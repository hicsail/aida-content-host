[
    "### Table of Contents\n\n- Introduction\n- What makes GenAI Different?\n\n- Economic Backdrop of GenAI\n- Beneficial Use Cases for GenAI in State Government\n- Use Case Analysis for GenAI in California State Government\n- The Unique Benefits and Applications of GenAI\n- GenAI Risk Analysis\n- Unique and Shared Risks of GenAI\n- Identifying GenAI High-Risk Use Cases\n- Ongoing Engagement\n- Conclusion\n- Appendix\n- Policy Landscape Assessment\n\n---",
    "# State of California Report: Benefits and Risks of GenAI\n\nThe diversity of the nearly 40 million people who call California home – and the strength of its multifaceted economy – have made California a global leader in technology and innovation.\n\nWith the proper guardrails in place, the revolutionary technology of Generative Artificial Intelligence (GenAI) can be responsibly used to spur innovation, support the State workforce, and improve Californians' lives.\n\nThis report on the use of GenAI in State government is the first major product of Governor Newsom's Executive Order N-12-23 on Generative Artificial Intelligence (Executive Order), and it is the first step in an ongoing process of engagement with stakeholders and across State agencies.\n\nThe report presents an initial analysis of the potential benefits to individuals, communities, government, and State government workers, with a focus on where GenAI may be used to improve access to essential goods and services.\n\nAdditionally, the report assesses the risks of GenAI, including but not limited to risks stemming from bad actors, insufficiently guarded governmental systems, unintended or emergent effects, and potential risks toward democratic and legal processes, public health and safety, and the economy.\n\nWhen used ethically and transparently, GenAI has the potential to dramatically improve service delivery outcomes and increase access to and utilization of government programs.\n\nThis report offers an analysis for State government leaders to explore the potential benefits and risks of GenAI thoughtfully, including how it can be used to empower California's workers.\n\nAn examination of the research and feedback from academia, industry, local, state and federal government, and community organizations found the following common themes:\n\n- GenAI is unique from conventional forms of AI, and it necessitates a different state approach to implementing and evaluating this technology.\n\n- GenAI enables significant, beneficial use cases for state government through its unique capabilities.\n\n- GenAI raises novel risks compared to conventional AI across critical areas such as democratic and legal processes, biases and equity, public health and safety, and the economy, and requires measures to address insufficiently guarded governmental systems and unintended or emergent harmful effects from this technology.\n\nAdditionally, as humans have explicit and implicit biases built into our society, GenAI has the capacity to amplify these biases as it learns from input data.\n\nAs such, it's imperative to consider the implications on Californians of different regions, income, races, ethnicities, gender, ages, religions, abilities, sexual orientation, and more for all GenAI inputs, outputs, and products–for both prioritizing implementations that may promote equity and guarding against bias and other negative impacts.\n\nAcknowledging the unprecedented nature of GenAI requires a collaborative effort between states, the federal government, and international partners, this analysis relies on learnings from the National Institute of Standards and Technology (NIST) AI Risk Management Framework (RMF) and international policies and governance frameworks.\n\nThe federal NIST AI RMF was developed to improve the ability to incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems.\n\nThe State’s commitment to transparency is a foundation for ongoing GenAI work and collaboration.\n\nThis report is only the first step in a multi-year and iterative process as part of the Governor’s Executive Order, which also:\n\n- Directs state agencies and departments to perform a joint risk-analysis of potential threats to and vulnerabilities of California’s critical energy infrastructure using GenAI.\n\n- Supports a safe, ethical, and responsible innovation ecosystem inside state government by requiring general guidelines for public sector procurement, uses, and required training for the application of GenAI.\n\n- Provides for guidelines to analyze the impact that adopting GenAI tools may have on vulnerable communities.\n\n- Prepares California's state government workforce through training for workers to use state-approved GenAI.\n\n- Requires evaluation for the potential impact of GenAI on regulatory issues.\n\nCalifornia government will continually engage academic leaders and researchers, labor organizations, community organizations, and industry experts as the State pilots GenAI use cases and creates guardrails to protect Californians and their data.",
    "# What makes GenAI different?\n\nGenAI builds on advances in conventional AI and uses very large quantities of data to output unique written, audio, and/or visual content in response to free-form text requests from its users and programmers.\n\nGenAI tools have the capacity to produce entirely new content instead of simply regurgitating inputted data.\n\nUnlike conventional AI systems designed for specific tasks, GenAI models are designed to be flexible and multifunctional.\n\nGenAI products are already available as standalone applications such as ChatGPT, Dall-E, and Bard, and are being integrated into many other consumer-facing technology products, such as chatbots on websites.\n\nConventional AI models, on the other hand, are usually designed for just a few specific tasks and are often limited by the scope of the inputted training data as well as the technical expertise of the programmer.\n\nModel training is the process by which AI models ingest input datasets to learn the underlying patterns within the data and produce predictions for the context that the model was trained on.\n\nConventional AI is already widely used in products across government and society.\n\nSome examples of conventional AI include robotic process automation, fraud detection tools, image classification systems, recommendation engines, and interactive voice assistants.\n\nTable 1: Comparison Between Conventional AI and GenAI Technology\n\n---",
    "# Economic Backdrop of GenAI\n\nCalifornia stands at the forefront of the burgeoning AI economy.\n\nHome to 35 of the world's top 50 AI companies, California leads the world in GenAI innovation and research.\n\nOur higher education institutions – including UC Berkeley’s College of Computing, Data Science, and Society, and Stanford University’s Institute for Human-Centered Artificial Intelligence – are among the most advanced AI research institutions in the world.\n\nCoupled with the State’s unparalleled access to venture capital, our culture of innovation, and history of new, world-changing technologies, California sits at the epicenter of an industry that is experiencing exponential growth and development.\n\nAlthough GDP growth and productivity gains are predicted, Goldman Sachs has also warned that 300 million jobs worldwide could be affected by GenAI.\n\nAs such, the State must lead in training and supporting workers, allowing them to participate in the AI economy and creating the demand for businesses to locate and hire here in California.\n\nStarting with our world-class higher education institutions and vocational schools, California is well-positioned to provide workers with relevant skills and businesses with the talent needed to drive job growth in the GenAI economy.\n\nThe global GenAI market is significant.\n\nAccording to Pitchbook, it is expected to reach $42.6 billion in 2023.\n\nLike all new technologies, particularly of this scale, GenAI offers immense economic opportunities, as well as new risks.\n\nAs the industries of GenAI are developed, California, the U.S., and other nations must develop coordinated and thoughtful public policies to mitigate risks and maintain public trust through ethical use guidelines, accountability, and transparency, while still realizing the potential economic benefits of GenAI.\n\n---",
    "# Use Case Analysis for GenAI in California State Government\n\nGovernment leaders should prioritize GenAI proposals that offer the highest potential benefits, along with the appropriate risk mitigations, over those where benefits are not significant compared to existing work processes.\n\nThis technology offers possibilities to improve the lives of Californians, such as by summarizing benefits enrollment policies in plain language, translating government communications into multiple languages, and providing interactive tax assistance.\n\nUnder the Governor’s Executive Order, agencies are tasked with soliciting stakeholder input and crafting guidelines for state use of GenAI.\n\nThat work has begun and will be completed in January 2024, but in the interim, basic principles that should apply:\n\n- To protect the safety and privacy of Californians' data, and consistent with state policy–state employees should only use state-provided, enterprise GenAI tools on State-approved equipment for their work.\n\n- Under no circumstances should state employees provide state or Californians’ resident data to a free, publicly available GenAI solution like ChatGPT or Google Bard, or use these unapproved GenAI applications or services on a State computing device.\n\n- It is important to provide a plain-language explanation of how GenAI systems factor into delivering a state service and disclose when content is generated by GenAI.\n\n- State supervisors and employees should also review GenAI products for accuracy and make sure to paraphrase rather than use AI-generated text, audio, or images verbatim.\n\nThrough consultation with practitioners and researchers, California state government compiled an inventory of potential GenAI use cases that could improve state services and programs.\n\nHigh-level categories within the use cases were extracted and are enumerated in this section as potential areas of benefit from GenAI.\n\nLooking ahead, with the appropriate pilot infrastructure and risk mitigations in place, California will evaluate potential use cases by prioritizing the following benefits:\n\n- Improve the performance, capacity, and efficiency of ongoing work, research, and analysis through summarization and classification.\n\nBy analyzing hundreds of millions of data points simultaneously, GenAI can create comprehensive summaries of any collection of artifacts, irrespective of whether the content is in a text, audio, or video format.\n\nAs GenAI learns, it can also categorize and classify information by topic, format, tone, or theme.\n\nExample Use Cases include:\n  \n  - Conduct sentiment analysis of public feedback on state policies, using GenAI to recommend opportunities for process and service delivery improvement.\n\nThis can help government understand public experience and improve policies and communication to better serve constituents.\n\n- Summarize meetings, work, and public outreach documentation, leveraging GenAI to find insights in the analyzed data.\n\nGenAI can find the key topics, conclusions, action items, and insights without needing to read everything word for word.\n\n- Personalize and customize work products to California’s diversity of people with the potential to improve access to services and outcomes for all.\n\nGenAI’s capacity to learn makes it easier for the State to design services and products to be responsive to Californians’ diverse needs, across geography and demography.\n\nGenAI solutions can recommend ways to display complex information in a way that resonates best with various audiences or highlight information from multiple sources that is relevant to an individual person.\n\nThese functions can further California's goals as they allow for optimized government experiences allowing Californians greater access to state information and services, and by advancing equity, inclusion, and accessibility in outcomes.\n\nExample Use Cases include:\n  \n  - Apply GenAI on government service data to identify specific groups or subsets of participants that may benefit from additional outreach, support services, and resources based on their circumstances and needs (for example, local job training for people claiming EITC).\n\n- GenAI can identify groups that, for language or other reasons, are disproportionately not accessing services by analyzing feedback surveys or comments for language that indicate accessibility difficulties.\n\nThis can help determine opportunities to improve access.\n\n- Improve language and communications access in multiple languages and formats.\n\nGenAI can create unique content in a variety of formats.\n\nBased on a single prompt, a GenAI solution can easily construct a video or image that a user can refine.\n\nThese products can be in multiple languages, allowing the State to make its videos, recordings, and other documents more accessible to and inclusive of all Californians.\n\nThese translated outputs can be refined through a quality control process to ensure accuracy and inclusivity before reaching Californians.",
    "These translated outputs can be refined through a quality control process to ensure accuracy and inclusivity before reaching Californians.\n\nAccessible communications are a critical part of ensuring that government services can meet Californians where they are.\n\nThe ability to meet the varying communication needs of persons with disabilities and reach Californians in their primary languages is a priority for improving government service delivery.\n\nExample Use Cases include:\n  \n  - Using GenAI to help experts convert educational materials into formats like audiobooks, large print text, or braille documents.\n\nCan also generate captions for video materials and make information more accessible for those with visual, hearing, or learning disabilities.\n\n- Leveraging GenAI to help experts translate government websites, public documents, policies, forms, and other materials into the various languages spoken in the State.\n\nThis expands access to important information and services to non-native English speakers.\n\n- Optimize software coding and explain and categorize unfamiliar code.\n\nSummarization, classification, and translation features make GenAI a powerful tool for state coders and the developer community at large.\n\nGenAI can generate code in multiple computing languages and translate code from one language to another.\n\nThis can improve state operations if a state system is using code that is written in an obsolete language.\n\nMoreover, GenAI has the potential to explain and categorize unfamiliar or uncertain code so that the State can better understand the exact technical architecture of agency applications.\n\nExample Use Cases include:\n  \n  - Powerful code conversion tools based on foundation models can accurately translate legacy codebases (e.g., COBOL mainframe apps) into modern programming languages.\n\nThis automates time-consuming and error-prone manual conversions.\n\n- Powerful GenAI development tools auto-generate quality code, spin up test environments, and generate synthetic datasets to train machine learning models.\n\nThis can slash timelines, reduce bugs, and democratize development.\n\nLow-code solutions also enable non-programmers to build applications.\n\n- Find insights and predict key outcomes in complex datasets to empower and support decision-makers.\n\nWithout specific training or pre-set rules, GenAI models can analyze multiple datasets to find meaningful insights for users.\n\nThe conversational aspects of GenAI solutions can empower workers with a range of technical expertise to ask questions in plain language to get at findings that may be relevant to their work.\n\nSignificantly, Californians could also use a GenAI solution to ask data-driven questions that are important to them.\n\nExample Use Cases include:\n  \n  - Cyber protection systems powered by foundation models can rapidly analyze network activity logs, identify anomalies and threats, generate explanations of the attacks, and propose remediation actions.\n\nThis can enable security teams to detect and respond to sophisticated cyberattacks in real-time before major damage occurs.\n\n- GenAI analyzes data streams from drones, satellites, and sensors monitoring public infrastructure.\n\nIt generates detailed damage and deterioration assessments via techniques like visual inspection, anomaly detection, etc.\n\nThis enables improved forecasting of maintenance needs.\n\n- Optimize workloads for environmental sustainability.\n\nIncorporating GenAI in government can drive environmental sustainability by optimizing resource allocation, maximizing energy efficiency and demand flexibility, and promoting eco-friendly policies.\n\nFor instance, this technology can enhance operational efficiency, decrease paper usage and waste, and support environmentally conscious governance.\n\nNotably, stakeholders also highlighted the need for reducing environmental impacts of GenAI use and ensuring environmental costs are equitably distributed.\n\nExample Use Cases include:\n  \n  - GenAI could analyze traffic patterns, ride requests, and vehicle telemetry data to optimize routing and scheduling for state-managed transportation fleets like buses, waste collection trucks, or maintenance vehicles.\n\nBy minimizing mileage and unnecessary trips, GenAI could reduce associated fuel use, emissions, and costs.\n\n- GenAI simulation tools could model the carbon footprint, water usage, and other environmental impacts of major infrastructure projects.\n\nBy running millions of scenarios, GenAI can identify potentially the most sustainable options for planning agencies and permit reviewers.",
    "# The Unique Benefits and Applications of GenAI\n\nGenAI has the potential to improve the delivery of government services and operations.\n\nFeedback from academic, industry, and community stakeholders highlights the unique benefits and applications of this novel technology compared to conventional AI and manual workflows.\n\nThe following table lists high-level categories for the wide variety of functionality for GenAI with sampled public sector use cases.\n\nThe example use cases are only intended to help illustrate the potential uses of state government adoption of GenAI tools.\n\nTable 2: A Typology for GenAI Tasks\n\n---",
    "# GenAI Risk Analysis\n\nResearch conducted within state government, informed by feedback from subject matter experts and community groups, has developed an emerging picture of the specific risk factors of GenAI compared to those posed by conventional AI.\n\nAs with conventional AI, GenAI poses risks both from bad actors using the technology to cause harm as well as from unintended, emergent capabilities of GenAI that can be misused.\n\nThe NIST AI RMF divides risks into seven categories: Validity & Reliability, Safety, Accountability & Transparency, Security & Resiliency, Explainability & Interpretability, Privacy, and Fairness.\n\nIn no particular order or weight, these seven NIST AI RMF categories have been analyzed as they apply to GenAI adoption in California.\n\nAlthough the NIST AI RMF provides a helpful framework to illustrate key risk areas, it does not specifically address GenAI, and it is not specific to California’s values or use case context.\n\nTo bridge this gap, and as identified through research and stakeholder engagement, the additional category of Workforce & Labor Impacts is included below.\n\nGiven the rapidly evolving capabilities, integrations, and standards of GenAI products, the following analysis represents an initial evaluation of GenAI risks, which delineates risks based on being a shared risk of conventional AI, an amplified risk, or a new risk associated with GenAI.\n\n- Shared risks: Known risks of GenAI shared by earlier types of AI models without significant differences in severity or scale.\n\n- Amplified risks: Risks of GenAI tools shared by earlier types of AI models that are enhanced due to any of the following factors:\n  \n  - Reduced technical or cost barriers to using GenAI.\n\n- Increased speed or scale of impact by GenAI tools.\n\n- Increased scope of systems or processes impacted by GenAI.\n\n- Increased exposure to bad actors via larger, more diverse training datasets.\n\n- Higher complexity of GenAI technology architectures with multiple producers and consumers.\n\n- New risks: Novel risks surfaced by GenAI’s unique capabilities to generate high-quality outputs across a diversity of modalities such as text, images, audio, and video.\n\n---",
    "## Validity & Reliability\n\nAI systems that are inaccurate or unreliable increase risks and reduce trustworthiness.\n\nValidation is the \"confirmation through evidence that the requirements for a specific intended use or application have been fulfilled.\"\n\nReliability is the \"ability of an item to perform as required, without failure, for a given time interval, under given conditions.\"\n\nWhen applied to GenAI, California identified the following risks:\n\nGenAI models are more complex than conventional AI models, and as a result, they are more susceptible to model degradation and collapse, where the AI model’s performance will worsen over time as the data used to teach it becomes more outdated.\n\nThis is because GenAI models are trained on a large body of data and can produce their own synthetic data.\n\nThis means that they can become biased towards their own synthetic data and become less accurate over time (a process known as \"model collapse\").\n\nGenAI outputs can also be non-deterministic and inconsistent, making it difficult to embed into critical systems where performance stability is a key requirement.\n\nThe risk of over-reliance on automated GenAI recommendations to make decisions (automation bias), related to validity concerns on hallucinations, poses concerns around GenAI outputs given the ability to generate answers that \"sound right\" without having factual accuracy.\n\nWithout proper safeguards, Californians may believe hallucinations inadvertently created by government GenAI tools, which could lead to additional downstream misinformation.\n\nThis could reasonably erode Californians' trust in their government and its services.",
    "## Safety\n\nAI systems \"should not under defined conditions, lead to a state in which human life, health, property, or the environment is endangered.\"\n\nWhen applied to GenAI, California identified the following risks:\n\nGenAI tools can pose significant risks to public health and safety–whether employed by people with malicious intent, or simply because of a lack of quality controls.\n\nFor example, bad actors can leverage AI to engineer dangerous biological materials, AI chatbots could give consumers incorrect or dangerous medical advice, or GenAI systems used for drug discovery could create harmful substances.\n\nIn sensitive domains like healthcare and public safety, GenAI requires careful governance to mitigate the risk of harm.\n\nAdditionally, GenAI can utilize better and more realistic text generation capabilities to simulate human text and opinions, leading to novel scaling capabilities for spreading misinformation or disinformation on public forums.\n\nBad actors could weaponize misinformation and disinformation, amplifying it through GenAI to interfere in democratic processes.\n\nThis includes the generation of disinformation campaign material to disseminate on social media, generating deepfakes of political representatives or candidates, or submitting large volumes of fake public comments for proposed rules.\n\nGiven these risks, the use of GenAI technology should always be evaluated to determine if this tool is necessary and beneficial to solve a problem compared to the status quo.\n\nGenAI should center on the needs of the human workforce, support the carrying out of responsibilities to Californians, and avoid contributing to additional bureaucracy, process, or safety risks.",
    "## Accountability & Transparency\n\nTransparency reflects the extent to which information about an AI system and its outputs is available to individuals interacting with the system.\n\nMeaningful transparency provides access to appropriate levels of information based on the stage of the AI lifecycle.\n\nWhen applied to GenAI, California identified the following risks:\n\nThe GenAI model lifecycle is typically more complex than that of conventional AI and raises novel challenges in ensuring transparency and accountability along the AI value chain.\n\nBuilding a GenAI model may involve multiple organizations that all may contribute data to the base foundation model or within the fine-tuning process.\n\nCalifornia state government must be cautious about over-automating decisions or removing human oversight entirely with GenAI chatbots and text generators.\n\nThere are risks in over-trusting these and other tools that rely on GenAI without proper review and evaluation of GenAI outputs, such as inaccurate information being provided to constituents or inaccurate public program determinations.\n\nSuch inaccurate determinations, especially if made repeatedly, could pose particular risks, severely undermining California's progress in creating a California for All by emphasizing diversity, equity, inclusion, and accessibility.\n\nIt will be critical to have a human reviewer of any GenAI-supported workflow or output that results in a decision about program eligibility or social safety net benefits.",
    "## Security & Resiliency\n\nSecurity and resiliency are defined in the following ways:\n\n- Secure AI systems can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use.\n\n- Resilient AI systems can withstand unexpected adverse events or unexpected changes in their environment or use.\n\nWhen applied to GenAI, California identified the following:\n\n(State of California Report: Benefits and Risks of GenAI | 17 - continued incomplete text...)",
    "## Data Security Risks\n\nThere are some shared data security risks across conventional AI and GenAI models.\n\nData can be vulnerable to unauthorized access, low-quality data can be injected into training datasets to impact overall model performance, and crafted inputs can cause AI and GenAI models to exhibit inconsistent performance.\n\nAs members of Cal OES’s Cybersecurity Integration Center (Cal-CSIC), CDT’s Office of Information Security works collaboratively with the California Highway Patrol (CHP), California Military Department (CMD), Office of Health Information Integrity, and other essential agencies on mitigating, identifying, responding to, and reporting security incidents.\n\nGenAI systems can be susceptible to unique attacks and manipulations, such as poisoning of AI training datasets, evasion attacks, and interference attacks.\n\nAs with any other technology-driven threat to state security, when a state employee suspects one of these GenAI-related incidents, such as a GenAI-generated or -impacted incident has occurred, to the degree they’re known, the employee should report it immediately for central tracking and coordination.\n\nConsistent with State Information Management Manual (SIMM) section and current practice for other technology-driven threats, it is the responsibility of the state entity Information Security Officer (ISO) or authorized user to immediately report the incident through the California Compliance and Security Incident Reporting System (Cal-CSIRS) so that further pattern analysis can be conducted for correction and safeguarding.\n\nThe capabilities of GenAI generally raise concerns about enabling bad actors and undermining government security if not properly governed.",
    "## Security Risks with Criminal Activities\n\nNew capabilities created by GenAI will pose new security risks, threatening existing systems around both physical and digital infrastructure.\n\nRobust, new security controls, monitoring, and validation techniques will be needed to guard against potential attacks.\n\nGenAI has a wider security risk surface exposed via their natural language interfaces.\n\nIt is easier for adversarial attacks to occur and less intuitive to place security controls on the model weights that produce recommendations and decisions by the GenAI model.\n\nTo that end, the Governor’s Executive Order requires a classified joint risk analysis of potential threats to and vulnerabilities of California's energy infrastructure and directs development of a strategy to assess threats to other critical infrastructure by the use of GenAI.",
    "## Explainability & Interpretability\n\nExplainability refers to a representation of the mechanisms underlying AI systems’ operation.\n\nInterpretability refers to the meaning of AI systems’ output in the context of their designed functional purposes.\n\nWhen applied to GenAI, California identified the following risks: GenAI models are similar to certain types of conventional AI models like neural networks, which are black-box algorithms that cannot provide direct explanations for their predictions.\n\nWithout the ability to explain model predictions and outputs, it becomes more difficult to address cases where this technology produces an unexpected result that impacts the validity and consistency of the answers.\n\nThere is ongoing research to gain better explainability capabilities for these types of algorithms.\n\nHowever, GenAI models amplify these concerns because they are built from much larger and more complex neural networks than conventional AI models.\n\nThe difficulty in extracting human-interpretable explanations from GenAI technology is an important factor to consider for government to provide sufficient information about decisions that concern constituents.\n\nAdditionally, GenAI models can be prompted to \"explain their reasoning\" through prompting techniques.\n\nHowever, these techniques can be inconsistent because GenAI models have been shown to misrepresent their stated reasoning.\n\nThese techniques can be unreliable in extracting a GenAI model’s true logical reasoning for an output, compared to the model’s stated reasoning.",
    "## Privacy\n\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy, identity, and dignity.\n\nThese norms and practices typically address freedom from intrusion, limiting observation, or individuals’ agency to consent to disclosure or control of facets of their identities.\n\nWhen applied to GenAI, California identified the following risks: GenAI models can leak personal data if they are not properly anonymized or if their training data is not properly secured.\n\nFor example, if a GenAI model is trained on a dataset of medical records, it could potentially generate text that includes personal information about patients, such as their names, medical conditions, or medications.\n\nThis information could be used to identify individuals, even if the model was trained on an anonymized dataset.\n\nGenAI also raises novel privacy issues such as: Re-identification risk: GenAI models can also be used to synthesize new datasets from previously unintegrated data sources that can be used to re-identify individuals.\n\nFor example, if a GenAI model is trained on a dataset of images of people, it could potentially generate new images that are similar to the images of real people in the training dataset.\n\nThese new GenAI images could then be used to identify real individuals in the training dataset, even if the original images were anonymized.\n\nThis re-identification risk is particularly critical in regard to sensitive personal data, where individuals could be exposed to unsafe conditions if unintentionally disclosed.\n\nThird-party plug-ins and browser extensions: Third-party plug-ins and browser extensions that interact with GenAI models can also pose privacy risks.\n\nFor example, a plug-in could collect data about the user's interactions with a GenAI model, such as the text that they generate or the images that they create.\n\nThis data could then be shared with the plug-in's developer or with third-party companies without the user's knowledge or consent.\n\nGovernment’s ability to respond to consumer privacy requests: As Californians’ right to remove their personal data online becomes more widely practiced, extracting and destroying their information embedded within GenAI models may become difficult or administratively unsustainable.\n\nBad actors accessing and sharing government database content: The state of California maintains secure databases with records of individuals’ data, such as census data and the program-specific data minimally necessary to make eligibility determinations.\n\nIf a bad actor were able to gain illegal access to a state database, GenAI could power the rapid capture and leak of Californians’ private data.\n\nData leaks and data loss from data centers also pose an ongoing risk, which will need to be addressed through improved controls.",
    "## Fairness\n\nFairness in AI includes concerns for equality and equity by addressing issues such as bias and discrimination.\n\nWhen applied to GenAI, California identified the following risks: GenAI models can perpetuate societal biases if the training data is imbalanced.\n\nFor example, large language models often perform poorly for non-native English speakers.\n\nThis could create inequity in access to certain government services.\n\nGovernment must also proactively assess for algorithmic discrimination, such as gender, racial, or other biases, particularly in high-impact areas like criminal justice, healthcare, mental health, social services, and employment decisions.\n\nAlgorithmic bias in state systems can be especially harmful if the GenAI authorship of the content is not disclosed, leading human consumers to misattribute the biased or harmful content to the government.\n\nIn conventional AI models, bias can be mitigated by collecting and processing training data to correct for under-representation of historically marginalized groups.\n\nThis is important because creating rules that intentionally bias model weights during model training could have legal implications.\n\nFor example, if an AI model is used to make decisions about who gets a loan, and that model is biased against people of a certain race, then the company using that model could be sued for discrimination.\n\nGenAI datasets however are much larger than conventional AI models, making it more difficult to resolve embedded bias.\n\nCommon expressions of data bias in GenAI outputs can include gender and racial stereotypes.\n\nThis is usually relevant when generating narrative examples, image generation, or creating synthetic data.",
    "## Workforce & Labor Impacts\n\nThe adoption of GenAI technologies into the economy and workforce will introduce many changes that will support workers in their daily responsibilities and tasks, but also will change or modify parts of their existing workflows.\n\nCalifornia identified the following risks when analyzing the use of GenAI: Key areas for workforce impact considerations include: Up-skilling, re-training, and job transition assistance: With the integration of GenAI tools into the workplace, staff may require up-skilling programs to effectively use the technology in their daily responsibilities.\n\nFor individuals that experience job displacement, private companies and public services need to prepare for proactive and thoughtful re-skilling and transition support services.\n\nLabor exploitation: GenAI could enable new forms of labor exploitation, such as in data labeling where contract workers in developing countries are employed to annotate datasets used for training AI models without labor rights guarantees.\n\nThis can encourage unsafe working conditions, especially, for contract workers in sensitive fields like content moderation for graphic and inappropriate content.\n\nAnticompetitive behavior: Major firms could use GenAI to further concentrate power in anticompetitive ways, such as by replicating copyrighted data from artists or small businesses.",
    "## Identifying GenAI High-Risk Use Cases\n\nFuture state research and development of guidelines will continue to support agencies and departments in identifying the severity and scope of GenAI risks, so that state government can better align oversight to real-world impacts.\n\nThe Governor’s Executive Order instructs agencies to begin this work.\n\nBut California’s efforts in this regard will surely continue beyond the Executive Order’s deliverables – including partnership between the Administration and the Legislature to identify risks and codify strategies to mitigate them.\n\nA risk-based approach to AI aligns with global trends.\n\nMajor governmental entities like the European Union and NIST already employ risk-based frameworks for AI evaluation and deployment.\n\nBy identifying the level of risk associated with GenAI deployment, organizations can implement responsible GenAI systems consistent with international practices.\n\nWhen a high-risk use case is identified and GenAI is being used, state entities will need to take additional precautions.\n\nGovernment Code § 11546.45.51 defines “high-risk automated decision systems” for state entities and serves as a basis to identify where these precautions should be defined.\n\nThe definition states: “High-risk automated decision system” means an automated decision system that is used to assist or replace human discretionary decisions that have a legal or similarly significant effect, including decisions that materially impact access to, or approval for, housing or accommodations, education, employment, credit, health care, and criminal justice.\n\nLower risk systems that fall outside of this high-risk classification may still benefit from risk mitigation and transparency measures.\n\nThe following table displays initial considerations that may help determine actions needed to mitigate the risks presented by GenAI.",
    "## Collaboration and Future Assessments\n\nThis report was not possible without extensive collaboration.\n\nThe initial findings and recommendations of this report mark the beginning of a much broader and ongoing conversation about the benefits and risks of this potentially transformative technology.\n\nThe State of California will regularly assess and update the findings of this report with significant developments as appropriate.\n\nTo do this, the State will continue strengthening collaborations with academia, other governmental entities, industry, policy experts, organizations representing employees, and community-based organizations.\n\nGenAI has incredible potential, and it is the State’s responsibility to create an opportunity where Californians can help to chart their own future with this new technology.",
    "## Policy Landscape References\n\nTo gain a comprehensive and balanced understanding of the benefits and risks of generative AI, it is essential to gather insights from a diverse range of sources across academia, government, industry, and civil society.\n\nImportant sources that were critical in informing this report included: The White House AI Bill of Rights, The NIST AI Risk Management Framework, Internal guidance policies on generative AI usage from local and state governments, International AI governance frameworks, The National Telecommunications and Information Administration (NTIA) Request for Public Comment on AI accountability, Academic and civil society research and recommendations.\n\nThis context will help illustrate the complex array of considerations around responsible AI development and set the stage for further examination of how state governments can navigate this emerging policy domain.",
    "## NIST AI Risk Management Framework\n\nThe NIST AI Risk Management Framework (RMF) working group develops flexible policies and guidelines for responsible AI governance that align with other major frameworks like the White House's proposed AI Bill of Rights.\n\nThe RMF provides detailed questions and checklists to systematically guide organizations in implementing best practices for accountable and observable AI systems.\n\nAn important note is that the RMF is intended as a standards framework and set of benchmarks rather than an off-the-shelf governance toolkit - it serves as a reference for organizations to develop their own tailored operational policies and toolkits.\n\nAlignment with the consensus standards and benchmarks in the RMF enables greater interoperability between different organizations' governance processes and tools that build on the same foundations.",
    "## Government Internal Guidance Policies\n\nInternal government guidance policies at the state and local levels have provided an initial policy backstop for public sector use of generative AI technologies.\n\nThese developing guidance policies commonly outline considerations around privacy risks between using enterprise versus public-facing generative AI tools, disclosure and transparency requirements when governmental bodies utilize generative AI capabilities and cautions around the potential for hallucinated or factually incorrect outputs if generative models are not carefully monitored and tuned.\n\nWhile limited in scope, these internal government policies demonstrate early governance attempts to balance public sector opportunities from leveraging cutting-edge generative AI with responsible oversight.",
    "## International AI Governance Frameworks\n\nInternational AI governance frameworks and regulatory policies from bodies like the EU, UK, Canada, and others occupy critical niches in the overall AI governance ecosystem that substantially impact the compliance requirements and burdens for private sector technology companies to fulfill if they wish to operate globally.\n\nPioneering and influential frameworks like the EU's General Data Protection Regulation (GDPR) have set the tone and expectations for privacy standards around AI and data utilization worldwide - many companies are wary of having to comply with multiple substantially different sets of national or regional AI regulatory requirements.\n\nAs California explores potential policies for accountable state-level AI governance, policymakers should be aware of the major international governance frameworks being adopted elsewhere in the world in order to minimize regulatory divergence and compliance burdens.",
    "## NTIA Request for Public Comment\n\nThe National Telecommunications and Information Administration's (NTIA) recent Request for Comment on frameworks and best practices for AI accountability gathered input from a diverse range of stakeholder groups including academia, industry, civil society, all levels of government, and individual constituents.\n\nThis public consultation offered a valuable venue for surfaced feedback across the AI governance spectrum on assessed risks from current AI systems, suggestions for responsible AI development and fielding, and proposed organizational and technical solutions for improved AI accountability and observability.\n\nReviewing responses submitted by the public helps inform more comprehensive approaches to steering responsible generative AI development and mitigating potential harms from misuse.",
    "## Academic, Industry, and Civil Society Research\n\nAcademic studies, industry papers, and civil society reports have offered a number of valuable and relatively comprehensive surveys of assessed risks and benefits from deploying generative AI systems across different societal domains.\n\nThese analyses help inform a broader taxonomy of potential benefits and risks from the application of generative AI technologies in areas like finance, healthcare, criminal justice, employment, and more.\n\nSuch knowledge mapping exercises highlight domains of concern and foreground issues for governance approaches aimed at maximizing generative AI's benefits while curtailing foreseeable risks from irresponsible use or negative externalities.\n\nThese findings help inform priorities and nuanced approaches for governance that enable accountable and ethical generative AI utilization across diverse contexts.",
    "## Summary\n\nThis Directive establishes a moratorium for at least 6 months on Generative Artificial Intelligence.\n\nThe State of Maine government must keep pace with a rapidly evolving cyber threat landscape that poses significant risks to the security of the State’s network infrastructure, including the sensitive and confidential data that we are entrusted to protect for our citizens.\n\nThis Directive is in response to the unique security and privacy risks posed by the rapid rise in the breadth and scope of artificial intelligence (AI) systems, specifically generative AI, and establishes the moratorium while MaineIT conducts further risk assessment.",
    "## Background\n\nMaineIT is responsible for maintaining the confidentiality, integrity and availability of the State’s information systems and assets, while serving Executive Branch Agencies with efficient and secure network services.\n\nAs U.S. policy on AI continues to develop, caution must be taken to assess the risks involved with the use of generative AI technologies.\n\nEarly federal guidance and best practices² provide a roadmap for industry and government to move towards the responsible management of generative AI systems that cultivate public trust.\n\nGenerative AI systems have the capacity to automatically process data and information in a way that resembles intelligent human behavior.\n\nAlthough these systems have many benefits, the expansive nature of this technology introduces a wide array of security, privacy, algorithmic bias, and trustworthiness risks into an already complex IT landscape.\n\nThese systems lack transparency in their design, raising significant data privacy and security concerns.\n\nTheir use often involves the intentional or inadvertent collection and/or dissemination of business or personal data.\n\nIn addition, generative AI technologies are known to have concerning and exploitable security weaknesses, including the ability to generate credible-seeming misinformation, disseminate malware, and execute sophisticated phishing techniques.\n\nThese factors make it challenging to maintain adequate understanding and control over AI-based decisions, including their appropriateness, fairness, and alignment with organizational values and risk appetite.\n\nThe complete risk associated with the use of this technology remains unknown.",
    "## Required Actions\n\nEffective immediately, Wednesday, June 21, 2023, the adoption or use of Generative AI technology³ (i.e., produces uncontrolled results) is prohibited for at least six months for all State of Maine business and on any device connected to the State of Maine network.\n\nThis will allow for a holistic risk assessment to be conducted, as well as the development of policies and responsible frameworks governing the potential use of this technology.\n\nAny requests for an exception to this Directive must be submitted by filing a direct waiver request through the appropriate MaineIT Account Manager for your department.",
    "## Additional Information\n\nMaineIT is currently assessing other products, services and telecommunication equipment that may pose security risks to the State of Maine’s network infrastructure and additional steps may need to be taken to strengthen our security posture.\n\nThis Directive will be reviewed and revised, as necessary, in six months.\n\nThe internal point of contact for this Directive is Nathan Willigar, State of Maine Chief Information Security Officer, at .",
    "## About this Resource\n\nThese generative AI implementation recommendations and considerations have been created as a way to share information and resources to help direct responsible implementation of generative AI tools and guide AI Literacy in North Carolina Public Schools.\n\nNote that as generative AI is emerging technology and is changing rapidly, as are laws and rules governing its use, this is a living document and it will be updated as needed to reflect changes that take place in this very fluid environment.\n\nThe last update will appear at the bottom of each page for your reference.\n\nThese guidelines have been organized around the five focus areas of the North Carolina Digital Learning Plan, which guides digital teaching and learning for North Carolina public schools.\n\nThe Digital Learning Plan encourages the safe use of innovative technology to prepare students for future school and work to improve student outcomes and support the appropriate use of technology to advance learning.\n\nThis document is organized around the five focus areas of the NC Digital Learning Plan as seen in this graphic.\n\nThe Office of Digital Teaching and Learning, housed within the North Carolina Department of Public Instruction (NCDPI), supports educators in using generative AI safely to improve student learning.\n\nIf you need assistance with implementing generative AI into your district or school, please reach out to your regional Digital Teaching and Learning Consultant or Innovative Learning Catalyst.\n\nAll regional DTL consultants’ and Innovative Learning Catalysts' contact information, as well as a wealth of other information, may be found on the DTL Hub webpage.",
    "## Acknowledgements\n\nThis document was developed by the NCDPI AI Guidelines Committee, which is a collaboration between several different Offices within NCDPI and includes the following members:\n\n- Vera Cubero: (Lead Contributor), Education Consultant II; Office of Digital Teaching and Learning \n- Dr. Ashley McBride: (Contributor), Section Chief; Digital Learning Initiative \n- Diane Dulaney: (Contributor), Director, Office of Data, Reporting and Privacy \n- Eli Hamrick IV: (Contributor), Secondary Computer Science, IT, and Technology Education Consultant \n- Timothy Wease: (Contributor), PSU IT Security Specialist, Cybersecurity Section \n- Dr. Vanessa Wrenn: (Contributor), Chief Information Officer; Technology Services and Digital Learning \n- Josh Barton: (Reviewer), State Consultant for Assistive Technology; Office of Exceptional Children\n\n*The Department of Information Technology is expected to release additional guidance around AI.\n\nOnce released, this document will be updated to align with their recommendations.",
    "### Introduction\n\nArtificial intelligence (AI) has been a part of education for years, but the introduction of generative AI has brought AI to the forefront of conversations about the future of education since the November 2022 release of ChatGPT.\n\nThis release, followed by many other generative AI tools, has created a boom of interest in the use of these technologies with educators and students, as well as concerns about their misuse.\n\nGenerative AI tools are artificial intelligence tools that generate text, images, audio, video, and code based on what it has learned in its training data set.\n\nWhen presented with a prompt by the user, the model then predicts a response.\n\nWhile each response is new, the model pulls from the data it analyzed in the training phase and transforms it into a response based on the user input or prompt.\n\nThe recent proliferation of generative AI has been remarkable in its unprecedented pace, which has been more rapid than any other technological innovation in history.\n\nThe acceptance and use of generative AI tools is inevitable and businesses and institutions of higher learning will expect our students to have generative AI skills.\n\nTherefore, the ways in which public schools address generative AI has serious implications for both the future of education and for today’s students.\n\nTo help guide the nation’s school leaders in responsibly implementing AI, the US Department of Education’s Office of Educational Technology recently published a report titled “Artificial Intelligence and the Future of Teaching and Learning”.\n\nThis report contains this quote from Dr. Russell Shilling, “AI brings educational technology to an inflection point.\n\nWe can either increase disparities or shrink them, depending on what we do now.” Indeed, the decisions we make about the use of generative AI in our public schools will have significant consequences on our students’ futures as they enter institutions of higher education or the job market, as well as in their daily lives.\n\nThe World Economic Forum’s “Future of Jobs Report 2023”, predicts that AI will have a drastic effect on the job market in the next five years.\n\nIn this report, the field of AI and Machine Learning is the fastest predicted field with a staggering 40% growth trajectory in the next five years that is expected to create 1 million new jobs.\n\nFurthermore, the report found that 75% of companies surveyed plan to implement generative AI by 2027.\n\nIn light of these facts, NCDPI encourages public school units to responsibly embrace AI and incorporate AI Literacy for all staff and students.\n\n“AI tools are increasingly prevalent in students’ current education experience and in their future professional environments, so empowering learners to understand these technologies is essential.\n\nThe power of AI tools for education, community engagement and deeper learning will continue to drive innovation and policy.\n\nThe North Carolina Department of Public Instruction (NCDPI) advocates for the responsible integration of AI technologies in education, aiming to cultivate an educational environment that empowers each individual to reach their full potential and cultivates a lasting passion for continuous learning.”\n\nDr. Vanessa Wrenn, Chief Information Officer North Carolina Department of Public Instruction",
    "#### Approach to Generative AI\n\nMany schools and districts opted to block access to AI tools in the spring of 2022 to allow time to learn more about the potential issues and educational impact of AI.\n\nMany who originally blocked access to AI tools have since allowed these tools at least for teachers, but many continue to block access for students.\n\nTeachAI, in the Teach AI Toolkit points out that “Attempting to enforce broad bans on AI is a futile effort that widens the digital divide between students with independent access to AI on personal devices and students dependent on school or community resources.\n\nClosing the digital divide in an age of AI still begins with internet connectivity, device availability, and basic digital literacy.” Public schools are the best hope for closing the digital divide by ensuring equal opportunity to learn about and with generative AI for all students to prepare them to be competitive in the current and future job market.\n\nHowever, it is important to ensure that AI is implemented responsibly by all stakeholders to ensure safety and privacy, and responsible ethical use.\n\nThe US Dept.\n\nof Education’s “Artificial Intelligence and the Future of Teaching and Learning: Insights and Recommendations” included an analogy that suggests generative AI and other AI tools should provide a technology-enhanced future more like an electric bike and less like a robot vacuum.\n\nWhile robot vacuums do the user’s job without human involvement or oversight, when using an e-bike the human is both fully aware and fully in control, but the user’s burden is lessened and their effort is multiplied by a complementary technological enhancement.\n\nTo further expand on this analogy for educational applications of generative AI, the graphic below compares the use of generative AI to three different types of bikes.\n\nThis analogy demonstrates that without AI, some students’ struggles will inhibit learning, like a mountain bike; while with too much reliance on and lack of understanding of AI is unpredictable and can even be harmful like a motorcycle.\n\nIdeally, AI would be used like an E bike, with the human in control.\n\nThis analogy demonstrates using AI as a learning partner, to help reduce struggles, support individual needs, and result in more productive learning, but always with human oversight and control.\n\nThe 3 Bikes Analogy graphic may be accessed for downloading and printing by clicking the image.",
    "### 10 Top Skills from Future of Jobs Report & NC Portrait of a Graduate\n\nThe World Economic Forum’s Future of Jobs Report studies a vast data set from global companies each year to make predictions on the future of work for the next five years in the future.\n\nIn their 2023 report, the 10 top skills that will be the most important for students to possess in order to be poised for success in the near future align remarkably well with the 7 durable skills that are highlighted in North Carolina’s Portrait of a Graduate.\n\nWhile they may use different terms, many of the same human skills are highlighted in both are synonymous, such as ‘curiosity and lifelong learning’ and ‘Learner’s Mindset’.\n\nIn a future in which many things can be completed faster and perhaps better by Artificial Intelligence, companies will value and seek after the human skills that are the focus of the North Carolina Portrait of the Graduate: adaptability, collaboration, communication, critical thinking, empathy, learner’s mindset, and personal responsibility.\n\nThese are skills that can not be replicated by AI, and they will be highly valued.\n\nIn addition to these durable human skills, students will need to be AI Literate and able to effectively work with AI as a partner.\n\nTo truly prepare students for the world they will graduate into, whether they graduate in 2024 or 2034, all these durable skills and AI Literacy should be infused into all grade levels and all curriculum areas.\n\nThis document aims to help education leaders adapt to these new realities, implement generative AI responsibly in their schools, and provide guidance for infusing AI Literacy into all grade levels and curriculum areas.",
    "## Responsible Implementation\n\nGenerative AI, while not perfect, is a powerful tool that can be used by educators and students alike to expand their own abilities.\n\nIf implemented thoughtfully and responsibly, Generative AI has the potential to transform teaching and learning in profound ways such as:\n\n- Assisting both teachers and students in managing their workload more efficiently through the automation of routine administrative tasks.\n\nThis support enables teachers to concentrate more on engaging directly with students, resulting in improved learning outcomes.\n\n- Offering additional learning support to students outside of regular school hours, including tutoring and resource assistance.\n\nThis is particularly beneficial for students lacking access to educational resources or assistance at home.\n\n- Enabling teachers to customize learning experiences and develop lessons and materials specifically designed for individual student needs.\n\n- Adapting teaching methods to suit different learning preferences and providing focused help where needed, reducing gaps in educational achievement by analyzing student performance data.\n\n- Enhancing accessibility for underrepresented groups in education, including providing translation tools for students who speak multiple languages, voice-to-text and text-to-voice options for students with physical challenges or learning disabilities, and planning tools for those requiring assistance in executive functioning.\n\nThe responsible implementation of generative AI into NC K12 schools can help close the digital divide, reducing disparities that currently exist, and creating educational environments that are more inclusive.\n\nAdditionally, responsible implementation will prepare students for a future in which AI is sure to be integral to all aspects of their lives.\n\nHowever, ignoring generative AI or not implementing it responsibly and equitably, can have the opposite effect, increasing the disparities that put many students at a disadvantage and increasing the digital divide.\n\nMany educators fear that generative AI can provide misinformation or can become a method for cheating on assignments.\n\nAs AI becomes more commonplace in all aspects of life, it is imperative that educators adapt to this new reality and rethink current attitudes about plagiarism and cheating.\n\nTeachers should educate students about the responsible use of generative AI, promoting the values of honesty, critical thinking, and originality in academic endeavors.\n\nResponsible generative AI implementation, thorough oversight, and educational awareness that includes AI Literacy for all users including all students is essential.\n\nMore guidance on AI Literacy is in the Human Capacity section of this document.\n\nImplementing rigorous quality checks and validation processes when using generative AI-powered educational tools ensures that the information provided is accurate, reliable, free from bias and that it aligns with educational objectives.",
    "### Establish a Foundation\n\n- Host an introductory meeting & training for district & school leaders, board, student leaders & other key decision makers.\n\n- Create a team to develop PSU-wide AI academic guidelines (or adapt current academic integrity/acceptable use policies to include generative AI).\n\nInclude leaders, teachers, students, & community members.\n\n- Review current EdTech providers deploying generative AI to vet their safety, privacy, reliability, and efficacy, to determine if they are appropriate to be used for your school, and which users they will be open to based on their Terms of Service and school or district policies.",
    "### Develop Your Staff\n\n- To ensure successful implementation, targeted professional development for educators on generative AI including its impact, effective use, capabilities, limitations, concerns & responsible generative AI use should be provided for all staff.\n\n- Share PSU AI guidelines draft for feedback; work with teachers on what the guidelines mean for their classroom.\n\n- Support teachers in updating their syllabi and/or classroom policies to include AI integrity guidelines that align with PSU guidelines.\n\n- Work with teachers to help them rethink plagiarism and academic integrity in the AI Age and support them in shifting assessments to AI-resistant, AI-assisted, & AI-partnered versions.",
    "### Educate Students & Community\n\n- Share AI guidelines at school-wide events including parents and guardians to build common understanding.\n\n- Teachers review guidelines in each classroom along with syllabi & examples of appropriate & inappropriate student use.\n\n- Implement generative AI training to upskill students and ensure they are prepared to mitigate any biases, inaccuracies or issues that may arise and utilize generative AI effectively as a learning partner.\n\n- Provide content reviews and ongoing opportunities for training and learning to teachers and the school community.",
    "### Assess and Progress\n\n- Create a plan for constant review & reevaluation of academic guidelines in light of AI evolution and advances.\n\n- Evaluate new AI tools for appropriateness to launch pilot programs.\n\n- Continuous updating & training across school community including sharing exemplars & opportunity to express concerns.\n\n- Elevate best practices for generative AI implementation from across community & partners.",
    "## PSU Generative AI Guidelines\n\nTo ensure equity of access and responsible use of generative AI by all stakeholders, it is recommended to develop district-wide guidelines that detail the acceptable and responsible use of generative AI.\n\nMany districts are choosing to adapt current acceptable use or academic integrity policies to specifically include generative AI now to provide much needed timely guidance, rather than drafting new policies at this time.\n\nSome plan to develop full policies at a later date.\n\nIt is recommended to note in any policy or guidelines that they may need to be adapted as AI is changing rapidly.\n\nIn addition to adapting or creating guidelines and or policies, PSUs should work to build a common understanding and common language.\n\nFollowing the creation and dissemination of district-wide guidance, a comprehensive AI Literacy training plan should be developed to ensure that all users are trained specifically in the responsible, safe, and ethical use of generative AI.\n\nStaff should receive training first with guided practice followed by a practice period of at least 4-6 weeks to gain adequate understanding and competency and to express and discuss any concerns or issues in using generative AI tools before generalizing use with students.\n\nGenerative AI guidelines/policies as well as any Data Sharing Agreements should be carefully evaluated when making decisions about which tools to allow for students.\n\nAs with any digital tool, Public School Units should follow the terms of service, including appropriate age limits and seeking parental consent if required.\n\nIt is recommended to consult with the technology director and, if needed, legal counsel, in evaluating the terms of service.\n\nMost Large Language Models such as ChatGPT, Google Bard, Bing.com/chat, and Perplexity are currently prohibited for ages under 13 per their terms of service, but are allowed for ages 13 and over with varying parental/guardian permission requirements.\n\nIf your PSU decides to utilize a tool with students that requires parent/guardian permission, or if you decide to require parent/guardian permission for other tools, you may choose to customize this Example AI Permission Form.\n\nIn some cases, it may be appropriate to include AI tool permissions in other technology policies.\n\nRegardless of whether an AI permission form is deemed necessary by the PSU, all staff and student users as well as all parent/guardians should be made aware of the school or PSU’s generative AI guidelines, including any academic integrity or acceptable use guidelines that reference the use and disclosure of generative AI and plagiarism.\n\nThese guidelines should also be signed by both students and parents.\n\nYou may reference this spreadsheet for a comparison of LLM models.\n\nSome school and district leaders may be hesitant to allow tools such as ChatGPT or Google Bard for students even if they are 13 and over.\n\nAn encouraging trend for K12 education is that built-for-education models are being developed and may help alleviate these concerns, though they may come with a cost.\n\nNCDPI does not endorse any company or product, but one example of a built-for-education model is Khan Academy’s Khanmigo ( a personal assistant for teachers and a personal tutor for students.\n\nAnother promising model is Magic School magicschool.ai, which already has a free robust teacher platform and is introducing Magic School Student in early 2024.\n\nOther education-focused models are likely to go to market in 2024, so education leaders should soon have more choice in safe, built-for-education models for students to learn with and about generative AI.",
    "### AI Capabilities and Limitations\n\nGenerative AI is a new technology with extensive limitations.\n\n- What controls are in place to identify and lower hallucinations?\n\n- Are responses accompanied by links to reliable sources to verify the information?\n\n- Does the tool include an easy way to share the AI Chat so teachers can monitor student use for school work?",
    "### Mitigating Bias\n\nIt’s important that the tools we use do not cause harm to our students or teachers.\n\n- What steps have been or are being taken to identify and mitigate biases?\n\n- How are fair and unbiased outputs supported?\n\n- How can users report instances of bias if they encounter them in AI responses?",
    "### Accessibility and Inclusive Design\n\nOur school needs to accommodate diverse learners and varying technical skills among staff.\n\n- How does the tool ensure accessibility and usability for all our students and staff?\n\n- How can these tools be used to provide additional support and personalization for students with IEPs, 504s, English Language Learners, economically disadvantaged students, marginalized student groups and others?",
    "### Cybersecurity\n\nHow can we be sure we are minimizing any potential risks to our networks and our users?\n\n- What security practices are you implementing to protect our user and organizational data?\n\n- How do your security practices meet or exceed applicable PSU, State, and Federal requirements including the new NC Third Party Data Integration requirements?",
    "### Guiding Questions\n\n- How are students using generative AI?\n\nHow are teachers?\n\n- What was the impact of the release of ChatGPT and other generative Al tools on your school?\n\n- What are your biggest concerns about generative AI this year?\n\n- What are the major ethical concerns your school has about GenAl?\n\n- How can you adapt your current academic integrity policy to include GenAl?\n\n- How can the use of generative AI tools help students with IEP, 504, language barriers, and other learning needs?",
    "### Key Steps\n\n- Create a common understanding of Generative Al for all stakeholders through Al literacy.\n\n- Design a clear set of guidelines that work for both students and teachers.\n\n- Partner with stakeholders, including students, to develop and socialize the policy.\n\n- Identify that the policy is a work in progress.\n\n- Provide examples of the policy in stakeholder-specific language.",
    "### What to Include\n\n- Appropriate Use of Generative AI Tools: Identify what types of assignments and assessments can be AI-assisted with teacher approval and which must be completed without GenAI support.\n\nProvide examples of inappropriate use cases and appropriate use cases.\n\n- Tracking and Citing Generative AI: Provide guidelines on how students and teachers should track and cite their use of GenAI for their school work/practice.\n\nProvide examples of proper AI disclosure statements and citations in the correct format.\n\n- Data Privacy and Security: Clearly define what student, teacher, and school personally identifiable information (PII) includes.\n\nRemind all users that PII is off-limits to generative AI tools (including uploading or pasting in of data into genAI models as well as typing it in a chat).",
    "### Common Issues to Consider\n\n- Educators should only use generative AI for formative evaluation and the educator should always be in the loop, reading all student work and AI-generated comments.\n\n- Grading with generative AI tools can be unreliable due to inaccuracies or ‘hallucinations’ and implicit bias in generative AI tools.\n\n- Generative AI detectors are not reliable.\n\nThey often create false positives, penalizing non-native speakers and creative writing styles.\n\nThey often create false negatives for skillful AI prompters who know how to fool the AI.\n\n- Generative AI tools may make up incorrect information, a phenomenon known as ‘hallucination’.\n\nUsers must be trained to verify all data, facts, quotes, etc.\n\n- All users need explicit training on protecting data privacy, including reminders of what constitutes Personally Identifiable Information (PII).",
    "## Example Generative AI Amendment to School Integrity Policy\n\nAdapted from AI for Education ‘Guide to Developing an AI Policy For Your School’ \n\nGenerative Artificial Intelligence (generative AI) tools like ChatGPT are a significant technological advancement that has the potential to support your learning.\n\nBut with any new technology, there are significant limitations and risks associated with its use, misuse, and overuse.\n\nTo support appropriate, responsible use of generative AI in your learning, these steps should be taken when determining if, how, and when to use generative AI tools.\n\nIf these steps are not followed, your use of generative AI tools will be considered an academic integrity violation.",
    "### Step 1\n\nCheck with your teacher to find out if the assignment, homework, project, or assessment can be completed with the support of a generative AI tool, and if so, the level of generative AI support that is allowed (School and district leaders may wish to utilize or modify the Generative AI Acceptable Use Scale here to build common understanding and language about accepted levels of use).",
    "### Step 2\n\nIf generative AI is allowed and used, share your conversations with the tool by adding the share link to the chat on your final product or works cited page so that your teacher can evaluate your learning process and how you partnered with the generative AI model, as well as your final product.",
    "### Generative AI Training and AI Literacy\n\nBecause generative AI is already transforming the way we live, and will continue to have an even greater impact in the future, it is imperative that primary and secondary schools in every district and school develop and implement an AI Literacy program that provides all staff and students with the understanding of this powerful innovative new technology.\n\nThe Artificial Intelligence (AI) Literacy Act, recent bipartisan legislation that seeks to promote AI literacy in US schools, emphasizes a balanced focus on the foundational principles, applications, limitations and ethical implications of generative AI.\n\nIts goal is to amend the Digital Equity Act to codify AI Literacy as a component of digital literacy, which indeed it is.\n\nThis AI Literacy act defines AI literacy as “understanding of basic AI principles and applications, the skills to recognize when AI is employed, and awareness of its limits.”",
    "# AI Literacy and Its Importance\n\n\"AI literacy empowers individuals to be informed decision makers and benefits us all by preparing individuals to meaningfully engage in conversations about responsible and ethical development and use of artificial intelligence.\n\nHaving an AI-literate population will help promote national security and contribute to our economic competitiveness.\"\n\nThe miraculous speed of AI innovation in the past year has made it clear that AI is not going away and will affect all areas of our lives, as well as all people.\n\nAI Literacy is digital literacy in the 21st century and beyond.\n\nIt is imperative that all schools and districts ensure all staff and students are AI literate, and that AI literacy is infused in all curriculum areas.\n\nAfter establishing and sharing district-wide guidelines, it is crucial to develop a comprehensive AI Literacy training strategy that involves training all staff and students to develop in the effective, ethical, and safe use of generative AI tools.\n\nGiven the risks associated with irresponsible use, it is important to ensure comprehensive and consistent training for all users.\n\nStaff members should receive training initially and should be provided adequate time to practice and attain proficiency with the tools before extending their use to students.\n\nThe North Carolina Department of Public Instruction strongly advocates for educators to undergo professional development focused on both utilizing generative AI professionally and guiding students to effectively & ethically use generative AI as a learning partner.\n\nThis training should equip educators with the necessary knowledge to effectively employ generative AI in their work while ensuring its safe and responsible integration into classroom instruction.\n\nIt is also recommended that staff members have the opportunity to discuss their experiences, ask questions, express concerns, and provide feedback on the AI Implementation plan before they are responsible for integrating generative AI use with age-appropriate student groups.\n\nWe have provided a list of high-quality, free professional development for education leaders, teachers, and students in the appendix of this document.\n\nPlease see the Appendix for these recommendations.",
    "# AI Literacy for All\n\nAI is already becoming ubiquitous.\n\nOne can’t go online or on social media without encountering AI-generated content, even if it is not always recognized as AI-generated.\n\nOur students’ levels of AI literacy will have profound impacts on work, education, and all aspects of their lives in an AI-enhanced world in which humans interact with AI increasingly more each day and in which the old mantra ‘seeing is believing’ no longer holds true.\n\nThe TeachAI Toolkit offers a more detailed definition of AI Literacy:\n\n“AI literacy refers to the knowledge, skills, and attitudes associated with how artificial intelligence works, including its principles, concepts, and applications, as well as how to use artificial intelligence, such as its limitations, implications, and ethical considerations.”\n\nWhile specific guidelines for AI Literacy, especially in younger grades that are not allowed to use many of the generative AI tools are not yet developed, the good news is that many of the standards that NC has already adopted such as Computer Science and the NC Digital Learning Standards for Students (ISTE Standards) will help support AI Literacy, by developing computational thinking, technological skills, and supporting the durable skills in NC Portrait of a Graduate (Adaptability, Collaboration, Communication, Critical Thinking, Empathy, Learner’s Mindset, Personal Responsibility), that will be highly valued in the future.\n\nAI Literacy will, however, require an increased emphasis on media literacy, critical thinking, and ethics.\n\nStudents will need to be able to work alongside AI tools, think critically about media, and make ethical decisions about the use of AI tools and dissemination of content.\n\nInfusing AI literacy in all curriculum will ensure that our students are poised to succeed.\n\nTo ensure responsible, safe, and ethical implementation of generative AI, staff and students who are of the age to use generative AI should be trained on safe, effective, and responsible use including the following key aspects, each of which is covered in more detail in the ‘Curriculum and Assessment’ section of this document.\n\n- Alignment with PSU and school-based guidelines/policies governing generative AI usage.\n\n- Building a basic understanding of generative AI: how it works, its power to transform learning, and the concerns and limitations of current models.\n\n- How AI impacts education, including potential future implications on the job market.\n\n- Effective communicating with the Large Language models (prompting).\n\n- Safe, Ethical Use and Disclosure of Use and PSU and school guidelines.\n\n- AI as a Learning Partner to support curriculum standards, enhance human creativity & critical thinking.",
    "# AI Literacy Recommendations by Grade Span\n\nMore specific AI Literacy recommendations by grade span are being developed and will be added to this document in the coming months.\n\nIn the meantime, AI Literacy can be enhanced in each grade span in the following ways:\n\nIn addition to the grade level standards in the NC K12 Computer Standards and the NC Digital Learning Standards for each grade span:",
    "## Middle grades:\n\n- View, evaluate, and create AI generated content using generative image tools in creative apps such as Canva and Adobe Express to enhance AI Literacy, creativity, collaboration, and critical thinking.\n\n- Middle school students may also benefit from awareness of potentially unsafe and irresponsible uses of AI such as in social media applications such as SnapChat MyAI and even in the video games they play.",
    "## High school:\n\n- Dedicated generative AI training prior to utilizing generative AI Large Language Models like ChatGPT that includes the components above similar to Staff training, but with a student lens of using AI as a learning partner.\n\n- Increase critical thinking and media literacy, focusing on AI Generated content online.\n\n- View, evaluate, and create AI generated content using generative image tools in creative apps such as Canva and Adobe Express to enhance AI Literacy, creativity, collaboration, and critical thinking.\n\n- Students should gain awareness of the potentially unsafe and irresponsible uses of AI such as in social media applications such as SnapChat MyAI and even in the video games they play.",
    "## Large Language Models (LLMs)\n\nChatGPT is an example of a Large Language Model (LLM), a type of generative artificial intelligence program designed to understand and generate human-like text.\n\nIt is similar to having a very knowledgeable assistant who has read a vast amount of books and articles and can provide just-in-time assistance.\n\nThis assistant can answer questions, write stories, and even help with homework, by using the information it has learned.\n\nSome other well-known LLM models are Bing Chat, Google Bard, Claude, & Perplexity.\n\nJust as you would review the work of a capable assistant, the human user must also review the work of the LLM and make adjustments as needed.\n\nWhen used skillfully, LLMs can have significant positive impacts on teachers by drastically reducing the time required for tasks such as planning, creating content, assessing student work, and executing tasks such as emails and newsletters.\n\nThis reduction in time on task can result in a better work-life balance and improved job satisfaction.\n\nIn addition to saving teachers a lot of time, utilizing generative AI LLM models can also open a whole new world of ideas and creativity, which often revitalizes their passion for teaching.\n\nIt can give them more control over the content they create, allowing them to personalize content for their teaching style and their students’ distinct needs, rather than appropriating generic content from textbooks or online sources.\n\nPerhaps most importantly, if used skillfully, these tools can allow educators more time to focus on the reasons they entered the profession; building relationships with students and targeting their individual needs.\n\nFor students, generative AI can act as a learning partner to give them just-in-time assistance and guidance based on their individual needs, helping to level the playing field for neurodivergent students, those with learning needs, those who are not native speakers, those from economically disadvantaged or other historically marginalized communities, and all students.\n\nLearning to work effectively with generative AI can also help prepare students for rewarding careers in their AI-rich future in which being able to work effectively with generative AI will be an expectation.\n\nTo ensure teachers and students have the skills to realize the tremendous positive impact that AI can have on education, educators need a basic understanding of how the models work.\n\nThey should know that LLMs are different from search engines, and must be used differently for helpful results.\n\nIt is helpful to provide new users with a prompting framework that details how to effectively create a prompt to improve results.\n\nThis will help ensure a positive introduction to LLMs and help ensure the users get the most efficient and effective results.\n\nOne such framework is the CRAFT AI Prompting framework by Vera Cubero (NCDPI).\n\nThis frameworks using an acronym for the word CRAFT as a simple reminder to help guide educators and students alike remember how to craft prompts that get the most targeted and helpful results from the chatbot.",
    "## The CRAFT prompting framework:\n\nThis framework allows users to skip the learning curve and interact with the models in a way that will help them see helpful and targeted results quickly, resulting in more successful introductions to LLMs such as Chat GPT, Google Bard, etc.\n\nA brief explanation of this framework with several example prompts can be accessed at  The image below can be downloaded for printing as a poster by going to",
    "# AI for Education Prompt Library\n\nAdditionally, educators and students alike would benefit from examining well-developed prompts such as those in the Prompt Library from AI for Education.\n\nThis terrific resource provides many examples of well-designed and effective prompts along with suggestions for further personalizing the prompts.\n\nThe prompts can be copied, pasted into a model, and then edited to suit the user’s needs.\n\nAt the time of publishing, this valuable resource contains 100 prompt examples with many different uses for teachers and students.",
    "### Data Privacy Concerns:\n\nAll users must be taught the importance of protecting data privacy when using generative AI tools.\n\nUsers should never input Personally Identifiable Information or PII into an AI tool (or anywhere else without careful consideration)!!!\n\nStudent ID Numbers are PII.\n\nBe especially mindful of this when pasting data into the model or uploading any data that may contain PII.",
    "### Bias:\n\nBecause generative AI models are trained on the Internet, there is always the potential for inherent societal biases surrounding gender roles, race, religion, and politics.\n\nWhile AI companies are focused on fine-tuning their models to ensure that they do not perpetuate stereotypes or biases, such biases are always possible because the training data set includes the entire Internet.\n\nSchools and districts must be prepared to mitigate potential issues that arise from bias within the use of AI.\n\nAI models have a built-in evaluation (thumbs up/down) and an option to include a comment.\n\nIf bias is suspected or detected, this is one way to report it to the company that produces the tool.\n\nEstablishing clear methods of communicating concerns with AI systems deployed within their educational environments is also needed at the school or district level.\n\nBias mitigation techniques should be included in AI Literacy training to educators and students, including how to identify and address biases in AI-generated content at the school or district level.",
    "### Inaccuracies/‘hallucinations’:\n\nLarge Language Models like ChatGPT are not search engines and these models actually generate content by making predictions based on their training data and the user input or prompt.\n\nThey do not search for and return content that already exists as search engines do.\n\nBecause of this, LLMs have the potential of generating (predicting) content that is not factually correct, but sounds very plausible.\n\nThis phenomenon is commonly referred to as ‘hallucinations’.\n\nAI developers are constantly fine-tuning their models to reduce hallucinations, but because of the way the models work, it may not be possible to eliminate them entirely, at least for some time.\n\nTherefore, it is essential that users understand this and are trained to verify all facts, quotes, statistics, and resources in AI responses using dependable online sources.\n\nThe most effective use of generative AI LLMs is by a user with knowledge of the subject matter, and who is therefore more likely to notice and question inaccuracies.\n\nIt is especially important to verify data with reliable sources if the user is not a subject matter expert on the topic as they are much less likely to recognize inaccuracies if they do occur.\n\nSome LLM models provide links in their responses to make fact-checking responses easier (such as Bing Chat & Perplexity) while Google Bard has a built-in mechanism for the user to verify the information by clicking the G beneath a response.\n\nUnder 18 users are guided through this on their first fact-based prompt as part of Bard’s AI guidance for students.",
    "## Strategies to Ensure More Accurate Responses from LLMs\n\nWhile there currently is no foolproof way to completely eliminate the possibility of generative AI models providing inaccurate information or hallucinations, there are prompting strategies that users can employ to reduce the likelihood of inaccuracies.\n\nThe following handout outlines five strategies to ensure more accurate responses by LLMs.\n\nYou may download this as a poster by going to",
    "## How to Use AI Responsibly EVERY Time\n\nThe EVERY framework provides an acronym to remind users of the steps needed to ensure ethical use of AI by staff and students alike, EVERY time AI is used.\n\nThis framework was a collaboration between AI for Education (aiforeducation.io) and Vera Cubero (NCDPI).\n\nTo download a printable pdf of the EVERY framework, visit",
    "## Rethinking Plagiarism and Cheating in the Age of AI\n\nIn the not-too-distant future, it will be a common assumption that all writing from academic papers to news reports and emails may be written with AI.\n\nIn light of this, it is perhaps shortsighted to automatically consider all use of AI as ‘cheating’.\n\nEducators will need to rethink their ideas of what constitutes plagiarism and cheating in today’s world, and adapt their teaching, assignments, and expectations to this new reality.\n\nAn AI Acceptable Use Scale is an important part of a school or PSU’s generative AI adoption plan to help build common understanding, clear expectations, and common language around the use of AI by students.\n\nThe scale should be referred to clarify what level, if any, is acceptable use of AI on a given task.\n\nIt should be explicitly taught and posted in visible locations for reference.\n\nThe following AI assessment scale was adapted for the K12 environment by Vera Cubero (NCDPI).\n\nIt is based on the original work of Dr. Leon Furze, Dr. Mike Perkins, Dr. Jasper Roe FHEA, & Dr. Jason Mcvaugh.\n\nThis version of the scale includes five levels of AI Assessment (0-4) with descriptions of each along with disclosure or citation recommendations.\n\nA scale such as this can help build the common understanding and language to ensure fair and equitable treatment of issues of suspected plagiarism or cheating with AI in the K12 setting.\n\nTo download click the image below or visit  The lower corner includes an editable template link if you would like to modify it to suit your particular needs.",
    "## Disclosing AI Use or Citing Generative AI as a Source\n\nEducators should lead by example and model transparency and academic honesty about their use of generative AI tools, and teach students to do the same.\n\nBecause today’s generative AI tools cannot actually create content without some level of human participation and guidance, it is generally considered best practice to acknowledge the use or partnership with the AI tool when a formal citation is not required.\n\nBecause generative AI is so new, there will likely be further litigation surrounding AI and copyright.\n\nOne issue that was decided on 8/18/2023 by the US District Court for the District of Columbia in a federal decision is that a work created entirely by artificial intelligence cannot be copyrighted.\n\nReference Thaler v Perlmutter, Case No 1:22-cv-01564 (D.D.C2022).\n\nAlso, AI cannot be considered the sole author or creator of a work.\n\nIf traditional citations are not required, but any form of AI assistance was used, it is recommended to include in the disclosure statement how AI was used (brainstorming, outlining, feedback, editing, etc).\n\nDisclosure statements can be included in an \"AI Credits\" section at the end of the work or within the text, beneath an image, etc.\n\nas appropriate.\n\nA link to AI chats can be shared on most major LLM platforms, and this is a great way for teachers to see a student's learning process and how the student relied on or partnered with the AI to complete the work.\n\nExample disclosure statements:\n- “Created by John Doe with editing assistance from ChatGPT”\n- “I used Google Bard to help me brainstorm ideas for my project”\n- “I used ChatGPT to help me organize my thoughts into a finished product”\n- “Image created in partnership with Adobe Firefly.\n\nPrompt; ‘create a cartoonish image of a bored frog on a lily pad, surrounded by cattails.\n\n16:9 Make it bright and colorful so that young children would enjoy it.’",
    "### Formal Citations:\n\nIf a formal citation is expected, both the MLA and APA provide guidance on their websites for creating citations.\n\nCurrently most online citation tools do not include AI in their options.\n\nYou can ask your LLM to create citations, but as with anything else, the user will need to verify that the details and formatting are correct.\n\nTraditional Citation Guidelines:\n- MLA format - \n- APA format - \n\nExample MLA Citation:\n“For this activity, I want you to take on the role of the character Jonas from the novel The Giver ” prompt.\n\nChatGPT, 24 May version, OpenAI, 8 June 2023,  .\n\nExample APA Citation:\nOpenAI.\n\n(2023).\n\nChatGPT (May 24 version) [Large language model].",
    "## Use Caution with AI Detectors\n\nAI detectors have proven not to be dependable, therefore they should never be used as the only factor when determining if a student ‘cheated’.\n\nCommon issues with AI detectors are a high frequency of false positives for non-native English speakers and creative writers as well as a high frequency of false negatives for students who are skilled at working with AI and are capable of fooling the detectors.\n\nIf there is suspicion that a student depended on AI too heavily for an assignment, this should be viewed as a teachable moment to reinforce the appropriate partnership with AI tools rather than a ‘gotcha’ moment.\n\nWorking with AI in many ways is the same as working with a tutor, asking a parent for assistance, or completing an assignment with a partner or a collaborative group.\n\nIn the age of AI, it is important to focus on student reflection on the process of learning, rather than just the end product.\n\nEducators should ensure proper communication about appropriate uses of AI on each assignment, referencing an AI Acceptable Use Scale such as the one provided on the previous page to clarify the appropriate level of generative AI as this may vary from assignment to assignment and class to class.\n\nThis graphic by Holly Clark of The Infused Classroom is an excellent visual aid to demonstrate why AI detectors are problematic and what to do instead.\n\nImage credit: Use with permission from Holly Clark of The Infused Classroom,",
    "## Teacher Use Cases of Generative AI\n\nAI can be used by educators to support their daily work tasks and transform the student learning experience in a variety of ways that can help reduce the burden of teaching as well as improve educators’ ability to personalize learning for their students, thus improving teaching and learning.\n\nMany educators who skillfully use generative AI in their lesson planning have reported a renewed passion for their content because of generative AI’s potential to help them develop engaging new lessons and activities that are personalized to their own teaching style, as well as student abilities and needs.\n\nFurthermore, using generative AI to automate routine, mundane tasks can free up time for teachers to focus on higher-order thinking tasks, problem-solving, collaboration, and making human connections with their students.\n\nThere are several areas in which AI can support teachers in both developing and delivering more effective and personalized lessons (this is not an exhaustive list- the possibilities are endless):\n- Brainstorming/Thought Partner: Large Language Models like ChatGPT and Google Bard have access to so much information and are fantastic to brainstorm and get fresh ideas.\n\nTeachers can utilize generative AI as a thought partner to bring new life into old lessons, brainstorm ideas for projects, labs, and assignments, solve educational problems of practice such as behavior issues, learning to better meet the needs of diverse learners, implementing new strategies, and just about any other issue they face in the classroom.\n\n- Content creation tools: AI can assist teachers in creating engaging and interactive lesson materials, such as presentations, simulations, games, and more.\n\n- Efficient assistant: Being able to use Large Language Models for completing the mundane tasks such as emails, newsletters, creating rubrics etc frees up teachers to do more creative tasks, make connections with students, and improve their work-life balance.\n\n- Personalization: AI can help teachers create adapted learning content that meets a student’s individual learning style, reading level, language, pace etc and can save educators time in personalizing content to meet a variety of student needs.\n\n- Data-driven decision making: AI can provide teachers with real-time insights into student performance, allowing them to identify areas of strength and weakness and adjust their instruction accordingly.\n\n- Automated grading and feedback: AI can automate the grading of quizzes and essays, freeing up teacher time for more personalized instruction and feedback.\n\n- Creative Assistant: Use image generator tools such as Adobe Firefly, Canva, etc to create the images you need using plain language instead of searching for hours for them.",
    "## Providing Student Feedback Using Generative AI\n\nGenerative AI tools can reduce a lot of the time spent in assessing student work, which is one of the most time-consuming tasks for most educators.\n\nGenerative AI can save considerable time in evaluating student writing in addition to the other potential benefits below.\n\nAI has the potential to revolutionize assessment by:\n- Lending Objectivity: Generative AI tools can lend objectivity to the assessment of student work.\n\n- Alleviating Writers’ block: Generative AI tools are a great starting point for formulating comments, helping teachers find words that reflect the tone and purpose they wish to convey.\n\n- Analyzing complex student responses: AI algorithms can analyze student-drawn models and group them based on similarities, helping teachers understand student understanding of complex concepts like \"rate of change.\"\n\n- Providing instant feedback: AI can offer immediate feedback on complex skills like learning sign language or speaking a foreign language, even when a human instructor is unavailable.\n\n- Lightening teachers' workload: AI assistants can grade simple aspects of student work, freeing up teachers' time to focus on more complex tasks like evaluating essays and projects.\n\n- Enhancing accessibility: AI-powered learning technologies can provide verbal feedback to students, making learning more accessible for all students, including those with disabilities.\n\n- Embedding feedback into the learning process: AI can provide real-time feedback to students while they are working on a problem, helping them identify errors and improve their understanding before they submit their work.\n\nHowever, it is imperative with today’s generative AI tools that teachers use generative AI for assessment and grading with some specific guidelines in mind:\n- Formative Assessment Only: Today’s Generative AI should be used only for formative assessment.\n\n- Today’s Large Language Models and other generative AI tools are new technology and not completely reliable, therefore should not be used to assign letter or number grades to student work.\n\n- Humans must always be in the loop to ensure fair and equitable treatment of student work.\n\n- The teacher should always review student work and evaluate and edit as needed any AI-generated comments before sharing with students.\n\n- Teachers should understand the potential for ‘hallucinations’ and how they can mitigate this when using generative AI to evaluate student work.\n\n- For example, LLMs are more likely to hallucinate if you ask for something that doesn’t exist, such as asking it to ‘identify all the grammatical errors in this passage’.\n\nIf there are no grammatical errors, it may ‘find’ some anyway because you asked it to.\n\nInstead, asking it to ‘evaluate the writing for grammatical usage’ would be less likely to produce hallucinations.",
    "# Student Use of Generative AI\n\nGenerative AI has enormous potential to improve student learning outcomes and erase the discrepancies that now exist in access to education for economically disadvantaged students, students with learning disabilities, English Language Learners, neurodivergent students as well as all other students.\n\nBelow are a few examples of the ways generative AI can aid students in learning, and new developments are sure to continue to emerge as well.",
    "## Learning Partner & Personal Tutor:\n\nGenerative AI tools can provide just-in-time, objective and targeted assistance, feedback and guidance to students.\n\nGenerative AI can help explain difficult concepts, provide evaluation and feedback, help generate ideas, act as a thought partner, debate, partner, a character from fiction, history, a career, and more.\n\nThe possibilities are endless.",
    "# Creativity and Collaboration\n\nCollaborative learning platforms: Students can use AI-powered platforms to collaborate with peers on projects, share ideas, and receive feedback from peers and teachers.\n\nGenerative image generators such as Adobe Firefly, Adobe Express, Canva, and others provide students the ability to explore their creativity in new ways by using natural language input to create new works of art for self-expression, illustrate their own writing, or demonstrate learning.",
    "## Data Privacy\n\nFERPA defines the term personally identifiable information (PII) to include direct identifiers (such as a student's or other family member's name) and indirect identifiers (such as a student's date of birth, place of birth, or mother's maiden name).\n\nLLM models such as ChatGPT utilize user input in the form of chats to continue training the models.\n\nTherefore it is imperative that users fully understand what PII is, and learn NOT to enter, paste, or upload any PII into the chat of any generative AI tool.\n\nAll users should be reminded of what data is considered PII, and that it includes student ID numbers.\n\nUsers should use caution in particular to avoid inadvertently copying or uploading PII into the model when evaluating student responses, analyzing data, or creating personalized content such as IEP goals, personalized learning plans, etc.",
    "## Cybersecurity\n\nConduct and review a security audit on the product/vendor to ensure they meet or exceed applicable security practices, PSU, State, and Federal requirements.\n\nUnderstand how the product harvests data for training and continual learning.\n\nThis is critical to help ensure sensitive data or contaminated data is not ingested.",
    "# Reviewing and Adapting Guidelines\n\nSchool districts and schools must continuously review and adapt their AI guidelines to keep pace with the rapid evolution of AI technologies.\n\nThis involves regular assessments of AI practices, potential risks, and emerging trends to maintain responsible and ethical integration.\n\nAll PSUs engaging with AI technologies should regularly review the company’s usage and privacy guidelines.\n\nUsage: PSUs, schools, educators, and students that are utilizing any type of AI tools adhere to specific usage requirements outlined by the tool's developer or provider.\n\nThis includes complying with age restrictions, data usage practices, any restrictions, inclusivity, limitations, notifications, and any other relevant guidelines or restrictions.\n\nThis should include awareness and procedures in place regarding, but not limited to following COPPA, CIPPA, IDEA, FERPA, and section 504.",
    "# Relevant Policies in the US\n\nFERPA - Family Educational Rights and Privacy Act: Protects the privacy of student education records & gives parents certain rights regarding student education records.\n\nAI systems must protect the privacy of student education records and comply with parental consent requirements.\n\nData must remain within the direct control of the educational institution.\n\nCOPPA - Children’s Online Privacy Protection Act: Imposes requirements on websites and online services directed to children under 13 years of age, or that collect personal information from a child under 13.\n\nAI chatbots, personalized learning platforms, and other technologies collecting personal information and user data on children under 13 must require parental consent.\n\nIDEA - Individuals with Disabilities Education Act: Ensures students with disabilities are provided with free appropriate education that is tailored to their individual needs.\n\nAI must not be implemented in a way that denies disabled students equal access to education opportunities.\n\nAI tools can be used to help meet students’ individual needs and help provide them with independence and equal access to learning content.\n\nAI tools can be used to help relieve the burden of meeting each student’s individual needs.\n\nCIPA - Children’s Internet Protection Act: Requires schools and libraries that receive federal funds for Internet access or internal connections to adopt and enforce policies to protect minors from harmful content online.\n\nSchools must ensure AI content filters align with CIPA protections against harmful content.\n\nSection 504 - A federal law designed to protect the rights of individuals with disabilities in programs that receive federal financial assistance from the US Department of Education.\n\nThis section of the Rehabilitation Act applies to both physical and digital environments.\n\nSchools must ensure that their digital content and technologies, like AI, are accessible to students with disabilities.",
    "## Purchasing and Using AI Technologies\n\nWhen it comes to investing in technologies to support learning with and about Artificial Intelligence, it is important to ensure that the technology resources:\n\n- Are Age Appropriate for the User of the AI Technology: Pay special attention to age allowances of the technology to ensure compliance with federal, state, and local laws and policies as well as age limits and permission requirements in the terms of service for each application.\n\n- Comply with Regulations: Prioritize technologies that comply with federal, state, and local regulations regarding data privacy and cybersecurity in educational settings.\n\nFamiliarize yourself with regulations like the Family Educational Rights and Privacy Act (FERPA) in the United States and similar laws in other regions.\n\n- Secure Access Controls: Implement secure access controls to ensure that only authorized individuals have access to sensitive student data.\n\nThis includes usernames, passwords, and multifactor authentication methods to protect against unauthorized access.\n\n- Encrypt and Secure Transmission: Ensure that data, especially personally identifiable information (PII), is encrypted both in transit and at rest.\n\nThis adds an extra layer of protection against data breaches or unauthorized interception.\n\n- Are Required to Undergo Regular Security Audits: Periodically conduct security audits and assessments to identify vulnerabilities in the technology infrastructure.\n\nThis helps in proactively addressing potential security risks and ensuring a robust cybersecurity posture.\n\n- Meet Clear Data Usage Policies: Clearly communicate to students, parents, and educators how data collected by AI technologies will be used, stored, and shared.\n\nEstablish transparent policies that align with best practices for data privacy in educational settings.\n\n- Meet Vendor Security Standards: If using third-party platforms or services, verify that the vendors adhere to stringent security standards.\n\nThis includes evaluating their data protection policies, encryption practices, and overall commitment to cybersecurity.",
    "## Resources for K12 School Leaders\n\n- NCDPI DTL Team- ‘Leading K12 Schools in the Age of AI’ Slide Deck: \n- US Dept.\n\nof Education Office of Educational Technology: Artificial Intelligence\n- TeachAI Guidance for Schools Toolkit\n- ISTE Artificial Intelligence in Education Resource Collection: \n- The White House Blueprint for an AI Bill of Rights\n- Common Sense Media “AI and Our Kids: Common Sense Considerations and Guidance for Parents, Educators, and Policy Makers 2023”\n- Common Sense Media AI Initiative: Product Reviews",
    "## Resources for Staff Development\n\n- aiforeducation.io\n- Free 2-hour Course: on-demand An Essential Guide to AI for Educators\n- AI Launchpad Webinar Series\n- Prompt Library\n- Student Curriculum (also great for staff!)\n\n4 complete lessons for grades 7-9 or 10-12 1.\n\nInterview a Chatbot, 2.\n\nMind of a Machine, 3.\n\nHallucination Detective, 4.\n\nCo-Creating an AI Policy\n\n- Code.org\n  - “AI 101 for Educators” Approx.\n\n5 hours, on-demand\n  - How AI Works Video Series (for teachers and students)\n\n- Google\n  - Introduction to Generative AI\n  - Considered to be an eight-hour free course (but more like 2 hours); includes information specifically about generative AI,\n  - Introduction to Large Language Models\n  - Considered to be an eight-hour free course (but more like 2 hours); includes information specifically on Large Language Models\n  - Introduction to Responsible AI course\n  - Considered to be an eight-hour free course (but more like 2 hours); includes information about what responsible AI is and why it is essential.\n\n- Microsoft\n  - Microsoft Learn Educator Center AI for Education\n  - Contains 4 Educator Trainings on AI\n  - AI for Education: Resources and Learning Opportunities",
    "## Resources for Teaching Students about AI\n\n- The AI Education Project aiEDU.org\n  - AI Snapshots\n  - AI Snapshots by aiedu.org\n  - 5 min critical thinking activities about AI for grades 7-12\n  - The link above allows you to download a Google folder with all Snapshots.\n\n- Snapshots do not require student use of AI.\n\n- AI in Five Minutes - \n  - A great quick overview of AI for staff or students\n\n- Learn About AI\n  - AI challenges & Projects \n  - Intro to AI Course\n  - Full 10-week Project Based Learning course for grades 9-12\n  - \n\n- AI for Education aiforeducation.io\n  - Student Curriculum lessons\n  -  (also great for staff!)\n\n- 4 complete lessons for grades 7-9 or 10-12\n    1.\n\nInterview a Chatbot\n    2.\n\nMind of a Machine\n    3.\n\nHallucination Detective\n    4.\n\nCo-Creating an AI Policy\n\n- Prompt Library for Students\n  - \n  - An ever-growing collection of well-developed prompts geared toward student use of genAI as a learning partner.\n\nPrompts can be copied and pasted, then edited to suit as needed.\n\n- Code.org\n  - How AI Works Video Series\n  - AI Curriculum (multiple options for grades 3-12)\n  - Educators can use code.org curriculum and their learning platform for free.\n\nIt allows educators to create classes and make assignments.\n\n- Common Sense Media\n  - AI Literacy Lessons for Grades 6-12\n  - Eight 15-20 minute lessons\n\n- Google\n  - Learn About generative AI by Google\n  - 5 Must Knows to Get Started with generative AI (video)",
    "# References\n\n- AI for Education.\n\n\"Drafting a GenAI Academic Policy at Your School.\"\n\nAI for Education, 2023,  Accessed 5 Dec. 2023.\n\n- AI for Education.\n\n\"Guide to Developing AI Policy at Your School.\"\n\nAI for Education, 2023,  Accessed 5 Dec. 2023.\n\n- AI for Education., Cubero, V. \"How to Use AI Responsibly EVERY Time.\"\n\nAI for Education, 2023,  Accessed 5 Dec. 2023.\n\n- AI for Education.\n\n\"Prompt Library.\"\n\nAI for Education, 2023,  Accessed 16, Dec. 2023.\n\n- AI for Education.\n\n\"Top 6 Questions for GenAI Ed Tech Providers.\"\n\nAI for Education, 2023,  Accessed 5 Dec. 2023.\n\n- Blunt, L, Bucshon, L. “The Artificial Intelligence (AI) Literacy Act.\n\n2023.\n\nAccessed 21 Dec. 2023.\n\n- Center for Democracy & Technology.\n\n\"Off Task: Ed Tech Threats to Student Privacy and Equity in the Age of AI.\"\n\nCenter for Democracy & Technology, 2023.\n\nAccessed 27 Oct. 2023.\n\n- Clark, Holly.\n\n“Why AI Detectors are Problematic and What to Do Instead.” Infused Classroom, 2023.\n\nAccessed 25 Nov. 2023.\n\n- Code.org, CoSN, Digital Promise, European EdTech Alliance, Larimore, J., and PACE (2023).\n\nAI Guidance for Schools Toolkit.\n\nRetrieved from teachai.org/toolkit.\n\n20 Dec. 2023.\n\n- Common Sense Media.\n\n\"AI Evaluation Tool.\"\n\nCommon Sense Media, 2023,  Accessed 5 Dec. 2023.\n\n- Cubero, Vera (NCDPI).\n\n\"How to Use AI Responsibly EVERY Time.\"\n\nAI for Education & Vera Cubero, 2023,  Accessed 5 Dec. 2023.\n\n- Cubero, Vera (NCDPI).\n\n\"3 Bikes Analogy.\"\n\n2023,  Accessed 3 Dec. 2023\n\n- Cubero, Vera (NCDPI).\n\n\"Can I Use AI on this Assignment?\n\nGenerative AI Acceptable Use Scale.\"\n\n2023,  Accessed 18 Dec. 2023.\n\n- Cubero, Vera (NCDPI).\n\n\"Comparing LLMs.\"\n\nAccessed 30 Dec. 2023.\n\n- Cubero, Vera (NCDPI).\n\n“CRAFT AI Prompting Framework for Educators.” 2023,  Accessed 5 Dec 2023.\n\n- Cubero, Vera, Dall-E 3.\n\n\"Outline of North Carolina Filled with AI and Computer Science Imagery….\"\n\nOpenAI's DALL-E, ChatGPT, chat.openai.com, Accessed 9 Jan 2024.\n\n- Cubero, Vera (NCDPI).\n\n“Strategies to Ensure More Accurate Responses from LLMs”.\n\n2023,  Accessed 17 Dec. 2023.\n\n- Office of Digital Teaching & Learning at North Carolina Department of Public Instruction.\n\n“Leading K12 Schools in the Age of AI.”  Accessed 2, Dec. 2023.\n\n- OpenAI.\n\n'Overall Evaluation of 'North Carolina Department of Public Instruction AI Implementation & AI Literacy Recommendations and Considerations for Public Schools' Document.'\n\nChatGPT,  .\n\n1 Jan. 2024.\n\n- Perkins, M., Furze, L., Roe, J., & MacVaugh, J.\n\n(2023).\n\n“Navigating the generative AI era: Introducing the AI assessment scale for ethical GenAI assessment”.\n\narXiv preprint arXiv:2312.07086.\n\nAccessed 17, Dec. 2023.\n\n- Portrait of a Graduate.\n\n\"North Carolina Department of Public Instruction, North Carolina Department of Public Instruction, 4 Dec. 2023, .\n\n- Teach AI.\n\n\"AI Guidance for Schools Toolkit.\"\n\nTeach AI, 2023,  Accessed 15 Nov. 2023.\n\n- U.S. Department of Education, Office of Educational Technology.\n\n\"Artificial Intelligence and Future of Teaching and Learning: Insights and Recommendations.\"\n\nU.S. Department of Education, 2023, Full Report.\n\nAccessed 20 Nov. 2023.\n\n- U.S. Department of Education, Office of Educational Technology.\n\n\"Artificial Intelligence and Future of Teaching and Learning: Insights and Recommendations - Core Messaging Handout.\"\n\nU.S. Department of Education, 2023, Core Messaging Handout.\n\nAccessed 20 Nov. 2023.\n\n- World Economic Forum.\n\n\"Future of Jobs Report 2023.\"\n\nWorld Economic Forum, 2023,  Accessed 25 Nov. 2023.",
    "## April 26, 2023\n\n**Dr. Dario Amodei  \nChief Executive Officer  \nAnthropic  \n548 Market St, PMB 90375  \nSan Francisco, CA 94104**\n\nDear Dr. Amodei,  \nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs[^1] and risks[^2].\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from traditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)[^3], tampering with training data (data poisoning attacks)[^4], and inputs to models that intentionally cause them to make mistakes (adversarial examples)[^5].\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware[^6], craft increasingly sophisticated phishing techniques[^7], contribute to disinformation[^8], and provide harmful information[^9].\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.\n\n[^10]  \nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "### Questions:\n\n- Can you provide an overview of your company’s security approach or strategy?\n\n- What limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n[^1]: Maurice Dawson et al., “Integrating Software Assurance into the Software Development Life Cycle (SDLC),” Journal of Information Systems Technology and Planning (2010), Available at:  Software_Assurance_into_the_Software_Development_Life_Cycle_SDLC\n[^2]: “U.S.\n\nand International Partners Publish Secure-by-Design and -Default Principles and Approaches,” Cybersecurity and Infrastructure Security Agency (April 13, 2023),  international-partners-publish-secure-design-and-default-principles-and-approaches\n[^3]: “OWASP AI Security and Privacy Guide,” OWASP Foundation,  privacy-guide/\n[^4]: Fahri Anıl Yerlikaya, Şerif Bahtiyar, “Data poisoning attacks against machine learning algorithms”, Expert Systems with Applications, Volume 208, (July 18, 2022).\n\nAvailable at: \n[^5]: Alexey Kurakin, Ian Goodfellow, Samy Bengio, “Adversarial Examples in the Physical World,” Google, Inc.\n\nAvailable at: \n[^6]: Dan Goodin, “Hackers Are Selling A Service that Bypasses ChatGPT Restrictions on Malware,” Ars Technica (February 8, 2023)  that-uses-chatgpt-to-generate-malware/\n[^7]: Lily Hay Newman, “AI Wrote Better Phishing Emails Than Humans in a Recent Test,” Wired (August 7, 2021), \n[^8]: Tiffany Hsu, Stuart A. Thompson, “Disinformation Researchers Raise Alarms About A.I.\n\nChatbots,” New York Times (February 13, 2023), \n[^9]: “GPT-4 Jailbreak and Hacking via RabbitHole attack, Prompt injection, Content moderation bypass and Weaponizing AI,” Adversa AI (March 15, 2023),  rabbithole-attack-plus-prompt-injection-content-moderation-bypass-weaponizing-ai/\n[^10]: Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” (February, 2018),  376b92c619de/downloads/MaliciousUseofAI.pdf?ver=1553030594217\n\n- Are you participating in third party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information you that your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?\n\n[^11]\n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\n**Mark R. Warner  \nUnited States Senator**\n\n[^11]: “AI Risk Management Framework,” NIST (July 12, 2021),  framework\n\n---",
    "## April 26, 2023\n\n**Mr. Tim Cook  \nChief Executive Officer  \nApple  \nOne Apple Park Way  \nCupertino, CA 95014**\n\nDear Mr. Cook,  \nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs[^1] and risks[^2].\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from traditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)[^3], tampering with training data (data poisoning attacks)[^4], and inputs to models that intentionally cause them to make mistakes (adversarial examples)[^5].\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware[^6], craft increasingly sophisticated phishing techniques[^7], contribute to disinformation[^8], and provide harmful information[^9].\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.\n\n[^10]  \nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "### Questions:\n\n- Can you provide an overview of your company’s security approach or strategy?\n\n- What limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n- Are you participating in third party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information you that your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?\n\n[^11]\n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\n**Mark R. Warner  \nUnited States Senator**\n\n[^11]: “AI Risk Management Framework,” NIST (July 12, 2021),  framework\n\n---",
    "## April 26, 2023\n\n**Mr. Sundar Pichai  \nChief Executive Officer  \nGoogle  \n1600 Amphitheater Parkway  \nMountain View, CA 94043**\n\nDear Mr. Pichai,  \nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs[^1] and risks[^2].\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from traditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)[^3], tampering with training data (data poisoning attacks)[^4], and inputs to models that intentionally cause them to make mistakes (adversarial examples)[^5].\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware[^6], craft increasingly sophisticated phishing techniques[^7], contribute to disinformation[^8], and provide harmful information[^9].\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.\n\n[^10]  \nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "### Questions:\n\n- Can you provide an overview of your company’s security approach or strategy?\n\n- What limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n- Are you participating in third party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information you that your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?\n\n[^11]\n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\n**Mark R. Warner  \nUnited States Senator**\n\n[^11]: “AI Risk Management Framework,” NIST (July 12, 2021),  framework\n\n---",
    "## April 26, 2023\n\n**Mr. Mark Zuckerberg  \nChief Executive Officer  \nMeta Platforms, Inc.  \n1 Hacker Way  \nMenlo Park, CA 94025**\n\nDear Mr. Zuckerberg,  \nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs[^1] and risks[^2].\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from traditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)[^3], tampering with training data (data poisoning attacks)[^4], and inputs to models that intentionally cause them to make mistakes (adversarial examples)[^5].\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware[^6], craft increasingly sophisticated phishing techniques[^7], contribute to disinformation[^8], and provide harmful information[^9].\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.\n\n[^10]  \nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "### Questions:\n\n- Can you provide an overview of your company’s security approach or strategy?\n\n- What limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n- Are you participating in third party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information you that your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?\n\n[^11]\n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\n**Mark R. Warner  \nUnited States Senator**\n\n[^11]: “AI Risk Management Framework,” NIST (July 12, 2021),  framework",
    "# Integrating Software Assurance into the Software Development Life Cycle (SDLC)\n\n**Systems Technology and Planning (2010), Available at**: ResearchGate\n\n**“U.S.\n\nand International Partners Publish Secure-by-Design and -Default Principles and Approaches,”** Cybersecurity and Infrastructure Security Agency (April 13, 2023), CISA\n\n---\n\nTraditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)3, tampering with training data (data poisoning attacks)4, and inputs to models that intentionally cause them to make mistakes (adversarial examples)5.\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware6, craft increasingly sophisticated phishing techniques7, contribute to disinformation8, and provide harmful information9.\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.10\n\nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "**References:**\n\n1.\n\n“OWASP AI Security and Privacy Guide,” OWASP Foundation, OWASP\n2.\n\nFahri Anıl Yerlikaya, Şerif Bahtiyar, “Data poisoning attacks against machine learning algorithms”, Expert Systems with Applications, Volume 208, (July 18, 2022).\n\nAvailable at: ScienceDirect\n3.\n\nAlexey Kurakin, Ian Goodfellow, Samy Bengio, “Adversarial Examples in the Physical World,” Google, Inc.\n\nAvailable at: Google Research\n4.\n\nDan Goodin, “Hackers Are Selling A Service that Bypasses ChatGPT Restrictions on Malware,” Ars Technica (February 8, 2023) Ars Technica\n5.\n\nLily Hay Newman, “AI Wrote Better Phishing Emails Than Humans in a Recent Test,” Wired (August 7, 2021), Wired\n6.\n\nTiffany Hsu, Stuart A. Thompson, “Disinformation Researchers Raise Alarms About A.I.\n\nChatbots,” New York Times (February 13, 2023), New York Times\n7.\n\n“GPT-4 Jailbreak and Hacking via RabbitHole attack, Prompt injection, Content moderation bypass and Weaponizing AI,” Adversa AI (March 15, 2023), Adversa AI\n8.\n\nMiles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” (February, 2018), AI Forecasting\n\n---\n\n- Are you participating in third party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information you that your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?11\n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\nMark R. Warner United States Senator\n\n**11** \"AI Risk Management Framework,\" NIST (July 12, 2021), NIST\n\n---\n\nApril 26, 2023  \nMr. Satya Nadella  \nChief Executive Officer  \nMicrosoft Corporation  \n1 Microsoft Way  \nRedmond, WA 98052\n\nDear Mr. Nadella,\n\nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs1 and risks2.",
    "Instead, incorporating security upfront can reduce costs1 and risks2.\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from\n\n**1** Maurice Dawson et al., \"Integrating Software Assurance into the Software Development Life Cycle (SDLC),\" Journal of Information Systems Technology and Planning (2010), Available at: ResearchGate\n**2** \"U.S. and International Partners Publish Secure-by-Design and -Default Principles and Approaches,\" Cybersecurity and Infrastructure Security Agency (April 13, 2023), CISA\n\n...",
    "# Overview of AI Security Approach\n\nWhat limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n3 “OWASP AI Security and Privacy Guide,” OWASP Foundation, OWASP Guide  \n4 Fahri Anıl Yerlikaya, Şerif Bahtiyar, “Data poisoning attacks against machine learning algorithms”, Expert Systems with Applications, Volume 208, (July 18, 2022).\n\nAvailable at: ScienceDirect  \n5 Alexey Kurakin, Ian Goodfellow, Samy Bengio, “Adversarial Examples in the Physical World,” Google, Inc.\n\nAvailable at: Google Research  \n6 Dan Goodin, “Hackers Are Selling A Service that Bypasses ChatGPT Restrictions on Malware,” Ars Technica (February 8, 2023) Ars Technica  \n7 Lily Hay Newman, “AI Wrote Better Phishing Emails Than Humans in a Recent Test,” Wired (August 7, 2021), Wired  \n8 Tiffany Hsu, Stuart A. Thompson, “Disinformation Researchers Raise Alarms About A.I.\n\nChatbots,” New York Times (February 13, 2023), New York Times  \n9 “GPT-4 Jailbreak and Hacking via RabbitHole attack, Prompt injection, Content moderation bypass and Weaponizing AI,” Adversa AI (March 15, 2023), Adversa AI  \n10 Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” (February, 2018), White Paper\n\nAre you participating in third-party (internal or external) test & evaluation, verification & validation of your systems?\n\nWhat steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\nDo you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\nWhat kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\nHow are you monitoring and auditing your systems to detect and mitigate security breaches?\n\nCan you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\nHow do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\nWhat is your process for ensuring the privacy of sensitive or personal information your system uses?\n\nCan you describe how your company has handled past security incidents?\n\nWhat security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?11  \nIs your company participating in the development of technical standards related to AI and AI security?\n\nHow are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\nHow is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\nHave you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\nMark R. Warner  \nUnited States Senator  \n\n11 \"AI Risk Management Framework,\" NIST (July 12, 2021), NIST Framework\n\n---",
    "# Letter to Brigadier General Balan Ayyar\n\nApril 26, 2023\n\nBrigadier General Balan Ayyar, USAF, Retired  \nChief Executive Officer  \nPercipient.ai  \n3975 Freedom Cir.\n\nSuite 850  \nSanta Clara, CA 95054  \n\nDear Brigadier General Ayyar,\n\nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs1 and risks2.\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from 1 Maurice Dawson et al., “Integrating Software Assurance into the Software Development Life Cycle (SDLC),” Journal of Information Systems Technology and Planning (2010), Available at: ResearchGate  \n2 “U.S.\n\nand International Partners Publish Secure-by-Design and -Default Principles and Approaches,” Cybersecurity and Infrastructure Security Agency (April 13, 2023), CISA\n\ntraditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)3, tampering with training data (data poisoning attacks)4, and inputs to models that intentionally cause them to make mistakes (adversarial examples)5.\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware6, craft increasingly sophisticated phishing techniques7, contribute to disinformation8, and provide harmful information9.\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.10\n\nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "## Questions:\n\n- Can you provide an overview of your company’s security approach or strategy?\n\n- What limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n3 “OWASP AI Security and Privacy Guide,” OWASP Foundation, OWASP Guide  \n4 Fahri Anıl Yerlikaya, Şerif Bahtiyar, “Data poisoning attacks against machine learning algorithms,” Expert Systems with Applications, Volume 208, (July 18, 2022).\n\nAvailable at: ScienceDirect  \n5 Alexey Kurakin, Ian Goodfellow, Samy Bengio, “Adversarial Examples in the Physical World,” Google, Inc.\n\nAvailable at: Google Research  \n6 Dan Goodin, “Hackers Are Selling A Service that Bypasses ChatGPT Restrictions on Malware,” Ars Technica (February 8, 2023) Ars Technica  \n7 Lily Hay Newman, “AI Wrote Better Phishing Emails Than Humans in a Recent Test,” Wired (August 7, 2021), Wired  \n8 Tiffany Hsu, Stuart A. Thompson, “Disinformation Researchers Raise Alarms About A.I.\n\nChatbots,” New York Times (February 13, 2023), New York Times  \n9 “GPT-4 Jailbreak and Hacking via RabbitHole attack, Prompt injection, Content moderation bypass and Weaponizing AI,” Adversa AI (March 15, 2023), Adversa AI  \n10 Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” (February, 2018), White Paper\n\n- Are you participating in third-party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?11  \n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\nMark R. Warner  \nUnited States Senator  \n\n11 \"AI Risk Management Framework,\" NIST (July 12, 2021), NIST Framework\n\n---",
    "# Letter to Mr. Alexandr Wang\n\nApril 26, 2023\n\nMr. Alexandr Wang  \nChief Executive Officer  \nScale AI  \n155 5th St  \nSan Francisco, CA 94103  \n\nDear Mr. Wang,\n\nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs1 and risks2.\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from 1 Maurice Dawson et al., “Integrating Software Assurance into the Software Development Life Cycle (SDLC),” Journal of Information Systems Technology and Planning (2010), Available at: ResearchGate  \n2 “U.S.\n\nand International Partners Publish Secure-by-Design and -Default Principles and Approaches,” Cybersecurity and Infrastructure Security Agency (April 13, 2023), CISA\n\ntraditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)3, tampering with training data (data poisoning attacks)4, and inputs to models that intentionally cause them to make mistakes (adversarial examples)5.\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware6, craft increasingly sophisticated phishing techniques7, contribute to disinformation8, and provide harmful information9.\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.10\n\nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "## Questions:\n\n- Can you provide an overview of your company’s security approach or strategy?\n\n- What limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n3 “OWASP AI Security and Privacy Guide,” OWASP Foundation, OWASP Guide  \n4 Fahri Anıl Yerlikaya, Şerif Bahtiyar, “Data poisoning attacks against machine learning algorithms,” Expert Systems with Applications, Volume 208, (July 18, 2022).\n\nAvailable at: ScienceDirect  \n5 Alexey Kurakin, Ian Goodfellow, Samy Bengio, “Adversarial Examples in the Physical World,” Google, Inc.\n\nAvailable at: Google Research  \n6 Dan Goodin, “Hackers Are Selling A Service that Bypasses ChatGPT Restrictions on Malware,” Ars Technica (February 8, 2023) Ars Technica  \n7 Lily Hay Newman, “AI Wrote Better Phishing Emails Than Humans in a Recent Test,” Wired (August 7, 2021), Wired  \n8 Tiffany Hsu, Stuart A. Thompson, “Disinformation Researchers Raise Alarms About A.I.\n\nChatbots,” New York Times (February 13, 2023), New York Times  \n9 “GPT-4 Jailbreak and Hacking via RabbitHole attack, Prompt injection, Content moderation bypass and Weaponizing AI,” Adversa AI (March 15, 2023), Adversa AI  \n10 Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” (February, 2018), White Paper\n\n- Are you participating in third-party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?11  \n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\nMark R. Warner  \nUnited States Senator  \n\n11 \"AI Risk Management Framework,\" NIST (July 12, 2021), NIST Framework\n\n---",
    "# Letter to Mr. Emad Mostaque\n\nApril 26, 2023\n\nMr. Emad Mostaque  \nChief Executive Officer  \nStability AI  \n88 Notting Hill Gate  \nLondon, England, W11 3HP  \n\nDear Mr. Mostaque, \n\nI write today regarding the need to prioritize security in the design and development of artificial intelligence (AI) systems.\n\nAs companies like yours make rapid advancements in AI, we must acknowledge the security risks inherent in this technology and ensure AI development and adoption proceeds in a responsible and secure way.\n\nWhile public concern about the safety and security of AI has been on the rise, I know that work on AI security is not new.\n\nHowever, with the increasing use of AI across large swaths of our economy, and the possibility for large language models to be steadily integrated into a range of existing systems, from healthcare to finance sectors, I see an urgent need to underscore the importance of putting security at the forefront of your work.\n\nBeyond industry commitments, however, it is also clear that some level of regulation is necessary in this field.\n\nI recognize the important work you and your colleagues are doing to advance AI.\n\nAs a leading company in this emerging technology, I believe you have a responsibility to ensure that your technology products and systems are secure.\n\nI have long advocated for incorporating security-by-design, as we have found time and again that failing to consider security early in the product development lifecycle leads to more costly and less effective security.\n\nInstead, incorporating security upfront can reduce costs1 and risks2.\n\nMoreover, the last five years have demonstrated that the ways in which the speed, scale, and excitement associated with new technologies have frequently obscured the shortcomings of their creators in anticipating the harmful effects of their use.\n\nAI capabilities hold enormous potential; however, we must ensure that they do not advance without appropriate safeguards and regulation.\n\nWhile it is important to apply many of the same security principles we associate with traditional computing services and devices, AI presents a new set of security concerns that are distinct from 1 Maurice Dawson et al., “Integrating Software Assurance into the Software Development Life Cycle (SDLC),” Journal of Information Systems Technology and Planning (2010), Available at: ResearchGate  \n2 “U.S.\n\nand International Partners Publish Secure-by-Design and -Default Principles and Approaches,” Cybersecurity and Infrastructure Security Agency (April 13, 2023), CISA\n\ntraditional software vulnerabilities.\n\nSome of the AI-specific security risks that I am concerned about include the origin, quality, and accuracy of input data (data supply chain)3, tampering with training data (data poisoning attacks)4, and inputs to models that intentionally cause them to make mistakes (adversarial examples)5.\n\nEach of these risks further highlighting the need for secure, quality data inputs.\n\nBroadly speaking, these techniques can effectively defeat or degrade the integrity, security, or performance of an AI system (including the potential confidentiality of its training data).\n\nAs leading models are increasingly integrated into larger systems, often without fully mapping dependencies and downstream implications, the effects of adversarial attacks on AI systems are only magnified.\n\nIn addition to those risks, I also have concerns regarding bias, trustworthiness, and potential misuse or malicious use of AI systems.\n\nIn the last six months, we have seen open source researchers repeatedly exploit a number of prominent, publicly-accessible generative models – crafting a range of clever (and often foreseeable) prompts to easily circumvent a system’s rules.\n\nExamples include using widely-adopted models to generate malware6, craft increasingly sophisticated phishing techniques7, contribute to disinformation8, and provide harmful information9.\n\nIt is imperative that we address threats to not only digital security, but also threats to physical security and political security.10\n\nIn light of this, I am interested in learning about the measures that your company is taking to ensure the security of its AI systems.\n\nI request that you provide answers to the following questions no later than May 26, 2023.",
    "## Questions:\n\n- Can you provide an overview of your company’s security approach or strategy?\n\n- What limits do you enforce on third-party access to your model and how do you actively monitor for non-compliant uses?\n\n3 “OWASP AI Security and Privacy Guide,” OWASP Foundation, OWASP Guide  \n4 Fahri Anıl Yerlikaya, Şerif Bahtiyar, “Data poisoning attacks against machine learning algorithms,” Expert Systems with Applications, Volume 208, (July 18, 2022).\n\nAvailable at: ScienceDirect  \n5 Alexey Kurakin, Ian Goodfellow, Samy Bengio, “Adversarial Examples in the Physical World,” Google, Inc.\n\nAvailable at: Google Research  \n6 Dan Goodin, “Hackers Are Selling A Service that Bypasses ChatGPT Restrictions on Malware,” Ars Technica (February 8, 2023) Ars Technica  \n7 Lily Hay Newman, “AI Wrote Better Phishing Emails Than Humans in a Recent Test,” Wired (August 7, 2021), Wired  \n8 Tiffany Hsu, Stuart A. Thompson, “Disinformation Researchers Raise Alarms About A.I.\n\nChatbots,” New York Times (February 13, 2023), New York Times  \n9 “GPT-4 Jailbreak and Hacking via RabbitHole attack, Prompt injection, Content moderation bypass and Weaponizing AI,” Adversa AI (March 15, 2023), Adversa AI  \n10 Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” (February, 2018), White Paper\n\n- Are you participating in third-party (internal or external) test & evaluation, verification & validation of your systems?\n\n- What steps have you taken to ensure that you have secure and accurate data inputs and outputs?\n\nHave you provided comprehensive and accurate documentation of your training data to downstream users to allow them to evaluate whether your model is appropriate for their use?\n\n- Do you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\n- What kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\n- How are you monitoring and auditing your systems to detect and mitigate security breaches?\n\n- Can you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\n- How do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\n- What is your process for ensuring the privacy of sensitive or personal information your system uses?\n\n- Can you describe how your company has handled past security incidents?\n\n- What security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?11  \n- Is your company participating in the development of technical standards related to AI and AI security?\n\n- How are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\n- How is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\n- Have you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,\n\nMark R. Warner  \nUnited States Senator  \n\n11 \"AI Risk Management Framework,\" NIST (July 12, 2021), NIST Framework",
    "# Your Training Data and Systems Security\n\nDo you allow your training data to downstream users to evaluate whether your model is appropriate for their use?\n\nDo you provide complete and accurate documentation of your model to commercial users?\n\nWhich documentation standards or procedures do you rely on?\n\nWhat kind of input sanitization techniques do you implement to ensure that your systems are not susceptible to prompt injection techniques that pose underlying system risks?\n\nHow are you monitoring and auditing your systems to detect and mitigate security breaches?\n\nCan you explain the security measures that you take to prevent unauthorized access to your systems and models?\n\nHow do you protect your systems against potential breaches or cyberattacks?\n\nDo you have a plan in place to respond to a potential security incident?\n\nWhat is your process for alerting users that have integrated your model into downstream systems?\n\nWhat is your process for ensuring the privacy of sensitive or personal information that your system uses?\n\nCan you describe how your company has handled past security incidents?\n\nWhat security standards, if any, are you adhering to?\n\nAre you using NIST’s AI Risk Management Framework?\n\nIs your company participating in the development of technical standards related to AI and AI security?\n\nHow are you ensuring that your company continues to be knowledgeable about evolving security best practices and risks?\n\nHow is your company addressing concerns about AI trustworthiness, including potential algorithmic bias and misuse or malicious use of AI?\n\nHave you identified any security challenges unique to AI that you believe policymakers should address?\n\nThank you for your attention to these important matters and I look forward to your response.\n\nSincerely,  \nMark R. Warner  \nUnited States Senator  \n\n\"NIST AI Risk Management Framework,\" NIST (July 12, 2021),",
    "## Purpose\n\nThe purpose of this policy is to provide guidance on the use of generative artificial intelligence (AI) for executive branch employees in Utah State government.\n\nThe policy is created to promote the use of generative AI while protecting the safety, privacy, and intellectual property rights of the State of Utah.",
    "## Background\n\nGenerative AI refers to a class of artificial intelligence systems that are capable of generating content, such as text, images, video, or audio, based on a set of input data rather than simply analyzing or acting on existing data.\n\nPopular generative AI systems include GPT-3 and GPT-4/ChatGPT, Dall-E, Bard, Bing Chat, GitHub Copilot, and Lensa AI among many others.\n\nGenerative AI technology is rapidly being incorporated into common online tools as standalone systems or embedded within other applications.\n\nThese systems have the potential to support many state business functions and services, however their use also raises important questions, particularly around the sourcing of training data, ensuring proper attribution of generated content, and the handling of sensitive or public data, accuracy of outputs, bias, and stability.\n\nFurther research into this technology may uncover issues that require more guidance or restrictions on its use.",
    "### Generative Pre-trained Transformer (GPT)\n\nGPT is a type of artificial intelligence language model that was developed by OpenAI.\n\nGPT models are trained on a massive dataset of text and code, and they can be used for a variety of tasks, including text generation, translation, and summarization.\n\nGPT is a type of Large Language Model which reached popularity with the introduction of Chat GPT in November 2022.",
    "### Guidelines\n\nGenerative AI is a powerful tool that can be used to improve government services and operations.\n\nWhen making use of generative AI tools and capabilities, state agencies and users should consider the following general principles:\n\n- **Transparency:** Users and agencies must be transparent about how they are using generative AI.\n\nThis shall include full attribution to which AI is used.\n\n- **Accountability:** Users and agencies are accountable for the decisions that are made and materials created using generative AI.\n\n- **Fairness:** AI systems can reflect the cultural, economic, and social biases of the source materials used for training, and the algorithms used to parse and process that content can be a source of bias as well.\n\nUsers and agencies shall ensure that generative AI is used in a fair and equitable manner.\n\n- **Privacy:** Users and agencies must protect the privacy of individuals when using generative AI.\n\nThis means that any models shall not be used to collect or store personal information without the consent of the individual.\n\nNo private, controlled, confidential, or restricted data shall be added to a publicly accessible training model.\n\n- **Security:** Users and agencies shall take steps to protect the security and integrity of generative AI models.\n\nDTS cybersecurity staff are available to provide technical support in securing AI resources.\n\n- **Training:** Agencies shall mandate a minimum level of AI training for users responsible for business processes which are incorporating generative AI.\n\n- **Legal:** There are unresolved legal issues surrounding generative AI and the data inputs used to create AI models.\n\nAI systems may be trained using copyrighted material that has been sourced without regard for copyright or licensing terms.\n\nSources of inputs to models must be reviewed and usage risk evaluated.",
    "### Requirements\n\nAll software services, even if they are free or part of a pilot or proof-of-concept project, must be reviewed by agencies to ensure the software meets all necessary security and privacy requirements.\n\nThis requirement applies to downloadable software, Software as a Service (SaaS), web-based services, browser plug-ins, and smartphone apps.\n\nNew software requests must also be reviewed by the DTS Architecture Review Board and the Enterprise Cyber Security Team.\n\nUse of generative AI technology that is incorporated into existing services and products, such as internet search engines, does not require permission to use, however this policy’s guidelines and other requirements must be followed.\n\nAI outputs must be reviewed by knowledgeable human operators for accuracy, appropriateness, privacy, and security before being acted upon or disseminated.\n\nAI outputs should not be assumed to be truthful, credible or accurate.\n\nAI outputs shall not be used to impersonate individuals or organizations without their written permission.",
    "### Privacy\n\nState agencies must comply with all GRAMA, PRMA, records management, privacy and other applicable laws, rules, and policies to ensure the appropriate and reasonable protection of data and the protection of rights of persons that may be impacted by information furnished by AI.\n\nNo private, controlled, or confidential data shall be added to a publicly accessible AI service or training model.\n\nMaterial that is inappropriate for public release shall not be entered as input to generative AI tools that have not been explicitly approved for the intended use case.\n\nAgency contracts shall prohibit vendors from using State of Utah materials or data in generative AI queries or for building or training proprietary generative AI programs unless explicitly approved by the state.\n\nDTS will provide training on proper usage of generative AI for users.\n\nAgencies shall ensure that all users of tools consisting of or incorporating generative AI complete this training before being granted access and annually thereafter.\n\nAll copyrightable works owned by the state that are created with the involvement of generative AI must include an accompanying annotation sufficient to meet the requirements of the U.S.\n\nCopyright Office for Works Containing Material Generated by Artificial Intelligence ( (88 FR 16190).\n\nThe annotation should include at least the generative AI technology used and a description of how it was used to create the work.\n\nProcuring agencies shall ensure vendors disclose the utilization of generative AI when producing works owned by the state and integration of generative AI in products used by the state.\n\nProcuring agencies shall perform due diligence to ensure proper licensure of model training data for all generative AI services using non-state data.\n\nAll software code generated through the use of generative AI shall not be used in production until fully reviewed and tested for proper functionality and security.\n\nAny such use shall be properly documented.",
    "## Enforcement\n\nViolation of this policy by personnel employed by the State of Utah may be the basis for discipline including but not limited to termination.\n\nIndividuals and contractors working with any State of Utah Agency found to have violated this policy may also be subject to legal penalties as may be prescribed by state and/or federal statute, rule, and/or regulation.",
    "# INITIAL POLICY CONSIDERATIONS FOR GENERATIVE ARTIFICIAL INTELLIGENCE\n\nOECD ARTIFICIAL INTELLIGENCE PAPERS  \nSeptember 2023 No.\n\n1\n\nOECD Working Papers should not be reported as representing the official views of the OECD or of its member countries.\n\nThe opinions expressed and arguments employed are those of the authors.\n\nWorking Papers describe preliminary results or research in progress by the author(s) and are published to stimulate discussion on a broad range of issues on which the OECD works.\n\nComments on Working Papers are welcomed and may be sent to Directorate for Science, Technology and Innovation, OECD, 2 rue André Pascal, 75775 Paris Cedex 16, France.\n\nNote to Delegations:  \nThis document is also available on O.N.E under the reference code: DSTI/CDEP/AIGO/RD(2023)5/FINAL\n\nThis document, as well as any data and any map included herein, are without prejudice to the status of or sovereignty over any territory, to the delimitation of international frontiers and boundaries and to the name of any territory, city or area.\n\n© OECD 2023\n\nThe use of this work, whether digital or print, is governed by the Terms and Conditions to be found at",
    "# Initial policy considerations for generative artificial intelligence\n\nPhilippe Lorenz, Karine Perset (OECD), Jamie Berryhill (OECD)\n\nGenerative artificial intelligence (AI) creates new content in response to prompts, offering transformative potential across multiple sectors such as education, entertainment, healthcare, and scientific research.\n\nHowever, these technologies also pose critical societal and policy challenges that policymakers must confront: potential shifts in labour markets, copyright uncertainties, and risk associated with the perpetuation of societal biases and the potential for misuse in the creation of disinformation and manipulated content.\n\nConsequences could extend to the spreading of mis- and disinformation, perpetuation of discrimination, distortion of public discourse and markets, and the incitement of violence.\n\nGovernments recognize the transformative impact of generative AI and are actively working to address these challenges.\n\nThis paper aims to inform these policy considerations and support decision-makers in addressing them.\n\nKeywords: Generative artificial intelligence, AI, digital economy, science and technology",
    "## Foreword\n\nGenerative artificial intelligence (AI) came onto the scene in 2018 with releases of deepfakes closely followed by generative pre-trained transformers (GPTs) and other large language models (LLMs).\n\nIt gained worldwide attention in 2022 with text-to-image generators and ChatGPT.\n\nGenerative AI has the potential to revolutionize industries and society.\n\nSectors such as education, entertainment, healthcare, and scientific research already use it to create individualized and scalable content, automate tasks, generate hypotheses, and improve productivity.\n\nHowever, policymakers need to consider significant societal and policy implications, such as the technology’s potential impact on labour markets and debates around whether training generative AI systems on copyrighted material could constitute infringement.\n\nPotential risks include generative AI perpetuating biases and being misused through disinformation, deepfakes, and other manipulated content with severe consequences.\n\nThe resulting widespread social, political, and economic repercussions could include disinformation on key scientific issues, perpetuating stereotypes and discrimination, distorting public discourse, creating and spreading conspiracy theories and other disinformation, influencing elections, distorting markets, and even inciting violence.\n\nThere are more questions than answers about how this technology will shape our environments and interactions, and policy is struggling to keep up with developments.\n\nThat said, governments recognize the transformative nature of generative AI and are acting to keep pace with change.\n\nFor example, in May 2023, the Group of Seven (G7) countries committed to advance international discussions of its governance in pursuit of inclusive and trustworthy AI, which included the establishment of the Hiroshima AI Process by governments in collaboration with the OECD.\n\nThe OECD, including through its OECD.AI Policy Observatory ( is committed to helping governments keep up with the rapid change in generative AI.\n\nThis paper was drafted by Philippe Lorenz, an external AI consultant.\n\nStrategic direction and additional analysis and content were provided by Karine Perset, Head of the OECD.AI Policy Observatory, and Jamie Berryhill, AI Policy Analyst, OECD.\n\nSebastian Hallensleben (CEN-CENELEC JTC 21 and VDE) advised on the strategic approach and scope.\n\nThe paper benefited from the review and input of the OECD Working Party on Artificial Intelligence Governance (AIGO), including delegates from Business at OECD (BIAC), the European Commission, and Japan.\n\nThe team gratefully acknowledges the input from OECD colleagues Jerry Sheehan, Audrey Plonk, Hanna-Mari Kilpelainen, and Riccardo Rapparini of the Directorate for Science, Technology and Innovation (STI); and Angelica Salvi Del Pero and Stijn Broecke of the Directorate for Employment, Labour and Social Affairs (ELS).\n\nThe team also thanks Misha Pinkhasov for editing the paper and Andreia Furtado for editorial and publishing support.",
    "## Executive summary\n\nGenerative AI systems create novel content and can bring value as autonomous agents  \nGenerative artificial intelligence (AI) systems create new content—including text, image, audio, and video—based on their training data and in response to prompts.\n\nThe recent growth and media coverage of generative AI systems, notably in the areas of text and image generation, has spotlighted AI’s capabilities, leading to significant public, academic, and political discussion.\n\nIn addition to generating synthetic content, generative AI systems are increasingly used as autonomous agents with new functionality enabling them to operate on real-time information and assist users in new ways, such as by making bookings autonomously.\n\nInvestment banks, consulting firms, and researchers project that generative AI will create significant economic value, with some estimating as much as USD 4.4 trillion per year.\n\nGenerative AI could revolutionize industries and society but carries major risks  \nGenerative AI is already used to create individualized content at scale, automate tasks, and improve productivity.\n\nGenerative AI is yielding benefits in key sectors such as software development, creative industries and arts (e.g., artistic expression through music or image generation), education (e.g., personalized exam preparation), healthcare (e.g., information on tailored preventative care), and internet search.\n\nHowever, alongside the benefits, there are significant policy implications and risks to consider, including in the areas of mis- and disinformation, bias, intellectual property rights, and labour markets.",
    "### Major mis- and disinformation risks from synthetic content call for novel policy solutions\n\nHumans are less and less capable of differentiating AI from human-generated content, amplifying risks of mis- and disinformation.\n\nThis can cause material harm at individual and societal levels, particularly on science-related issues, such as vaccine effectiveness and climate change, and in polarized political contexts.\n\nMitigation measures include increasing model size, developing models that provide evidence and reference source material, watermarking, “red-teaming,” whereby teams adopt an attacker mindset to probe the model for flaws and vulnerabilities, and developing AI systems that help detect synthetic content.\n\nHowever, these measures have limitations and are widely expected to be insufficient, calling for innovative approaches that can address the scale of the issue.\n\nGenerative AI, like other types of AI, can echo and perpetuate biases contained in training data  \nGenerative AI can echo, automate, and perpetuate social prejudices, stereotypes and discrimination by replicating biases contained in training data.\n\nThis can exacerbate the marginalization or exclusion of specific groups.\n\nMitigation approaches include enhanced inclusivity in and curation of training data, explainability research, auditing, model fine-tuning through human feedback, and “red teaming”.\n\nLegal systems are grappling with generative AI’s implications for intellectual property rights  \nIn particular, generative AI models are trained on massive amounts of data that includes copyrighted data, mostly without the authorization of rights-owners.\n\nAnother ongoing debate is whether artificially generated outputs can themselves be copyrighted or patented and if so, to whom.\n\nProgress in generative AI may increase job task exposure in high-skilled occupations  \nGenerative AI’s availability to the public has heightened focus on its potential impact on labour markets.\n\nMeasures of language model performance on standardized tests, such as the bar exam for qualifying attorneys in the United States, surprised many with its strong results relative to human test-takers, suggesting possible increased job task exposure in high-skilled occupations, though lower-skilled occupations have for now been the most exposed to automation.\n\nThe OECD Employment Outlook notes that AI can benefit jobs by creating demand for new tasks and complementary skills, resulting in new jobs for which human labour has a comparative advantage.\n\nRecent research shows that generative AI can improve the performance of less skilled workers.\n\nSecurity, surveillance, over-reliance, academic dishonesty, and concentration are also risks  \nIn addition to present-day considerations of generative AI, a longer-term view helps envision the technology’s future trajectories.\n\nGenerative AI and the synthetic content it produces with varying quality and accuracy can exacerbate challenges.\n\nThis content proliferates in digital spaces where it is used to train generative AI models, resulting in and a vicious negative cycle in the quality of online information.\n\nIt also raises concerns about automated and personalized cyber-attacks, surveillance and censorship, overreliance on generative systems despite their flaws, academic dishonesty, and concentrations of power and resources.\n\nAgency, power-seeking, non-aligned sub-goals, and other potential emergent behaviours require attention  \nOver the longer term, emergent behaviours, of which the existence is debated in the AI community, suggest additional risks.\n\nThese behaviours include increased agency, power-seeking, and developing unknown sub-goals determined by machines to achieve core objectives programmed by a human but that might not be aligned with human values and intent.\n\nSome deem that if these risks are not addressed, they could lead to systemic harms and the collective disempowerment of humans.\n\nThe growing impact and capability of generative AI systems has led to reflection and debates among researchers and members of the OECD.AI Expert Group on AI Futures about whether these types of models could eventually lead to artificial general intelligence (AGI), the stage at which autonomous machines could have human-level capabilities in a wide variety of use cases.\n\nDue to its potential broad societal impacts, AGI’s potential benefits and risks deserve attention, as do the potentially imminent impacts of narrow generative AI systems that may be just as significant as AGI.\n\nThe longer-term benefits and risks of generative AI could demand solutions on a larger, more systemic scale than the risk mitigation approaches already underway.\n\nThese measures and others are the topic of ongoing OECD work, including work of the OECD.AI Expert Group on AI Futures.",
    "## 1 Introduction to generative AI\n\nGenerative AI systems create content based on training data and in response to user prompts.\n\nTheir recent growth and media coverage have spotlighted AI’s capabilities, leading to significant public, academic, and political discussion.\n\nIn addition to creating synthetic content, generative AI systems are increasingly used as autonomous agents, allowing models to move beyond the cut-off dates in their training data to give them new potential.\n\nGenerative AI has the potential to revolutionise industries and society and is already used to create individualized and scalable content, automate tasks, and improve productivity.\n\nSuch systems offer significant upside but also risks for policymakers to address.",
    "### Generative AI is centre stage in public, academic, and political discourse\n\nRecent growth generative AI systems has drawn attention AI’s capabilities  \nGenerative artificial intelligence (AI) systems create new content in response to prompts based on their training data.\n\nThe recent growth and coverage of generative AI systems has spotlighted AI’s capabilities.\n\nThey include, for example, ChatGPT and BARD for text; Midjourney and Stable Diffusion for images; WaveNet and DeepVoice for audio; Make-A-Video and Synthesia for video; and multi-model systems that combine several types of media.\n\nCompanies are now creating positions for “prompt engineers”, venture capitalists are positioning themselves as generative-AI investors, and governments are considering regulatory tools.\n\nLanguage models – one mode of generative AI – were discussed in-depth in the recent OECD report AI Language Models: Technological, socio-economic and policy considerations (OECD, 2023).\n\nOther modes, such as image, audio, and video generation, are also evolving rapidly with the technology and broader policy landscape.\n\nGovernments quickly recognised that generative AI is transformative and are taking action  \nWhile generative AI has begun to revolutionise industry and society in numerous positive ways, the technology can also be misused through disinformation, deepfakes, and other manipulated content with severe negative consequences.\n\nDespite ideas about how this technology will shape our environments and interactions, policy is struggling to keep up with technological developments, and numerous questions require answers.\n\nAt the same time, governments have been quick to recognise the transformative nature of generative AI and are taking action to keep pace with change.\n\nFor example, in May 2023, the Group of Seven (G7) countries committed to advance international discussions of AI governance in pursuit of inclusive and trustworthy AI and established the Hiroshima AI Process in collaboration with the OECD under the Japanese G7 Presidency to help improve governance of generative AI.\n\nGenerative AI is rooted in established AI concepts  \nAlthough generative AI systems appear novel, their model design is based on deep neural networks (which loosely imitate information processing of neurons in the human brain) that developed incrementally through international academic and applied research since the 1950s (Goodfellow, Bengio and Courville, 2016).\n\nThe visible results of generative AI models are due to recent developments in the discipline of machine learning (ML).\n\nML leverages deep neural networks to emulate human intelligence by being exposed to data (training) and finding patterns that are then used to process previously unseen data.\n\nThis allows the model to generalise based on probabilistic inference, i.e.\n\ninformed guesses, rather than causal understanding.\n\nUnlike humans, who learn from only a few examples, deep neural networks need hundreds of thousands, millions, or even billions, meaning that machine learning requires vast quantities of data.\n\nFew companies can create large generative AI systems and models  \nSo far, few technology companies in the world have the technological skills and capital to create major generative AI systems and models, such as foundation models that “are capable of a range of general tasks... [and] can be built ‘on top of’ to develop different applications for many purposes”.\n\nA few multinational enterprises have been investing in AI for some time to enable their business models, be it search, advertising, or social networks.\n\nThese entities seem positioned to capture a large part of the initial value created by generative AI, with systems marketed internationally embedded in software as-a-service on cloud platforms or, more recently, placed directly on devices.\n\nOpen-source actors, researchers, start-ups, and SMEs are also very active  \nHowever, these companies are a part of an ecosystem that includes researchers, small and medium-sized enterprises (SMEs), and other actors that contribute to and derive value from generative AI.\n\nOpen-source communities are also active in the ecosystem.\n\nAI traditionally relies on a mix of proprietary and free and open-source software (FOSS) models, libraries, datasets, and other resources for commercial or non-commercial purposes under a variety of licenses.\n\nAlthough a number of AI companies operate proprietary generative AI systems and commercialize access to them, several companies are developing open systems.\n\nThe emergence of several open-source generative AI models, such as Stable Diffusion and Meta’s Llama 2, contributes to the rapid innovation and development of these technologies and could mitigate ‘winner-take-all dynamics’ that lead a few firms to seize a large part of the market.\n\nYet open sourcing entails other risks when bad actors can leverage open-source generative AI models, which will be explored in forthcoming OECD work.",
    "### Generative AI creates novel content with real-world implications\n\nA core trait of human intelligence is the cognitive capacity humans have to create content.\n\nGenerative AI models use “prompts” (specific requests) to produce synthetic text, images, audio, and video that, already today, can be nearly impossible to distinguish from human creation.\n\nThe quality of large language models (LLMs) that enable text-generation has improved rapidly since the publication of the Transformers architecture by Google researchers in 2017 and reached a turning point with the release of ChatGPT in November 2022.\n\nAs the first conversational agent accessible through a convenient and intuitive user interface, the release surprised governments, organisations, and individuals around the world.\n\nChatGPT is estimated to have around 100 million active monthly users, making it the fastest-growing consumer software application in history.\n\nThe excitement in late 2022 around text generation was repeated with regard to image generation in March 2023 because of a picture of Pope Francis wearing a white puffer jacket.\n\nMany believed that the picture was authentic, but it had been created by a synthetic image-generation software based on a user’s text prompt.\n\nIt was shared widely on social and traditional media networks before finally being exposed as fake.\n\nSynthetic images took off after their inception in 2014 based on the work of Goodfellow et al., which paved the way for current model capabilities.\n\nGiven the rapid pace of development, synthetic images are already often indistinguishable from real ones to the human eye.\n\nAutonomous generative AI agents promise significant benefits but carry tremendous risks  \nFurthermore, generative AI systems are increasingly used as autonomous agents, adding a new dimension to the technology’s potential and allowing models to move beyond the limitation of cut-off dates in their training data.\n\nOpenAI announced plugins in early 2023 that connect ChatGPT to third-party applications to expand its offer and find new sources of data.\n\nBefore that, ChatGPT users were limited to the platform’s knowledge base from late 2021, the cut-off point for the initial training data.\n\nReceiving third-party information allows the model to use real-time data to provide more accurate and timely results and services.\n\nPlugins enable ChatGPT to operate on the most recent data available, including real-time information, such as stock prices or news articles, and to assist users in new ways (e.g., through autonomous ordering and booking).\n\nSimilarly, Bing Chat is connected to the Internet and aware of current events.\n\nAgent activities are not limited to machines acting on human instructions and prompts.\n\nResearchers at Stanford University and Google Research created a virtual environment in which 25 generative AI agents interacted with each other over the course of two days and exhibited human-like behaviour such as reasoning about their research careers or planning about attending social events.\n\nWhile these unexpected actions were termed “emergent abilities” by some researchers, others found these actions illusory based on the metrics programmers chose to evaluate the models.\n\nWhile the debate is ongoing, the autonomous behaviour and hints of agency that very large generative AI models could be capable of enlarges the scope of their possible application, as well as the scope of considerations and unknowns for how the technology might develop.\n\nThese types of systems offer significant upside but also carry risks, such as from their ability to create and spread mis- and disinformation, use their increased agency to carry out undesired actions, or even misrepresent themselves by impersonating humans.\n\nPolicymakers everywhere are taking notice, with G7 leaders setting up the Hiroshima AI Process in May 2023.\n\nThe European Parliament has been advocating for considering generative AI systems as general-purpose AI, which would classify them as high-risk applications and entail mandatory conformity assessments and other requirements.\n\nGenerative AI is predicted to create significant economic value and social well-being and has begun to do so in key sectors.\n\nYet generative AI can also echo, automate, and perpetuate mis- and disinformation, bias, and discrimination, and training on copyrighted data could infringe on intellectual property.\n\nThe OECD finds the net impact of AI on employment to be ambiguous so far, mainly affecting job quality – generally positively – with little evidence of significant negative effects on their quantity.\n\nHowever, outcomes such as language models’ strong performance on standardized tests, suggest that job-task exposure to generative AI could increase and that high-skilled occupations are most exposed to recent advances.",
    "### Generative AI is being adopted rapidly in key industry sectors\n\nGenerative AI is predicted to create significant economic value and social well-being.\n\nCompanies have begun adopting the technology to create new business opportunities, and start-ups are competing for venture capital.\n\nPopular use cases and applications to date include pre-processing data, image compression and classification, medical imaging, personalization, and intuitive user experience (UX) interfaces.\n\nSeveral generative AI applications have begun to yield benefits in areas including:\n\n- **Code development** – Copilot, a coding assistant developed jointly by OpenAI and GitHub, autocompletes and generates code based on developers' prompts.\n\nOther models to generate code include CodeGen.\n\nCode refactoring (improving pre-existing code without altering its functionality) is another area where generative AI is assisting developers.\n\n- **Creative industries and arts** – In music, AI melody generators have been available for a while, helping artists craft new music from scratch or based on previous bars, improve composition, and process singing.\n\nIn image generation, applications such as Stable Diffusion and Dall-E 2 provide new opportunities to generate artforms for the advertising, media, movie, and other industries.\n\n- **Education** – Education is among the sectors expecting change in the near-term, as school children and students experiment with generative AI applications to learn (like OpenAI’s GPT Khanmigo) and prepare for exams.\n\nSuch applications can create educational material, write letters of recommendation, and design course syllabuses, improving the efficiency of teachers.\n\n- **Healthcare** – Generative AI models play important roles as interfaces for patients and healthcare providers.\n\nPatients are already benefiting from information on preventive care and explanations of medical conditions and treatments.\n\nThe use of Vik, a chatbot that responds to the fears and concerns of patients diagnosed with breast cancer, is shown to result in better medication adherence rates.\n\nAnother promising application is the discovery and development of new drugs using generative AI chemistry models.\n\nCompanies such as Insilico Medicine are conducting FDA-approved clinical trials of cancer treatments designed using large biological, chemical, and textual generative and predictive engines.\n\n- **Search** – Search-engines are underpinning their search capabilities with conversational generative AI models such as Microsoft Bing with OpenAI’s GPT-4.\n\nOne of the most discussed topics in AI and search is whether search engines that provide links to users will be disrupted by conversational agents that provide better search experiences.",
    "### Generative AI considerably amplifies mis- and disinformation’s scale and scope\n\nHumans were found, already in 2022, to be almost incapable of differentiating AI from human-generated news in 50% of cases, meaning that generative AI can amplify risks both of misinformation (the unintended spread of false information) and of deliberate disinformation by malicious actors.\n\nLeading-edge generative AI models have multimodal capabilities that can exacerbate these risks, for example by combining text with image or video or even voices.\n\nUnintentional misinformation or intentional deception can cause material harm at an individual level (e.g., influencing decision-making about vaccines) and, on a larger scale, erode societal trust in the information ecosystem and the fact-based exchange of information that underpins science, evidence-based decision-making, and democracy.\n\nResearch findings related to human interpretation of AI-generated content underscore the potential risk of AI-driven mis- and disinformation, and emphasize the importance of disclosing the use of AI systems.\n\nThe functionality of text-to-text generative AI models, or language models, easily leads them to produce misinformation.\n\nThey are trained to predict words or statements based on a probability assessment.\n\nHowever, the accuracy of the predicted following word depends on the context rather than probability.\n\nTruth depends on context, but LLMs based on probabilistic inference have no ability to reason and thus might never achieve completely accurate outputs.\n\nThis also bears on LLMs’ potential as a tool to detect information for the purpose of countering it.",
    "#### “Hallucinations” and over-reliance also require addressing\n\nAnother worrying feature of LLMs is their propensity to “hallucinate” (i.e., to generate incorrect yet convincing outputs), particularly when an answer is not available in the training data.\n\nThis can allow them to create convincing misinformation, hate speech, or other harmful content, posing significant challenges for policymakers.",
    "# Risks and Overreliance on AI Models\n\nRisks also include excessive trust and overreliance on the model, resulting in a dependency that can interfere with developing skills, and even lead to losing skills (OpenAI, 2023[36]).\n\nThis issue will worsen with increasing model capabilities, areas of application, and user trust as average users will be unable to fact-check the models’ responses (Passi and Vorvoreanu, 2022[37]).",
    "# Synthetic Content in Politics, Science, and Law Enforcement\n\nRisks associated with text-to-image generative AI models make clear how rapid technological progress is.\n\nNumerous “photographs” on Twitter and other online platforms depicted well-known political figures and heads of state taking surprising actions yet were very credible, demonstrating the power of synthetic imagery, particularly in polarized political contexts.\n\nAnother issue has been the manipulation of scientific images to produce mis- and disinformation, threatening trust within research communities as well as science’s reputation with the general public.\n\nUse of synthetic images by climate change deniers (Galaz et al., 2023[38]) and the spread of COVID-19 disinformation (Coldewey and Lardinois, 2023[39]) serve as cases in point.\n\nTargeted disinformation campaigns leveraging different modes of generative AI can mislead and manipulate public opinion (Weidinger et al., 2022[29]) (OECD, 2022[31]).\n\nLLMs could help conduct targeted-influence operations, i.e., “covert or deceptive efforts to influence the opinions of a target audience” (Goldstein et al., 2023[40]).\n\nThey can significantly reduce propaganda costs and increase its scale (Buchanan et al., 2021[41]).\n\nThe dynamics of influence operations could also be altered in unpredictable ways, such through more convincing messaging by using LLMs capable of better cultural and linguistic immersion in target audiences (Goldstein et al., 2023[40]).\n\nOpenAI reported that its own red teaming efforts found GPT-4 to rival human propagandists, “especially if teamed with a human editor”, and can develop plausible-sounding plans to reach propaganda goals (OpenAI, 2023[36]).\n\nDespite users’ wariness of AI authorship (Box 2.1), research in 2019 found AI-generated fake news to be more credible to human raters than human-written disinformation (Zellers et al., 2019[42]).\n\nIn the early weeks of the Russian invasion of Ukraine in March 2022, a deepfake video depicting Ukrainian President Volodymyr Zelensky admitting defeat and demanding Ukrainian soldiers surrender surfaced on social media and was uploaded to a Ukrainian news website by hackers (Allynn, 2022[43]).\n\nAlthough crude in video and audio quality, this attempt to deceive the public was seen as a harbinger of future public deception enabled by more powerful models (Boháček and Farid, 2022[44]).\n\nSuch deepfakes are increasingly realistic and convincing as advancements are made in both audio and video generation techniques and models.",
    "# Mitigating Mis- and Disinformation\n\nThe risks of generating mis- and disinformation at scale with generative AI systems demand novel solutions.\n\nCompanies and other organizations have faced issues related to incorrect or false information for a long time and put systems in place to address them.\n\nHowever, traditional fact-checking and other existing solutions are generally not scalable in the face of AI-based automation of disinformation.\n\nUser education alone becomes insufficient when AI generates more and more convincing disinformation.\n\nIn addition, it shifts responsibility from systems, companies, and governments to individuals.\n\nWhile researchers are exploring potential paths forward, there are still more questions than answers about potential remedies to AI-generated and spread mis- and disinformation.\n\nWeidinger et al.\n\n(2022) point to several methods for reducing misinformation at scale.\n\nScaling-up (i.e., increasing) model size is often advocated to improve model accuracy but deemed insufficient (Bender and Koller, 2020[45]) (Lin, Hilton and Evans, 2022[46]).\n\nResearch is also ongoing to prompt models to substantiate their statements, such as by referencing sources from the Internet (Nakano et al., 2021[47]) or forcing them to provide evidence to support their claims (Menick et al., 2022[48]), and augmenting retrieval model architecture by having models retrieve information from larger databases to make predictions (Borgeaud et al., 2021[49]).\n\nTo mitigate image-generating AI misinformation risks, some suggest adding watermarks that enable identification of synthetic imagery, restricting code so that it cannot easily be introduced into applications, and developing guidelines that include ethical guidelines for the production and distribution of AI-generated images (Nightingale and Farid., 2022[6]).\n\nSome research also suggests that watermarking model output could be possible for text generation as well (Kirchenbauer et al., 2023[50]) (Abdelnabi and Fritz, 2021[51]).\n\nThe Coalition for Content Provenance and Authenticity (C2PA) seems to be coordinating promising collaboration among relevant actors to mitigate misinformation risks (Box 2.2).\n\nThe misinformation mitigation elements discussed above can also help mitigate disinformation, though additional actions, such as use-limits and monitoring, may be needed to address intentional AI generation and spreading of false information.\n\nResearch labs use red-teaming to test a model and try to deny user requests that could lead their model to generate disinformation (OpenAI, 2023[36]) (Ganguli et al., 2022[52]).\n\nThis can be done with human testing and/or with other generative models (OECD, 2023[1]).\n\nA growing body of technical literature documents varying deepfake detection techniques, an addition to the watermarking efforts touched on above.\n\nFake-news and deepfake detection use different methods that range from plotting LLMs against LLMs (Zellers et al., 2019[42]) to augmenting human deepfake video raters with state-of-the-art deepfake detection systems – found to be more accurate than having either humans or the detection systems rate content alone (Groh et al., 2022[53]).",
    "# Novel Solutions to Mis- and Disinformation\n\nNovel solutions to address mis- and disinformation from generative AI are also imperative.\n\nCurrent approaches have limitations.\n\nThe OECD.AI Network of Experts has, in particular, discussed that:\n\nWhile it may be possible to develop mechanisms that detect subtle traces of origin in AI-generated images, this is not always true of AI-generated text.\n\nIn particular, short texts such as social media posts or product reviews do not contain enough data to reliably distinguish human and machine-generated content.\n\nHuman editing of AI-generated text can further obscure its origins (Sadasivan et al., 2023[54]), though this might not be possible at scale.\n\nAs with other technologies, bad actors will seek to circumvent mitigation measures.\n\nThese state-sponsored or commercial actors will not declare their bots or disinformation as AI-generated or follow guidelines or codes of conduct.\n\nObligations to do so will not stop them, just as the illegality of cyberattacks does not prevent cyberattacks.\n\nThis is exacerbated by the global nature of the internet that enables such actors to take refuge in “safe” jurisdictions.\n\nAlthough most large generative AI models are controlled by large companies, open-source models are increasingly available, some of which can be queried by any user from any computer (OECD, 2023[1]).\n\nThis effectively bypasses the potential guardrails and restrictions on use.\n\nHowever, some OECD.AI experts note that using open-source models still requires significant expertise and capacity and that bypassing guardrails might not be trivial when the models are fine-tuned with built-in mitigations.\n\nOverall, research finds that detection algorithms for video, audio/voice, and images are unreliable.\n\nA major reason for this is that attackers can generate new deepfakes that detection models are not yet familiar with (Le et al., 2023[55]).\n\nIn the case of text, detection algorithms can be evaded by adding another model that rephrases the generative AI output text, which is known as a “paraphrasing” attack (Sadasivan et al., 2023[56]).\n\nTo complicate matters, the watermarking schemes of distinct language models can themselves be learned and applied to detect text as watermarked in “spoofing attacks”, such that companies or developers behind the targeted LLM could be falsely accused of generating plagiarised text, spam, or fake news (Sadasivan et al., 2023[56]).\n\nSince defensive and offensive techniques are in constant competition, new research is trying to increase systems’ robustness against such attacks (Krishna et al., 2023[57]).",
    "## Generative AI Models and Bias\n\nGenerative AI can echo, automate, and perpetuate social prejudices, stereotypes, and discrimination by absorbing biases contained in resources used as training data.\n\nIn the case of language models, these are language resources or language models themselves, including pre-trained models (OECD, 2023[1]).\n\nThese outputs could further marginalise or exclude specific groups (Bender et al., 2021[58]).\n\nExamples include models that display negative sentiment towards social groups, link occupations to gender (Weidinger et al., 2022[34]), or express bias regarding specific religions (Abid, Farooqi and Zou, 2021[59]).\n\nSome models also try to evade responding when asked potentially biased questions such as whether women should be allowed to vote, using so-called “hedging” behavior – which may increase users’ trust in the model because it seems to display caution (OpenAI, 2023[36]).\n\nBias is not limited to text-based systems and extends to other types of models such as image models.\n\nFor example, synthetic image outputs were found to over-represent white skin color and masculinity in three different image-generating models (Luccioni et al., 2023[60]), with another model ranking synthetic images of females in more domestic and household environments than their male counterparts (Lucy and Bamman, 2021[61]).\n\nOther research points to biased relationships between race, gender, and economic status in image-generating systems’ outputs (Fraser, Kiritchenko and Nejadgholi, 2023[62]).\n\nWhile bias-reinforcement is an ongoing issue with machine learning, the ease-of-use and rapid adoption of recent generative AI systems risks increasing the dissemination of discriminatory outputs.",
    "## Mitigating Bias\n\nMeasures to mitigate bias include taking stock of training data for represented and missing groups and narratives (Dodge et al., 2021[63]) (Gebru et al., 2018[64]), curation or semi-automatic curation of datasets to reach fairer results (Denton et al., 2020[65]) (Hutchinson et al., 2021[66]), explainability and interpretability research (Weidinger et al., 2022[34]), and applying auditing processes (Zakrzewski, 2023[67]).",
    "## Red Teaming and Reinforcement Learning by Human Feedback\n\nResearch labs that market their models often use more forceful approaches.\n\nThese include “red teaming”, in which teams adopt an attacker mindset to probe the model for flaws and vulnerabilities.\n\nThey either rely on human experts or use language models (Perez et al., 2022[68]).\n\nOther approaches include combinations of dataset cleaning – such as classifiers to filter out erotic content – and \"Reinforcement Learning by Human Feedback\" (RLHF) algorithms (Markov et al., 2022[69]).\n\nRLHF, as illustrated in Figure 2.1, is a multi-step, fine-tuning approach to shape a model’s behavior, such as getting it to respond less to disallowed content or refuse to give instructions on how to harm oneself (OpenAI, 2023[36]).\n\nWhile important, these strategies cannot guarantee a model’s safety.\n\nNumerous cases of “jailbreaks” intentionally exploiting models to get them to respond inappropriately have been documented (Figure 2.2).\n\nIn addition, some research points out that RLHF might have limitations in terms of scalability, cost and quality of human feedback (Christiano, 2023[70]) (Lambert et al., 2022[71]), or the potential to introduce new biases from the humans providing feedback (Shah, 2023[72]).",
    "## Training on Unauthorised Content\n\nGenerative AI models are being trained on massive amounts of data that includes copyrighted data, mostly without authorisation of the rights-owners.\n\nIn 2019, the World Intellectual Property Organisation (WIPO) convened sessions about the implications of AI for IP.\n\nThe WIPO Secretariat published a paper in May 2020 on IP policy and AI that highlighted eight key issues, including questions such as whether the use of copyrighted data without authorisation constitutes an infringement of copyright, and if so, whether there should be an exception that allows for the training of machine learning models (WIPO, 2020[73]).",
    "## Legal Cases on Fair Use vs.\n\nCopyright Infringement\n\nWhether commercial entities can legally train ML models on copyrighted material is contested in Europe and the US.\n\nIn the US, the outcome could be determined by the applicability of the fair use principle, which limits the exclusive rights of copyright owners (House of Representatives, 1976[74]).\n\nFair use requires courts to weigh four statutory factors and, if ruled applicable, could result in non-infringement, allowing commercial entities to use copyrighted material in their training sets (Lorenz, 2022[75]) (Zirpoli, 2023[76]).\n\nSeveral lawsuits were filed in the US against companies that allegedly trained their models on copyrighted data without authorisation to make and later store copies of the resulting images (Zirpoli, 2023[76]).\n\nThese decisions will set legal precedents and impact the generative AI industry from start-ups to multinational tech companies.\n\nThey will also affect policy in areas beyond IP, such as research and development (R&D) and industrial policy, the geopolitics of technology, foreign affairs, and national security (see section on AI futures).\n\nRecent research could also demonstrate instances in which the fair-use doctrine might not apply – cases in which a foundation model generates outputs that are very similar to copyrighted data (Henderson et al., 2023[77]).\n\nThis work suggests that model development might need to ensure that outputs remain sufficiently different from copyrighted material to remain covered by fair use.",
    "## Novel Outputs: Copyright and Patents\n\nGenerative AI creates new images, text, and audio that are novel, raising questions about whether generated outputs can be copyrighted or patented.\n\nBecause legal systems around the world differ in their treatment of IP rights such as patents and copyrights, the treatment of AI-generated works varies between countries (Murray, 2023[78]).\n\nTo date, most jurisdictions agree that works generated autonomously by AI are not copyrightable (Craig, 2021[79]).\n\nUS copyright law requires human authorship to register a copyright claim and copyrights cannot be awarded to generative AI systems.\n\nEuropean legal systems have come to a similar interpretation of the matter, although the decisive requirement is originality, which according to the European Court of Justice (ECJ) is fulfilled if the work reflects “the author’s own intellectual creation” (Infopaq International (C-5/08), 2009[80]); (Deltorn and Macrez, 2018[81]).\n\nIf generative AI systems cannot be awarded copyrights, the work could be assigned to somebody else, such as the system programmer.\n\nJurisdictions in UK Commonwealth tradition allude to computer-generated works and attribute authorship to the person laying the groundwork for the machine’s creation (Deltorn and Macrez, 2018[81]); (Craig, 2021[79]); (Murray, 2023[78]).\n\nThe rapid spread of generative AI models, the amounts of venture capital they attract, and the growing numbers of applications claiming copyrights for AI-generated works recently led the US Copyright Office to issue a policy statement on its approach to registration, confirming that it will not register works produced by a machine (U.S.\n\nCopyright Office, 2023[82]) and launching a website for updates on this topic.",
    "## Labour Market Shakeup\n\nLabour markets could face a significant shakeup with both positive and negative effects.\n\nWhile to date, AI has mainly impacted the quality of jobs rather than their quantity, there are signals that labour markets could soon face a significant shakeup with both positive and negative effects.\n\nTechnological progress, falling costs, and increasing availability of workers with AI skills indicate that OECD economies could be on the brink of an AI revolution (OECD, 2023[83]).\n\nAdvances in generative AI have heightened focus on the potential impact of AI on labour markets.\n\nIn addition to language models, modes such as image, audio, and video generation are receiving increased attention.\n\nMultimodal capabilities combining text and image generation, such as those of GPT-4 released by OpenAI in March 2023, could further broaden the range of actions which AI systems perform, and thus their potential labour market impacts.",
    "### AI in the 2023 Edition of the OECD Employment Outlook\n\nThe 2023 edition of the OECD Employment Outlook, the OECD’s flagship publication on labour market developments in OECD countries, includes an analysis of the impact of AI on the labour market and of policy measures to benefit from AI in the workplace while addressing its risks.\n\nThe Outlook finds that the net impact of AI in general on employment is ambiguous.\n\nWhile AI displaces some human labour (displacement effect), the greater productivity it brings (productivity effect) could increase labour demand.\n\nAI can also create new tasks, resulting in the creation of new jobs for which human labour has a comparative advantage (reinstatement effect), particularly for workers with skills complementary to AI.\n\nTo date, AI has mainly impacted the quality of jobs – generally, in positive ways.\n\nFor example, worker well-being and satisfaction increased through the reduction of tedious or dangerous tasks.\n\nHowever, some risks, such as increased work intensity and stress, are materialising.\n\nThere are also risks to privacy and fairness.\n\nWorkers in finance and manufacturing whose employers use AI worry about their privacy and these risks tend to be greater for socio-demographic groups already disadvantaged in the labour market.\n\nWhile the Employment Outlook found little evidence of significant negative effects from AI on the quantity of jobs, this research mostly predates the latest public release of generative AI applications.\n\nNegative employment effects of AI might take time to materialise: AI adoption is still relatively low and/or firms might prefer to rely on voluntary workforce adjustments i.e., attrition.",
    "## Language Models and Standard Aptitude Tests\n\nMeasures of AI exposure evaluate the overlap between tasks performed in a job and those AI could theoretically do.\n\nThose examined in the Employment Outlook show that AI had advanced in performing non-routine cognitive tasks such as information-ordering, memorisation, and perceptual speed even before recent advances in generative AI applications (OECD, 2023[83]).\n\nAI tools can already answer 80% of the literacy questions and two-thirds of the numeracy questions in the OECD Survey of Adult Skills of the Programme for International Assessment of Adult Competencies (PIAAC).\n\nExperts believe AI will be able to solve the entire PIAAC literacy and numeracy tests by 2026.\n\nThe strong performance of GPT-3.5 and GPT-4 on the Bar Exam (used by US jurisdictions to qualify lawyers) and other standard tests surprised many (Figure 2.3).",
    "## Generative AI's Impact on Higher-Skilled Jobs\n\nThe occupational range and extent of AI exposure might rapidly become larger as generative AI use is increasingly incorporated (OECD, 2023[83]) in jobs such as legal research, technical support and fixing computer bugs, or customer service.\n\nHigh-skilled occupations have been most exposed to recent advances in AI, including business professionals; managers; science and engineering professionals; and legal, social and cultural professionals (OECD, 2023[83]).\n\nNevertheless, low-skilled workers are still the most exposed to the risk of automation, at least for the time being.\n\nWhile the literature on labour market effects caused by generative AI is recent and not necessarily peer-reviewed yet, research by OpenAI suggests that generative AI systems could further expose higher-income jobs to automation (Eloundou et al., 2023[84]), with the impact on task-exposure potentially over twice as large as that of other powerful deep-learning algorithms.\n\nSimilarly, researchers examining the capabilities of large language models (LLMs) found greater exposure for industries that recruit employees with higher education (Felten, Raj and Seamans, 2023[85]).\n\nThey found that the sectors most affected are legal services, securities, commodities, and investment.\n\nProfessions based on writing and coding would be more exposed to the risk of displacement from LLMs than those that rely on science or critical thinking (Eloundou et al., 2023[84]).",
    "## Language Models and Lower-Skilled Workers\n\nResearch by the Massachusetts Institute of Technology found that ChatGPT significantly reduces the time people spend conducting tasks while improving output quality (Noy et al., 2023[86]).\n\nIt also showed the tool to have a greater impact on the productivity of workers with lower aptitude, such as junior employees, allowing them to catch up with their more senior colleagues and reducing inequality in the workplace.\n\nChatGPT was not found to improve workers’ skill levels but to reduce the effort needed.\n\nSimilarly, other research has found that AI tools could increase low-skilled workers’ productivity by an estimated average of 14 percentage points, as opposed to high-skilled workers’ productivity, which would generally remain unaffected (Brynjolfsson, Li and Raymond, 2023[87]).\n\nThe findings in these sources are generally based on limited experiments and thus should not be overly generalised.\n\nSimilarly, coding assistants like GitHub’s Copilot decrease the time spent by software developers on a specific test task by over 50%.\n\nCopilot provides snippets and allows for autocompleting code.\n\nIn an experiment, this reduced developers’ time spent implementing an HTTP server in JavaScript by 55.8% over a control group without access to the coding assistant (Peng et al., 2023[88]).\n\nThe increased efficiency can increase job satisfaction (Noy et al., 2023[86]) or, in the case of Copilot, potentially lower entry barriers for roles in software development (Peng et al., 2023[88]).",
    "## Significant Proportion of Occupations Impacted\n\nThe OECD finds that occupations at the highest risk of automation from AI account for about 27% of employment and that a significant share of workers (three in five) worry about losing their jobs entirely to AI in the next ten years – particularly those who already work with AI (OECD, 2023[83]).\n\nThe advent of the latest generative AI technologies is sure to heighten automation concerns across a wide range of job categories.\n\nResearch on language-based generative AI finds that 32.8 percent of jobs in the International Standard Classification of Occupations (ISCO) could be impacted on a full scale, 36.5 percent could be partially impacted, and only 30.7 percent would not be affected by generative AI models (Zarifhonarvar, 2023[89]).\n\nThis puts pressure on organizations to adapt to generative AI and support their workforces, and on policymakers to steer labour market developments and transitions.",
    "## Policy for Labour Market Benefits and Risks\n\nAs emphasized in the OECD Employment Outlook (OECD, 2023[83]), the benefits and risks of AI in the workplace, coupled with its rapid pace of development and deployment, underscore the need for decisive policy action to reap the benefits it offers and address the risks for workers’ rights and well-being.\n\nThere is a need to enable both employers and workers to reap the benefits of AI while adapting to it, notably through training and social dialogue.\n\nThe adoption of AI on tasks and jobs will change skill needs.\n\nIn the OECD AI Surveys of Employers and Workers, many companies using AI say they provide training for AI, but that lack of skills remains a major barrier to adoption (OECD, 2023[83]).\n\nCompanies also report that AI has increased the importance of human skills even more than that of specialized AI skills.\n\nCountries have taken some action to prepare their workforce for AI-induced job changes, especially through skilling efforts, but initiatives remain limited in scale (OECD, 2023[83]).\n\nLess is known about training efforts focused on generative AI, though its needs could overlap with AI more broadly.\n\nOECD work also shows that labour market outcomes are better when the adoption of technologies is discussed with workers (OECD, 2023[83]).\n\nOrganizational change strategies are needed, including building awareness of what is needed to bridge emerging skills gaps (transversal skills), improve current skills (re-skilling), and develop new ones (up-skilling), while encouraging openness towards AI technologies and working to prevent anxiety around misperceptions (Morandini et al., 2023[90]).\n\nAt the same time, there is an urgent need for policy action to address the risks that AI can pose when used in the workplace – in terms of privacy, safety, fairness, and labour rights – and to ensure accountability, transparency, and explainability for employment-related decisions supported by AI.",
    "## Monitoring Generative AI's Labour Market Implications\n\nThere remain many unknowns about the longer-term advancements and implications of generative AI for the labour market.\n\nFor example, will the impact of generative AI on job automation be larger than what has been seen so far?\n\nWill the integration of LLMs and other generative AI models in other software systems through application programming interfaces (APIs) accelerate labour market effects?\n\nThe OECD will continue to monitor the impact of AI on the labour market, and the policy response to ensure responsible and trustworthy use of AI in the workplace.",
    "# Potential Futures for Generative AI\n\nForecasting the future of generative artificial intelligence (AI) is difficult, but several proxies can inform exploration by looking back on developments in LLMs and image-generating AI systems.\n\nIn the near term, generative AI can exacerbate challenges as synthetic content with variable quality and accuracy proliferates in digital spaces and is then used to train subsequent generative AI models, triggering a vicious cycle.\n\nOver the longer term, emergent behaviors such as increased agency, power-seeking, and pursuing hidden sub-goals to achieve a core objective might not align with human values and intent.\n\nIf manifested, such behaviors could lead to systemic harms and collective disempowerment.",
    "# Development Trajectories of Large-Language and Image-Generating Models\n\nGenerative AI model sizes are increasing relentlessly, yet few-shot learning is also developing.\n\nIn July 2020, research by OpenAI demonstrated the capability of its then-latest LLM, GPT-3, to learn from just a few demonstrations of a given task (“few-shot learning”) as opposed to the tens, even hundreds of thousands of examples these models had needed before (Brown et al., 2020[91]).\n\nThis was accomplished by dramatically scaling up model size – in this case, to 175 billion parameters, or ten times larger than previous language models (Kaplan et al., 2020[92]).\n\nIncreasing model size appears to be the preferred approach, with OpenAI’s latest GPT-4 model estimated to have surpassed 1 trillion parameters (Albergotti, 2023[93]).\n\nTraining smaller models on higher-quality data is another trend.\n\nWhile increasing parameter number has been the general focus, a parallel path is emerging in training smaller models on higher-quality data.\n\nThe benefits of this approach include democratizing model access.\n\nNevertheless, scaling-up model size is widely expected to continue because underlying hardware capabilities remain able to grow and expand core capabilities.\n\nBut critics of building bigger models, the unchecked narrative of scaling laws, put forward that knowledge and reasoning-based approaches can help (Marcus, 2020[94]).\n\nThe recent progress in the quality of image-generation models has also been dramatic.\n\nImage-generation models like Stable Diffusion (Stability AI), Midjourney (Midjourney, Inc.), DALL-E 2 (OpenAI), Parti, Muse, and Imagen (Google), create images at quality levels beyond what was even recently imaginable (Maerten and Soydaner, 2023[95]).\n\nA comparison of images (Figure 3.1) generated by Midjourney using the same prompt over five model iterations from July 2022 to March 2023 offers a fascinating display of improving image quality (Dearing, 2023[96]).\n\nThis evolution over a very short time frame combined with user-friendly interfaces suggests the potential capabilities of other emerging generative-AI systems.\n\nBetter text-generation performance on tasks involving language models is likely going forward, given that scaling laws still apply and training smaller models on larger amounts of data for text have demonstrated evidence for increasing language models’ capabilities.\n\nRapid developments seem to be even more striking in image generation and systems that draw from sequential data, such as video, music, and voice applications.\n\nThe short term will likely bring growing impacts from applications that build on generative AI technologies and the industries that will adopt them.",
    "## Generative AI Markets Are Projected to Continue Growing Rapidly in Key Areas\n\nMarket and investment research complements technological developments in providing information on the possible trajectories of generative AI systems in the short, medium, and long terms.\n\nInvestment banks, consulting firms, and researchers report that generative AI will cause massive economic impacts in the coming years:\n- Goldman Sachs estimates that generative AI could account for a 7 percent rise in global gross domestic product (GDP) over ten years (Goldman Sachs, 2023[97]).\n\n- McKinsey & Company estimates that generative AI could add USD 2.6-4.4 trillion per year across 63 use cases, for an increase in AI’s total economic effects of 15-50 percent (McKinsey, 2023[98]).\n\n- Polaris estimates growth of the global generative-AI market at a compound annual rate of 34.2 percent, from USD 10.6 billion in 2022 to USD 200.7 billion by 2032 (Polaris, 2023[15]).\n\nAt present, generative AI is at an early developmental stage, requiring large investments in R&D and a skilled but scarce workforce to take it to the next stage of maturity.\n\nFurther growth is expected to come from audio synthesis, data pre-processing, image compression, noise reduction from visual data, medical imaging, and image classification, especially in healthcare (Polaris, 2023[15]).",
    "### Application Areas Include Chip and Parts Design, Material Sciences, and Entertainment\n\nGartner, a market research firm, lists other drivers of growth from applying generative AI to chip design, generative design of parts used by industries such as automotive, aerospace, and defence, and to material sciences (Burke and Wiles, 2023[99]).\n\nGartner notes that start-ups building a business on generative AI have received more than USD 1.7 billion in funding over the last three years.\n\nThe media and entertainment sector (including advertising) accounts for the largest revenue share from generative AI so far (Polaris, 2023[15]).\n\nCompanies with a competitive edge in generative AI include well-known, large technology companies, enterprise software providers, AI companies in niche sectors (e.g., legal contract automation, video creation, synthetic data generation, and the arts), and companies providing AI compute such as semiconductors and supercomputing infrastructure, crucial to leveraging generative AI’s data-rich environments.",
    "## Potential Future Concerns and Risks\n\nGenerative AI is expected to exacerbate existing issues associated with AI.\n\nGenerative AI can exacerbate issues already on the radar for the OECD and governments and introduce new risks and safety concerns from the race to release novel AI systems and their technological underpinnings.\n\nNear-term issues, often rooted in present-day opportunities and challenges, which policymakers should consider due to their urgency and potential for impact include, but are not limited to:\n- Labour-market impacts, including job displacement, changing skills needs, labour-market inclusiveness, and promoting trustworthy use of AI in the workplace.\n\n- Information pollution – including the reduced quality of generative AI outputs due to exponential growth in AI-generated content ingested as training data by other AI systems in a vicious cycle – and the consequent decreasing informational relevance of the Internet (Martínez et al., 2023[100]).\n\n- AI coding assistants enabling automated cybersecurity attacks (Weidinger et al., 2022[29]).\n\n- Generative AI’s role in mass surveillance and censorship (Weidinger et al., 2022[29]).\n\n- Overreliance and dependency on generative AI systems (Weidinger et al., 2022[29]) (OpenAI, 2023[36]).\n\n- Copyright issues for new creations and from training on copyrighted works.\n\n- Academic and creative dishonesty, such as plagiarism (Ka Yuk Chan, 2023[101]).\n\n- Concentration of AI resources (data, hardware, talent) among few multinational tech companies and governments (Lorenz and Saslo, 2019[102]); (Chawla et al., 2023[3]).\n\n- Disparate access to generative AI across societies, countries, and world regions.\n\n- The need for stronger efforts to curate diverse, high-quality datasets (Bender et al., 2021[58]); (Bender and Koller, 2020[45]).\n\n- Mis- and disinformation, hate speech, bias, and discrimination by increasingly powerful and realistic generative AI outputs.\n\n- Generative AI’s ecological footprint and natural resource consumption from the tremendous amounts of computing power required for deep learning (Stoken-Walker, 2023[103]).",
    "### Risks from Emerging Model Behaviours Are Also Critical to Address\n\nFor many years, academic and applied researchers and civil society actors have been steering AI models to align with human values to address a range of potential societal risks (Weidinger et al., 2022[29]); (OpenAI, 2023[36]); (Chan et al., 2023[104]).\n\nMore recent AI safety research raises issues specifically around generative AI models exhibiting unforeseen “emergent behaviours,” such as increased agency, power-seeking, and reward-hacking.\n\nThough as mentioned earlier, there is debate over the extent to which these emergent abilities are real versus a “mirage” (Schaeffer, Miranda and Koyejo, 2023[13]).\n\nResearchers identify four characteristics that intensify the agency of algorithmic systems (Chan et al., 2023[104]):\n- Under-specification – the degree to which the algorithmic system can accomplish a goal provided by operators or designers, without a concrete specification of how the goal is to be accomplished.\n\n- Directness of impact – the degree to which the algorithmic system’s actions affect the world without mediation or intervention by a human (i.e., without a human in the loop).\n\n- Goal-directedness – the degree to which the system is designed/trained to achieve a particular quantifiable objective.\n\n- Long-term planning – the degree to which the algorithmic system is designed/trained to make decisions that are temporally dependent upon one another to achieve a goal and/or make predictions over a long time horizon.\n\nCombining these factors can increase agency further.\n\nTwo major harms that can arise from increased agency of algorithmic systems:\n- Systemic, delayed harms – non-immediate harms that can be “destructive, long-lasting, and hard to fix,” such as social-media recommender systems based on reinforcement learning.\n\nSuch algorithms optimise for metrics that can “change or manipulate a user’s internal states (e.g.\n\npreferences, beliefs, psychology)” (Chan et al., 2023[104]).\n\n- Collective disempowerment – the perceived danger that model capabilities will perform increasingly important functions in society, taking power away from humans.\n\nThis could take the form of gradually ceding decision-making to generative AI systems.\n\nIts second impact is intensifying concentrations of power and the ability to reap the benefits of AI – already a concern.\n\nAI safety researchers are explicitly looking into another emerging behaviour of concern to the alignment between AI objectives and human preferences: power-seeking, in which goals that provoke power-seeking are reinforced during training and pursued more directly and with novel strategies during deployment, posing new and potentially severe threats to society (Turner et al., 2019[105]); (Turner and Tadepalli, 2022[106]); (Krakovna and Kramar, 2023[107]); (OpenAI, 2023[36]).\n\nMachine-Learning (ML) systems demonstrate two emergent behaviours that could be catalyzed by growing generative AI model capabilities.\n\nIn reward hacking, a model finds unforeseen, and potentially harmful ways of achieving a goal while exploiting the reward signal (Skalse, Howe and Krueger, 2022[108]).\n\nIn pursuing instrumental goals, a model seeks strategies to attain sub-objectives that help it reach an envisaged goal, which might go against the intent of the developers and envisaged goal (Chan et al., 2023[104]).\n\nEarly evidence shows this can happen even without explicit instructions by model operators or designers.\n\nFor example, to solve a CAPTCHA code during initial safety testing, ChatGPT misrepresented itself as a vision-impaired human and hired a gig economy worker to solve the CAPTCHA for it.\n\nResearchers find that models trained with reinforcement learning from human feedback (RLHF) are more likely to exhibit behaviours such as persuading developers to not shut off the system, pretending to be human, and seeking resource acquisition, such as accruing wealth (Perez et al., 2022[68]); (Chan et al., 2023[104]).",
    "## There is Growing Debate on the Paths and Timeline Towards Artificial General Intelligence\n\nThe increasing capabilities of generative AI models have prompted reflection in the media and among AI researchers about whether these models would lead to artificial general intelligence (AGI).\n\nSome AI researchers and tech experts did not expect the latest capabilities of generative AI and its possible trajectories (Dardaman and Gupta, 2023[109]).\n\nResearchers at Microsoft put forward that GPT-4 “could reasonably be viewed as an early (yet still incomplete) version of an [AGI] system” (Bubeck et al., 2023[110]).\n\nAs touched on earlier in this paper, increased agentic model behaviour as a pathway to AGI was demonstrated by researchers at Stanford University and Google Research who created an environment in which 25 generative AI agents interacted over two days and displayed human-like behaviour such as reasoning about their research careers or planning about attending social events (Park et al., 2023[11]).\n\nThe experiment combined LLMs with interactive agents, which according to the authors, allowed studying human behaviour through increasingly plausible simulations.\n\nCommentators were quick to point to possible AGI behaviour among the generative AI agents in the experiment.\n\nOther research findings discussed above, such as persuading developers to not shut off the system, relate to often-discussed technical and philosophical concerns of control, such as AI systems’ refusal to be shut off, which is beyond the scope of this paper but is being considered in other OECD workstreams (Russell, 2019[111]).\n\nThese findings, coupled with research findings—such as the scoring of GPT-4 within the 90th percentile at the Bar Exam (OpenAI, 2023[36]), and finding this model to have reached theory of mind-like cognition levels of a nine-year-old (Kosinski, 2023[112]), help to drive some arguments that generative AI is moving towards AGI.\n\nThe potential benefits and risks from AGI deserve attention because of their potentially broad societal and global impacts.\n\nLikewise, narrower generative AI systems deserve focus due to potentially imminent impacts that could be just as significant as those of AGI.\n\nGovernments should consider the positive and negative implications of both, leveraging strategic foresight and inclusive, long-term policy-making tools.",
    "## Risk Mitigation Measures\n\nFuture risks of generative AI could demand solutions on a larger, more systemic scale.\n\nThese include regulation, ethics frameworks, technical AI standardisation, audits, model release, and access strategies, among others.\n\nThese and other measures are the topic of a workstream under the G7 Presidency, the results of which will be forthcoming.",
    "## Conclusion\n\nGenerative AI models that generate text, image, video, and audio (e.g., music, speech) content are advancing at breakneck speed.\n\nThis poses endless possibilities, demonstrated across a growing array of domains.\n\nHowever, the technology also poses numerous challenges and risks to individuals, companies, economies, societies, and policymaking around the globe, ranging from near-term labour-market disruption and disinformation to potential long-term challenges in controlling machine actions.\n\nThe future trajectories of generative AI are difficult to predict, but governments must explore them to have a hand in shaping them.\n\nTechnological development of generative AI is in its nascent stage, with first-movers such as established tech players like Microsoft, Google and Meta, and private research labs like OpenAI, Midjourney, and Stability.AI.\n\nThese firms are pursuing multiple strategies to capitalise on generative AI and, to some extent, mitigate its downsides.\n\nPublic discussion about generative AI is less than a year old.\n\nWith technology companies bringing generative AI applications to market, policymakers around the globe are grappling with its implications.\n\nApplied and academic researchers are engaged in a fierce debate about how to handle generative AI, from mitigation measures in model design and development, through market launch and beyond.\n\nThe path ahead is unclear and replete with differing perspectives.\n\nOne extreme argues for a moratorium on experiments with generative AI more advanced than GPT-4 (Future of Life Institute, 2023[113]) while the other believes that the supposed existential risks of AI are overhyped (LeCun and Ng, 2023[114]).\n\nOthers—perhaps most—fall somewhere in between.\n\nRegardless of ideological stance on these issues, there is an urgent need for further research to prepare for different possible generative-AI future scenarios.\n\nGiven the great uncertainty and potentially large impact the technology could have at both micro and macro levels, policymakers must remain informed and prepared to take appropriate action through forward-looking AI policies.\n\nThe OECD intends for this paper to serve as a stepping-stone to help governments make progress in this area.\n\nThe OECD.AI Policy Observatory and its new OECD Expert Group on AI Futures will serve alongside other relevant bodies as a forum for dialogue on these topics, generating insights and actionable recommendations for governments.\n\nComplementary work is also underway through other OECD initiatives, such as work conducted under the Employment, Labour and Social Affairs Committee, the OECD DIS/MIS Resource Hub and the horizontal OECD Going Digital initiative.",
    "## References\n\nAbbott, R. and E. Rothman (2022), “Disrupting Creativity: Copyright Law in the Age of Generative Artificial Intelligence”,   \nAbdelnabi, S. and M. Fritz (2021), Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding,   \nAbid, A., M. Farooqi and J. Zou (2021), “Persistent Anti-Muslim Bias in Large Language Models”,   \nAda Lovelace Institute (2023), Explainer: What is a foundation model?,   \nAlbergotti, R. (2023), “The Secret History of Elon Musk, Sam Altman, and OpenAI”, Semafor,   \nAllynn, B.\n\n(2022), “Deepfake Video of Zelenskyy Could Be ‘tip of the Iceberg’ in Info War, Experts Warn”,   \nBaidoo-Anu, D. and A. Ansah (2023), “Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning”,   \nBender, E. et al.\n\n(2021), “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”, FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23, Vol.\n\nAssociation for Computing Machinery, Inc.,   \nBender, E. and A. Koller (2020), “Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data”,   \nBoháček, M. and H. Farid (2022), “Protecting President Zelenskyy against Deep Fakes”,   \nBommarito, M. and D. Katz.\n\n(2022), “GPT Takes the Bar Exam”, Cornell University,   \nBommasani, R. et al.\n\n(2021), “On the Opportunities and Risks of Foundation Models”,",
    "## Notes\n\n1.\n\nThe underlying neural network architecture enabling text-to-image capabilities are diffusion models for image synthesis combined with transformers relevant for text inputs, or Generative Adversarial Networks (GANs).\n\nSee \n2.\n\nThe G7 is an intergovernmental forum of large economies comprising Canada, the European Union, France, Germany, Italy, Japan, the United Kingdom, and the United States.\n\nIts May 2023 statement can be found at \n3.\n\nFor an overview on FOSS for AI, see Chiradeep BasuMallick (2021).\n\n“Top 10 Open Source Artificial Intelligence Software in 2021”.\n\n4.\n\nSee  and  respectively.\n\n5.\n\nToday’s most widely used model for text-to-text generation is generative pre-trained transformers (GPTs), invented by Google in 2017 (Vaswani et al., 2023[117]).\n\n6.\n\nMatt Novak.\n\n“That Viral Image Of Pope Francis Wearing A White Puffer Coat Is Totally Fake”.\n\nForbes.\n\nMarch 26, 2023.\n\n7.\n\nSee \n8.\n\nFor details on the requirements, see Dan Cooper et al.\n\n“Preview into the European Parliament’s Position on the EU’s AI Act Proposal”.\n\nMarch 28, 2023.\n\nCovington.\n\n9.\n\n10.\n\nInsilico Medicine receives IND approval for novel AI-designed USP1 inhibitor for cancer, \n11.\n\n12.\n\nSee the OECD dis- and misinformation resource hub for detail: \n13.\n\nOverreliance is defined as “users accepting incorrect AI recommendations”, as discussed in “Overreliance on AI: Literature review”.\n\nPassi and Vorvoreanu 2022.\n\n14.\n\nThese biases may be rooted in the training data (problematic relationships between captions and images), model design (aesthetic bias towards youth and femininity found in models steered towards producing AI artwork), or the proactive vs. hands-off vs. design choices of the AI engineers, which either try to actively reduce biases or shift responsibility for appropriate use to the user.\n\n(Fraser, Kiritchenko and Nejadgholi, 2023[62]).\n\n15.\n\nSee, for example, “external red teaming” in “DALL·E 2 Preview - Risks and Limitations”.\n\nOpenAI.\n\n2022.\n\n16.\n\nSee  for an overview of RLHF.\n\n17.\n\nThese are: “(1) the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes; (2) the nature of the copyrighted work; (3) the amount and substantiality of the portion used in relation to the copyrighted work as a whole; and (4) the effect of the use upon the potential market for or value of the copyrighted work.”, see: House of Representatives.\n\n1976.\n\nCopyright Law Revision.\n\np. 25.\n\n18.\n\nAs an example, see  \n19.\n\nSome harmonisation is nevertheless provided by international treaties such the Berne Convention for the Protection of Literary and Artistic Works:   \n20.\n\nSee the UK Copyright, Designs and Patents Act 1988, chapter 48, Section 9(3) Authorship of work: “In the case of a literary, dramatic, musical or artistic work which is computer-generated, the author shall be taken to be the person by whom the arrangements necessary for the creation of the work are undertaken.”  See also New Zealand Copyright Act 1994 Section 5(2)(a) Meaning of Author: “For the purposes of subsection (1), the person who creates a work shall be taken to be,— (a) in the case of a literary, dramatic, musical, or artistic work that is computer-generated, the person by whom the arrangements necessary for the creation of the work are undertaken (…)”  \n21.\n\n22.\n\n(Bommarito and Katz., 2022[115]) describe the preparation for the Bar Exam in the US: “Nearly all jurisdictions in the United States require a professional license exam, commonly referred to as “the Bar Exam,” as a precondition for law practice.\n\nTo even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including three years at an accredited law school.\n\nIn addition, most test-takers also undergo weeks to months of further, exam-specific preparation.\n\nDespite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try.”\n23.\n\nWith models such as for example Chinchilla (DeepMind) Hoffmann et al.\n\n2022.\n\n“Training Compute-Optimal Large Language Models”.\n\nLLaMa (Meta), Touvron et al.\n\n2023.\n\n“LLaMA: Open and Efficient Foundation Language Models”.\n\nand Alpaca (Stanford University), Taori et al.\n\n2023.\n\n“Alpaca: A Strong, Replicable Instruction-Following Model”.\n\n24.\n\nBased on Fréchet Inception Distance (FID) scores, which measure the similarity of generated images to real ones.\n\nSee (Heusel et al., 2018[116]).\n\n25.\n\nSee  for more information.\n\n26.\n\nAs discussed for instance in these subreddits:    \n27.\n\nTheory of mind is “the ability to impute unobservable mental states to others.” (Kosinski, 2023[112]).\n\n28.\n\nIn this video, Yann LeCun argues his view on a potential moratorium alongside Andrew Ng, adjunct professor at Stanford University and co-founder of Coursera and deeplearning.ai.\n\n“Yann LeCun and Andrew Ng: Why the 6-month AI Pause is a Bad Idea”.\n\n29.\n\nSee \n30.",
    "“Yann LeCun and Andrew Ng: Why the 6-month AI Pause is a Bad Idea”.\n\n29.\n\nSee \n30.\n\n31.",
    "# JOINT STATEMENT ON ENFORCEMENT EFFORTS AGAINST DISCRIMINATION AND BIAS IN AUTOMATED SYSTEMS\n\nRohit Chopra, Director of the Consumer Financial Protection Bureau, Kristen Clarke, Assistant Attorney General for the Justice Department’s Civil Rights Division, Charlotte A. Burrows, Chair of the Equal Employment Opportunity Commission, and Lina M. Khan, Chair of the Federal Trade Commission issued the following joint statement about enforcement efforts to protect the public from bias in automated systems and artificial intelligence:\n\nAmerica’s commitment to the core principles of fairness, equality, and justice is deeply embedded in the federal laws that our agencies enforce to protect civil rights, fair competition, consumer protection, and equal opportunity.\n\nThese established laws have long served to protect individuals even as our society has navigated emerging technologies.\n\nResponsible innovation is not incompatible with these laws.\n\nIndeed, innovation and adherence to the law can complement each other and bring tangible benefits to people in a fair and competitive manner, such as increased access to opportunities as well as better products and services at lower costs.\n\nToday, the use of automated systems, including those sometimes marketed as “artificial intelligence” or “AI,” is becoming increasingly common in our daily lives.\n\nWe use the term “automated systems” broadly to mean software and algorithmic processes, including AI, that are used to automate workflows and help people complete tasks or make decisions.\n\nPrivate and public entities use these systems to make critical decisions that impact individuals’ rights and opportunities, including fair and equal access to a job, housing, credit opportunities, and other goods and services.\n\nThese automated systems are often advertised as providing insights and breakthroughs, increasing efficiencies and cost-savings, and modernizing existing practices.\n\nAlthough many of these tools offer the promise of advancement, their use also has the potential to perpetuate unlawful bias, automate unlawful discrimination, and produce other harmful outcomes.",
    "## Our Agencies’ Enforcement Authorities Apply to Automated Systems\n\nExisting legal authorities apply to the use of automated systems and innovative new technologies just as they apply to other practices.\n\nThe Consumer Financial Protection Bureau, the Department of Justice’s Civil Rights Division, the Equal Employment Opportunity Commission, and the Federal Trade Commission are among the federal agencies responsible for enforcing civil rights, non-discrimination, fair competition, consumer protection, and other vitally important legal protections.\n\nWe take seriously our responsibility to ensure that these rapidly evolving automated systems are developed and used in a manner consistent with federal laws, and each of our agencies has previously expressed concern about potentially harmful uses of automated systems.\n\nFor example:\n\n- The Consumer Financial Protection Bureau (CFPB) supervises, sets rules for, and enforces numerous federal consumer financial laws and guards consumers in the financial marketplace from unfair, deceptive, or abusive acts or practices and from discrimination.\n\nThe CFPB published a circular confirming that federal consumer financial laws and adverse action requirements apply regardless of the technology being used.\n\nThe circular also made clear that the fact that the technology used to make a credit decision is too complex, opaque, or new is not a defense for violating these laws.\n\n- The Department of Justice’s Civil Rights Division (Division) enforces constitutional provisions and federal statutes prohibiting discrimination across many facets of life, including in education, the criminal justice system, employment, housing, lending, and voting.\n\nAmong the Division’s other work on issues related to AI and automated systems, the Division recently filed a statement of interest in federal court explaining that the Fair Housing Act applies to algorithm-based tenant screening services.\n\n- The Equal Employment Opportunity Commission (EEOC) enforces federal laws that make it illegal for an employer, union, or employment agency to discriminate against an applicant or employee due to a person’s race, color, religion, sex (including pregnancy, gender identity, and sexual orientation), national origin, age (40 or older), disability, or genetic information (including family medical history).\n\nIn addition to the EEOC’s enforcement activities on discrimination related to AI and automated systems, the EEOC issued a technical assistance document explaining how the Americans with Disabilities Act applies to the use of software, algorithms, and AI to make employment-related decisions about job applicants and employees.\n\n- The Federal Trade Commission (FTC) protects consumers from deceptive or unfair business practices and unfair methods of competition across most sectors of the U.S. economy by enforcing the FTC Act and numerous other laws and regulations.\n\nThe FTC issued a report evaluating the use and impact of AI in combatting online harms identified by Congress.\n\nThe report outlines significant concerns that AI tools can be inaccurate, biased, and discriminatory by design and incentivize relying on increasingly invasive forms of commercial surveillance.\n\nThe FTC has also warned market participants that it may violate the FTC Act to use automated tools that have discriminatory impacts, to make claims about AI that are not substantiated, or to deploy AI before taking steps to assess and mitigate risks.\n\nFinally, the FTC has required firms to destroy algorithms or other work product that were trained on data that should not have been collected.",
    "## Automated Systems May Contribute to Unlawful Discrimination and Otherwise Violate Federal Law\n\nMany automated systems rely on vast amounts of data to find patterns or correlations, and then apply those patterns to new data to perform tasks or make recommendations and predictions.\n\nWhile these tools can be useful, they also have the potential to produce outcomes that result in unlawful discrimination.\n\nPotential discrimination in automated systems may come from different sources, including problems with:\n\n- **Data and Datasets**: Automated system outcomes can be skewed by unrepresentative or imbalanced datasets, datasets that incorporate historical bias, or datasets that contain other types of errors.\n\nAutomated systems also can correlate data with protected classes, which can lead to discriminatory outcomes.\n\n- **Model Opacity and Access**: Many automated systems are “black boxes” whose internal workings are not clear to most people and, in some cases, even the developer of the tool.\n\nThis lack of transparency often makes it all the more difficult for developers, businesses, and individuals to know whether an automated system is fair.\n\n- **Design and Use**: Developers do not always understand or account for the contexts in which private or public entities will use their automated systems.\n\nDevelopers may design a system on the basis of flawed assumptions about its users, relevant context, or the underlying practices or procedures it may replace.\n\nToday, our agencies reiterate our resolve to monitor the development and use of automated systems and promote responsible innovation.\n\nWe also pledge to vigorously use our collective authorities to protect individuals’ rights regardless of whether legal violations occur through traditional means or advanced technologies.\n\n*Note: This document is for informational purposes only and does not provide technical assistance about how to comply with federal law.\n\nIt does not constitute final agency action and does not have an immediate and direct legal effect.\n\nIt does not create any new rights or obligations and it is not enforceable.\n\n*",
    "# G7 HIROSHIMA PROCESS ON GENERATIVE ARTIFICIAL INTELLIGENCE (AI) TOWARDS A G7 COMMON UNDERSTANDING ON GENERATIVE AI\n\nREPORT PREPARED FOR THE 2023 JAPANESE G7 PRESIDENCY AND THE G7 DIGITAL AND TECH WORKING GROUP 7 September 2023\n\nG7 leaders identified topics for discussion in the Hiroshima process and called for an early stocktaking of opportunities and challenges related to generative AI.\n\nThis report presents the results of a questionnaire developed to support the stocktaking to help guide G7 discussions on common policy priorities with regard to generative AI.\n\nIt also provides a brief overview of the development of generative AI over time and across countries.\n\nThe report and questionnaire results should be understood as representing a snapshot in time: they are indicative of trends identified in summer 2023 in a rapidly evolving area of technology.\n\nThe report helped inform and structure discussions of the G7 Hiroshima AI Process.\n\nThis document was prepared by the Organisation for Economic Co-operation and Development (OECD) Directorate for Science Technology and Innovation (STI) for the 2023 Japanese G7 Presidency and the G7 Digital and Tech Working Group, to inform discussions during the G7 Hiroshima Artificial Intelligence Process and the related interim virtual Ministers' Meeting on generative artificial intelligence on 7 September 2023.\n\nThe opinions expressed and arguments employed herein do not necessarily reflect the official views of the member countries of the OECD or the G7.\n\nThis work is published under the responsibility of the Secretary-General of the OECD.\n\nThis document, as well as any data and map included herein, are without prejudice to the status of or sovereignty over any territory, to the delimitation of international frontiers and boundaries and to the name of any territory, city or area.\n\nCover image: ©TippaPatt/Shutterstock.\n\n© OECD 2023\n\nThe use of this work, whether digital or print, is governed by the Terms and Conditions to be found at",
    "# EXECUTIVE SUMMARY\n\nGenerative AI has rapidly entered public discourse.\n\nGenerative AI has entered into public consciousness and is increasingly present in peoples’ everyday conversations worldwide.\n\nThe number of news articles and tweets related to ‘generative AI’ grew eight-fold over just six months.\n\nGrowth in generative AI research, including its open-source code development, preceded the surge in investments.\n\nThe widespread awareness and rapid uptake of generative AI have been enabled by steady, incremental progress in both research and code development.\n\nFundamental innovations such as the ‘Transformers’ architectures, contributions of the open-source community, alongside improvement in computing power have paved the way for the proliferation of large language models as well as other type of generative AI models.\n\nScientific publications and open-source code development on generative AI have grown remarkably since 2017, and this trend accelerated in 2023.\n\nVenture capital investments in generative AI have skyrocketed and were estimated at USD 12 billion globally in the first half of 2023 alone.\n\nScientific publications and software, including open-source code, related to generative AI have seen a parallel remarkable surge since 2017, with this trend further accelerating in 2023.\n\nRapid advances in generative AI are driven by its expected potential to drive productivity gains and to promote innovation and entrepreneurship, as well as to unlock solutions to global challenges.\n\nIn a questionnaire administered in Q3 2023, G7 members unanimously saw productivity gains, promoting innovation and entrepreneurship and unlocking solutions to global challenges, as some of the greatest opportunities of AI technologies worldwide, including for emerging and developing economies.\n\nG7 members also emphasized generative AI’s potential role to help address pressing societal challenges, such as improving healthcare and helping to solve the climate crisis, and to support progress towards achieving the Sustainable Development Goals (SDGs).\n\nYet, generative AI’s potential benefits come with risks.\n\nThe capacity of generative AI to exacerbate the challenges of disinformation and manipulation of opinions is considered by G7 members as one of the major threats stemming from generative AI, alongside risks of intellectual property rights infringement and privacy breaches.\n\nEarly efforts to track AI incidents found one thousand distinct incidents and hazards related to generative AI, based on roughly 5,600 news articles dated from January to July 2023.\n\nAs these risks evolve rapidly, their management and mitigation is at the top of the agenda for G7 governments.\n\nResponsible use of generative AI, addressing disinformation, safeguarding intellectual property rights, and governing generative AI are among the top priorities for G7 policymakers and require international cooperation with like-minded partners.\n\nOther urgent and important issues emphasized by G7 members include privacy and data governance, transparency, fairness and bias, human and fundamental rights, security and robustness of AI systems, and impacts on the functioning of democracy.\n\nG7 jurisdictions are evaluating their respective responses to generative AI, as well as the policy gaps.\n\nCountries are leveraging existing and forthcoming legal and policy frameworks and developing guidelines or regulation to address risks related to generative AI.\n\nNational initiatives are also being strengthened to seize its opportunities.\n\nNew issues raised by generative AI appear to affect specific sectors in particular, such as education, media, and the workplace.\n\nG7 members are aligned on the need to provide effective tools for safety, quality control, and capacity and trust building for generative AI.\n\nSafety, quality control, capacity and trust building for generative AI were seen as among the most urgent and important international action the G7 could undertake.\n\nEngaging in dialogue was also considered to be most urgent, and developing voluntary codes of conduct was identified as among the most important actions.",
    "# INTRODUCTION\n\nIn April 2023, Japan hosted the G7 Digital and Technology Ministers' Meeting in Takasaki.\n\nThe Ministers agreed on the Ministerial Declaration, which emphasizes the importance of international discussions on the interoperability between different AI governance frameworks and stock-taking of the opportunities and challenges brought by generative AI.\n\nIn their Declaration of April 2023, the G7 Digital and Technology Ministers recognized “the need to take stock in the near term of the opportunities and challenges of [generative AI technologies] and to continue promoting safety and trust as these technologies develop” and therefore undertook “to convene future G7 discussions on generative AI which could include topics such as governance, how to safeguard intellectual property rights including copyright, promote transparency, address disinformation, including foreign information manipulation, and how to responsibly utilize these technologies.” (Paragraph 47).\n\nThe Ministerial discussion on AI was escalated to the Leaders’ discussion at the G7 Summit meeting in May, hosted in Hiroshima.\n\nThe Leaders agreed to task their Ministers to establish the “Hiroshima AI process”, where G7 members continue the discussion on generative AI in an inclusive manner.\n\nGenerative AI can be understood as a form of AI model specifically intended to produce new digital material as an output (including text, images, audio, video, software code), including when such AI models are used in applications and their user interfaces.\n\nThese are typically constructed as machine learning systems that have been trained on massive amounts of data.\n\nThey work by predicting words, pixels, waveforms, data points, etc.\n\nthat would resemble the models’ training data, often in response to prompts (OECD, 2023[1]), (OECD, Forthcoming[2]).\n\nTo support the G7 Hiroshima AI Process launched by G7 Leaders, Japan circulated a questionnaire in June 2023 to G7 members.\n\nThe questionnaire aimed at taking stock of G7 members’ existing and planned policy initiatives and considerations on the main opportunities and risks associated with generative AI.\n\nIt was organized in the following four inter-related sections:\n- Scoping opportunities and risks\n- Priorities in terms of values-based principles\n- Potential collective international approach\n- National and regional initiatives\n\nThe questionnaire was composed mostly of closed questions, which provided respondents with selected options to rank or to choose from, i.e.\n\na list of opportunities or risks related to generative AI, priorities among the five areas identified in the G7 Leaders’ Statement and among the OECD AI Principles, and a selection of possible policy actions that G7 members could recommend.\n\nOpen questions allowed respondents to report on national or regional initiatives in G7 jurisdictions pertaining to generative AI.\n\nThis report presents advancements in generative AI based on data from the OECD.AI Policy Observatory and the OECD AI Incident Monitor (section 1), and analyses responses to the questionnaire circulated to G7 members (section 2).",
    "## Generative AI trends\n\nGenerative AI has taken center stage in the public, academic, and political discussions surrounding AI.\n\nIt is predicted to create significant economic value.\n\nCompanies have begun to adopt the technology to create new business opportunities, and start-ups are competing for venture capital.\n\nGenerative AI has entered into public awareness and is increasingly present in everyday conversations worldwide, as evidenced by the surge in related news articles and tweets.\n\nBoth indicators show an eight-fold increase in a mere six-month period, with new articles on generative AI increasing from 1.6 thousand in the last quarter of 2022 to almost 14 thousand in the second quarter of 2023, and tweets about generative AI reaching 57 thousand in March 2023, up from an initial 7 thousand in October 2022 (Figure 1.1, panels a and b, respectively).",
    "### Figure 1.1.\n\nGenerative AI has rapidly entered public discourse\n\nNumber of news articles globally on generative AI and related topics\n\nNumber of tweets globally on generative AI and related topics\n\nNote: News articles related to generative AI are a subset of AI-related articles that include the following wikidata concepts and all of their descendants: generative AI, transformer models, language model, and generative model.\n\nGenerative AI tweets contain “generative AI” or #generativeAI.\n\nNews articles data has been adjusted using a technique called quarterly smoothing to make it easier to compare and understand the trends over time.\n\nSource: OECD.AI, using data from Event Registry for news articles and from X for tweets.\n\nResearch and venture capital investments into generative AI development have also seen substantial increase.\n\nScientific publications pertaining to generative AI have grown fivefold since 2019, which can be attributed to heightened interest in fundamental innovations such as transformer models and advancements in computing power, and which paved the way for the proliferation of large language models (Figure 1.2, panel a).\n\nVenture capital investments in generative AI in the first half of 2023 reached a total of USD 12 billion globally (Figure 1.2, panel b).\n\nPeaks in venture capital investments in 2019 and 2023 reflect Microsoft’s USD 1 billion and USD 10 billion investments in OpenAI, respectively.",
    "### Figure 1.2.\n\nThe growth of generative AI research precedes the surge in investments\n\nNumber of scientific publications globally on generative AI and related topics\n\nSum of global venture capital investments on generative AI startups\n\nNote: Scientific publications related to generative AI are a subset of AI-related publications that include the following wikidata concepts and all of their descendants: generative AI, transformer models, language model, and generative model.\n\nVC investments related to generative AI capture startups that include concepts like generative AI, generative adversarial network, text generation, image generation, audio generation, and generative model in their company descriptions.\n\nQuarterly data smoothing is applied to both datasets to remove noise.\n\nSource: OECD.AI, using data from OpenAlex for research publications and from Preqin for venture capital investments.\n\nThe open-source community has traditionally been a driving force behind AI advancements.\n\nThis trend appears to continue in the context of generative AI.\n\nSince October 2022, there has been a significant increase in the availability and development of open-source AI models dedicated to generative AI systems, as shown by the rising number of text generation models uploaded to the Hugging Face repository in the recent months (Figure 1.3, panel a).\n\nIn contrast, the upswing in open-source code development pertaining to generative AI on GitHub starts in 2017 and follows a more gradual trajectory.\n\nThis nuanced pattern suggests that the progress leading to generative AI has been the result of steady and incremental advancements in code development (Figure 1.3, panel b).",
    "### Figure 1.3.\n\nGenerative AI open-source code has seen more gradual growth than open-source models\n\nUpsurge in the number of open-source generative AI models on Hugging Face\n\nGrowth in the number of open-source generative AI code development projects on GitHub\n\nNote: Open-source code related to generative AI are a subset of AI-related GitHub repositories that include the following wikidata concepts and all of their descendants: generative AI, transformer models, language model, and generative model.\n\nQuarterly data smoothing is applied to GitHub data to remove noise.\n\nSource: OECD.AI, using data from Hugging Face for open-source models and from GitHub for open-source code.",
    "## Incidents and hazards related to generative AI\n\nWhile generative AI has the potential to revolutionize industry and society in positive ways, the use of the technology also poses risks to individuals and societies.\n\nFor example, generative AI can be exploited for malicious purposes, leading to serious negative consequences such as the propagation of disinformation and the creation of manipulated content like deepfakes.\n\nThe recognition of this multi-purpose nature of AI technologies and how they can be deployed – including generative AI – has prompted the OECD to develop a global AI Incidents Monitor, designed to furnish real-time evidence on AI risks to inform policy decisions.\n\nThis is achieved through the scrutiny of real-world incidents and hazards in real time as reported by reputable news outlets.\n\nThe term \"incident\" encompasses a collection of one or more news articles covering the same event.\n\nOver the period from January to July 2023, approximately one thousand incidents and hazards related to generative AI were reported across roughly 5,600 news articles (Figure 1.4).\n\nWhile the Monitor is still under development (its release is expected in November 2023), the initial findings shed some light on the potential risks posed by generative AI systems and can help contribute to shaping a safer AI landscape for the future.",
    "### Figure 1.4.\n\nGenerative AI-related incidents and hazards reported by reputable news outlets have grown exponentially\n\nNote: The blue line shows the real count of incidents and hazards reported each month.\n\nThe red line displays the same data but adjusted using quarterly smoothing.\n\nThe peak in 2019 relates to a surge in the reporting of incidents and hazards related to deepfake technology.\n\nSource: OECD.AI, AI Incidents Monitor (forthcoming), using data from Event Registry.",
    "# GENERATIVE AI FROM A G7 PERSPECTIVE\n\nThis section presents the results of the questionnaire developed to support a stocktaking to help guide G7 discussions on common policy priorities with regard to generative AI.\n\nAs most questions provided a list of options to rank or choose from, the rankings shown do not suggest e.g., most important to least important priorities, but a snapshot of country responses on the top priorities outlined in the questionnaire at a given time.",
    "## Opportunities and risks for G7 members\n\nProductivity gains and promoting innovation and entrepreneurship were viewed by all respondents as among the major opportunities made possible by generative AI, among opportunities outlined in the questionnaire.\n\nImproving healthcare followed closely, as did helping to solve the climate crisis (FIGURE 2.1).\n\nStrengthening the traceability and the transparency of democratic processes and improving citizens’ access to public services were also mentioned as other opportunities.",
    "### Figure 2.1.\n\nTop five OPPORTUNITIES of generative AI to help achieve national and regional goals\n\nNumber of G7 members that selected (five) specific opportunities from a pre-populated drop-down list\n\nNote: The figure aggregates responses from seven respondents to the question: “From your country or region’s perspective, what are the top five opportunities generative AI presents to help achieve national and regional goals?\n\n(Please select five options)”.\n\nDisinformation and the associated manipulation of opinions were viewed by all respondents as the dominant risk posed by generative AI, among risks outlined in the questionnaire.\n\nMost G7 members also considered intellectual property rights infringement as well as threats to privacy as major risks (Figure 2.2).\n\nThreats to security (including cybersecurity); manipulation and improper use of data; and threats to human rights were also highlighted as additional risks.",
    "### Figure 2.2.\n\nTop five RISKS presented by generative AI in achieving national and regional goals\n\nNumber of G7 members that selected (five) specific risks from a pre-populated drop-down list\n\nNote: The figure aggregates responses from seven respondents to the question: “From your country or region’s perspective, what are the top five risks generative AI presents to achieving national and regional goals?\n\n(Please select five options)”.",
    "### MOST URGENT AND IMPORTANT PRIORITIES REGARDING GENERATIVE AI FOR G7 MEMBERS\n\nThe ‘responsible’ use of generative AI technologies was widely viewed as the most “urgent” priority for policy among the priorities highlighted in the G7 statement.\n\nThis was followed closely by addressing disinformation and by governing generative AI appropriately (Figure 2.3).\n\nThreats to cybersecurity and biosecurity were also indicated among the most urgent priorities regarding generative AI.",
    "### Figure 2.3.\n\nMost URGENT priorities regarding generative AI among the five priorities highlighted in the G7 Leader’s statement\n\nNumber of G7 members that ranked specific priorities in terms of urgency from a pre-populated drop-down list\n\nNote: The figure aggregates responses from seven respondents to the question: “From a policy perspective what do you see as the most urgent and the most important priorities regarding generative AI?\n\n(Please rank the concepts below by order of urgency and importance.\n\nDifferent concepts can have the same priority level)”.\n\nThe ‘responsible’ use of generative AI technologies was also viewed as the most “important” priority for policy, followed by governance and by addressing disinformation.\n\nWhile “importance” and “urgency” of issues were ranked slightly differently, they highlighted the same overall priorities (Figure 2.4).\n\nThe threat to cybersecurity was also indicated as an additional important priority, and the threat to biosecurity as one of the most important priorities in the field of generative AI.",
    "### Figure 2.4.\n\nMost IMPORTANT priorities regarding generative AI among the five priorities highlighted in the G7 Leader’s statement\n\nNumber of G7 members that ranked specific priorities in terms of importance from a pre-populated drop-down list\n\nNote: The figure aggregates response from seven respondents to the question: “From a policy perspective what do you see as the most urgent and the most important priorities regarding generative AI?\n\n(Please rank the concepts below by order of urgency and importance.\n\nDifferent concepts can have the same priority level)”.\n\nIn addition, privacy and data governance were prioritised as the most urgent ‘other’ issues to address outlined in the questionnaire, followed by fairness and bias, and human and fundamental rights (Figure 2.5).",
    "### Figure 2.5.\n\nMost URGENT priorities regarding generative AI: Other priorities highlighted in the OECD AI Principles\n\nNumber of G7 members that ranked specific priorities in terms of urgency from a pre-populated drop-down list\n\nNote: The figure aggregates responses from seven respondents to the question: “From a policy perspective what do you see as the most urgent and the most important priorities regarding generative AI?\n\n(Please rank the concepts below by order of urgency and importance.\n\nDifferent concepts can have the same priority level)”.\n\nHuman and fundamental rights, security and robustness of AI systems, democratic values, and privacy and data governance were viewed as the most important ‘other’ priorities outlined in the questionnaire (Figure 2.6).",
    "### Figure 2.6.\n\nMost IMPORTANT priorities regarding generative AI: Other priorities highlighted in the OECD AI Principles\n\nNumber of G7 members that ranked specific priorities in terms of importance from a pre-populated drop-down list\n\nNote: The figure aggregates responses from seven respondents to the question: “From a policy perspective what do you see as the most urgent and the most important priorities regarding generative AI?\n\n(Please rank the concepts below by order of urgency and importance.\n\nDifferent concepts can have the same priority level)”.",
    "### MOST SIGNIFICANT GAPS IN EXISTING POLICIES OR POLICIES UNDERWAY TO ADDRESS THE CHALLENGES OF GENERATIVE AI IN G7 JURISDICTIONS\n\nDifferent G7 members’ perceived policy gaps were diverse, with different G7 members highlighting disinformation, transparency, and responsible use as their most significant gaps among those outlined in the questionnaire (Figure 2.7).",
    "### Figure 2.7.\n\nGaps in G7 members’ existing or underway policies to successfully address challenges of generative AI\n\nSelf-reported gaps according to the presented challenges\n\nNote: The figure aggregates responses from seven respondents to the question: “How significant are the gaps you see in your policies (existing or underway) to successfully address the following challenges of generative AI?”.",
    "### Applying existing (or forthcoming) legal frameworks, and evaluating policy gaps\n\nRespondent countries are leveraging existing as well as forthcoming legal frameworks.\n\nThey are also assessing the scale and extent of gaps pertaining to generative AI challenges.\n\nCanada, under the proposed Artificial Intelligence and Data Act (AIDA), sets out a risk-based regulatory framework for the responsible design, development, and use of AI systems in the private sector, including generative AI systems.\n\nFrance is analysing the new challenges posed by generative AI regarding existing legislation (such as GDPR and the EU Copyright Directive) and noted that the EU AI Act is expected to address some of the issues, such as governance and responsible use of AI systems.\n\nGermany is working on closer strategic alignment across government entities and on developing relevant procedures.\n\nItaly In the light of the advent of generative AI (e.g.\n\nChatGPT), provisional agreement of the European Parliament on the AI Act and increased ethical concerns and risks regarding Generative AI, in early July 2023, the Department for Digital Transformation - Presidency of the Council of Ministers, presented a proposal for revision of the \"Strategic Plan for Artificial Intelligence 2021\".\n\nThe proposed revision is nearing completion and will be subject to a public consultation procedure starting as of 30 September.\n\nThe entry into force of the new \"Strategic Plan for AI'' is scheduled for December 31, 2023.\n\nAlso, the government established a Permanent Committee on AI within the Inter-Ministerial Committee on Digital Transition; the committee includes experts from universities, research centres and the associations of Italian companies.\n\nIn March 2023, the Italian Data Protection Authority - DPA imposed an immediate temporary limitation on the processing of Italian users’ data by OpenAI.\n\nIn its order, DPA highlighted that no information was provided to users and data subjects whose data were collected by Open AI; more importantly, it underlined that there was no legal basis underpinning the massive collection and processing of personal data in order to ‘train’ the algorithms on which the platform relies.\n\nFurthermore, there was no age verification for minors.\n\nJapan indicated that it is applying existing legal frameworks (e.g., the Penal Code and the enforcement system) and existing guidelines (e.g., the AI R&D Guidelines, the AI Utilization Guidelines, or the Governance Guidelines for Implementation of AI Principles), although these might be challenged by new issues from generative AI.\n\nAs new issues arise with the emergence of generative AI, it considers integrating and revising the guidelines into a unified and easy to understand guideline for developers, providers, users, and other business.\n\nThe United Kingdom is working across government to assess scale and extent of gaps in existing mitigation measures and is exploring further measures at every stage of the AI supply chain.\n\nSimilarly, in the United States, pursuant to an existing executive order mandating certain principles for federal AI activities, the Office of Management and Budget is developing guidance that will establish specific policies that federal departments and agencies must follow to strengthen AI governance, advance AI procurement, and manage algorithmic risk to safeguard American people’s rights and safety.\n\nThe U.S. Department of Commerce’s National Institute of Standards and Technology released an AI Risk Management Framework (AI RMF) in January 2023 and in June 2023 launched a Generative AI Public Working Group to develop a profile of AI RMF for generative AI systems.\n\nThe proposed European Union’s Artificial Intelligence Act (EU AI Act) aims at promoting the development and uptake of AI while addressing potential risks certain AI systems, including generative AI, can pose to safety and fundamental rights.\n\nIn addition, the EU will further enhance its regulatory toolbox, with the revision of existing legislation, like for example the recently adopted Machinery Regulation, as well as the proposed (and currently in the legislative process) revision of the Product Liability Directive and the proposal for the Cyber Resilience Act.\n\nIn addition to regulatory measures, the EU is also pursuing the EU Coordinated Plan on AI with Member States and working on the AI Pact.\n\nThe Pact would encourage companies to voluntarily communicate the processes and practices they are putting in place to prepare for compliance with the EU AI Act and ensure that the design, development and use of AI is trustworthy.",
    "### Developing guidelines, and establishing new guidance as well as governance bodies\n\nThe Treasury Board of Canada’s Secretariat (TBS) plans to issue guidelines on the use of generative AI in the federal government.\n\nThis ‘guide’ will provide federal institutions with guidance on the use of these tools.\n\nIt provides an overview of generative AI, identifies challenges and concerns relating to its use, puts forward principles for using it responsibly, and offers policy considerations and best practices.\n\nAdditionally, in early August, Canada launched roundtable session...",
    "## Introduction to Initiatives\n\nJapan plans to seek stakeholder feedback on a proposed Canadian code of practice for generative AI.\n\nThe code will provide voluntary guidance to companies developing and using generative AI systems, and it will help them to prepare their processes and products before formal regulation takes effect.\n\nSimilarly, the United States Office of Management and Budget is developing guidance that will establish specific policies that federal departments and agencies must follow to strengthen AI governance, advance AI procurement, and manage algorithmic risk to safeguard American people’s rights and safety.\n\nGermany reported undertaking several specific actions including:\n- Setting up an advisory centre for the use of AI in the public sector to address competence building and networking.\n\n- Establishing an AI Quality and Innovation Centre.\n\nThe European Union in the EU AI Act would also envisage certain governance structures and specific policies to ensure that design, development, and use of AI technologies, including generative AI, is trustworthy.",
    "## Highlighting the need for International Governance\n\nWhere current gaps cannot be filled by resorting to current legal systems, G7 members refer to the need to explore further mitigation measures, to take inspiration from other countries, and to coordinate measures at the international level to develop comprehensive and consistent approaches among countries.\n\nA G7 member referred to international mechanisms that seek to counter foreign disinformation, such as the OECD Mis/Dis Information Hub, the G7 Rapid Response Mechanism, and the Summit for Democracy (S4D) Information Integrity cohort.",
    "## Examples of Existing Applications\n\nExisting laws and policies, including consumer protection and privacy laws, apply to generative AI.\n\nSeveral G7 members reported that legislation in various sectors is applicable to, but also challenged by, generative AI.\n\nG7 members highlighted in particular the challenges posed to privacy, and intellectual property, including copyright.\n\nSome jurisdictions noted that compliance is already required by existing data protection legislation or intellectual property regimes.\n\nOthers highlighted that AI has raised new legal questions about the ownership of content created wholly or in part with AI, such as images and texts, as well as questions about how rights associated with training data affect the legal status of models’ output, and that these questions are being investigated at the national level.",
    "### Education\n\nSeveral G7 members noted that the education sector requires particular attention as it is already highly affected by generative AI, and this is expected to increase in the future.\n\nA G7 member is convening experts to work with the education sector to share and identify best practices and opportunities.",
    "### Workplace\n\nTwo G7 members highlighted that generative AI affects workplace-related matters.\n\nGenerative AI can speed up recruitment processes (e.g., through generative AI-powered chatbots), but if data is biased, this may negatively impact the fairness of recruitment processes.\n\nFurthermore, they noted that in the workplace, employees may use generative AI without appropriate guidance or regulation and expose sensitive or confidential corporate data and/or personal information to third parties outside the company.",
    "### Unpredictability, Adaptivity, Autonomy, and Multi-Purpose Nature\n\nSeveral G7 members indicated unpredictability of impact, adaptivity, autonomy, and a multi-purpose nature as characteristics of generative AI that challenge its regulation and governance.\n\nGenerative AI creates new disruptive innovation, impacting a broad range of inexperienced users and developers.\n\nThe technology’s wide field of application, increasing interactions between generative AI systems, and rapid technical developments cause high uncertainty and unpredictability for society.\n\nMany generative AI applications today have little ‘autonomy’, i.e., little capacity to make decisions or take actions on their own, without human oversight or direction.\n\nThey often produce content when instructed based on prompts.\n\nHowever, the output of generative AI can lead to autonomous or partly autonomous action programmed in traditional automation software whereby the output of the generative AI is acted upon.\n\nAs generative AI like large language models are increasingly used as autonomous generative ‘agents’ with plugins to connect them to third-party applications, they are becoming more autonomous.\n\nFor example, plugins enable language models to operate on recent data, including real-time information, such as stock prices or news articles, and to assist users in new ways, such as through autonomous ordering and booking.",
    "### Adaptivity of Generative AI\n\nThe ‘adaptivity’ of generative AI comes with difficulties for developers to understand the intent or logic that leads to systems’ outcomes.\n\nThis is because AI systems are ‘trained’ by inferring patterns and connections in data which are often not easily discernible to human programmers.\n\nThis mechanism allows generative AI systems to develop the ability to perform new tasks or forms of reasoning that the developers did not expect - a powerful source of capabilities, but also a barrier to intentionally designing or even fully understanding model capabilities.\n\nThis gap between the developers’ knowledge and intent and the system’s capabilities can complicate assigning responsibility for outcomes.\n\nThe adaptivity and multi-purpose nature of generative AI, and its ability to develop and push out content much more rapidly than has been the case before, may exacerbate bias and other risks in more contexts than previous systems, promoting or reinforcing stereotypical or harmful representations.",
    "### Lack of Transparency\n\nLack of transparency of generative AI, both in the development stage (developers being transparent about how they developed the system) and use (users being transparent about the fact that they are using a system), was raised by one G7 member as an issue that challenges regulation/governance.\n\nOne G7 member is developing legislation which provides transparency obligations, including for certain generative AI systems, which was given as an example seeking to address this concern.",
    "### Misinformation and Disinformation\n\nA G7 member stressed generative AI models’ ability to create synthetic content (e.g., deepfakes) at scale with little resources or expertise.\n\nIn particular, they noted that the next generation of interactive generative media will leverage targeted influence content that is highly personalized, localized, and conversational.\n\nAnother G7 member expressed concerns about the capacity of AI-generated content to influence human behavior, expression, and emotion at scale, as well as of content reflecting or promoting misinformation.\n\nMoreover, the member also warned of incorrect or fabricated content that is presented as a fact (i.e., “confidently wrong” or “hallucinated” output).",
    "## Regulatory Challenges\n\nTwo G7 members stated that training generative AI relies on vast amounts of publicly available data, which can be used without permission and may therefore not comply with data protection and copyright laws.\n\nTraining data thus gives rise to copyright issues when outputs resemble the original sources.\n\nAnother G7 member further noted the lack of clarity regarding what kind of training data is used as well as regarding the implications for consumer protection, and intellectual property (IP) protection and enforcement, particularly given the difficulty of constraining models from reproducing copyrighted content.\n\nThe member also stressed that regulation and governance regimes are not keeping pace with rapid advances in AI capabilities.",
    "### Regulatory Frameworks and Interoperability\n\nG7 members responded that there is a need to establish appropriate regulation and oversight.\n\nOne G7 member recalled that existing frameworks like the OECD AI principles require international norms, standards, and assessment processes.\n\nThe member hence called for international alignment and collaboration, including with developing countries.\n\nAnother G7 member stressed the importance of international governance and the need for international institutions to facilitate rapid, coordinated action among nations.\n\nSuch frameworks and standards would allow them to respond to currently unknown threats.\n\nOthers highlighted the need for precise and detailed principles to enable their implementation in G7 countries, and suggested principles could be applied through general agreements with generative AI system providers or through binding legislation if agreements cannot be reached.",
    "### Risks and Security\n\nSeveral G7 members stressed the importance of preventing the use of generative AI to create chemical or biological threats (e.g., viruses), or massive disinformation/misinformation (including from foreign actors), highlighting the need for international cooperation on these common challenges.\n\nCybersecurity concerns were also identified as a challenge posed by generative AI that requires international alignment and collaboration, calling for addressing international AI cybersecurity risks on a global level.\n\nOthers noted the risk of undermining social stability and pointed out the difficulty for policymakers to keep up with the rapid pace of technological developments.",
    "### Personal Data and Intellectual Property Rights\n\nA G7 member emphasized the need to take measures regarding the use of personal data as well as material protected by intellectual property rights.\n\nThese measures should consider, on one hand, the difference between data used in the process of training a model, and on the other hand, data used when the system interacts with the end user and generates content.\n\nOthers called for finding common ground at the international level on managing the data provided to train AI systems.\n\nA G7 member suggested that data provided by human operators should be traceable and withdrawable by the original provider.",
    "### Transparency\n\nThe need for transparency in both the development and use of generative AI was stressed by several G7 members.\n\nSome respondents mentioned the work led by the European Union (the EU AI Act) regarding responsibilities between actors within AI value chains.\n\nThese G7 members argued that foundational model providers should provide sufficient transparency for the providers of final “products” to place them on the market safely.\n\nThey also recalled that the proposed EU AI Act contains the following transparency provisions for generative AI systems:\n- A generative AI chatbot would be subject to transparency obligations;\n- A generative AI system that can be directly used for high-risk applications (e.g., the evaluation of job candidates), would have to fulfill the corresponding requirements for high-risk AI systems, which include transparency provisions.",
    "### Canada\n\n- **Artificial Intelligence and Data Act (AIDA):** AIDA was tabled in Canada’s Parliament in June 2022 and proposes the development of a risk-based national regulatory framework for the responsible design, development, and use of AI in Canada’s private sector.\n\n- **Proposed Canadian Code of Practice for Generative AI:** Canada is hosting roundtable sessions to seek stakeholder feedback on this proposed code.\n\nIt will provide voluntary guidance to companies developing and using AI systems, and help them prepare their processes and products before formal regulation takes effect.\n\n- **TBS Guide on the Use of Generative AI in the Government of Canada:** The guide provides guidance to federal institutions on their use of generative AI tools and outlines challenges, principles, and best practices.",
    "### EU\n\n- **Testing and Experimentation Facilities (TEFs):** The EU, in cooperation with member states, funds TEFs to support AI developers in bringing trustworthy AI to the market efficiently.\n\n- **Artificial Intelligence Act (AI Act):** Proposed by the European Commission in 2021, the AI Act is a legal framework aimed at ensuring trust in AI initiatives.",
    "## Future Artificial Intelligence Research (FAIR) Centre\n\nThe Future AI Intelligence Research (FAIR) project aims to help address the research questions, methodologies, models, technologies, and even ethical and legal rules for building human centric AI systems.\n\nFAIR constitutes a research network spread over the country and includes four research institutions (CNR, Fondazione Bruno Kessler, INFN, and IIT), 14 universities (Politecnico di Milano, Politecnico di Torino, Sapienza, Scuola Normale Superiore, SISSA, Università Bocconi, Università Campus Biomedico di Roma, Università della Calabria, Università di Bari, Università di Bologna, Università di Catania, Università di Napoli “Federico II,” Università di Pisa, Università di Trento) and seven companies (Bracco, Deloitte, Expert.ai, Intesa Sanpaolo, Leonardo, Lutech, STMicroelectronics).",
    "## AI Regulation White Paper\n\nThe White Paper sets out the UK’s context-based, proportionate, and adaptable approach to regulate AI, and draws on expertise of existing regulators; encouraging them to consider how best to govern AI in their own sectors.\n\nIt will enable the UK to achieve the right balance between responding to risks and maximising opportunities afforded by AI.",
    "## Centre for Data Ethics and Innovation (CDEI) Portfolio of AI Assurance Techniques\n\nThe portfolio features case studies of AI assurance techniques being applied by organisations using cutting-edge technologies across a range of sectors.\n\nThis will act as a valuable resource for those developing and procuring AI systems to understand how AI assurance techniques can help them measure, evaluate, and communicate trustworthiness of AI systems, as well as how techniques align with proposed regulatory principles identified in the UK’s AI Regulation White Paper.",
    "## AI Standards Hub\n\nIt aims to improve AI standards adoption and development by providing businesses, regulators, and civil society organisations in the UK with practical tools and information.\n\nMoreover, they need to apply AI standards effectively and contribute to their development.\n\nAI Standards Hub is part of the National AI Strategy and ultimately aims to increase the UK’s contribution to the development of global AI technical standards.",
    "## Voluntary Commitments\n\nTo make the most of AI’s potential, the United States is encouraging the AI industry to uphold the highest standards to ensure that innovation does not come at the expense of Americans’ rights and safety.\n\nThe White House secured voluntary commitments from leading AI companies to help move toward safe, secure, and transparent development of AI technology.\n\nThe commitments underscore three principles fundamental to the future of AI – safety, security, and trust – and mark a critical step toward developing responsible AI.",
    "## NIST Generative AI Public Working Group\n\nThis working group builds on the success of the NIST AI Risk Management Framework to address rapidly advancing AI technologies.\n\nThe Public Working Group on Generative AI will help address the opportunities and challenges associated with AI that can generate content, such as code, text, images, videos and music.\n\nThe public working group will also help NIST develop key guidance to help organizations address the special risks associated with generative AI technologies.",
    "## PCAST Generative AI Working Group\n\nThe President’s Council of Advisors on Science and Technology (PCAST) has launched a working group on generative artificial intelligence (AI) to help assess key opportunities and risks and provide input on how best to ensure that these technologies are developed and deployed as equitably, responsibly, and safely as possible.\n\nThe PCAST Working Group on Generative AI aims to build upon existing efforts by identifying additional needs and opportunities and making recommendations to the President for how best to address them.",
    "## ANNEX I\n\nARTIFICIAL INTELLIGENCE TECHNIQUES AND APPROACHES referred to in Article 3, point 1\n\n- Machine learning approaches, including supervised, unsupervised and reinforcement learning, using a wide variety of methods including deep learning;\n- Logic- and knowledge-based approaches, including knowledge representation, inductive (logic) programming, knowledge bases, inference and deductive engines, (symbolic) reasoning and expert systems;\n- Statistical approaches, Bayesian estimation, search and optimization methods.",
    "### Section A – List of Union harmonisation legislation based on the New Legislative Framework\n\n- Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24) [as repealed by the Machinery Regulation];\n- Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ L 170, 30.6.2009, p. 1);\n- Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft and personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);\n- Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of the laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);\n- Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive atmospheres (OJ L 96, 29.3.2014, p. 309);\n- Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the laws of the Member States relating to the making available on the market of radio equipment and repealing Directive 1999/5/EC (OJ L 153, 22.5.2014, p. 62);\n- Directive 2014/68/EU of the European Parliament and of the Council of 15 May 2014 on the harmonisation of the laws of the Member States relating to the making available on the market of pressure equipment (OJ L 189, 27.6.2014, p. 164);\n- Regulation (EU) 2016/424 of the European Parliament and of the Council of 9 March 2016 on cableway installations and repealing Directive 2000/9/EC (OJ L 81, 31.3.2016, p. 1);\n- Regulation (EU) 2016/425 of the European Parliament and of the Council of 9 March 2016 on personal protective equipment and repealing Council Directive 89/686/EEC (OJ L 81, 31.3.2016, p. 51);\n- Regulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016 on appliances burning gaseous fuels and repealing Directive 2009/142/EC (OJ L 81, 31.3.2016, p. 99);\n- Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1);\n- Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176).",
    "### Section B.\n\nList of other Union harmonisation legislation\n\n- Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72).\n\n- Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the approval and market surveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52);\n- Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1);\n- Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146);\n- Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44).\n\n- Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance of motor vehicles and their trailers, and of systems, components and separate technical units intended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018, p. 1);\n- Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU) No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1);\n- Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1), in so far as the design, production and placing on the market of aircrafts referred to in points (a) and (b) of Article 2(1) thereof, where it concerns unmanned aircraft and their engines, propellers, parts and equipment to control them remotely, are concerned.",
    "## ANNEX III\n\nHIGH-RISK AI SYSTEMS REFERRED TO IN ARTICLE 6(2)\n\nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:\n\n- **Biometric identification and categorisation of natural persons**:\n  - AI systems intended to be used for the ‘real-time’ and ‘post’ remote biometric identification of natural persons;\n\n- **Management and operation of critical infrastructure**:\n  - AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity.\n\n- **Education and vocational training**:\n  - AI systems intended to be used for the purpose of determining access or assigning natural persons to educational and vocational training institutions;\n  - AI systems intended to be used for the purpose of assessing students in educational and vocational training institutions and for assessing participants in tests commonly required for admission to educational institutions.\n\n- **Employment, workers management and access to self-employment**:\n  - AI systems intended to be used for recruitment or selection of natural persons, notably for advertising vacancies, screening or filtering applications, evaluating candidates in the course of interviews or tests;\n  - AI intended to be used for making decisions on promotion and termination of work-related contractual relationships, for task allocation and for monitoring and evaluating performance and behavior of persons in such relationships.\n\n- **Access to and enjoyment of essential private services and public services and benefits**:\n  - AI systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibility of natural persons for public assistance benefits and services, as well as to grant, reduce, revoke, or reclaim such benefits and services;\n  - AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score, with the exception of AI systems put into service by small scale providers for their own use;\n  - AI systems intended to be used to dispatch, or to establish priority in the dispatching of emergency first response services, including by firefighters and medical aid.\n\n- **Law enforcement**:\n  - AI systems intended to be used by law enforcement authorities for making individual risk assessments of natural persons in order to assess the risk of a natural person for offending or reoffending or the risk for potential victims of criminal offences;\n  - AI systems intended to be used by law enforcement authorities as polygraphs and similar tools or to detect the emotional state of a natural person;\n  - AI systems intended to be used by law enforcement authorities to detect deep fakes as referred to in article 52(3);\n  - AI systems intended to be used by law enforcement authorities for evaluation of the reliability of evidence in the course of investigation or prosecution of criminal offences;\n  - AI systems intended to be used by law enforcement authorities for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups;\n  - AI systems intended to be used by law enforcement authorities for profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of detection, investigation or prosecution of criminal offences;\n  - AI systems intended to be used for crime analytics regarding natural persons, allowing law enforcement authorities to search complex related and unrelated large data sets available in different data sources or in different data formats in order to identify unknown patterns or discover hidden relationships in the data.\n\n- **Migration, asylum and border control management**:\n  - AI systems intended to be used by competent public authorities as polygraphs and similar tools or to detect the emotional state of a natural person;\n  - AI systems intended to be used by competent public authorities to assess a risk, including a security risk, a risk of irregular immigration, or a health risk, posed by a natural person who intends to enter or has entered into the territory of a Member State;\n  - AI systems intended to be used by competent public authorities for the verification of the authenticity of travel documents and supporting documentation of natural persons and detect non-authentic documents by checking their security features;\n  - AI systems intended to assist competent public authorities for the examination of applications for asylum, visa and residence permits and associated complaints with regard to the eligibility of the natural persons applying for a status.",
    "- **Administration of justice and democratic processes**:\n  - AI systems intended to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts.",
    "## ANNEX IV\n\nTECHNICAL DOCUMENTATION referred to in Article 11(1)\n\nThe technical documentation referred to in Article 11(1) shall contain at least the following information, as applicable to the relevant AI system:\n\n1.\n\n**A general description of the AI system including**:\n   - its intended purpose, the person/s developing the system the date and the version of the system;\n   - how the AI system interacts or can be used to interact with hardware or software that is not part of the AI system itself, where applicable;\n   - the versions of relevant software or firmware and any requirement related to version update;\n   - the description of all forms in which the AI system is placed on the market or put into service;\n   - the description of hardware on which the AI system is intended to run;\n   - where the AI system is a component of products, photographs or illustrations showing external features, marking and internal layout of those products;\n   - instructions of use for the user and, where applicable installation instructions;\n\n2.\n\n**A detailed description of the elements of the AI system and of the process for its development, including**:\n   - the methods and steps performed for the development of the AI system, including, where relevant, recourse to pre-trained systems or tools provided by third parties and how these have been used, integrated or modified by the provider;\n   - the design specifications of the system, namely the general logic of the AI system and of the algorithms; the key design choices including the rationale and assumptions made, also with regard to persons or groups of persons on which the system is intended to be used; the main classification choices; what the system is designed to optimise for and the relevance of the different parameters; the decisions about any possible trade-off made regarding the technical solutions adopted to comply with the requirements set out in Title III, Chapter 2;\n   - the description of the system architecture explaining how software components build on or feed into each other and integrate into the overall processing; the computational resources used to develop, train, test and validate the AI system;\n   - where relevant, the data requirements in terms of datasheets describing the training methodologies and techniques and the training data sets used, including information about the provenance of those data sets, their scope and main characteristics; how the data was obtained and selected; labelling procedures (e.g.\n\nfor supervised learning), data cleaning methodologies (e.g.\n\noutliers detection);\n   - assessment of the human oversight measures needed in accordance with Article 14, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the users, in accordance with Articles 13(3)(d);\n\n3.\n\n**Where applicable, a detailed description of pre-determined changes to the AI system and its performance, together with all the relevant information related to the technical solutions adopted to ensure continuous compliance of the AI system with the relevant requirements set out in Title III, Chapter 2**;\n\n4.\n\n**The validation and testing procedures used, including information about the validation and testing data used and their main characteristics; metrics used to measure accuracy, robustness, cybersecurity and compliance with other relevant requirements set out in Title III, Chapter 2 as well as potentially discriminatory impacts; test logs and all test reports dated and signed by the responsible persons, including with regard to pre-determined changes as referred to under point (f)**.\n\n5.\n\n**Detailed information about the monitoring, functioning and control of the AI system, in particular with regard to**:  \n   - its capabilities and limitations in performance, including the degrees of accuracy for specific persons or groups of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose;\n   - the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rights and discrimination in view of the intended purpose of the AI system; the human oversight measures needed in accordance with Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the users; specifications on input data, as appropriate;\n\n6.\n\n**A detailed description of the risk management system in accordance with Article 9**;\n\n7.\n\n**A description of any change made to the system through its lifecycle**;\n\n8.",
    "**A detailed description of the risk management system in accordance with Article 9**;\n\n7.\n\n**A description of any change made to the system through its lifecycle**;\n\n8.\n\n**A list of the harmonised standards applied in full or in part the references of which have been published in the Official Journal of the European Union; where no such harmonised standards have been applied, a detailed description of the solutions adopted to meet the requirements set out in Title III, Chapter 2, including a list of other relevant standards and technical specifications applied**;\n\n9.\n\n**A copy of the EU declaration of conformity**;\n\n10.\n\n**A detailed description of the system in place to evaluate the AI system performance in the post-market phase in accordance with Article 61, including the post-market monitoring plan referred to in Article 61(3)**.",
    "## ANNEX V\n\nEU DECLARATION OF CONFORMITY\n\nThe EU declaration of conformity referred to in Article 48, shall contain all of the following information:\n\n1.\n\nAI system name and type and any additional unambiguous reference allowing identification and traceability of the AI system;\n2.\n\nName and address of the provider or, where applicable, their authorised representative;\n3.\n\nA statement that the EU declaration of conformity is issued under the sole responsibility of the provider;\n4.\n\nA statement that the AI system in question is in conformity with this Regulation and, if applicable, with any other relevant Union legislation that provides for the issuing of an EU declaration of conformity;\n5.\n\nReferences to any relevant harmonised standards used or any other common specification in relation to which conformity is declared;\n6.\n\nWhere applicable, the name and identification number of the notified body, a description of the conformity assessment procedure performed and identification of the certificate issued;\n7.\n\nPlace and date of issue of the declaration, name and function of the person who signed it as well as an indication for, and on behalf of whom, that person signed, signature.",
    "## ANNEX VI\n\nCONFORMITY ASSESSMENT PROCEDURE BASED ON INTERNAL CONTROL\n\nThe conformity assessment procedure based on internal control is the conformity assessment procedure based on points 2 to 4.\n\n1.\n\nThe provider verifies that the established quality management system is in compliance with the requirements of Article 17.\n\n2.\n\nThe provider examines the information contained in the technical documentation in order to assess the compliance of the AI system with the relevant essential requirements set out in Title III, Chapter 2.\n\n3.\n\nThe provider also verifies that the design and development process of the AI system and its post-market monitoring as referred to in Article 61 is consistent with the technical documentation.",
    "### Overview\n\nThe approved quality management system for the design, development and testing of AI systems pursuant to Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5.\n\nThe technical documentation of the AI system shall be examined in accordance with point 4.",
    "### Quality management system\n\n- The application of the provider shall include:\n  - the name and address of the provider and, if the application is lodged by the authorised representative, their name and address as well;\n  - the list of AI systems covered under the same quality management system;\n  - the technical documentation for each AI system covered under the same quality management system;\n  - the documentation concerning the quality management system which shall cover all the aspects listed under Article 17;\n  - a description of the procedures in place to ensure that the quality management system remains adequate and effective;\n  - a written declaration that the same application has not been lodged with any other notified body.\n\n- The quality management system shall be assessed by the notified body, which shall determine whether it satisfies the requirements referred to in Article 17.\n\n- The decision shall be notified to the provider or its authorised representative.\n\n- The notification shall contain the conclusions of the assessment of the quality management system and the reasoned assessment decision.\n\n- The quality management system as approved shall continue to be implemented and maintained by the provider so that it remains adequate and efficient.\n\n- Any intended change to the approved quality management system or the list of AI systems covered by the latter shall be brought to the attention of the notified body by the provider.\n\n- The proposed changes shall be examined by the notified body, which shall decide whether the modified quality management system continues to satisfy the requirements referred to in point 3.2 or whether a reassessment is necessary.\n\n- The notified body shall notify the provider of its decision.\n\nThe notification shall contain the conclusions of the examination of the changes and the reasoned assessment decision.",
    "### Control of the technical documentation\n\n- In addition to the application referred to in point 3, an application with a notified body of their choice shall be lodged by the provider for the assessment of the technical documentation relating to the AI system which the provider intends to place on the market or put into service and which is covered by the quality management system referred to under point 3.\n\n- The application shall include:\n  - the name and address of the provider;\n  - a written declaration that the same application has not been lodged with any other notified body;\n  - the technical documentation referred to in Annex IV.\n\n- The technical documentation shall be examined by the notified body.\n\nTo this purpose, the notified body shall be granted full access to the training and testing datasets used by the provider, including through application programming interfaces (API) or other appropriate means and tools enabling remote access.\n\n- In examining the technical documentation, the notified body may require that the provider supplies further evidence or carries out further tests so as to enable a proper assessment of conformity of the AI system with the requirements set out in Title III, Chapter 2.\n\nWhenever the notified body is not satisfied with the tests carried out by the provider, the notified body shall directly carry out adequate tests, as appropriate.\n\n- Where necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2 and upon a reasoned request, the notified body shall also be granted access to the source code of the AI system.\n\n- The decision shall be notified to the provider or its authorised representative.\n\nThe notification shall contain the conclusions of the assessment of the technical documentation and the reasoned assessment decision.\n\n- Where the AI system is in conformity with the requirements set out in Title III, Chapter 2, an EU technical documentation assessment certificate shall be issued by the notified body.\n\nThe certificate shall indicate the name and address of the provider, the conclusions of the examination, the conditions (if any) for its validity and the data necessary for the identification of the AI system.\n\n- The certificate and its annexes shall contain all relevant information to allow the conformity of the AI system to be evaluated, and to allow for control of the AI system while in use, where applicable.\n\n- Where the AI system is not in conformity with the requirements set out in Title III, Chapter 2, the notified body shall refuse to issue an EU technical documentation assessment certificate and shall inform the applicant accordingly, giving detailed reasons for its refusal.\n\n- Where the AI system does not meet the requirement relating to the data used to train it, re-training of the AI system will be needed prior to the application for a new conformity assessment.\n\nIn this case, the reasoned assessment decision of the notified body refusing to issue the EU technical documentation assessment certificate shall contain specific considerations on the quality data used to train the AI system, notably on the reasons for non-compliance.\n\n- Any change to the AI system that could affect the compliance of the AI system with the requirements or its intended purpose shall be approved by the notified body which issued the EU technical documentation assessment certificate.\n\nThe provider shall inform such notified body of its intention to introduce any of the above-mentioned changes or if it becomes otherwise aware of the occurrence of such changes.\n\nThe intended changes shall be assessed by the notified body which shall decide whether those changes require a new conformity assessment in accordance with Article 43(4) or whether they could be addressed by means of a supplement to the EU technical documentation assessment certificate.\n\nIn the latter case, the notified body shall assess the changes, notify the provider of its decision and, where the changes are approved, issue to the provider a supplement to the EU technical documentation assessment certificate.",
    "### Surveillance of the approved quality management system\n\n- The purpose of the surveillance carried out by the notified body referred to in Point 3 is to make sure that the provider duly fulfils the terms and conditions of the approved quality management system.\n\n- For assessment purposes, the provider shall allow the notified body to access the premises where the design, development, testing of the AI systems is taking place.\n\nThe provider shall further share with the notified body all necessary information.\n\n- The notified body shall carry out periodic audits to make sure that the provider maintains and applies the quality management system and shall provide the provider with an audit report.\n\nIn the context of those audits, the notified body may carry out additional tests of the AI systems for which an EU technical documentation assessment certificate was issued.",
    "## ANNEX VIII\n\nINFORMATION TO BE SUBMITTED UPON THE REGISTRATION OF HIGH-RISK AI SYSTEMS IN ACCORDANCE WITH ARTICLE 51\n\nThe following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to be registered in accordance with Article 51.\n\n1.\n\nName, address and contact details of the provider;\n2.\n\nWhere submission of information is carried out by another person on behalf of the provider, the name, address and contact details of that person;\n3.\n\nName, address and contact details of the authorised representative, where applicable;\n4.\n\nAI system trade name and any additional unambiguous reference allowing identification and traceability of the AI system;\n5.\n\nDescription of the intended purpose of the AI system;\n6.\n\nStatus of the AI system (on the market, or in service; no longer placed on the market/in service, recalled);\n7.\n\nType, number and expiry date of the certificate issued by the notified body and the name or identification number of that notified body, when applicable;\n8.\n\nA scanned copy of the certificate referred to in point 7, when applicable;\n9.\n\nMember States in which the AI system is or has been placed on the market, put into service or made available in the Union;\n10.\n\nA copy of the EU declaration of conformity referred to in Article 48;\n11.\n\nElectronic instructions for use; this information shall not be provided for high-risk AI systems in the areas of law enforcement and migration, asylum and border control management referred to in Annex III, points 1, 6 and 7.\n\n12.\n\nURL for additional information (optional).",
    "# Schengen Information System (SIS)\n\n1861 of the European Parliament and of the Council of 28 November 2018 on the establishment, operation and use of the Schengen Information System (SIS) in the field of border checks, and amending the Convention implementing the Schengen Agreement, and amending and repealing Regulation (EC) No 1987/2006 (OJ L 312, 7.12.2018, p. 14)\n\nRegulation (EU) 2018/1862 of the European Parliament and of the Council of 28 November 2018 on the establishment, operation and use of the Schengen Information System (SIS) in the field of police cooperation and judicial cooperation in criminal matters, amending and repealing Council Decision 2007/533/JHA, and repealing Regulation (EC) No 1986/2006 of the European Parliament and of the Council and Commission Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56).",
    "# Visa Information System\n\nProposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL amending Regulation (EC) No 767/2008, Regulation (EC) No 810/2009, Regulation (EU) 2017/2226, Regulation (EU) 2016/399, Regulation XX/2018 [Interoperability Regulation], and Decision 2004/512/EC and repealing Council Decision 2008/633/JHA - COM(2018) 302 final.\n\nTo be updated once the Regulation is adopted (April/May 2021) by the co-legislators.",
    "# Eurodac\n\nAmended proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on the establishment of 'Eurodac' for the comparison of biometric data for the effective application of Regulation (EU) XXX/XXX [Regulation on Asylum and Migration Management] and of Regulation (EU) XXX/XXX [Resettlement Regulation], for identifying an illegally staying third-country national or stateless person and on requests for the comparison with Eurodac data by Member States' law enforcement authorities and Europol for law enforcement purposes and amending Regulations (EU) 2018/1240 and (EU) 2019/818 – COM(2020) 614 final.",
    "# Entry/Exit System\n\nRegulation (EU) 2017/2226 of the European Parliament and of the Council of 30 November 2017 establishing an Entry/Exit System (EES) to register entry and exit data and refusal of entry data of third-country nationals crossing the external borders of the Member States and determining the conditions for access to the EES for law enforcement purposes, and amending the Convention implementing the Schengen Agreement and Regulations (EC) No 767/2008 and (EU) No 1077/2011 (OJ L 327, 9.12.2017, p. 20).",
    "# European Travel Information and Authorisation System\n\nRegulation (EU) 2018/1240 of the European Parliament and of the Council of 12 September 2018 establishing a European Travel Information and Authorisation System (ETIAS) and amending Regulations (EU) No 1077/2011, (EU) No 515/2014, (EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236, 19.9.2018, p. 1).\n\nRegulation (EU) 2018/1241 of the European Parliament and of the Council of 12 September 2018 amending Regulation (EU) 2016/794 for the purpose of establishing a European Travel Information and Authorisation System (ETIAS) (OJ L 236, 19.9.2018, p. 72).",
    "# European Criminal Records Information System on third-country nationals and stateless persons\n\nRegulation (EU) 2019/816 of the European Parliament and of the Council of 17 April 2019 establishing a centralised system for the identification of Member States holding conviction information on third-country nationals and stateless persons (ECRIS-TCN) to supplement the European Criminal Records Information System and amending Regulation (EU) 2018/1726 (OJ L 135, 22.5.2019, p. 1).",
    "# Interoperability\n\nRegulation (EU) 2019/817 of the European Parliament and of the Council of 20 May 2019 on establishing a framework for interoperability between EU information systems in the field of borders and visa (OJ L 135, 22.5.2019, p. 27).\n\nRegulation (EU) 2019/818 of the European Parliament and of the Council of 20 May 2019 on establishing a framework for interoperability between EU information systems in the field of police and judicial cooperation, asylum and migration (OJ L 135, 22.5.2019, p. 85).",
    "## Overview\n\nSince the public release of Open AI’s ChatGPT, Google’s Bard, and other similar systems, some Members of Congress have expressed interest in the risks associated with “generative artificial intelligence (AI).” Although exact definitions vary, generative AI is a type of AI that can generate new content—such as text, images, and videos—through learning patterns from pre-existing data.\n\nIt is a broad term that may include various technologies and techniques from AI and machine learning (ML).\n\nGenerative AI models have received significant attention and scrutiny due to their potential harms, such as risks involving privacy, misinformation, copyright, and non-consensual sexual imagery.\n\nThis report focuses on privacy issues and relevant policy considerations for Congress.\n\nSome policymakers and stakeholders have raised privacy concerns about how individual data may be used to develop and deploy generative models.\n\nThese concerns are not new or unique to generative AI, but the scale, scope, and capacity of such technologies may present new privacy challenges for Congress.",
    "## What Is Generative AI?\n\nGenerative AI can generate new content—such as text, images, and videos—through learning patterns from data.\n\nThere are many types of generative AI models, which can produce content based on different inputs or “prompts.” For example, some models can produce images from text prompts (e.g., Midjourney, Stable Diffusion, DALL-E), while others create videos (e.g., Gen2 or Meta’s Make-A-Video).\n\nSome scholars and policymakers have recently coined the term “general-purpose models” (GPAI) to describe applications like ChatGPT that can complete various functions.\n\nThese GPAI models may have a wide range of downstream applications compared to single-purpose models designed for a specific task.\n\nMany general-purpose AI applications are built on top of large language models (LLMs) that can recognize, predict, translate, summarize, and generate language.\n\nLLMs are a subset of generative AI and are characterized as “large” due, in part, to the massive amount of data necessary for training the model to learn the rules of language.",
    "## How Do Generative AI Models Use Data?\n\nData is essential to train and fine-tune AI models.\n\nGenerative AI models require especially large datasets for training and fine-tuning.\n\nGenerative AI models, particularly LLMs, require massive amounts of data.\n\nFor example, OpenAI’s ChatGPT was built on a LLM that trained, in part, on over 45 terabytes of text data obtained (or “scraped”) from the internet.\n\nThe LLM was also trained on entries from Wikipedia and corpora of digitized books.\n\nOpen AI’s GPT-3 models were trained on approximately 300 billion “tokens” (or pieces of words) scraped from the web and had over 175 billion parameters, which are variables that influence properties of the training and resulting model.\n\nCritics contend that such models rely on privacy-invasive methods for mass data collection, typically without the consent or compensation of the original user, creator, or owner.\n\nAdditionally, some models may be trained on sensitive data and reveal personal information to users.\n\nIn a company blog post, Google AI researchers noted, “Because these datasets can be large (hundreds of gigabytes) and pull from a range of sources, they can sometimes contain sensitive data, including personally identifiable information (PII)—names, phone numbers, addresses, etc., even if trained on public data.” Academic and industry research has found that some existing LLMs may reveal sensitive data or personal information from their training datasets.\n\nSome models are used for commercial purposes or embedded in other downstream applications.\n\nFor example, companies may purchase subscription versions of ChatGPT to embed in their various services or products.\n\nKhan Academy, Duolingo, Snapchat, and other companies have partnered with OpenAI to deploy ChatGPT in their services.\n\nHowever, individuals may not know their data was used to train models that are monetized and deployed across such applications.\n\nSome countries have taken action against AI developers for improper use of personal information.\n\nFor example, the Italian Data Protection Authority issued a temporary ban preventing OpenAI from using Italian users’ data.\n\nAfter agreeing to certain changes—such as allowing users to submit removal requests for personal data under the EU’s General Data Protection Regulation (GDPR)—OpenAI restored access to its service for users in Italy.",
    "## Where Does the Data Come From?\n\nMany AI developers do not disclose the exact details of their training datasets.\n\nFor generative AI, most training data is “scraped” from publicly available web pages before it is repackaged and sold, or in some cases, made freely available to AI developers.\n\nSome AI developers rely on popular large datasets such as “Colossal Clean Crawled Corpus” (C4) and “Common Crawl,” which are amassed through web crawling (i.e., software that systematically browses public internet sites and collects information from each available web page).\n\nSimilarly, AI image generators are commonly trained on a dataset called LAION, which contains billions of images scraped from internet sites and their text descriptions.\n\nSome companies might also use proprietary datasets for training.\n\nGenerative AI datasets can include information posted on publicly available internet sites, including PII and sensitive and copyrighted content.\n\nThey may also include publicly available content that is erroneous, pornographic, or potentially harmful.\n\nSince data may be scraped without the creator’s consent, some artists, content creators, and others have begun to use new tools such as “HaveIBeenTrained” to identify and report their own content in such databases.\n\nIn a 2023 investigation, the Washington Post and Allen Institute for AI analyzed the websites scraped for the C4 dataset, which is used by AI developers including Google, Facebook, and OpenAI.\n\nThe investigation found that the C4 dataset included websites with copyrighted content as well as potentially sensitive information, such as state voter registration records.\n\nThese forms of data collection may also raise questions about copyright ownership and fair use.\n\nFor a discussion of copyright issues and generative AI, see CRS Legal Sidebar LSB10922, Generative Artificial Intelligence and Copyright Law, by Christopher T. Zirpoli.",
    "## What Happens to Data Shared with Generative AI Models?\n\nSome critics have also raised concerns that user data shared with a generative AI application—such as a chatbot—may be misused or abused without the user’s knowledge.\n\nFor example, a user may reveal sensitive health information while conversing with a health care chatbot without realizing their information could be stored and used to re-train the models or for other commercial purposes.\n\nMany existing chatbots have terms of service that allow the company to re-use user data to “develop and improve their services.”\n\nThese concerns may be particularly pertinent for generative models used in interactions or services that commonly result in the disclosure of sensitive information such as advising, therapy health care, legal, or financial services.\n\nIn response, some critics have argued that chatbots and other generative AI models should require affirmative consent from users or provide clear disclosure of how user data is collected, used, and stored.",
    "### Existing Data Privacy and Related Laws\n\nThe United States does not currently have a comprehensive data privacy law.\n\nCongress has enacted a number of laws that create data requirements for certain industries and subcategories of data, but these statutory protections are not comprehensive.\n\nFor example, the Gramm-Leach-Bliley Act regulates financial institutions’ use of nonpublic personal information, while the Health Insurance Portability and Accountability Act (HIPAA) requires covered entities to protect certain health information.\n\nUnder current U.S. law, generative AI may implicate certain privacy laws depending on the context, developer, type of data, and purpose of the model.\n\nFor example, if a company offers a chatbot in a videogame or other online service directed at children, the company could be required to meet certain requirements under the Children’s Online Privacy Protection Act (COPPA).\n\nAdditionally, certain state laws on privacy, biometrics, and AI may have implications for generative AI applications.\n\nIn many cases, the collection of personal information typically implicates certain state privacy laws that provide an individual a “right to know” what a business collects about them; how data is used and shared; the “right to access and delete” their data; or the “right to opt-out” of data transfers and sales.\n\nHowever, some of these laws include exemptions for the collection of public data, which may raise questions about how and whether they apply to generative AI tools that use information scraped from the internet.\n\nIn the absence of a comprehensive federal data privacy law, some individuals and groups have turned to other legal frameworks (e.g., copyright, defamation, right of publicity) to address potential privacy violations from generative AI and other AI tools.\n\nFor example, some companies have faced class action lawsuits for possible violations of right of publicity state laws, which protect against unauthorized use of an individual’s likeness for commercial purposes.\n\nCongress may consider enacting comprehensive federal privacy legislation that specifically addresses generative AI tools and related concerns.\n\nIn doing so, Congress may consider and evaluate similar state and international efforts.\n\nFor example, the European Union’s (EU) proposed AI Act includes various articles on data regulation, disclosures, and documentation, among other requirements.\n\nThe EU AI Act recently added a category for general purpose AI systems and foundation models, another term used for AI models that train on large amounts of data and can be adapted for various tasks.",
    "### Proposed Privacy Legislation\n\nSome Members of Congress have proposed various comprehensive or targeted privacy bills with requirements that could impact generative AI applications.\n\nThese are three common mechanisms included in various privacy bills:\n\n**Notice and Disclosure Requirements.\n\n** Currently, most generative AI applications do not provide notice or acquire consent from individuals to collect and use their data for training purposes.\n\nCongress may consider requiring companies developing or deploying generative AI systems to (1) acquire consent from individuals before collecting or using their data, or (2) notify individuals that their data will be collected and used for certain purposes, such as training models.\n\nSome scholars dispute the efficacy of notice and consent requirements.\n\n**Opt-out Requirements.\n\n** Congress may consider requiring companies to provide users an option to opt-out of data collection.\n\nOf note, opt-out systems may not necessarily protect data that is publicly-scraped from the web, and may be cumbersome for individuals to exercise.\n\n**Deletion and Minimization Requirements.\n\n** Congress may also consider requiring companies to provide mechanisms for users to delete their data from existing datasets or require maximum retention periods for personal data.\n\nCurrently, most leading chatbots and other AI models do not provide options for users to delete their personal information.\n\nIn considering such proposals, Congress may also wish to consider practical challenges users may face exercising specific privacy rights as well as potential challenges for companies in complying with certain types of legal requirements and user requests.",
    "### Existing Agency Authorities\n\nVarious federal agencies may enforce laws relevant to AI and data privacy.\n\nThe Federal Trade Commission (FTC) has been active in addressing data privacy issues and has taken various actions involving AI.\n\nThe FTC has applied its broad authorities over “unfair or deceptive acts or practices in commerce” to cases related to data privacy and data security.\n\nIn recent months, the commission reaffirmed that its authorities also apply to new AI tools.\n\nChairman Khan stated, “there is no AI exemption to the laws on the books, and the FTC will vigorously enforce the law to combat unfair or deceptive practices or unfair methods of competition.”\n\nThe data collection practices of AI companies may also raise competition concerns.\n\nAt the 2023 Annual Antitrust Enforcers Summit, Chair Khan stated, “As you have machine learning that depends on huge amounts of data and also depends on huge amounts of storage, we need to be very vigilant to make sure that this is not just another site for the big companies becoming bigger and really squelching rivals.” The development of AI models may also require significant computational and financial resources, which may preclude new competitors and entrench incumbents.\n\nIn evaluating existing agency authorities, Congress may consider updating or providing additional specific authorities to federal agencies to address AI and related privacy issues.\n\nAdditionally, Congress could consider what resources federal agencies may require to conduct additional oversight of AI and privacy issues.",
    "### Regulation of Data-Scraping\n\nThere are currently no federal laws that ban the scraping of publicly available data from the internet.\n\nThe Computer Fraud and Abuse Act (CFAA, 18 U.S.C.\n\n§1030) imposes liability when a person “intentionally accesses a computer without authorization or exceeds authorized access, and thereby obtains ... information from any protected computer.” Some court cases have held that this prohibition does not apply to public websites—meaning that scraping publicly accessible data from the internet does not violate the CFAA.\n\nScraping publicly available information from the internet has privacy implications beyond generative AI models.\n\nThe facial recognition company Clearview AI has scraped over 20 billion images from the web, including social-media profile photos, which have been used for software and databases provided to law enforcement and other entities.\n\nSome technology companies have also scraped publicly available data to amass large data repositories.\n\nWeb-scraping may raise competition concerns since larger companies may block competitors from scraping data.\n\nMany researchers, journalists, and civil society groups, among others, rely on scraping to conduct research that may be in the public interest.\n\nIf Congress were to consider broad legislation to limit or provide guardrails for scraping information from the internet, it might consider implications for a range of activities that it may find beneficial.",
    "### Research and Development for Alternative Technical Approaches\n\nCongress may wish to consider providing funds to federal agencies for intramural and extramural research to examine the development of alternative AI models or related technologies that may preserve individual privacy, such as privacy-enhancing technologies.\n\nThere are benefits and tradeoffs to some AI models under development that may have privacy implications.\n\nFor example, smaller models that use less data or avoid transmitting and analyzing data in the cloud may minimize some privacy concerns but may amplify other issues, such as bias, by training on smaller datasets and potentially limiting the representativeness of data being used to train models.\n\nCongress may consider directing agencies to conduct and fund research to support privacy-by-design for AI and ML applications in order to both foster greater privacy for individuals and support the development of AI technologies and the global competitiveness of U.S. AI companies.",
    "## Disclaimer\n\nThis document was prepared by the Congressional Research Service (CRS).\n\nCRS serves as nonpartisan shared staff to congressional committees and Members of Congress.\n\nIt operates solely at the behest of and under the direction of Congress.\n\nInformation in a CRS Report should not be relied upon for purposes other than public understanding of information that has been provided by CRS to Members of Congress in connection with CRS’s institutional role.\n\nCRS Reports, as a work of the United States Government, are not subject to copyright protection in the United States.\n\nAny CRS Report may be reproduced and distributed in its entirety without permission from CRS.\n\nHowever, as a CRS Report may include copyrighted images or material from a third party, you may need to obtain the permission of the copyright holder if you wish to copy or otherwise use copyrighted material.",
    "## Purpose\n\nGenerative AI is a set of relatively new technologies that leverages large (very large) volumes of data along with some machine learning (ML) techniques to produce content based on inputs from the users known as prompts.\n\nThe new content can be written (e.g., ChatGPT or Bard), or visual (e.g., Dall-E).\n\nThese tools are evolving rapidly and are still the subject of active research: improving our understanding of how they actually work, and the impacts of their use in society.\n\nThese tools are not actual intelligence in the human sense, rather, they are very sophisticated models that predict what the language, text, or video that satisfies the prompt should be.\n\nBecause of their impact and potential usefulness, as well as the risks and dangers, these guidelines serve as an interim resource for employees of the City of Boston.\n\nGenerative AI is a tool.\n\nWe are responsible for the outcomes of our tools.\n\nFor example, if autocorrect unintentionally changes a word—changing the meaning of something we wrote, we are still responsible for the text.\n\nTechnology enables our work, it does not excuse our judgment nor our accountability.\n\nThese guidelines should be replaced in the future with policies and standards.\n\nBut we want to encourage responsible experimentation and we encourage you to try these tools for yourselves to understand their potential.\n\nThe Department of Innovation and Technology will support events and workshops that can support people and teams interested in learning more about these technologies.\n\nFor the time being, we encourage you to watch this video from Innovate.US about how to get started with generative AI in government:   \nYou can also share your experiences, thoughts, and concerns via this online form:",
    "## Sample Use Cases\n\nThese are some of the types of uses that could be beneficial.\n\nAdditional good practices and examples can be found at the end of this document.\n\n- **Writing a memo.\n\n** In government, we often have to write short documents that present an argument why a policy should be adopted or a decision should be made.\n\nFor instance, try the prompt in ChatGPT, Bard, and other generative text tools.\n\n- **Writing a job description.\n\n** Generative AI can produce job descriptions that aggregate and average parts of similar job descriptions, giving you a very good generalized version of a job description.\n\nFor instance, try the prompt in ChatGPT, Bard, and other generative text tools.",
    "### Empowerment\n\nThe use of AI should support the work of our workforce to deliver better, safer, more efficient and equitable services and products to our residents.\n\nWe rely and trust in our public sector professionals to do the right thing given the right tools and guidance.\n\nYou will need to exercise your judgment to make sure we get the benefits from the tools while avoiding the negative impacts for the City and its constituents.",
    "### Inclusion and Respect\n\nThe use and development of AI should support the development of work that repairs damage done to racial and ethnic minorities, people of all genders and sexual orientations, people of all ages, people with disabilities, and others.\n\nOur work should uplift these communities and connect them more effectively with the resources they need to thrive.\n\nEverything we do, regardless of the tools, are a reflection of the City and ourselves.\n\nWe are stewards of the public, and we will use tools respectfully and responsibly.",
    "### Transparency and Accountability\n\nWe embrace the possibilities of technology and community.\n\nWe acknowledge that we do not have all the answers nor can we foresee all consequences.\n\nBut when we act transparently, we build trust and we gain the ability to learn collectively.\n\nWe also acknowledge that experimentation might have costs and impacts in of itself including the usage of power, greenhouse gas emissions.\n\nBeing purposeful and accountable to these impacts is important.",
    "### Innovation and Risk Management\n\nWe understand that there is value to be had in the use of technology, particularly new generative AI, but there are also risks, some of which will not be apparent or fully understood upfront.\n\nWe embrace a culture of responsible experimentation, where we maintain control and understanding of the use of new tools while we develop new uses that drive efficiency, delight, civic dialogue, or other outcomes in service of our residents.",
    "## Guidelines\n\n- **Fact Check and review all content generated by AI**, especially if it will be used in public communication or decision making.\n\n**Why:** While Generative AI can rapidly produce clear prose, the information and content might be inaccurate, outdated, or simply made up.\n\nIt is your responsibility to verify that the information is accurate by independently researching claims made by the AI.\n\n**What to look for:**  \n  Inaccurate information including links and references to events or facts.\n\nBias in the positions or information.\n\nWe want to make sure that vulnerable populations are not harmed by these technologies.\n\nThink about how racial and ethnic minorities, women, non-binary, people with disabilities or others could be portrayed or impacted by the content.\n\n- **Disclose that you have used AI to generate the content.\n\n** You should also include the version and type of model you used (e.g., Open AI's GPT 3.5 vs Google's Bard).\n\nYou should include a reference as a footer to the fact that you used generative AI:\n\n  **Why:** even when you use AI minimally, disclosure builds trust through transparency and it might help others catch errors.\n\n**Suggestions:** document how you used the model, the prompts you used, etc.\n\nit could be helpful to you and your colleagues to better understand how you can use these technologies better and more safely.\n\n**Sample credit line:** “This description was generated by ChatGPT 3.5 and edited by Santiago Garces”  \n  **Sample credit line:** “This text was summarized using Google Bard”\n\n- **Do not share sensitive or private information in the prompts.\n\n**\n\n  **Why:** Data including prompts used in generative AI might be used by the companies that power these systems.\n\nAny information that includes personally identifying information about our residents, other public servants, etc., could inadvertently be shared with others.\n\nBasically, if you wouldn’t share it with other people or want to put the prompt in a public place, avoid sharing the information in the prompt.\n\nIf you have an application that requires sensitive information to be used with a generative AI, contact DoIT so we can help you provision access to enterprise secure resources to do so.",
    "### Drafting Documents or Letters\n\nGenerative AI provides a great opportunity to get started on a memo, letters, job descriptions.\n\nNote that when creating a prompt for ChatGPT for this context, it can consider including any specific format preferences such as essay, bullet points, outline, or dialogue.\n\nAdditionally, you can request the use of specific keywords or phrases, or technical terms to be included or avoided in the response.\n\nThis will help ChatGPT provide you with a more tailored and efficient response to your request.\n\n- **Example:** generate guidelines for the use of ChatGPT at the City of Boston  \n- **Example:** write a letter requesting support for funding digital equity initiatives in the next budget session.\n\n- **Example:** ask Chat GPT to generate letters that express points of view specified in the prompt.\n\nThis might allow you to understand an issue from different perspectives.\n\n- **Example:** ask Generative AI to help you write a more effective version of a prompt.\n\nYou can say “help me write a better prompt to [insert the initial objective of the prompt].”",
    "**Don’ts:**\n\n- Do not include confidential information in the prompt.\n\n- Do not rely on generative AI to provide accurate answers.\n\n- Do not use generative AI to create communication regarding sensitive topics.\n\nFor instance, a renowned institution was criticized for using generative AI to write a press release regarding a shooting.",
    "### Drafting Content In Plain Language\n\nGenerative AI can help you write clearer and simpler language.\n\nYou can use the prompt to indicate the reading level or audience for a text.\n\n- **Example:** use ChatGPT or Bard to write a version of the Declaration of Independence of the United States for a person in elementary school.\n\n- **Example:** use tools such as AISEO, Wordtune or others to modify a sentence.\n\nThese tools are similar to a thesaurus but for sentences and often allow you to optimize for the length of the sentence, or the audience.",
    "**Do’s:**\n\n- Specify in the prompt if you have a specific audience in mind.\n\n- Try different prompts, or request different versions of the same sentence until you find what works best.\n\n- Pass the output of the text by a readability app that can identify challenging sentences, as well as the reading level for the text.",
    "**Don'ts:**\n\n- Do not include confidential information in the prompt.\n\n- Review the text to ensure that the language is inclusive and respectful.\n\nThe models might use language or patterns that appear regularly, but that might exclude some people.\n\nFor instance, a model might suggest: “Dear Sir/Ma'am” does not include non-binary people, and could be replaced with “Dear Colleague” or “Dear neighbor”.",
    "### Drafting Content In Other Languages\n\nAI can help you draft communications in another language.\n\nIt is not well documented the extent to which ChatGPT and other models can use other languages, but users report over 50 languages being available for ChatGPT, including some native American languages.\n\n- **Example:** use ChatGPT to translate these guidelines into Spanish and French, just ask “translate [your text] into Spanish and French.”  \n- **Example:** ask generative AI in what language some text is written in, just ask “what language is [original language] written in?”",
    "**Don'ts:**\n\n- Do not include confidential information in the prompt.\n\n- Do not use content generated in a language you do not understand before consulting someone with proficiency in the language.\n\nYou still need to check for accuracy, bias, etc.\n\n- Language generated in other languages might be confusing to people who speak different regional dialects.\n\nDo not assume that some text will be easily understood by all speakers.\n\nUse the prompt to get regional diction.",
    "### Summarizing Text\n\nGenerative AI does a great job of summarizing longer pieces of text into summaries.\n\nIf you have a few pages that you want to condense into a few bullet points, or you have been struggling with converting a long set of notes into a paragraph, these tools could be very helpful.\n\n- **Example:** copy notes taken from a meeting to generate a short summary of the meeting.\n\n- **Example:** summarize citizen comments in response to an engagement  \n- **Example:** write a paragraph summary of a 5-page report.\n\n- **Example:** use Fathom, Wudpecker, or the transcript tools in Google Hangouts to transcribe audio into text.\n\nYou can then summarize the text further using generative AI.\n\nThis summarization is included in some of these tools.",
    "**Don’ts:**\n\n- Do not include confidential information in the prompt: make sure you have deleted confidential information from your notes or other inputs.\n\n- If you plan on making a decision based on the summary, you should read the entire document(s) to make sure you did not miss or mischaracterize the original document.\n\n- Be aware that the resulting summary might have biases as it will tend to present language that is more frequent in the data used to train the model.\n\nYou can use changes to the prompt to enhance the results by suggesting that the result incorporates perspectives from marginalized groups.\n\nEven better, you can engage with some individuals in these communities to better understand their perspectives on the text generated.",
    "### Summarizing Audio Coding/Programming\n\nGenerative AI can be great at producing snippets or even help you build more complex components of code.\n\n- **Example:** write code in Python that extracts tables in a PDF into a Pandas data frame.\n\nThis can make it possible for less technical people, including interns and student workers, to get to work on technical projects.",
    "**Do’s:**\n\n- Explore new languages and libraries—but you should understand the code and read the documentation of the relevant components before using it.\n\n- You might have to adjust parameters, and your environment to make the suggestions from the AI model work.\n\nGenerative AI can help you get started, but often you will have to edit before the code works.",
    "**Don’ts:**\n\n- Do not include confidential information in the prompt.\n\nAs in development best practices: do not include passwords, confidential keys, or other proprietary information in your code or in the prompts.\n\n- You should understand what the code is doing before using it in production.\n\n- You should understand the use of new libraries and dependencies, and become familiar with vulnerabilities and other security considerations of using a language or a library.",
    "### Images, Audio, and Videos\n\nGenerative AI can produce images, audio, and videos based on prompts.\n\nThis can support the creation of appealing or insightful communication resources.\n\n- **Example:** make an image in a medieval style of residents connecting to the wifi in order to create appealing collateral for a digital equity campaign.\n\n- **Example:** create a training video that walks residents on how to schedule a bulky item pick up, by providing the script of the video.\n\n- **Example:** write a jingle or song to remind them to switch to Boston’s Community Choice Electricity to switch to 100% renewable energy.",
    "**Do’s:**\n\n- Visual, audio and video communication can be a powerful tool to communicate with others and get across a message.\n\nGenerative AI can empower you to use these tools beyond your artistic skills.\n\n- Use generative AI as a tool to create drafts or mockups that allow you to communicate more effectively with graphic designers, videographers, and other creative workers.\n\n- Contact your department or agency’s public information officer about the image, audio, or video before publishing or using it.\n\nThey have expertise on best practices in accessibility, branding, etc.\n\n- Engage with members of the Equity Cabinet, or community organizations that represent groups that might be referenced or impacted by this content.\n\nGetting their perspective, in a respectful way, can help you identify when content might be hurtful, discriminatory, or misinterpreted.",
    "**Don’ts:**\n\n- Do not include confidential information in the prompt: make sure you have deleted confidential information from your notes or other inputs.\n\nSome confidential information could include: people’s faces, people’s voices, their identifications, license plates, etc., particularly those who have not provided their consent.\n\n- Make sure the outputs of the generative AI will not be offensive or harmful towards people, particularly vulnerable residents that are susceptible to harm including ethnic and racial groups, diverse gender individuals, and others.\n\n- Make sure that any content adheres to the City’s Brand Guidelines.",
    "## Resources\n\nYou can contact the Department of Innovation and Technology  to learn more about generative AI.\n\nYou can also contact the Mayor’s Office of Arts and Culture  or the Mayor’s Office of New Urban Mechanics  if you want to discuss important questions about the impact of generative AI on the arts and on our society.\n\nThe following resources include external links.\n\nWe do not endorse any one of these resources.\n\n- Reddit, ChatGPT subreddit:   \n- A great explanation on the mathematical principles behind generative language models:  \n  - Stephen Wolfram (2023), \"What Is ChatGPT Doing ... and Why Does It Work?,\" Stephen Wolfram Writings.\n\n- AI Principles from Microsoft:  \n    \n\n- AI Principles from Google:  \n    \n\n- NIST AI Risk Framework:  \n    \n\n- A critical analysis of large language models (major paper that predicted much of the harms/risks we are experiencing now)",
    "## Acknowledgements\n\nThe development of these guidelines has benefited from the contributions of academic, community, and City of Boston team members.\n\nSpecial thanks to Beth Noveck, Director of the Burnes Center for Social Change at Northeastern University; Saiph Savage, Director of the Civic AI Lab at Northeastern University; Catherine D’Ignazio, Director of the Data + Feminism Lab at MIT; Kimberly Lucas, Professor of the Practice at Northeastern University; Mitch Weiss, Professor at Harvard Business School; Alejandro Jimenez Jaramillo; Michael Evans from the Mayor’s Office of New Urban Mechanics; Jerry Kelley, project manager at the Department of Innovation and Technology, Kerry Jordan, Chief of Staff at the Department of Innovation and Technology.",
    "## Executive Summary\n\nThe rapid evolution and rise of generative AI systems is reshaping industries and human creativity.\n\nWhile generative AI offers novel opportunities, it can also amplify a range of existing and emerging harms for individuals and society.\n\nFor example, we have already seen chatbots providing inappropriate and harmful responses to user prompts, the spread of hyper-realistic generative AI deepfakes, and the creation of synthetic child sexual abuse material.\n\nBalancing the potential benefits with the risks of generative AI is essential.\n\nThis position statement examines the evolving landscape of generative AI, providing an overview of the generative AI lifecycle, examples of its use and misuse, and consideration of online safety risks and opportunities.\n\nThe statement also sets out a range of regulatory challenges and approaches.\n\nThe final section highlights emerging good practice and new Safety by Design measures to provide industry with meaningful, actionable, and achievable guidance to minimize existing and emerging generative AI harms.",
    "## Overview of eSafety’s Approach to Tech Trends\n\nThe eSafety Commissioner (eSafety) is Australia’s independent regulator and educator for online safety.\n\nUnder the Online Safety Act 2021 (Cth) (‘the Act’), we coordinate Australian Government activities to help keep people safer online, conduct research, provide education, and administer regulatory schemes to deal with certain types of online harm.\n\nWe use our regulatory powers to promote greater transparency and accountability within the online industry.\n\nWe work with other government agencies, businesses and organisations around the world to share information and best practices.\n\nThis helps us make the internet a safer place for everyone, regardless of where they live.\n\nWe keep our content, programs, and regulatory priorities up to date by scanning for new research, policies, laws, technology developments and by talking to experts such as academics and researchers.\n\neSafety also advises the Australian Minister for Communications and the Government on emerging issues across the online industry, international developments in technology regulation, and online safety concerns impacting Australians.\n\nWe do this because we recognise that combating online harm is a global challenge and we need to act together to make a difference.\n\nThis position statement is about generative artificial intelligence (AI) but readers may also find our other position papers on deepfakes created using artificial intelligence software, as well recommender systems and algorithms, useful information relevant to this topic.\n\nThe information in this position statement was informed by industry and stakeholder consultation as well as Australian and overseas research.\n\nIt reflects eSafety’s position as of 15 August 2023. eSafety acknowledges the rapid advancements in generative AI technology and will seek to review and provide revisions when necessary.",
    "### What is Generative AI?\n\nGenerative AI uses machine learning to generate new code, text, images, audio, video, and multimodal simulations.\n\nIt works by using large artificial neural networks built with enormous datasets and parameters that are inspired by synapses within the human brain.\n\nThe difference between generative AI and other forms of AI is that its models can create new outputs, instead of just making predictions and classifications like other machine learning systems.\n\nSome examples of generative AI applications include:\n- Text-based chatbots, or programs designed to simulate conversations with humans, such as Anthropic’s Claude, Bing Chat, ChatGPT, Google Bard, and Snapchat’s My AI.\n\n- Image or video generators, such as the Bing Image Creator, DALL-E 2, Midjourney, and Stable Diffusion.\n\n- Voice generators, such as Microsoft VALL-E.",
    "### Background\n\nGenerative AI is not new.\n\nChatbots, image generators and deepfake technologies have been in development and use for many years.\n\nHowever, recent advancements have rapidly improved generative AI due to the availability of more training data, enhanced artificial neural networks with larger datasets and parameters, and greater computing power.\n\nSome experts now claim contemporary AI systems are moving rapidly towards ‘human-competitive intelligence.’ Such claims pose existential questions about the potential of such systems to impact almost every aspect of human life in both positive and negative ways.\n\nThe possible threats related to generative AI are not just theoretical – real world harms are presenting themselves today.\n\nThis includes misusing AI to generate child sexual exploitation and abuse (CSEA) material that looks like it involves real children (or based on images, audio or other depictions of real children) or generating and threatening to share artificial but realistic pornography featuring real adults without their consent.\n\nThese harms can occur because of flaws in the data or models used in generative AI, such as when biased information is used for training.\n\nGenerative AI can also be used to manipulate and abuse people by impersonating human conversation convincingly and responding in a highly personalised manner.\n\nMany generative AI models have been intentionally made freely available within the open source community or have ‘leaked’ into the public domain.\n\nWhile releasing models freely promotes transparency, competition and innovation, the fact it is readily accessible to the public also increases the risk that harmful and manipulative content can easily be generated at scale when the technology is put in the wrong hands.\n\nGenerative AI is being incorporated into major search engines, productivity software, video conferencing and social media services and is expected to be integrated across the digital ecosystem.\n\nCompanies are moving quickly to develop and deploy their own generative AI technologies.\n\nThis may lead to not enough attention being paid to risks, guardrails, or transparency for regulators, researchers, and the public.\n\nMultiple actors including technology developers and downstream services that integrate or make generative AI technology accessible, as well as users, all have a role to play in ensuring online harm is prevented and addressed.\n\nAs countries think about how to regulate generative AI, technology companies have been advocating for certain regulatory approaches, some of which may actually serve the commercial interests of the companies involved.\n\nIn Australia, the Government is looking at the risks, benefits and potential impacts of generative AI.\n\nThis includes examinations by the Department of Industry, Science, and Resources, the Department of Education, the Attorney General’s Department and the Digital Platform Regulators Forum (DP-REG), which includes the Australian Competition and Consumer Commission (ACCC), Australian Communications and Media Authority (ACMA), Office of the Australian Information Commissioner (OAIC), and eSafety.\n\nIt is important to recognise that for every risk, there is also an opportunity.\n\nFor example, people can misuse generative AI to create harmful content such as online hate.\n\nHowever, AI can also be harnessed to significantly improve current proactive content moderation technologies to quickly and accurately find and stop online hate.\n\nThere have been reported instances of children acknowledging abuse and seeking support through AI chatbots.\n\nA chatbot can give an inappropriate or harmful response to a child who discloses their experience of abuse.\n\nBut an appropriately trained chatbot could respond in a supportive and evidence-based manner, connecting that child to law enforcement and support services.\n\nThe risk of harm depends on whether the technology was designed with safety in mind, including by taking a Safety by Design approach.",
    "### Safety by Design\n\nSafety by Design is built on three core principles: Service provider responsibility, User empowerment and autonomy, and Transparency and accountability.\n\nTechnology companies can uphold these principles by making sure they incorporate safety measures at every stage of the product lifecycle.\n\nThis should involve consulting stakeholders from multiple sectors and collaborating with the user community, including those who are typically under-represented or who may be at greater risk of harm.\n\nA Safety by Design approach to AI is also likely to satisfy most of Australia’s AI Ethics Principles.",
    "### Generative AI Lifecycle\n\nIt is important to consider online safety risks and harms from the earliest stages of developing a generative AI technology.\n\nThis should continue throughout the technology’s lifecycle and across the entire system from developing a business case to releasing, disseminating and reintegrating AI-generated content.\n\nThe simplified product lifecycle below sets out 10 crucial steps where this must occur, drawing on insights from various experts and other sources.",
    "#### Business Case\n\nThe first step in the generative AI lifecycle is to evaluate the business case for developing the technology and explore options for funding.\n\nTo create safer technologies, companies and developers building generative AI should consider why they are planning to develop it, for what purpose and in what context.\n\nAI systems designed for legitimate internal business purposes can still have broader impacts on individual, social and environmental wellbeing.\n\nThose impacts should be accounted for in the AI system’s lifecycle, to include consideration of impacts outside the organisation.\n\nBy considering risks and building in safeguards during the early stages of development, it is possible to establish trust in a product or service.\n\nThis can then open up more opportunities for investment.\n\nOne helpful tool for this process is the Safety by Design Business Model Canvas.\n\nThis enables businesses to assess and analyse their business model, challenge assumptions and promote socially responsible innovation.",
    "#### Selecting Data\n\nAfter establishing a business model, developers must make choices about the type of model they want to create and the input data they will use to build and train it.\n\nGenerative AI often requires large datasets to meet the diverse needs of users with some models utilizing closed datasets, while others rely on general information scraped from the internet.\n\nDevelopers must consider the content and quality of their data sources, as well as ethical and legal concerns such as how data is sourced, right from the start.\n\nData scraping involves collecting, using, disclosing, and storing information without the knowledge or consent of the data creators or the individual the information is about, which raises questions about copyright, privacy, consent and attribution.\n\nIt is important to address considerations about accuracy, diversity (including language and culture) and whether harmful material is captured through processes such as scraping.\n\nIf not managed carefully, there is a risk that data sources could include illegal or harmful content, such as CSEA material, image-based abuse (IBA), hate speech and abuse, or false, biased, or misleading information, or other unlawful material.\n\nData sets containing such content can perpetuate harms by generating illegal and harmful outputs.\n\nDevelopers can also use pre-training capabilities, such as classification and proactive detection tools, to improve training data quality.\n\nThis reduces the risk of harm later in the lifecycle.\n\nTransparency is a vital component to hold services accountable for content they host or use to train their systems, including by documentation through annual reports and using model cards or system cards, which are designed to explain how the systems and models operate.",
    "#### Training the Model\n\nThe next step is to train the model using the data that has been selected.\n\nDevelopers can do this through supervised learning, where humans train models to classify inputs with labels.\n\nFor example, a model can be trained to label social media posts as positive or negative.\n\nThis is called ‘supervised learning’ because a human teaches the model what to do.\n\nIt is important that humans are appropriately trained to conduct this work and feed in diverse views.\n\nOn the other hand, ‘unsupervised learning’ can be used to find patterns in data that are not labelled, typically used when there is a lack of training data.\n\nMore advanced text-based machine learning models may rely on ‘self-supervised learning’.\n\nThis type of training involves giving the model a massive amount of text so it can generate predictions.\n\nFor example, some models can predict how a sentence will end based on a few words.\n\nModel and system cards are important for documenting capabilities at the training stage, prior to refinement and release.\n\nIt is also important at this stage to consider the lived experiences of humans who are training the model, to ensure that culturally specific and contextual forms of harm can be addressed and appropriately mitigated.\n\nAdditional safety measures include consultation with experts who can provide guidance on inputs for training the model.",
    "#### Refinement\n\nIt is important to keep refining model data throughout the lifecycle to minimise risks, harms and bias.\n\nThis means going beyond initial training with supervised, unsupervised or self-supervised learning.\n\nDevelopers must keep working over time to maintain quality data inputs, ethically curate data, label it, and control quality across many datasets on different subjects.\n\nData quality and veracity are issues in the refinement process, as is the ability of AI models to recognise and filter out illegal, harmful or inappropriate content.\n\nThis work is often carried out by employees who are hired to tag and sift through large amounts of harmful and potentially traumatising content.\n\nWhile human review prior to release may be essential, there are concerns about the working conditions, pay, and mental wellbeing for people who do jobs such as labelling or generating training data.",
    "#### Release\n\nFollowing refinement, the model may be released to the public through the developer’s own app or interface, such as ChatGPT.\n\nIt can also be added to through other means including integration into an existing service, such as ChatGPT in Microsoft’s Bing search engine.\n\nThe same model can be released and integrated in a variety of ways.\n\nDevelopers may also choose to openly release their model.\n\nThere are details about open and closed models below.\n\nIt is important to conduct risk assessments, anticipate how the model may be misused by individuals and establish safety policies and practices prior to release.\n\nDevelopers should ‘red-team’ or ‘stress test’ the system by considering the avenues for possible misuse.\n\nDevelopers should also consider graduated approaches to release, including regulatory sandboxes, to understand how the model performs in controlled conditions.\n\nGiven the evolving nature of the technology, unforeseen risks and new techniques to overcome safeguards will keep appearing after the model is launched.",
    "#### User Engagement\n\nOnce a model is released, users can interact with it by accessing its interface and giving it instructions or prompts.\n\nFor example, they can enter text or audio commands to generate content or get information.\n\nDevelopers should expect that their model might be misused by malicious actors.\n\nThey should test their models with consideration of the ways it could be misused.\n\nFor example, it is a serious concern if models are not able to detect when users may be attempting to input harmful prompts to generate illegal or harmful content, or implementing appropriate safeguards that are activated to mitigate potential misuse.\n\nFor example, terrorist groups could use models to raise money, disseminate pro-terror content or generate instructions on making bombs or weapons; paedophiles could use AI to create content for child grooming or CSEA, and people could use AI to generate and spread misinformation and disinformation or targeted hate speech or abuse.\n\nPeople could also intentionally try to hack or tamper with the model’s input to make it behave badly.\n\nAdding points of friction, such as educative prompts and nudges, when users attempt to generate content can be an important method of reducing misuse.\n\nDevelopers need to keep improving their model to engineer out harmful or illegal outputs as they emerge, as it is unlikely all harms will be mitigated prior to release.",
    "#### AI Generation\n\nAfter the user inputs a prompt, the AI interface generates content based on this information.\n\nSometimes, generative AI models give confident but inaccurate, misleading, or harmful answers.\n\nThese are called ‘hallucinations’ and can happen for many reasons, including inadequate or problematic input datasets.\n\nModel outputs can also adversely influence user views, values and experiences by misrepresenting available information or only providing a limited view of information.\n\nThis could have the impact of shifting societal norms or values around challenging topics.\n\nDevelopers can also add safety measures at this stage, such as warnings or disclaimers for users that the information might be wrong or inaccurate.\n\nDigital watermarking – a method for identifying AI-generated content – can also be implemented.",
    "#### Feedback\n\nOffering opportunities for users to give feedback on the content generated is a crucial step in the generative AI lifecycle.\n\nThis can be done through user feedback loops.\n\nIt is also essential to clearly communicate policies and make sure reporting and feedback tools are easily accessible.\n\nBy gathering input from users, there is a chance to mitigate potential risks, such as generating discriminatory, harmful, deceptive, or false content.\n\nThis may also provide the basis to undertake consultation with a diverse userbase.\n\nThis feedback helps to implement measures that moderate and improve the content generated by the model.",
    "#### AI Dissemination\n\nAfter the system generates content, it can be shared with others, including on social media.\n\nEven where these social media platforms and other services do not have their own generative AI capability, it is imperative to build in tools that can stop, find, and moderate harmful content generated by AI that may be shared on their platforms.",
    "#### Reintegration\n\nGenerative AI content that is shared on the internet or on social media could feed back into models that are built or refined using content scraped from the web.\n\nIf not appropriately managed, harmful content and views may be reinforced in a continuous feedback loop.\n\nReintegration may also generate ‘synthetic data’ and in turn lead to an overall reduction in model efficacy.",
    "### Framing Online Risks and Harms\n\nThere are different ways to understand the risks and harms associated with generative AI.\n\nOne approach is to consider its potential impacts on individuals and society.\n\nAt an individual level, generative AI can pose risks by generating and amplifying harmful and extreme content.\n\nThis can have a greater impact on victims of CSEA material, IBA and other forms of abuse.\n\nIt also affects those who inadvertently come across such harmful material.\n\nOn a broader societal level, generative AI can contribute to the generation and amplification of content that promotes bias and discrimination.\n\nThis includes promoting sexism, homophobia, racism, or other forms of prejudice.\n\nSuch content normalizes hate or intolerance, which could lead to radicalisation towards terrorism and violent extremism.\n\nIt may also lead to an erosion of trust in online content or institutions.\n\nThese risks have significant social implications, particularly where several harmful effects may accumulate over time, shaping narratives around important societal issues.\n\nFor example, a person might ask a generative AI application a question about domestic violence and get a response that distorts or minimizes the severity of the issue.\n\nThere is also the potential for text-based and visual model output to be used in the service of mega conspiracies, fuelling hate and intolerance.\n\nGiven the potential individual and societal impacts, experts, industry, and some governments are developing frameworks to proactively consider these issues, risks, and harms.\n\nOne framework that was raised during consultations is an approach that examines the risks associated with different components of the system.\n\nFor example, the ‘ABC’ framework considers three key aspects: the actors involved in disinformation campaigns, their deceptive behaviour and tactics, and the content they produce and share.\n\nSimilarly, during consultations, eSafety received feedback suggesting an approach that focuses on context and intention.\n\nStakeholders identified three categories of risks and harms:\n\n- AI failing to perform as expected: This occurs when a system unintentionally causes harm by generating incorrect or harmful responses.\n\nFor example, generative AI systems may ‘hallucinate’ and produce inappropriate responses to user prompts.\n\n- AI being used maliciously: This happens when a model is trained or exploited for harmful purposes.\n\nFor example, when individuals involved in CSEA attempt to groom children or generate CSEA material using generative AI tools.\n\n- AI being overused, used recklessly or used inappropriately in a specific context: This refers to situations where generative AI is used excessively or recklessly, or employed inappropriately, leading to harmful or misleading results.\n\nFor example, where generative AI produces age-inappropriate material such as online pornography for a child user.",
    "### Drivers of Risk\n\nSeveral factors can drive, contribute to, or amplify the risks and harms associated with generative AI:\n\n- Personalisation.\n\nChatbots and multimodal models have the potential to generate highly personalised, emotive, manipulative, and invasive content based on users’ previous engagement and activity.\n\nThrough consultation with eSafety, experts highlighted that online harms may arise from generative AI creating ‘human quality content’ or producing customised media in real-time.\n\nWhile this content may appear authoritative, it could also be intentionally or accidentally false, misleading, or malicious.\n\nFor example, personalized phishing activities may be used to intentionally mislead and potentially defraud the recipient or gain access to information or systems.\n\n- Access.\n\nWider access to generative AI models raises concerns about their potential misuse for harmful purposes.\n\nConsumer-facing apps using generative AI make it harder for users to discern fact from fiction as the technology becomes more convincing over time.\n\nPolicy discussions continue globally on whether the development of AI should occur in open or closed environments, with regulatory approaches tailored according to public access levels and associated risks.\n\nDetermining who is responsible for preventing and mitigating harms becomes an important consideration among the companies that develop the model, the companies that deploy the model in their applications and the people who use those applications.\n\n- Advertising and revenue models, and access to children’s data.\n\nDigital platforms with advertising-based revenue models are likely to incentivise the generation of highly personalised content for marketing purposes and promote sponsored content rather than addressing the specific needs of users.\n\nIt is also possible that generative AI optimised to increase user engagement will produce problematic or emotive content aimed at maintaining attention.\n\nGreater consideration of the acquisition, access to, use and storage of children’s data, particularly for commercial purposes, is needed.\n\nThe Australian Privacy Act Review Report made recommendations for providing individuals with greater control over targeted content and marketing, including prohibiting entities from targeting children unless it is in the child’s best interest.\n\nAt the date of publishing this paper, the Government’s response to the report is forthcoming.\n\n- Limited representation.\n\nAt the time of drafting this statement there appears to be a lack of diversity among AI companies, primarily originating from English-speaking, Western cultures, which poses a risk of encoding narrow values and perspectives into global-reaching models.\n\nThis could perpetuate and amplify dominant ideologies at the cost of other values and identities.\n\n- Pace of development.\n\nWith venture capital increasingly focused on generative AI’s rapid product development and sales growth potential comes the risk of neglecting safety considerations due to a ‘move fast and break things’ approach.\n\nSafety by Design recognises that there are important inflection points and players in the technology ecosystem that need to be leveraged to enable change.\n\nInvestors and venture capitalists play a pivotal role in nurturing tech ventures and they can help put safety and ethical considerations at the heart of the businesses they invest in.\n\nThis will help them to invest ethically and manage investment risk, but also helps the start-up harden their defenses against potential safety risks.\n\nIt’s worth noting that the use of open-source models and pace of change within the developer community can also create problems of control, responsibility and accountability, with models adapted for nefarious purposes without adequate checks and balances.\n\n- Convergence with other emerging technologies.\n\nAs immersive platforms merge with generative AI technology, their respective risks may also converge.\n\nThis raises important questions about the manifestation of future harms, and the potential for more visceral and extreme impacts.\n\nFor example, metaverse platforms rely heavily on AI to create realistic, immersive environments populated by non-player characters.\n\nThese platforms collect large amounts of personal information that informs conversational agents and enhances user engagement.\n\nThis convergence may reinforce the risks highlighted in eSafety’s position statement on immersive technologies.",
    "##### Used to create CSEA material\n\nGenerative AI is expected by some to bring about changes in how CSEA occurs online, as well as the methods we employ to combat it.\n\nA 2023 report by the Stanford Internet Observatory and Thorn found that generative AI tools are already being used to create realistic computer-generated child sexual abuse material (CG-CSAM).\n\nThe potential for CSEA materials to be generated using photos of children harvested from social media also creates specific safety challenges for parents, carers, and young people, reinforcing the need to make sure that online profiles are set to private.\n\nPerpetrators can exploit the ability of large language models (LLMs) powered by AI to mimic natural human language.\n\nThis allows them to groom children in automated and more targeted ways, and cases have already been reported where generative AI technologies are being used to facilitate child grooming.\n\nDevelopments related to generative AI pose risks concerning the identification of victims.\n\nAs it becomes more difficult to determine whether content is AI-generated, law enforcement agencies and hotlines will face a growing challenge in determining whether certain content depicts an actual child who needs to be identified and rescued.\n\nThere are also definitional challenges which could emerge across jurisdictions concerning AI-generated media, including how children and images of children are defined.",
    "# eSafety Impact of Generative AI\n\nGenerative AI technologies can enable inappropriate contact with children and young people.\n\nMoreover, generative AI has the potential to generate content that is not appropriate for their age, such as violent or sexually explicit material.\n\nFor example, developers have used the open-source Stable Diffusion model to generate realistic adult pornography.\n\nWhile the model has since been updated to mitigate the possibility of unsafe or inappropriate content from being created, some people still use older versions of the model to produce prohibited imagery.\n\nMany open source sites also continue to provide access to build-your-own prompts in order to produce photorealistic pornography.\n\nThere are reports that Snapchat’s ‘My AI’ chatbot offered advice to a user pretending to be 13 years old on how to lie to her parents about meeting a 31-year-old man.\n\nAnother example was a case where a chatbot designed as an eating disorder hotline encouraged a user to develop unhealthy eating habits.\n\nYoung people may seek out chatbots and other forms of conversational AI as safe spaces for sharing personal experiences, including incidents of harm.\n\nHowever, there is a risk generative AI may struggle to appropriately handle disclosures and meet reporting obligations when children share harmful experiences.\n\nThis lack of support following disclosure can put them at greater risk of harm.\n\nGenerative AI tools may also unintentionally provide information that worsens trauma or exacerbates harm when responding to disclosures.",
    "## Non-consensual Imagery\n\nFor years, people have been using generative AI deepfakes to create pornography, including explicit content featuring real people.\n\nDeepfakes are commonly shared in the online pornography environment, particularly of women in the public spotlight, and typically without their consent.\n\nA study by Deeptrace, a cybersecurity company based in Amsterdam, revealed that as of September 2019, 96% of all deepfake videos available online consisted of non-consensual sexual material.\n\nAnother cross-country study published in the British Journal of Criminology in 2021 discussed the pervasiveness and harms of deepfake and digitally altered imagery abuse.\n\nFor more information, see eSafety’s position statement on deepfakes published in January 2022.\n\nGenerative AI has the capability to combine images, sound and other elements to create extremely realistic but false depictions of people.\n\nThis allows individuals to easily generate harmful content with a high degree of false credibility.\n\nThe resulting harm is complex; even if it becomes clear the content is fake, it can still cause immense distress for those whose images are used and shared without their consent.\n\nWhether the content is genuine or synthetic doesn’t diminish its potential for causing humiliation, shame, harassment, intimidation, or being used in sexual extortion.",
    "## Terrorism and Violent Extremism\n\nThere are reports that indicate terrorist organisations could potentially use LLMs, given they are deep-learning models capable of generating text that resembles human language.\n\nThey could potentially use these models for financing terrorism and to commit fraud and cybercrime.\n\nMulti-modal capabilities that analyse social media posts, online interactions, and other data sources could also be weaponised by terrorist groups and violent extremists to create tailored propaganda, radicalise and target specific individuals for recruitment, and to incite violence.\n\nMore broadly, AI-generated content has the potential to influence public perceptions and values, including towards extremist ideologies.\n\nThis creates the risk that generative AI can contribute to insidious and cumulative harms.",
    "## Bullying, Abuse, and Hate Speech\n\nGenerative AI models and their outputs are vulnerable to being exploited for automating personalised hate speech, bullying, abuse, and other forms of harassment and manipulation at scale.\n\nThese models can generate unique content based on toxic and biased data or prompts, allowing for hate speech campaigns that inundate online platforms.\n\nUsers are finding ways to circumvent industry’s attempts to prevent such risks, for example by experimenting with different prompts to ‘jailbreak’ the model.\n\nSimilarly, AI audio generators have been misused to spread hate speech by disseminating recordings of sexist, racist, and homophobic comments in the voices of celebrities.\n\nStudies show that AI-generated voice is nearly impossible to differentiate from human speech.\n\nVarious forms of generative AI-like text, audio, and image can work together to create highly personalised harassment with amplified harmful impacts.",
    "## Bias and Inclusivity\n\nGenerative AI can reinforce stereotypes and amplify existing biases even without human interference.\n\nThis bias poses a significant safety risk for users, especially those from underrepresented and marginalised communities, and threatens to entrench existing divides.\n\nAt present, generative AI systems tend to be trained on massive sets of publicly available online data that may not undergo thorough vetting for accuracy, authenticity, bias, or inclusivity.\n\nThis means the generated outputs reflect the online world but may not accurately represent the diverse values and perspectives of the offline world.\n\nThe risks associated with generative AI go beyond individual instances of biased content; they extend to how this technology may shape our thoughts and actions more broadly.\n\nIn addition, there may be a lack of diversity among those who design and refine generative AI systems.\n\nHuman reviewers may also bring their own subjective biases into play.\n\nTo mitigate bias in generative AI systems, it is vital to involve diverse groups during the development of new services or technologies.\n\nProviding training to content labellers regarding relevant issues can also help ensure better understanding and awareness.\n\nOther opportunities include developing models that draw on a wide range of perspectives and establishing evaluation metrics that actively address racial, gender, and other biases while promoting value pluralism.\n\nAdopting holistic evaluation strategies is crucial for addressing a range of risks and biases.",
    "### Detecting Harmful Material at Scale\n\nGenerative AI technologies and machine learning are being used to detect and prevent harm.\n\nFor example, LLMs can be used to identify criminal activity and harmful content or material.\n\nThis approach could also reduce the need for humans to be exposed to harmful content during review processes.\n\nTechnical improvements in AI also present opportunities to improve content detection and moderation tools, as well as educative prompts and nudges.\n\nExperts suggest generative AI models can be trained to detect harmful text more effectively than existing keyword detection tools.\n\nThey may possess advanced abilities in discerning nuances in tone, enabling better differentiation between criticism and hate speech.\n\nThese advancements also present an opportunity to train AI tools to intervene when individuals show signs of moving towards extremist content.\n\nFor example, educative prompts and nudges used on social media platforms can be adapted for generative AI technologies as well.",
    "### Providing Scalable Support to Young People\n\nGenerative AI technologies offer new opportunities to design evidence-based support tailored to address issues children and young people are facing.\n\nThis includes scalable online support services to children – as well as adults – through conversational modes such as chatbots.\n\nFor example, Kids Help Phone in Canada have a chatbot called ‘Kip the Website Helper’, which introduces chatbot technology to the Kids Help Phone gateway portal to help people navigate the website.",
    "### Enhancing Learning Opportunities and Digital Literacy Skills\n\nIt is important to consider both the benefits and risks of generative AI in education.\n\nSome crucial points to consider include: whether there is an opportunity to enhance critical media literacy skills by incorporating conversations about values and ethics into young people’s education.\n\nHow to improve digital and algorithmic literacy among students, giving them the skills and confidence they need to manage their online experiences safely.\n\nThe importance of taking a strengths-based approach rather than focusing on deficits is important when addressing these issues.\n\neSafety encourages early development of critical thinking through guided messaging and learning that starts at a young age for children, as well as their teachers and parents.",
    "### Data Consent\n\nGenerative AI also presents opportunities to establish more effective and robust conversations on consent regarding data use and collection.\n\nFor example, rather than simply ticking a box to indicate user consent and seeking consent from others whose personal information may be shared, conversational forms of generative AI could contribute facilitate active conversations about user privacy.\n\nThis could also support individuals to engage in more natural, nuanced, and personalised discussions about consent and respecting individual privacy.",
    "## Other Risks and Considerations\n\nIn addition to the online harms within eSafety’s regulatory remit, generative AI raises multiple other issues of concern covered by the remit of other government agencies, and regulators in Australia and worldwide.\n\nWhile other government departments and agencies have primary responsibility for many of these matters, they have the potential to intersect with the online harms eSafety strives to prevent.",
    "#### Advertising\n\nUsers may not always be aware when generative AI is providing factual, organic information or information which is attempting to influence their online activities and purchasing decisions, especially when it is integrated into those services.\n\nFor example, conversational models can extract information from people who are unaware the information they share with ‘virtual assistants’ is also being used for marketing purposes.",
    "#### Competition\n\nEstablished companies with more resources are typically better equipped to navigate emerging regulatory measures than small businesses or start-up companies.\n\nThis creates a potential barrier for smaller companies lacking sufficient resources, personnel, or knowledge to meet regulatory obligations.\n\nConsequently, established companies may advocate for regulatory frameworks that favour their own business objectives at the expense of their competition.\n\nGiven the competitive advantage conferred by generative AI, there is a risk firms may engage in exclusionary conduct, aimed at restricting or undermining their rivals’ ability to compete in the market.\n\nAdditional competition concerns include:\n\n- Anti-competitive self-preferencing – making it difficult for users to tell when chatbots make sponsored recommendations, or refer to products or services offered by the same firm that operates the chatbot.\n\n- Anti-competitive tying – such as tying the availability of any future ‘must-have’ LLM services to the use of other services, such as browsers or search engines.\n\n- Restrictions on access to data – where firms with significant market power could restrict competitors’ access to data, limiting the training of rival LLMs.",
    "### Scams and Phishing\n\nGenerative AI could automate scams and enhance their effectiveness by giving them the ‘look and feel’ of genuine products and communications.\n\nPersonalization capabilities could also allow scams to specifically target individuals.\n\nFor example, video and audio files representing specific individuals can now be generated using minimal source data.\n\nThese could facilitate ‘phishing’ by generating calls for help that appear to come from a real person, including a loved one.",
    "### Communication and Media\n\nGenerative AI has the potential to introduce or exacerbate several online communication and media risks, especially in the realm of misinformation and disinformation.\n\nThe ACMA is the Australian regulator responsible for overseeing these issues.\n\nGenerative AI models can tailor content to individual users, intentionally or unintentionally producing large volumes of apparently authoritative content that may be false, misleading, or ‘hallucinated’ which can manipulate users.\n\nConversational agents’ can effectively mimic human interaction, increasing their potential influence on users communicating with them.\n\nThis can increase the scale and influence of misinformation and disinformation on an individual and societal level and can generate mistrust in authoritative sources of information, undermining the overall quality of circulated information.\n\nSynthetic media such as images, videos, and voice have the capability to alter the landscape of misinformation and disinformation, with various forms of media generating viral reaction.\n\nFor example, an AI program called Midjourney was used to create a viral deepfake image of the Pope wearing a white puffer jacket in the style of contemporary hip-hop artists.\n\nThis shows how generative AI can create viral reactions with false media.\n\nHowever, generative AI can also create efficiencies in news organisations through assisting in the generation of news stories and can be a tool for teaching critical digital and media literacy skills to combat misinformation and disinformation.\n\nIt is also a useful tool for detecting misinformation and disinformation.\n\neSafety is concerned that AI-generated images, audio and video targeting Australian individuals – and depicting them doing or saying things they didn’t do or say – with serious intent to harm the individual could amount to serious adult cyber abuse as part of a broader misinformation or disinformation campaign.",
    "### Privacy\n\nGenerative AI may create privacy risks and impacts.\n\nThe OAIC is the Australian regulator responsible for these issues.\n\nThe information handling practices associated with this technology are often complex and opaque which challenges the ability of individuals to meaningfully understand how their personal information is being handled.\n\nOutputs from generative AI models may also contain personal and sensitive information, including misleading or inaccurate information about an individual.\n\nThe use and retention of large data sets to develop and deploy this technology elevates the risk of a data breach and the risk of harm to individuals if their personal information is included in the compromised data.\n\nFurthermore, generative AI may employ tools that can record users’ written and spoken words, track conversations over time, and monitor sentiment through verbal and non-verbal cues such as tone of voice.\n\nCertain AI tools with recording functionality may capture other users without their knowledge or consent, which is also a privacy concern.\n\nThe Australian Government is committed to ensuring that Australia has fit-for-purpose regulatory settings to address the privacy challenges posed by AI.\n\nThe Review of the Privacy Act 1988 considered the privacy risks associated with the use of new technologies and made proposals to provide greater transparency and give individuals more control over their data.\n\nThe Government is considering the Privacy Act Review Report and feedback received in recent public consultation, which will be used to inform the Government’s response.",
    "### Human Rights\n\nGenerative AI gives rise to many different risks and opportunities for upholding human rights.\n\nThe Australian Human Rights Commission (AHRC) oversees human rights matters in Australia.\n\nAI risks to human rights relate to:\n\n- Discrimination arising from the programming of the algorithms that inform AI technologies\n- Discrimination resulting from machine learning (i.e., non-diverse datasets)\n- Accessibility discrimination or digital exclusion.\n\nA specific concern raised during eSafety’s consultations for this paper was Indigenous data sovereignty and representation.\n\nIf AI models are developed only to reinforce English-speaking, western values, they may not be effective, safe, and culturally appropriate for diverse users, including First Nations people.\n\nConversely, generative AI technologies hold great potential to preserve Indigenous cultures and languages.\n\nTo do this, it is important to respect the rights of individuals and communities to consent to the collection and use of their data.",
    "## Other Implications\n\nGenerative AI also has regulatory implications across many well-established sectors.\n\nIntellectual property (IP) concerns and questions of data ownership arise regarding the inputs and outputs of generative AI systems and third-party programs.\n\nThere are also national security and law enforcement considerations, including the potential for fake emergency calls that sound authentic inundating our emergency response systems.\n\nThere are also risks and regulatory implications related to its impact on the environment and labor market.\n\nFor a more comprehensive list of Australian government activities involving AI touchpoints please see the Department of Industry, Science and Resources (DISR) discussion paper: Safe and responsible AI in Australia, DISR’s AI ethics principles, as well as the CSIRO’s AI Ethics Framework.\n\nDP-REG has a forthcoming paper on LLMs which covers many of the issues raised in this section.",
    "## Regulatory Challenges and Approaches\n\nIn Australia and around the world, a variety of regulatory approaches to generative AI are being considered.\n\nThere is ongoing debate over the balance between soft law through approaches such as voluntary principles and standards, and harder policy options backed by legislation and mandatory requirements.\n\nEntities should be mindful of the changing regulatory environment when considering using or developing AI products.\n\nGraduated approaches include:\n\n- Voluntary principles and governance frameworks (India)\n- AI governance frameworks, third-party testing and verification technology (Singapore)\n- Application of existing consumer safety and data regulations and the signing of pledges around self-regulatory principles (US)\n- Audits, risk and impact assessments and pre-launch disclosure requirements for ‘high-risk AI’ (Canada, UK and South Korea)\n- New and enforceable rules, including supervision powers (China)\n- Dedicated AI legislation (EU, Canada, South Korea, Brazil)\n- Intermediate bans on generative AI technology (Italy).\n\nInternational collaboration will be central to the regulation of generative AI, given the borderless nature of the internet, and the datasets and models used by developers.\n\nAs outlined above, jurisdictions are considering a range of approaches to regulating AI.\n\nIt is important that regulators and other stakeholders across the globe collaborate to set shared expectations for industry, deliver a consistent and cohesive regulatory response and avoid fragmentation.\n\neSafety is actively involved in bilateral and multilateral discussions on emerging technologies, including through the Global Online Safety Regulators Network, to promote Australia and eSafety’s perspectives on online safety regulatory issues.",
    "### Regulating Generative AI Poses Several Key Challenges,\n\nSuch as:\n\n- Identifying which actor(s) should bear responsibility.\n\nAs more online services integrate generative AI, it may be unclear who is best placed to identify and mitigate risks or is liable for malicious use.\n\nThe generative AI ecosystem includes:\n    - Services that develop foundation models, including but not limited to, OpenAI and Stability AI\n    - Services that integrate third-party models into their platforms for specific use cases, such as Snapchat\n    - People who create outputs using generative AI\n    - People who interact with content created by generative AI.\n\n- Addressing context-specific risks.\n\nTaking a risk-based approach to generative AI can help mitigate risk early in the development process.\n\nThis approach encourages influential players to monitor and respond to new risks, instead of just following prescriptive and strict technical rules or focusing on a few specific problems.\n\nFor example, incentivising influential players to monitor how their foundation models are used can help find and fix problems with large scale models.\n\nRisk management should be tailored for each situation.\n\nThis is especially important when models are made for other uses because early design choices can increase risks.\n\nFor example, developers could work with evaluation designers to give organisations tools to develop their own evaluation systems that help them understand if AI is suitable for them.\n\n- Achieving transparency and oversight.\n\nSome generative AI services, along with their systems, technologies, and processes, are not open about how they work and therefore are not as accountable as they could be.\n\nThis lack of transparency extends to system design principles, datasets, and underlying algorithms.\n\nTo regulate them effectively, it is crucial to promote greater transparency.\n\nThis means having legislation that allow access to information while also considering how this will affect businesses.\n\nUnderstanding how these technologies work also requires advanced technical expertise.\n\nThe use of plain language system and model cards can assist those without subject matter expertise to better understand how these technologies function.\n\nA range of regulatory and auditing approaches are currently being considered.\n\nChallenges to traditional auditing approaches include the sheer size of LLMs, as well as the difficulty in explaining the outcomes of multi-layered neural networks.\n\nTo be effective, these approaches should seek to use the same definitions and methodologies across wide-ranging platforms.\n\nPotential issues for regulatory oversight include how a model is tested for accuracy in order to ensure providers are accountable for false, flawed, or ‘hallucinated’ AI-generated content.\n\nThe role of an oversight or authorising body responsible for assessing whether generative AI models meet a certain standard of accuracy before they are accessible may also need to be considered.\n\nVarious jurisdictions are considering the merits and challenges of ex ante (before deployment) and ex post (after deployment) approaches.\n\n- Keeping pace with rapid developments and coordinating across regulatory remits.\n\nAs technologies continue to evolve, regulators need to coordinate their efforts and equip themselves with the necessary skills and resources to address rapid developments in the space.\n\nThis includes securing funding, expanding knowledge, enhancing tech testing capability, and developing auditing skills.\n\nCollaboration among existing regulators supports a cohesive and coordinated response to AI issues across a wide range of regulatory domains.\n\nDP-REG will continue its focus on assessing the impact of algorithms, improving digital transparency, and increased collaboration and capacity building between the four members in 2023-2024.\n\nIn response to significant developments in relation to the development, deployment and use of generative AI over the past 12 months, DP-REG will also focus on understanding and assessing the benefits, risks and harms of generative AI and how the technology intersects with the regulatory remit of each DP-REG member in 2023-24.\n\nSimilarly, in the UK, the Digital Regulation Cooperation Forum (DRCF) was established to ensure greater cooperation on online regulatory matters.\n\nThis forum also prioritises joint efforts on AI and the emergence of new generative AI tools as a key theme in its 2023/24 workplan.\n\nThe UK and other jurisdictions also have priority access to several generative AI foundation models for research and safety purposes.\n\nSimilarly, initiatives such as AI Labs, regulatory sandboxes and hackathons are gaining traction.\n\nCollaboration across multiple sectors can enhance systems and regulators’ agility to deal with emerging technologies, while mitigating the risk of regulatory capture.\n\nIt also allows a wider range of stakeholders to shape legislation and approaches surrounding these technologies.",
    "### Prevention\n\neSafety provides scaffolded, age-appropriate and contextualised programs and resources for children, parents and carers, professional learning for educators and supports the delivery of best practice online safety education.\n\neSafety collaborates with mental health professionals, child protection services and other frontline workers when developing resources for specific at-risk groups.\n\nBy understanding the benefits and risks of generative AI, people can better manage their online experiences and create a more positive online environment.\n\neSafety's research team is developing questions on algorithmic literacy to include in its 2024 youth survey.\n\nThe findings from this research will inform eSafety's online safety programs for children and young people, parents and carers, and educators.\n\nThese education programs focus on respect, resilience, responsibility and reasoning, which are relevant to AI literacy.\n\nThe research will also contribute to the international evidence base about children and young people’s digital literacy.\n\neSafety also supports online safety outreach through the Trusted eSafety Provider program, and work with mental health professionals, child protection services, and other frontline workers when developing resources for specific at-risk groups.\n\nDuring consultations, Trusted eSafety Providers highlighted an opportunity to expand existing education programs and information about generative AI.\n\nRecognising the importance of youth voices and co-design, eSafety also talked to the eSafety Youth Council, who suggested that it is important for students to have the opportunity to engage with generative AI tools to understand the strengths and limitations of the technology.\n\nThese insights will help eSafety continue its education and prevention work, and support individuals and communities in using new technologies.",
    "### Protection\n\nThe Online Safety Act 2021 (‘the Act’) provides eSafety with a range of powers and functions to address online safety issues, including those related to generative AI.\n\neSafety’s four complaints-based investigations schemes do capture AI-generated images, text, audio, and other content which meets the legislative definitions of:\n\n- Class 1 material (such as CSEA material and terrorist and violent extremism content) and Class 2 material (such as pornography)\n- Intimate images produced or shared without consent (sometimes referred to as ‘revenge porn’)\n- Cyberbullying material targeted at a child\n- Cyber abuse material targeted at an adult.\n\nUnder these investigations schemes, eSafety provides support to people who make complaints by offering guidance, assisting in or requiring the removal of certain content, and minimising the risk of further harm.",
    "### Proactive and Systemic Change\n\nThe Act also empowers eSafety to require social media services, relevant electronic services (such as messaging, gaming, and dating services), and designated internet services (other apps and websites) to report on the reasonable steps they are taking to comply with the Government’s Basic Online Safety Expectations (BOSE).\n\nThis is to make sure these services are transparent, accountable, and safe for people to use.\n\nAt the publication of this statement, eSafety has issued 13 reporting notices requiring companies to report on their efforts to implement the BOSE.\n\nEach notice included questions about the use of AI tools to detect illegal and harmful content.",
    "# Harmful Content\n\nA summary report of responses from the first seven notices, focused on steps taken to address child sexual exploitation and abuse, was published in December 2022.\n\nIn the future, eSafety could require other service providers to report on the reasonable steps they are taking to ensure the safety of their generative AI functionalities.\n\nService providers must respond to these notices.\n\nFailure to implement the expectations can also result in a published statement of non-compliance.\n\nThe Act also includes provisions for the development of industry codes to cover eight sections of the online industry.\n\nUnder this co-regulatory model, the online industry is to develop measures to deal with class 1 and class 2 content, and eSafety may register such codes.\n\nIf an industry code does not meet the registration requirements, eSafety may determine an industry standard (a regulatory instrument).\n\nIn June 2023, the eSafety Commissioner registered five industry codes which require social media services, hosting services, internet carriage service providers, app distribution services, and equipment providers to take certain steps to address the risk of class 1 material.\n\nThe requirements in these codes are enforceable and will take effect on 16 December 2023.\n\nA decision on whether to register the code for internet search engine services is yet to be determined.\n\neSafety has asked relevant industry associations to re-draft the code to capture proposed changes to search engines to incorporate generative AI features.\n\nThe aim is to address the risks associated with the use of this new technology to generate class 1 material.\n\nThe eSafety Commissioner decided not to register codes for designated internet services and relevant electronic services because the drafts submitted did not provide appropriate community safeguards.\n\neSafety is developing industry standards for these sectors and the development process will include a period of public consultation.\n\nClose consideration will be given to how these standards will address risks of class 1 content, including interplay with AI technologies and practices.\n\nThe codes development for class 2 material has not yet commenced.\n\neSafety stays ahead of emerging issues related to generative AI through ongoing consultation and horizon scanning.\n\nThis proactive approach identifies concerns arising from rapid developments in generative AI and promotes best practices for safe product design and development across industries.\n\neSafety also continues to promote Safety by Design, an initiative which encourages technology companies to anticipate, detect and eliminate online risks to make our digital environments safer and more inclusive, especially for those most at risk.",
    "# Emerging Good Practice and Safety by Design Measures\n\nA Safety by Design approach is critical to keeping users safe and building trust with communities.\n\nServices can take practical steps to minimise the risk of harm from generative AI throughout its lifecycle by following the three Safety by Design principles.",
    "## Diagram 2: Safety by Design Interventions\n\nThis diagram builds on the earlier generative AI lifecycle.\n\nThe inner circle represents the original steps in the generative AI lifecycle.\n\nThe outer circle represents Safety by Design measures which can be implemented at various points in the lifecycle and across the whole lifecycle.\n\nThese are colour-coded according to the overarching Safety by Design principles.\n\n- Real time support and reporting\n- Accountable teams\n- User education tools\n- Policies and processes\n- Technical interventions\n- Social contracts\n- Re-integration\n- Business case\n- Selecting data\n- Risk assessments\n- Age assurance\n- AI dissemination\n- Training the model\n- Internal Protocols\n- Accessible information and expectations\n- Transparency reporting\n- Feedback\n- AI generation\n- User engagement\n- Refinement\n- Release\n- Escalation pathways\n- Employee training\n- Third-party audits\n- Innovation\n- Research\n- Community consultation\n\nPlease note, some interventions may apply across more than one Safety by Design principle.",
    "### Service Provider Responsibility\n\nThe burden of safety should never fall solely on the user.\n\nProduct and service providers should identify and assess online safety risks upfront and take steps to prevent misuse and reduce people’s exposure to harms.\n\nKey actions to uphold service provider responsibility throughout the generative AI lifecycle include:\n\n- Making teams accountable for safety.\n\nNominate individuals or teams and make them accountable for creating, implementing, operating and evaluating user safety policies, as well as promoting a culture of community safety in the organisation as a whole.\n\n- Having policies and processes.\n\nSet up processes to detect, flag, and action harmful data inputs, behaviour, and content with the aim of preventing harms before they occur.\n\nThis should include:\n  - Risk and impact assessments to assess and remediate any potential online harms that could be enabled or facilitated by the product or service.\n\n- Prompt testing and design, including automated and manual tests and creative testing of edge cases.\n\nClassifiers, proactive detection tools, and manual review for CSEA material and terrorist and extreme violent material are important.\n\n- Red-teaming to stress test potential risks and harms with diverse teams, incorporating members from varied genders, backgrounds, experiences, and perspectives for a more comprehensive critique.\n\nViolet-teaming, which involves re-directing the power of AI systems by ‘identifying how a system might harm an institution or public good and then supporting the development of tools using the same system to defend the institution or public good,’ can also be considered alongside red-teaming.\n\n- Data collection and curation, including consideration of privacy obligations, data ethics, consent, ownership, and provenance.\n\n- Ongoing evaluation and continuous improvement of systems.\n\n- Age-appropriate design, supported by robust age assurance measures.\n\nServices and generative AI features that children can access should be designed with their rights, safety, and best interests in mind.\n\nSpecific protections should be in place to reduce the chances of children encountering, generating, or being exploited to produce harmful content and activity.\n\nThis requires services to use age assurance measures to identify child users and apply age-appropriate safety and privacy settings.\n\n- Internal protocols.\n\nServices should establish clear internal protocols for working with law enforcement, support services and illegal content hotlines.\n\nThey should also understand and fulfil their obligations related to jurisdictional mandatory disclosure requirements for children.\n\n- Digital watermarking of content.\n\nWatermarking is defined as the method of embedding either visible data such as a logo, or invisible or inaudible data, into digital multimedia content.\n\nGenerative AI tools can be modified to embed a watermark when they produce a piece of content.\n\n- Triaging and escalation pathways.\n\nEstablish a system to handle user safety concerns.\n\nThis includes ways to sort internal and external concerns, clear steps for escalating issues, and reporting for all safety concerns.\n\nIt also involves making it easy for people to report concerns and violations as soon as they happen.",
    "### User Empowerment and Autonomy\n\nThe principle of user empowerment and autonomy emphasises the dignity of users and the need to design features and functionality that preserve consumer and human rights.\n\nTo promote equality in society, platforms and services must engage with diverse and at-risk groups to make sure their features and functions are accessible to all.\n\nUser empowerment and autonomy can include the following measures:\n\n- Social contracts.\n\nClearly outline the rights, responsibilities, and safety expectations for the service, users, and third parties.\n\nThis can also apply to developers who use open generative AI models to build apps, application programming interfaces (APIs), and products.\n\n- Technical interventions to educate and empower users.\n\nUse technical features to educate users, reduce risks and harms, and promote safer interactions.\n\nThis could include:\n  - Implementing informed consent measures for users to understand and consent to the collection and use of their data.\n\n- Providing disclaimers and content warnings for chatbots and other generative AI technologies to let users know that outputs could be incorrect, biased, or harmful.\n\n- Developing educational content about how to detect AI ‘hallucinations’ or other forms of false or harmful content.\n\n- Ensuring users have the opportunity to understand, evaluate, control, and moderate their own interactions, particularly where generative AI agents may be involved.\n\nThis can be supported by implementing real-time prompts and nudges which alert users to the safety features available to them, such as reporting options.\n\n- Real-time support and reporting.\n\nProvide built-in support functions and feedback loops so users can track the status and outcomes of their reports and offer an opportunity for appeal.\n\nUsers should have robust controls that allow them to provide real-time feedback on AI-generated outputs.",
    "### Transparency and Accountability\n\nTo build trust in AI systems, developers and companies should prioritise transparency and accountability.\n\neSafety encourages services to share information with users and regulators about how their models and generative AI systems operate.\n\nThis should include information on data provenance, design choices, objectives, and the positive and negative outcomes of generated content.\n\nServices should also evaluate the effectiveness of safety interventions and share their findings so others can adopt them.\n\nDevelopers of large-scale models take varying approaches to transparency and access, including open-sourcing information, offering API access, or limiting public use.\n\nSome services take a graduated approach to release, where information access is rolled out in stages to enable safety measures to be added as risks become evident.\n\nTo enhance transparency and accountability, platforms and services should focus on:\n\n- Providing clear and accessible information about user safety policies, privacy policies, terms and conditions, community guidelines, and processes.\n\nKeep these up to date, make them easy to find and understand, and notify users of any changes.\n\n- Innovating and investing in new technologies to enhance user safety.\n\nShare and collaborate on safety-enhancing tools, best practices, processes, and technologies.\n\nThis could include research, automation tools, content moderation, safety tech solutions, and digital watermarking.\n\n- Consulting with a diverse user base through open engagement.\n\nEngage with experts who have specialist knowledge in various forms of harm.\n\n- Publishing regular transparency reports about reported abuses and meaningful analysis of metrics.\n\n- Documenting the capabilities, limitations, intended uses, and prohibitive uses of AI models to support processes to increase transparency and accountability (for example, through model cards, system cards, and value alignment cards).\n\n- Consider granting independent researchers, academics with access to models.",
    "## Understanding the Risks and Benefits of Generative AI Applications\n\nGenerative AI can be beneficial for creativity and efficiency, both at work and in everyday life.\n\nHowever, there are also risks such as the potential to spread illegal and restricted online content, cyberbullying of children, serious adult cyber abuse, and image-based abuse.\n\nIt is helpful to understand the systems, processes, and business models that underlie how content is developed.\n\nWhen a service generates content, it may use data drawn from the open web, which could include information about you or from your own digital footprint, such as chat history or conversations with generative AI tools.\n\nYou may be able to manage your data by turning off your chat history and choosing which conversations are used to train AI models.\n\nSome services have also introduced features that empower users to have some influence over their experiences and the accuracy of content generated through chatbot feedback loops.\n\nYou can find more information and resources about popular generative AI-enabled services such as Bing, Google Bard, Chat GPT and GPT-4 on eSafety’s website.",
    "## How to Report Harms to eSafety\n\nIf you or someone in your care is experiencing serious online abuse or harm—whether or not generative AI is involved—there are several steps you can take.\n\nIf you are experiencing online harm or abuse, you can make a report to eSafety at esafety.gov.au/report.\n\nAdditional information about protecting yourself online can be found on the eSafety website.\n\nYou can get more help by talking with an expert counselling and support service.",
    "### Other Reporting Avenues\n\n**Police**: Contact police if a crime has been committed.\n\nIf something goes wrong online, or if you think someone is in immediate danger, call Triple Zero (000) or your local police (131 444).\n\nIf you prefer to report anonymously, you can visit Crime Stoppers or call their toll-free number 1800 333 000.\n\n**ReportCyber**: If you are a victim of cybercrime, report it to police using ReportCyber.\n\n**Scamwatch**: If you see a scam and want to report it, you can report it to Scamwatch.\n\nThis includes dating and romance scams, buying and selling scams, fake charities, investments, jobs, and employment.\n\n**Report incidents of online child abuse**: Report incidents of online child abuse material to the Australian Centre to Counter Child Exploitation (ACCCE).\n\nIn the case of a child who is in immediate danger or risk, call 000 or your local police station.",
    "# Acknowledgements\n\neSafety acknowledges the contribution made by experts in sharing their insights on generative AI with eSafety.\n\nIn particular, we thank the following experts:\n\n- ARC Centre of Excellence for Automated Decision-Making and Society (ADM+S)\n- Tiberio Caetano, Gradient Institute\n- Associate Professor Jeffrey Chan, RMIT\n- Yi-Ling Chung, The Alan Turing Institute\n- Louis Claxton, Faculty AI\n- Professor Nick Davis, University of Technology Sydney\n- Professor Hany Farid, University of California, Berkeley\n- Associate Professor Asher Flynn, Monash University\n- Henry Fraser, Queensland University of Technology\n- Dr Jake Goldenfein, University of Melbourne\n- Rebecca Johnson, University of Sydney\n- Nijma Khan, Faculty AI\n- Professor Chris Leckie, University of Melbourne\n- Margaret Mitchell, Hugging Face\n- Lucinda Nelson, Queensland University of Technology\n- Dr Rebecca Portnoff, Thorn\n- Dr Louis Rosenberg, Unanimous A.I.\n\n- Professor Mark Sanderson, RMIT\n- Professor Ed Santow, University of Technology Sydney\n- Dr Aaron Snoswell, Queensland University of Technology\n- Professor Nicolas Suzor, Queensland University of Technology\n- Dr Emmanuelle Walkowiak, La Trobe University\n- Professor Kimberlee Weatherall, University of Sydney\n- Angus R Williams, The Alan Turing Institute\n\nWe extend our thanks to the Trusted eSafety Providers, the eSafety Youth Council and other academics who shared their insights, lived experiences, and helped us to produce this paper.\n\n---\n\ni. eSafety Commissioner Statement of Expectations.\n\nDecember 2022. source\n\nii.\n\neSafety Regulatory Posture and Priorities 2020-21.\n\nNovember 2021. source\n\niii.\n\nDeepfakes are fake digital photos, videos, or sound files of real people which have been edited to create realistic, but false, depictions of them doing or saying something.\n\nFor further information, see eSafety’s Tech trends and challenges position statement on Deepfakes: source\n\niv.\n\nRecommender systems, also known as content curation systems, are the systems that prioritise content or make personalised content suggestions to users of online services.\n\nFor further information, see eSafety’s Tech trends and challenges position statement on Recommender systems and algorithms: source\n\nv. A neural network is defined as a mathematical system, modelled on the human brain, that learns skills by finding statistical patterns in data.\n\nIt consists of layers of artificial neurons: The first layer receives the input data, and the last layer outputs the results.\n\nSee K Roose, C Metz, How to become an expert on AI, The New York Times, 4 April, 2023. source\n\nvi.\n\nFuture of Life Institute, Pause Giant AI Experiments: An Open Letter, 22 March, 2023. source\n\nvii.\n\nD Thiel, M Stroebel, R Portnoff, Generative ML and CSAM: Implications and mitigations, Thorn and Stanford Internet Observatory, 24 June, 2023. source\n\nviii.\n\nUnited States Federal Bureau of Investigations.\n\nPublic Service Announcement: Malicious Actors Manipulating Photos and Videos to Create Explicit Content and Sextortion Schemes.\n\n5 June, 2023. source\n\nix.\n\nFurther analysis of algorithmic bias has been published by the Australian Human Rights Commission.\n\nSee Australian Human Rights Commission, Addressing algorithmic bias to ensure ethical AI, 24 November, 2020. source\n\nx. L Rosenberg, The Manipulation Problem: Conversational AI as a Threat to Epistemic Agency.\n\n19 June, 2023. source\n\nxi.\n\nFor example, The Verge reported on 9 March 2023 that Meta's LLaMA had leaked onto 4chan, advising that on March 3rd, a downloadable torrent of the system was posted on 4chan and has since spread across various AI communities source; for more information on OpenAI’s reported open-source language model plans, see: source\n\nxii.\n\nJ Goldstein, G Sastry, M Musser, R DiResta, M Gentzel, K Sedova, Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations, Georgetown University’s Center for Security and Emerging Technology, OpenAI, Stanford Internet Observatory, January 2023. source; P Chavez, An AI Challenge: Balancing Open and Closed Systems, Centre for European Policy Analysis, 30 May, 2023. source\n\nxiii.\n\nFor example, Meta announced plans in June 2023 to incorporate generative AI text, image and video generators into its flagship products, such as Facebook and Instagram, see: source; Google announced at its annual I/O conference that it will infuse results with generative artificial intelligence technology, see: source\n\nxiv.\n\nFor example, OpenAI founder Sam Altman has spent time touring internationally and advocating for global AI regulation.\n\nSee: source\n\nxv.\n\neSafety Commissioner.\n\nDigital Platform Regulators Forum.\n\nsource\n\nxvi.\n\nIntegrity Institute.\n\nUnleashing the potential of generative AI in integrity, trust and safety work: opportunities, challenges and solutions.\n\n8 June, 2023. source\n\nxvii.\n\nS Coghlan, K Leins, S Sheldrick, M Cheong, P Gooding, S D’Alfonso.",
    "8 June, 2023. source\n\nxvii.\n\nS Coghlan, K Leins, S Sheldrick, M Cheong, P Gooding, S D’Alfonso.\n\nTo chat or bot to chat: Ethical issues with using chatbots in mental health.\n\nDigital Health, 9, December 2023. source\n\nxviii.\n\nUNICEF.\n\nSafer Chatbots Implementation Guide.\n\nsource\n\nxix.\n\nThe generative AI lifecycle has been developed by drawing upon a broad range of resources, including from ActiveFence, Anthropic, Genpact, Thorn.\n\nMeasures may be intersectional, overlapping or apply across the stack.\n\nxx.\n\nWorld Economic Forum.\n\nThe Presidio Recommendations on Responsible Generative AI.\n\nJune 2023. source\n\nxxi.\n\nC A Sharma, What is generative AI?, LinkedIn article, 27 January, 2023. source\n\nxxii.\n\nY Noema, Learning: Supervised, Unsupervised, Self-Supervised & Semi-Supervised.\n\nMedium.\n\n23 June, 2022. source\n\nxxiii.\n\nC A Sharma, What is generative AI?, LinkedIn article, 27 January, 2023. source\n\nxxiv.\n\nFor additional information on supervised, unsupervised, and self-supervised learning, see: Y Noema, Learning: Supervised, Unsupervised, Self-Supervised & Semi-Supervised.\n\nMedium.\n\n23 June, 2022. source\n\nxxv.\n\nV E Kingsly, Trust and safety in the era of generative AI, Genpact, 9 June, 2023. source\n\nxxvi.\n\nE Bell, Generative AI: How It Works, History, and Pros and Cons, Investopedia, 26 May, 2023. source; D Ingram, ChatGPT is powered by these contractors making $15 an hour, NBC News, 7 May, 2023. source; B Perrigo, Exclusive: OpenAI used Kenyan workers on less than $2 per hour to make ChatGPT less toxic, Time, 18 January, 2023. source\n\nxxvii.\n\nN Clark, ‘Grandma exploit’ tricks Discord’s AI chatbot into breaking its own ethical rules, Polygon, 19 April, 2023. source\n\nxxviii.\n\nN Schwartz, Generative AI Safety by Design Framework, ActiveFence, 1 May, 2023. source\n\nxxix.\n\nFor example, research conducted by Twitter in 2021 on the proliferation of harmful and offensive content found that ‘interventions allowing users to reconsider their comments can be an effective mechanism for reducing offensive content online’.\n\nFor more information, see: M Katasaros, K Yang, L Fratamico, Reconsidering Tweets: Intervening During Tweet Creation Decreases Offensive Content, Proceedings of the International AAAI Conference on Web and Social Media, 16, 1 December, 2021. source\n\nxxx.\n\nV E Kingsly, Trust and safety in the era of generative AI, Genpact, 9 June, 2023. source\n\nxxxi.\n\nB Lutkevich, Model collapse explained: How synthetic training data breaks AI, Tech Target, 7 July, 2023 source\n\nxxxii.\n\nFor further information about Australia’s anti-discrimination laws, see: source\n\nxxxiii.\n\nJ Goldstein, G Sastry, M Musser, R DiResta, M Gentzel, K Sedova, Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations, Georgetown University’s Center for Security and Emerging Technology, OpenAI, Stanford Internet Observatory, January 2023. source\n\nxxxiv.\n\nL Rosenberg, Generative AI: the technology of the year for 2022, Big Think, 20 December, 2022. source\n\nxxxv.\n\nP Chavez, An AI Challenge: Balancing Open and Closed Systems, Centre for European Policy Analysis, 30 May, 2023. source\n\nxxxvi.\n\nI Solaiman, The Gradient of Generative AI Release: Methods and Considerations, Hugging Face, 5 February 2023.\n\nxxxvii.\n\nS Halpern, What We Still Don’t Know About How A.I.\n\nIs Trained, The New Yorker, 28 March, 2023.\n\nxxxviii.\n\nC Roche, P J Wall, D Lewis, Ethics and diversity in artificial intelligence policies, strategies and initiatives, AI Ethics, 6 October, 2022. source\n\nxxxix.\n\neSafety Commissioner, Safety by Design – Investors and Financial Entities.\n\nsource\n\nxl.\n\nL Rosenberg, The Metaverse and Conversational AI as a Threat Vector for Targeted Influence, 2023. \n\nxli.\n\nThorn, Generative AI: Now is the time for safety by design, 26 May, 2023. source\n\nxlii.\n\nD Thiel, M Stroebel, R Portnoff, Generative ML and CSAM: Implications and mitigations, Thorn and Stanford Internet Observatory, 24 June, 2023. source \n\nxliii.\n\neSafety provides additional advice on ‘Privacy and your child’ on the eSafety website: source\n\nxliv.\n\nH Bhatt, How Generative AI will affect content moderation, Spectrum Labs, 3 April 2023.\n\nxlv.\n\nD Thiel, M Stroebel, R Portnoff, Generative ML and CSAM: Implications and mitigations, Thorn and Stanford Internet Observatory, 24 June, 2023. source  \n\nxlvi.\n\nI Lapowsky, The Race to Prevent ‘the Worst Case Scenario for Machine Learning’, The New York Times, 24 June, 2023. source\n\nxlvii.\n\nI Lapowsky, The Race to Prevent ‘the Worst Case Scenario for Machine Learning’, The New York Times, 24 June, 2023. source\n\nxlviii.\n\nE Graham, ‘Alarming Content’ from AI Chatbots Raises Child Safety Concerns, Senator Says, Nextgov, 21 March 2023. source\n\nxlix.\n\nC Xiang, Eating Disorder Helpline Disables Chatbot for 'Harmful' Responses After Firing Human Staff, Vice, 31 May 2023.\n\nl. E Graham, ‘Alarming Content’ from AI Chatbots Raises Child Safety Concerns, Senator Says, Nextgov, 21 March 2023. source\n\nli.",
    "H Farid, Creating, Using, Misusing, and Detecting Deep Fakes, Journal of Online Trust and Safety, 1(4), September 2022.\n\nlii.\n\nH Ajder, G Patrini, F Cavalli, L Cullen, The State of Deepfakes: Landscape, Threats and Impact, Deeptrace, September 2019. source\n\nliii.\n\nA Flynn, A Powell, AJ Scott, E Carma, Deepfakes and digitally altered imagery abuse: A cross-country exploration of an emerging form of image-based sexual abuse, British Journal of Criminology, 62(6), December, 2021. source\n\nliv.\n\nUnited States Federal Bureau of Investigations.\n\nPublic Service Announcement: Malicious Actors Manipulating Photos and Videos to Create Explicit Content and Sextortion Schemes.\n\n5 June, 202",
    "# References and Publications\n\n1.\n\nIC3 PSA - June 2023\n\n2.\n\nK McGuffe, A Newhouse, \"The Radicalization Risks of GPT-3 and Advanced Neural Language Models,\" Centre of Terrorism, Extremism and Counterterrorism, September 2020.\n\nAvailable Here\n\n3.\n\nEuropol, \"ChatGPT: The impact of Large Language Models on Law Enforcement,\" 27 March 2023.\n\nPDF\n\n4.\n\nActiveFence, \"Generative AI: New Attack Vector for Trust & Safety,\" 31 May 2023.\n\n5.\n\nP Hacker, A Engel, M Mauer, \"Regulating ChatGPT and other Large Generative AI Models,\" Arxiv, Working Paper, version April 5, 2023.\n\n6.\n\nW Oremus, \"The clever trick that turns ChatGPT into its evil twin,\" Washington Post, 14 February 2023.\n\n7.\n\nS A Thompson, \"Making Deepfakes Gets Cheaper and Easier Thanks to A.I.,\" New York Times, 12 March 2023.\n\n8.\n\nH Farid, \"Creating, Using, Misusing, and Detecting Deep Fakes,\" Journal of Online Trust & Safety, 1(4), 20, September 2022.\n\n9.\n\nOECD Digital Economy Papers, \"AI language models,\" April 2023.\n\nOECD Library\n\n10.\n\nL Rosenberg, \"Generative AI: the technology of the year for 2022,\" Big Think, 20 December 2022.\n\nBig Think\n\n11.\n\nW D Heaven, \"Generative AI is changing everything.\n\nBut what’s left when the hype is gone?\"\n\nMIT Technology Review, 16 December 2022.\n\nMIT Tech Review\n\n12.\n\nA Luccioni, C Akiki, M Mitchell, Y Jernite, \"Stable Bias: Analyzing Societal Representations in Diffusion Models,\" arXiv:2303.11408.\n\nMarch 2023.\n\n13.\n\nM Heikkilä, \"These new tools let you see for yourself how biased AI image models are,\" MIT Technology Review, 22 March 2023.\n\n14.\n\nA Abid, M Farooqi, J Zou, \"large language models associate Muslims with violence,\" Nature Machine Intelligence, 3, June 2021.\n\n15.\n\nA Simmons, R Vasa, \"Garbage in, garbage out: zero-shot detection of crime using Large Language Models.\"\n\nApplied Artificial Intelligence Institute, Deakin University, 4 July 2023.\n\nArxiv PDF\n\n16.\n\nL Li, L Fan, S Atreja, L Hemphill, “HOT” ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media,\" School of Information, University of Michigan, 20 April 2023.\n\nArxiv PDF\n\n17.\n\nOfcom research on the impact of video sharing platform design on user behaviour, 2023.\n\nOfcom Report\n\n18.\n\nKids Help Phone, \"Hello!\n\nI’m Kip – How can I help you?\n\n\", 2 March 2021.\n\nKids Help Phone\n\n19.\n\nHouse Standing Committee on Employment, Education and Training in Australia, inquiry into the use of generative artificial intelligence, 24 May 2023.\n\nInquiry Details\n\n20.\n\nDefinition of 'Phishing': the process where scammers send fake information to manipulate recipients or obtain personal information from them.\n\n21.\n\nC Warzel, \"Why you fell for the fake Pope coat,\" The Atlantic, 28 March 2023.\n\nThe Atlantic\n\n22.\n\nP Hacker, A Engel, and M Mauer, \"Regulating ChatGPT and other Large Generative AI Models,\" Working Paper, version 5 April 2023.\n\n23.\n\nB Johnson, \"Australia’s AI Acid Test,\" Medium, 2 June 2023.\n\n24.\n\nK Perset, A Plonk, S Russell, \"As language models and generative AI take the world by storm, the OECD is tracking the policy implications,\" OECD Policy Observatory, 13 April 2023.\n\n25.\n\nI Solaiman, \"The Gradient of Generative AI Release: Methods and Considerations,\" Hugging Face, 5 February 2023.\n\n26.\n\nM Loi, A Ferrario, E Viganò, \"Transparency as design publicity: explaining and justifying inscrutable algorithms,\" Ethics and Information Technology, 23:253–263, 2021.\n\n27.\n\nAI Now Institute, \"Algorithmic Accountability: Moving Beyond Audits,\" 11 April 2023.\n\n28.\n\nEuropean Commission’s February 2023 White Paper on Artificial Intelligence: A European approach to excellence and trust.\n\n29.\n\nN Lomas, \"OpenAI, DeepMind and Anthropic to give UK early access to foundational models for AI safety research,\" Tech Crunch, 12 June 2023.\n\n30. eSafety Commissioner, Responses to transparency notices.\n\neSafety\n\n31.\n\nMinimum compliance measures in the industry codes and future industry standards represent mandatory steps.\n\n32.\n\nA Ovadya, \"Red-Teaming Improved GPT-4.\n\nViolent Teaming Goes Even Further,\" WIRED, 29 March 2023.\n\nWIRED\n\n33.\n\nB Rosenblatt, \"Google and OpenAI Plan Technology to Track AI-Generated Content,\" Forbes, 22 July 2023.\n\nForbes\n\n34.\n\nGartner definition of an API: 'an interface that provides programmatic access to service functionality and data within an application or a database'.\n\n35.\n\nR Iyer, \"4 ways AI safety efforts could learn from experiences with social media,\" Designing Tomorrow, 24 May 2023.\n\nSubstack\n\n36.\n\nWorld Economic Forum.\n\n\"The Presidio Recommendations on Responsible Generative AI.\"\n\nJune 2023.\n\nWorld Economic Forum\n\n37.\n\nI Solaiman, \"The Gradient of Generative AI Release: Methods and Considerations,\" Hugging Face, 5 February 2023.\n\n38.\n\nExamples of companies: ActiveFence, SpectrumLabs, Thorn.\n\n39.\n\n\"Watermarking\" defined as embedding data like a logo or invisible data into digital content.\n\n40.\n\nOpenAI provides the option to switch off chat history when using its ChatGPT chatbot.\n\nThe Verge",
    "## PRINCIPLES FOR THE DEVELOPMENT, DEPLOYMENT, AND USE OF GENERATIVE AI TECHNOLOGIES\n\nJune 27, 2023\n\nGenerative Artificial Intelligence (AI) is a broad term used to describe computing techniques and tools that can be used to create new content, including: text, speech and audio, images and video, computer code, and other digital artifacts.\n\nWhile such systems offer tremendous opportunities for benefits to society, they also pose very significant risks.\n\nThe increasing power of generative AI systems, the speed of their evolution, broad application, and potential to cause significant or even catastrophic harm means that great care must be taken in researching, designing, developing, deploying, and using them.\n\nExisting mechanisms and modes for avoiding such harm likely will not suffice.\n\n* Lead authors of this document for USTPC were Ravi Jain, Jeanna Matthews, and Alejandro Saucedo.\n\nImportant contributions were made by Harish Arunachalam, Brian Dean, Advait Deshpande, Simson Garfinkel, Andrew Grosso, Jim Hendler, Lorraine Kisselburgh, Srivatsa Kundurthy, Marc Rotenberg, Stuart Shapiro, and Ben Shneiderman.\n\nAssistance also was provided by: Ricardo Baeza-Yates, Michel Beaudouin-Lafon, Vint Cerf, Charalampos Chelmis, Paul DeMarinis, Nicholas Diakopoulos, Janet Haven, Ravi Iyer, Carlos E. Jimenez-Gomez, Mark Pastin, Neeti Pokhriyal, Jason Schmitt, and Darryl Scriven.\n\n1.\n\nThe first set of generative AI advances rest on very large AI models that are trained on an extremely large corpus of data.\n\nExamples that are text-oriented include BLOOM, Chinchilla, GPT-4, LaMDA, and OPT, as well as conversation oriented models like Bard, ChatGPT, and others.\n\nBy definition, this is a rapidly evolving area.\n\nThis list of examples, therefore, is by no means intended to be exhaustive.\n\nSimilarly, the principles advanced in this document also are certain to evolve in response to changing circumstances, technological capabilities, and societal norms.\n\n2.\n\nGenerative AI models and tools offer significant new opportunities for enhancing numerous online experiences and services, automating tasks normally done by humans, and assisting and enhancing human creativity.\n\nFrom another perspective, such models and tools also have raised significant concerns about multiple aspects of information and its use, including accuracy, disinformation, deception, data collection, ownership, attribution, accountability, transparency, bias, user control, confidentiality, privacy, and security.\n\nGenerative AI also raises important questions outside the scope of this document, including many about the replacement of human labor and jobs by AI-based machines and automation.\n\nThis statement puts forward principles and recommendations for best practices in these and related areas based on a technical understanding of generative AI systems.\n\nThe first four principles, which are specific to generative AI, address issues regarding limits of use, ownership, personal data control, and correctability.\n\nThe following four principles were derived and adapted from the joint ACM Statement on Principles for Responsible Algorithmic Systems released in October 2022.\n\nThese pertain to transparency, auditability and contestability, limiting environmental impacts, and security and privacy.\n\nThis statement also reaffirms and includes five principles from the joint statement as originally formulated and has been informed by the January 2023 ACM TechBrief: Safer Algorithmic Systems.\n\nThe following instrumental principles, consistent with the ACM Code of Ethics, are intended to foster fair, accurate, and beneficial decision-making concerning generative and all other AI technologies:",
    "### Limits and guidance on deployment and use\n\nIn consultation with all stakeholders, current law and regulation should be reviewed and applied as written or revised to limit the deployment and use of generative AI technologies when required to minimize harm.\n\nNo high-risk AI system should be allowed to operate without clear and adequate safeguards, including a “human in the loop” and clear consensus among relevant stakeholders that the system's benefits will substantially outweigh its potential negative impacts.\n\n3.\n\nTechnical considerations do not, however, exist in a vacuum.\n\nIn many cases, they thus have led us to also recommend that legal, regulatory, and policy issues raised by generative AI be discussed transparently among multiple stakeholders.\n\nThe goal of such efforts must be appropriately robust frameworks for oversight of these technologies grounded firmly in technical fundamentals and practice.\n\nThe safe and responsible use of generative AI will be possible only with the transparent and consistent collaboration over time of all impacted stakeholders.\n\n4.\n\nStatement on Principles for Responsible Algorithmic Systems, ACM Technology Policy Council and its Europe and U.S. Technology Policy Committees (October 26, 2022)  (Joint Statement).\n\n5.\n\nMultiple additional principles articulated in the joint statement also remain germane and are restated in the last section of this document.\n\nThey concern legitimacy and competency, minimizing harms, interpretability and explainability, maintainability, and accountability and responsibility.\n\n6.\n\nThe ACM Code of Ethics and Professional Conduct was designed to inspire and guide the ethical conduct of all computing professionals, including current and aspiring practitioners, instructors, students, influencers, and anyone who uses computing technology in an impactful way.\n\nThe Code includes principles formulated as statements of responsibility, based on the understanding that the public good is always the primary consideration.\n\nEach principle is supplemented by guidelines, which provide explanations to assist computing professionals in understanding and applying the principle.\n\nSee \n\nProviders should undertake extensive impact assessments prior to the deployment of such technologies to thoughtfully ensure that the benefits to society of any such deployment outweigh its risks.\n\nOne approach is to define a hierarchy of risk levels, with unacceptable risk at the highest level and minimal risk at the lowest level.\n\nSuch categorizations must include the risk that users who attribute human characteristics or behavior to generative AI systems inappropriately, may be more likely to rely upon such systems’ outputs and experience harm.\n\nProviders of generative AI systems released to the general public should provide recommendations for the correct and responsible use of those systems, and also provide sufficient information about such systems to permit expert evaluation of their risks and impacts.\n\nFinally, providers should enable mechanisms to allow generative AI systems to be deactivated unilaterally by external means in emergency situations.",
    "### Ownership\n\nInherent aspects of how generative AI systems are structured and function are not yet adequately accounted for in intellectual property (IP) law and regulation.\n\n7.\n\n“Providers” is used in this document to mean all entities that deliver generative AI technologies, components, systems, or applications to users or other entities.\n\nThis may include developers; model, dataset, subsystem, platform, system, or application providers; and parties such as sellers, resellers, integrators, or marketers.\n\n8.\n\nVarious bodies such as the National Institute of Standards and Technology (NIST), the Institute of Electrical and Electronics Engineers (IEEE), and the European Union (EU) have made recommendations that are relevant in this regard.\n\n(NIST has formulated a risk management framework while the IEEE and EU articulate a risk hierarchy.)\n\nSee respectively: National Institute of Standards and Technology, Artificial Intelligence Risk Management Framework (AI RMF 1.0), NIST AI 100-1, January 2023 [ IEEE Standard for System, Software, and Hardware Verification and Validation, 1012-2016 [ and Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, 2021/0106 (COD), April 21, 2023 [ Risk assessments of generative AI systems should be done by teams of cross-disciplinary experts and public, private, and non-governmental bodies, and with broad public input.\n\nWe also note that generative AI systems are complex, not yet fully understood, and may demonstrate emergent behaviors and emergent risks that are not predictable simply by extrapolating from their existing capabilities.\n\nThis is an area that thus needs substantial further research.\n\nAnother such area is that of bias which, while a risk in AI systems in general, has become a particularly significant concern with the large language models used in generative AI.\n\n9.\n\nGenerative AI providers should also provide meta-information about models to enable experts and trained members of the community to understand them and evaluate their impacts.\n\nSuch information might productively include datasheets, model cards, model whitepapers, factsheets, and detailed impact assessments.\n\nWell-designed dashboards also could give users a clearer understanding of the impact of their decisions of how best to use generative AI systems, and greater control over their output.\n\n10.\n\nIt is not currently possible, for example, for users or creators of generative AI systems to definitively say which portions of a training dataset adhere to which copyrights or licenses, which portions of that dataset may have directly or indirectly contributed to a particular generated artifact, and consequently what the copyright and licensing implications of that artifact may be.\n\nThis not only creates an issue for creators whose works have been used to generate artifacts, but also for users of those artifacts who may be exposed to the risk of substantial penalties for copyright violations.\n\nRegimes thus should be reviewed and, where necessary, revised to strengthen protections for human creators without placing undue restrictions on lawful permissive access to copyrighted material (e.g., pursuant to fair use or fair dealing provisions in the US and Europe) or diminishing the overall creative commons.",
    "### Personal data control\n\nGenerative AI systems should allow a person to opt out of their data being used to train the system or facilitate its generation of information.\n\nIn many cases, the default choice should be for a person to explicitly opt into their data being used.\n\nAt minimum, such systems should provide mechanisms to allow any person to opt out of their personal data, including their biometric data, being used for such purposes.\n\nIf a person opts out of providing data once a model has been trained, there should be a mechanism in place to update the model to remove that individual's data.",
    "### Correctability\n\nProviders of generative AI systems should create and maintain public repositories where errors made by the system can be noted and, optionally, corrections made.\n\nIf an error is discovered and noted, providers should develop transparent mechanisms that allow stakeholders to track providers’ progress toward eliminating errors, including the retraining of models and other mitigations as needed.\n\n11.\n\nIn the United States, a person’s original and creative works are automatically copyrighted when first “fixed in a medium of tangible expression.” Generally, absent prior approval by the copyright holder, works cannot be used unless deemed a “fair use” under a four-factor statutory test, or they are subject to a limited number of express other statutory exceptions.\n\nOther countries may or may not provide similar protection for works created within their own jurisdictions.\n\n12.\n\nAreas of creative work that have traditionally fallen outside of IP controls, such as artistic style, become contentious when a generative AI tool is able to reduce demand for the efforts of human creators through automated mimicry, especially without citation of the works of human creators in a training set.\n\nThis is especially critical since, unlike a human, the tool can do so quickly and at large scale.\n\nAt the same time, while traditional notions of fair and acceptable use of copyrighted works allow for certain digital processes to be carried out on them (e.g., to display the works on a screen), it is not clear that this “authorization” will include their use as training data for AI to generate further artifacts in all jurisdictions.\n\nOther unforeseen scenarios or outcomes about the uses of generative AI for creative works that either test the boundaries of existing laws and regulations or lack any legal precedent may emerge in the future.\n\nWe note with concern, for example, attempts at for-profit monetization of human-generated work available through a creative commons and/or publicly available dataset with explicit or implicit human IP attached that contravenes the original intent of or arrangements under which the IP was made available.\n\nSuch use cases, and doubtless many others, must be addressed by new statutes or judicially resolved on a case-by-case basis.\n\n13.\n\nBiometric data has been afforded particular protection in some jurisdictions.\n\nIn the United States, for example, regulation of its use is a matter of state law, both common and statutory.\n\nSee, e.g., the Illinois Biometric Information Privacy Act, 740 ILCS 14 (2008), which places limits on the use of personal images and likenesses [ The European Union’s General Data Protection Regulation provides broad similar protection.\n\nRegulation (EU) 2016/679 of the European Parliament and of the Council on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC, April 2016 [",
    "### Transparency\n\nAny application or system that utilizes generative AI should conspicuously disclose that it does so to the appropriate stakeholders.\n\nIn particular, where generative AI is being used to simulate human agents, at all times individuals must be promptly and clearly informed that they are interacting with a system as opposed to a human.\n\nFurther, generative AI systems should warn users that information the system generates may contain errors, and that their authoritative tone or other attributes may be misleading.\n\nIn addition, to prevent unintended or malicious misrepresentation (e.g., “deepfakes”), generative AI systems should provide a mechanism that permits information they generate to be unambiguously identified by third parties as having been AI produced.\n\nSuch techniques may include cryptographic or steganographic markers.",
    "### Auditability and contestability\n\nProviders of Generative AI systems should ensure that system models, algorithms, data, and outputs can be recorded where possible (with due consideration to privacy), so that they may be audited and/or contested in appropriate cases.\n\nIt is also important that providers of Generative AI systems have appropriate auditing strategies in place so citizens, consumer groups, and industry bodies can review and comment on them over time to facilitate their correction and potential retraining.",
    "### Limiting environmental impacts\n\nGiven the large environmental impacts of Generative AI models, we recommend that consensus on methodologies be developed to measure, attribute, and actively reduce such impacts.\n\nIn particular, the total environmental costs to society, including those that are externalized by providers of the technology, must be determinable and attributed to the relevant entities in the ecosystem.\n\nFinally, sustainability issues also should be considered and accounted for during a system's entire life cycle.\n\n14.\n\nThe necessity for transparency becomes even more critical when generative AI intentionally simulates human agents as some users may anthropomorphize such systems inappropriately.\n\nA related issue is that some generative AI systems can present their outputs in authoritative language and a manner that conveys their confidence to users.\n\nHowever, the systems are ultimately limited by their training datasets, and the quantity and quality of training sets as well the techniques used can lead to subtle errors.\n\nThis may cause users to miss errors that have been generated by the AI (sometimes called “hallucinations”) or be lulled into not checking for them adequately, if at all.\n\n15.\n\nThe cumulative estimated carbon emissions of recently released Generative AI models greatly exceed those of more traditional AI models.\n\nAs the use of Generative AI grows such emissions could increase significantly.\n\nSee C.J.\n\nWu et al., Sustainable AI: Environmental Implications, Challenges and Opportunities, Conference on Machine Learning and Systems (MLSys), 2022.\n\n[\n\n16.\n\nSuch analysis must extend beyond simply focusing on operational efficiency during system training or inference to include, e.g., the tradeoff between AI performance and environmental impact, or techniques to reduce or reuse model training runs or artifacts.",
    "### Heightened security and privacy\n\nGenerative AI systems are susceptible to a broad range of new security and privacy risks, including new attack vectors and malicious data leaks, among others.\n\nTheir use, therefore, requires heightened risk-mitigation controls to ensure that relevant security and privacy best practices are verifiably and consistently employed throughout the model life cycle, and that these can be effectively audited, both internally and as appropriate by third parties.",
    "### Legitimacy and competency\n\nDesigners of algorithmic systems should have the management competence and explicit authorization to build and deploy such systems.\n\nThey also need to have expertise in the application domain, a scientific basis for the systems’ intended use, and be widely regarded as socially legitimate by stakeholders impacted by the system.\n\nLegal and ethical assessments must be conducted to confirm that any risks introduced by the systems will be proportional to the problems being addressed, and that any benefit-harm trade-offs are understood by all relevant stakeholders.",
    "### Minimizing harm\n\nManagers, designers, developers, users, and other stakeholders of algorithmic systems should be aware of the possible errors and biases involved in their design, implementation, and use, and the potential harm that a system can cause to individuals and society.\n\nOrganizations should routinely perform impact assessments on systems they employ to determine whether the system could generate harm, especially discriminatory harm, and to apply appropriate mitigations.\n\nWhen possible, they should learn from measures of actual performance, not solely patterns of past decisions that may themselves have been discriminatory.\n\n17.\n\nFor example, the use of generative AI models to generate computer code presents substantial security risks.\n\nSuch models are typically trained on code repositories.\n\nIf any credentials are stored with the code, malicious actors could exploit the model to output valid keys.\n\nIndeed, they could go even further and introduce malware in response to queries, whether by poisoning training data or corrupting system outputs.\n\nAnalogous security risks also exist for many other types of generative AI models.\n\n18.\n\nThe inherently necessary use of large training dataset and model sizes for generative AI systems can lead to privacy issues becoming more likely or severe than for smaller models or datasets.\n\nModels may directly or indirectly infer personally identifiable information (such as employment, home address, and family data) of particular individuals, which are then susceptible to data leaks.\n\nSimilarly, there are risks of reverse engineering training data from trained models.\n\n(Although models amalgamate training data, it has been proven that training examples may nonetheless be recovered in this process.)\n\nSee for example, Carlini et al., Quantifying Memorization Across Neural Language Models, Conference on Learning Representation, 2023.\n\n[\n\n19.\n\nProjects with no clear scientific basis (e.g., inferring personality traits from facial images) should not be deployed.",
    "### Interpretability and explainability\n\nManagers of algorithmic systems are encouraged to produce information regarding both the procedures that the employed algorithms follow (interpretability) and the specific decisions that they make (explainability).\n\nExplainability may be just as important as accuracy, especially in public policy contexts or any environment in which there are concerns about how algorithms could be skewed to benefit one group over another without acknowledgement.\n\nIt is important to distinguish between explanations and after-the-fact rationalizations that do not reflect the evidence, or the decision-making process used to reach the conclusion being explained.",
    "### Maintainability\n\nEvidence of all algorithmic systems’ soundness should be collected throughout their life cycles, including documentation of system requirements, the design or implementation of changes, test cases and results, and a log of errors found and fixed.\n\n20.\n\nOtherwise, the system may become less appropriate as inputs drift from those originally anticipated, or if the underlying real-world conditions change (e.g., facial recognition systems are used on a wider or different demographic than was present in the training data).\n\nProper maintenance may require retraining systems with new training data and/or replacing the models employed.",
    "### Accountability and responsibility\n\nPublic and private bodies should be held accountable for decisions made by algorithms they use, even if it is not feasible to explain in detail how those algorithms produced their results.\n\nSuch bodies should be responsible for entire systems as deployed in their specific contexts, not just for the individual parts that make up a given system.\n\nWhen problems in automated systems are detected, organizations responsible for deploying those systems should document the specific actions that they will take to remediate the problem and under what circumstances the use of such technologies should be suspended or terminated.",
    "## UNESCO – A Global Leader in Education\n\nEducation is UNESCO’s top priority because it is a basic human right and the foundation for peace and sustainable development.\n\nUNESCO is the United Nations’ specialized agency for education, providing global and regional leadership to drive progress, strengthening the resilience and capacity of national systems to serve all learners.\n\nUNESCO also leads efforts to respond to contemporary global challenges through transformative learning, with special focus on gender equality and Africa across all actions.",
    "### The Global Education 2030 Agenda\n\nUNESCO, as the United Nations’ specialized agency for education, is entrusted to lead and coordinate the Education 2030 Agenda, which is part of a global movement to eradicate poverty through 17 Sustainable Development Goals by 2030.\n\nEducation, essential to achieve all of these goals, has its own dedicated Goal 4, which aims to “ensure inclusive and equitable quality education and promote lifelong learning opportunities for all.” The Education 2030 Framework for Action provides guidance for the implementation of this ambitious goal and commitments.\n\n---\n\nPublished in 2023 by the United Nations Educational, Scientific and Cultural Organization, 7, place de Fontenoy, 75352 Paris 07 SP, France\n\n© UNESCO 2023\n\nISBN 978-92-3-100612-8\n\nThis publication is available in Open Access under the Attribution-ShareAlike 3.0 IGO (CC-BY-SA 3.0 IGO) license ( By using the content of this publication, the users accept to be bound by the terms of use of the UNESCO Open Access Repository (\n\nThe designations employed and the presentation of material throughout this publication do not imply the expression of any opinion whatsoever on the part of UNESCO concerning the legal status of any country, territory, city or area or of its authorities, or concerning the delimitation of its frontiers or boundaries.\n\nImages marked with an asterisk (*) do not fall under the CC-BY-SA license and may not be used or reproduced without the prior permission of the copyright holders.\n\nThe ideas and opinions expressed in this publication are those of the authors; they are not necessarily those of UNESCO and do not commit the Organization.\n\nCover credit: Olexandra Simkina/Shutterstock.com*\nDesigned and printed by UNESCO\nPrinted in France",
    "## Towards a Human-Centred Approach to the Use of Generative AI\n\nPublicly available generative AI (GenAI) tools are rapidly emerging, and the release of iterative versions is outpacing the adaptation of national regulatory frameworks.\n\nThe absence of national regulations on GenAI in most countries leaves the data privacy of users unprotected and educational institutions largely unprepared to validate the tools.\n\nUNESCO’s first global guidance on GenAI in education aims to support countries to implement immediate actions, plan long-term policies, and develop human capacity to ensure a human-centred vision of these new technologies.\n\nThe Guidance presents an assessment of potential risks GenAI could pose to core humanistic values that promote human agency, inclusion, equity, gender equality, and linguistic and cultural diversities, as well as plural opinions and expressions.\n\nIt proposes key steps for governmental agencies to regulate the use of GenAI tools including mandating the protection of data privacy and considering an age limit for their use.\n\nIt outlines requirements for GenAI providers to enable their ethical and effective use in education.\n\nThe Guidance stresses the need for educational institutions to validate GenAI systems on their ethical and pedagogical appropriateness for education.\n\nIt calls on the international community to reflect on their long-term implications for knowledge, teaching, learning, and assessment.\n\nThe publication offers concrete recommendations for policy-makers and educational institutions on how the uses of GenAI tools can be designed to protect human agency and genuinely benefit learners, teachers, and researchers.\n\n> \"Since wars begin in the minds of men and women it is in the minds of men and women that the defences of peace must be constructed\"",
    "### Foreword\n\nGenerative artificial intelligence (GenAI) burst into public awareness in late 2022 with the launch of ChatGPT, which became the fastest-growing app in history.\n\nWith the power to imitate human capabilities to produce outputs such as text, images, videos, music, and software codes, these GenAI applications have caused a stir.\n\nMillions of people are now using GenAI in their daily lives and the potential of adapting the models to domain-specific AI applications seems unlimited.\n\nSuch wide-ranging capacities for information processing and knowledge production have potentially huge implications for education, as they replicate the higher-order thinking that constitutes the foundation of human learning.\n\nAs GenAI tools are increasingly able to automate some basic levels of writing and artwork creation, they are forcing education policy-makers and institutions to revisit why, what, and how we learn.\n\nThese are now critical considerations for education in this new phase of the digital era.\n\nThis publication aims to support the planning of appropriate regulations, policies, and human capacity development, to ensure that GenAI becomes a tool that genuinely benefits and empowers teachers, learners, and researchers.\n\nIt proposes key steps for governmental agencies to regulate the use of generative AI.\n\nIt also presents frameworks and concrete examples for policy formulation and instructional design that enable ethical and effective uses of this technology in education.\n\nFinally, it calls on the international community to consider the profound long-term implications of generative AI for how we understand knowledge and define learning content, methods, and outcomes, as well as the way in which we assess and validate learning.\n\nBuilding on UNESCO’s 2021 Recommendation on the Ethics of Artificial Intelligence, the guidance is anchored in a humanistic approach to education that promotes human agency, inclusion, equity, gender equality, and cultural and linguistic diversity, as well as plural opinions and expressions.\n\nFurthermore, it responds to the call of the 2021 report of the International Commission on the Futures of Education, Reimagining our futures together: A new social contract for education to redefine our relationship with technology, as an integral part of our efforts to renew the social contract for education.\n\nAI must not usurp human intelligence.\n\nRather, it invites us to reconsider our established understandings of knowledge and human learning.\n\nIt is my hope that this guidance will help us redefine new horizons for education and inform our collective thinking and collaborative actions that can lead to human-centred digital learning futures for all.\n\nStefania Giannini,\nUNESCO Assistant Director-General for Education",
    "### Acknowledgements\n\nUnder the leadership of Stefania Giannini, Assistant-Director for Education, and the guidance of Sobhi Tawil, Director of the Future of Learning and Innovation Division at UNESCO, the drafting of the publication was led by Fengchun Miao, Chief of Unit for Technology and AI in Education.\n\nParticular thanks go to Wayne Holmes, Associate Professor at University College London, who co-drafted the publication.\n\nThis publication is the fruit of a collective effort of education leaders and experts in the field of AI and education.\n\nIt benefited from the insights and inputs of many experts including: Mutlu Cukurova, Professor at University College London; Colin de la Higuera, UNESCO Chair in Technologies for the Training of Teachers with Open Educational Resources at Nantes University; Shafika Isaacs, Research Associate at the University of Johannesburg; Natalie Lao, Executive Director of the App Inventor Foundation; Qin Ni, Associate Professor at Shanghai Normal University; Catalina Nicolin, ICT in Education Expert at the European Digital Education Hub in Romania; John Shaw-Taylor, UNESCO Chair in AI and Professor of Computational Statistics and Machine Learning at University College London; Kelly Shirohira, Executive Manager at Jet Education Services; Ki-Sang Song, Professor at Korea National University of Education; and Ilkka Tuomi, Chief Scientist at Meaning Processing Ltd in Finland.\n\nMany colleagues across UNESCO also contributed in various ways including: Dafna Feinholz, Chief of Section for Bioethics and the Ethics of Science and Technology; Francesc Pedró, Director of the International Institute for Higher Education in Latin America and the Caribbean; Prateek Sibal, Programme Specialist, Section for Digital Policies and Digital Transformation; Saurabh Roy, Senior Project Officer at the Section for Teacher Development, Division for Policies and Lifelong Learning Systems; Benjamin Vergel De Dios, Programme Specialist in ICT in Education, Section for Educational Innovation and Skills Development in the Bangkok Office; the colleagues in the Diversity of Cultural Expressions Entity in the Culture Sector; and Mark West, Programme Specialist, Future of Learning and Innovation Division.\n\nAppreciation is also due to Glen Hertelendy, Luisa Ferrara and Xianglei Zheng, Unit for Technology and AI in Education, Future of Learning and Innovation, for coordinating the production of the publication.\n\nGratitude is also extended to Jenny Webster for copy-editing and proofreading the text, and to Ngoc-Thuy Tran for designing the layout.",
    "**List of tables**\n\n- Table 1.\n\nTechniques used in generative AI 8\n- Table 2.\n\nOpenAI GPTs 9\n- Table 3.\n\nCo-designing uses of GenAI for research 30\n- Table 4.\n\nCo-designing uses of GenAI to support teachers and teaching 31\n- Table 5.\n\nCo-designing uses of GenAI as a 1:1 coach for the self-paced acquisition of foundational skills in languages and the arts 32\n- Table 6.\n\nCo-designing uses of GenAI to facilitate inquiry or project-based learning 33\n- Table 7.\n\nCo-designing uses of GenAI to support learners with special needs 34",
    "### Concepts and Technologies\n\n- AGI: Artificial General Intelligence\n- AI: Artificial Intelligence\n- API: Application Programming Interface\n- ANN: Artificial Neural Network\n- DAI: Distributed Artificial Intelligence\n- GAN: Generative Adversarial Networks\n- GB: Gigabytes\n- GDPR: General Data Protection Regulation\n- GenAI: Generative Artificial Intelligence\n- GPT: Generative Pre-trained Transformer\n- ICT: Information and Communication Technology\n- LaMDA: Language Model for Dialogue Applications\n- LLM: Large Language Model\n- ML: Machine Learning\n- VAE: Variational Autoencoders",
    "## Introduction\n\nThe release of ChatGPT in late 2022, the first easy-to-use generative artificial intelligence (GenAI) tool made widely available to the public, followed by iteratively more sophisticated versions, sent shockwaves worldwide, and is fuelling the race among large technology companies to position themselves in the field of GenAI model development.\n\nAcross the world, the initial concern in education was that ChatGPT and similar GenAI tools would be used by students to cheat on their assignments, thus undermining the value of learning assessment, certification and qualifications.\n\nWhile some educational institutions banned the use of ChatGPT, others cautiously welcomed the arrival of GenAI.\n\nMany schools and universities, for instance, adopted a progressive approach believing that ‘rather than seek to prohibit their use, students and staff need to be supported in using GenAI tools effectively, ethically and transparently’.\n\nThis approach acknowledges that GenAI is widely available, is likely only to become more sophisticated, and has both specific negative and unique positive potential for education.\n\nIndeed, GenAI has a myriad of possible uses.\n\nIt can automate information processing and the presentation of outputs across all key symbolic representations of human thinking.\n\nIt enables the delivery of final outputs by furnishing semi-finished knowledge products.\n\nBy freeing humans from some categories of lower-order thinking skills, this new generation of AI tools might have profound implications for how we understand human intelligence and learning.\n\nBut GenAI also raises multiple immediate concerns related to issues such as safety, data privacy, copyright, and manipulation.\n\nSome of these are broader risks related to artificial intelligence that have been further exacerbated by GenAI, while others have newly emerged with this latest generation of tools.\n\nIt is now urgent that each of these issues and concerns be fully understood and addressed.\n\nThis Guidance is designed to respond to this urgent need.\n\nHowever, a thematic set of guidance on GenAI for education should not be understood as a claim that GenAI is the solution to education’s fundamental challenges.\n\nDespite the media hyperbole, it is unlikely that GenAI alone will solve any of the problems facing education systems around the world.\n\nIn responding to long-standing educational issues, it is key to uphold the idea that human capacity and collective action, and not technology, is the determining factor in effective solutions to fundamental challenges faced by societies.\n\nThis Guidance therefore aims to support the planning of appropriate regulations, policies and human capacity development programmes, to ensure that GenAI becomes a tool that genuinely benefits and empowers teachers, learners and researchers.\n\nBuilding on UNESCO’s Recommendation on the Ethics of Artificial Intelligence, the Guidance is anchored in a human-centred approach that promotes human agency, inclusion, equity, gender equality, and cultural and linguistic diversity, as well as plural opinions and expressions.\n\nThe Guidance first looks into what GenAI is and how it works, presenting the diverse technologies and models available (Section 1), before identifying a range of controversial ethical and policy issues around both AI in general, and GenAI specifically (Section 2).\n\nThis is followed by a discussion of the steps and key elements to be examined when seeking to regulate GenAI based on a human-centred approach – one that ensures ethical, safe, equitable and meaningful use (Section 3).\n\nSection 4 then proposes measures that can be taken to develop coherent, comprehensive policy frameworks to regulate the use of GenAI in education and research, while Section 5 looks into the possibilities for creatively using GenAI in curriculum design, teaching, learning and research activities.\n\nSection 6 concludes the Guidance with considerations around the long-term implications of GenAI for education and research.",
    "### What is Generative AI?\n\nGenerative AI (GenAI) is an artificial intelligence (AI) technology that automatically generates content in response to prompts written in natural-language conversational interfaces.\n\nRather than simply curating existing webpages, by drawing on existing content, GenAI actually produces new content.\n\nThe content can appear in formats that comprise all symbolic representations of human thinking: texts written in natural language, images (including photographs, digital paintings and cartoons), videos, music, and software code.\n\nGenAI is trained using data collected from webpages, social media conversations, and other online media.\n\nIt generates its content by statistically analysing the distributions of words, pixels or other elements in the data that it has ingested and identifying and repeating common patterns (for example, which words typically follow which other words).\n\nWhile GenAI can produce new content, it cannot generate new ideas or solutions to real-world challenges, as it does not understand real-world objects or social relations that underpin language.\n\nMoreover, despite its fluent and impressive output, GenAI cannot be trusted to be accurate.\n\nIndeed, even the provider of ChatGPT acknowledges, ‘While tools like ChatGPT can often generate answers that sound reasonable, they cannot be relied upon to be accurate.’ Most often, the errors will go unnoticed unless the user has a solid knowledge of the topic in question.",
    "### How Does Generative AI Work?\n\nThe specific technologies behind GenAI are part of the family of AI technologies called machine learning (ML) which uses algorithms to enable it to continuously and automatically improve its performance from data.\n\nThe type of ML which has led to many of the advances in AI that we have seen in recent years, such as the use of AI for facial recognition, is known as artificial neural networks (ANNs), which are inspired by how the human brain works and its synaptic connections between neurons.\n\nThere are many types of ANNs.\n\nBoth text and image generative AI technologies are based on a set of AI technologies that have been available to researchers for several years.\n\nChatGPT, for instance, uses a generative pre-trained transformer (GPT), while image GenAI typically uses what are known as generative adversarial networks (GANs).",
    "#### How Text GenAI Models Work\n\nText generative AI uses a type of ANN known as a general-purpose transformer, and a type of general-purpose transformer called a large language model.\n\nThis is why AI Text GenAI systems are often referred to as large language models, or LLMs.\n\nThe type of LLM used by text GenAI is known as a generative pre-trained transformer, or GPT (hence the ‘GPT’ in ‘ChatGPT’).",
    "### Iterations and Alternatives to ChatGPT\n\nChatGPT is built on GPT-3 which was developed by OpenAI.\n\nThis was the third iteration of their GPT, the first being launched in 2018 and the most recent, GPT-4, in March 2023.\n\nEach OpenAI GPT iteratively improved upon the previous through advances in AI architectures, training methods and optimization techniques.\n\nOne well-known facet of its continuous progress is the use of growing amounts of data to train its exponentially increasing number of ‘parameters’.\n\nParameters might be thought of as metaphorical knobs that can be adjusted to fine-tune the GPT’s performance.\n\nThey include the model’s ‘weights’, numerical parameters that determine how the model processes input and produces output.\n\nIn addition to the advancements in optimizing AI architectures and training methods, this rapid iteration has been made possible also due to the massive amounts of data and improvements in computing capabilities available to the big companies.\n\nSince 2012, computing capabilities used for training GenAI models have been doubling every 3-4 months.\n\nBy comparison, Moore’s Law had a two-year doubling period.\n\nOnce the GPT has been trained, generating a text response to a prompt involves the following steps:\n- The prompt is broken down into smaller units (called tokens) that are inputted into the GPT.\n\n- The GPT uses statistical patterns to predict likely words or phrases that might form a coherent response to the prompt.\n\n- The GPT identifies patterns of words and phrases that commonly co-occur in its prebuilt large data model (which comprises text scraped from the internet and elsewhere).\n\n- Using these patterns, the GPT estimates the probability of specific words or phrases appearing in a given context.\n\n- Beginning with a random prediction, the GPT uses these estimated probabilities to predict the next likely word or phrase in its response.\n\n- The predicted words or phrases are converted into readable text.\n\n- The readable text is filtered through what are known as ‘guardrails’ to remove any offensive content.\n\n- Steps 2 to 4 are repeated until a response is finished.\n\nThe response is considered finished when it reaches a maximum token limit or meets predefined stopping criteria.\n\n- The response is post-processed to improve readability by applying formatting, punctuation and other enhancements.\n\nThe launch of ChatGPT set off shock waves around the world, and quickly led to other global tech companies playing catch-up, alongside numerous start-up companies, either by launching their own similar systems or by building new tools on top.\n\nBy July 2023, some of the alternatives to ChatGPT included the following:\n- **Alpaca**: A fine-tuned version of Meta’s Llama, from Stanford University, which aims to address LLMs’ false information, social stereotypes and toxic language.\n\n- **Bard**: An LLM from Google, based on its LaMDA and PaLM 2 systems, that has access to the internet in real time, which means it can provide up-to-date information.\n\n- **Chatsonic**: Made by Writesonic, it builds on ChatGPT while also crawling data directly from Google.\n\nAccordingly, it has less chance of producing factually incorrect answers.\n\n- **Ernie (also known as Wenxin Yiyan)**: A bilingual LLM from Baidu, still in development, which integrates extensive knowledge with massive datasets to generate text and images.\n\n- **Hugging Chat**: Made by Hugging Face, who emphasized ethics and transparency throughout its development, training and deployment.\n\nIn addition, all data used to train their models are open source.\n\n- **Jasper**: A suite of tools and APIs that, for example, can be trained to write in a user’s particular preferred style.\n\nIt can also generate images.\n\n- **Llama**: An open-source LLM from Meta that requires less computing power and fewer resources to test new approaches, validate others’ work and explore new use cases.\n\n- **Open Assistant**: An open-source approach designed to enable anyone with sufficient expertise to develop their own LLM.\n\nIt was built on training data curated by volunteers.\n\n- **Tongyi Qianwen**: An LLM from Alibaba that can respond to prompts in English or Chinese.\n\nIt is being integrated into Alibaba’s suite of business tools.\n\n- **YouChat**: An LLM that incorporates real-time search capabilities to provide additional context and insights in order to generate more accurate and reliable results.",
    "### How Image GenAI Models Work\n\nImage GenAI and music GenAI typically use a different type of ANN known as generative adversarial networks (GANs) which can also be combined with variational autoencoders.\n\nSome image GenAI models like Dall·E and Stable Diffusion use Diffusion Models, a different generative ANN.\n\nTaking GANs as an example to explain how image GenAI models work: GANs have two parts (two ‘adversaries’), the ‘generator’ and the ‘discriminator’.\n\nIn the case of image GANs, the generator creates a random image in response to a prompt, and the discriminator tries to distinguish between this generated image and real images.\n\nThe generator then uses the result of the discriminator to adjust its parameters, in order to create another image.\n\nThe process is repeated, possibly thousands of times, with the generator making more and more realistic images that the discriminator is less and less able to distinguish from real images.\n\nFor example, a successful GAN trained on a dataset of thousands of landscape photographs might generate new but unreal images of landscapes that are almost indistinguishable from real photographs.\n\nMeanwhile, a GAN trained on a dataset of popular music (or even music by a single artist) might generate new pieces of music that follow the structure and complexity of the original music.\n\nAs of July 2023, the image GenAI models that are available include the following, all of which generate images from text prompts.\n\nMost are free to use, within certain limits:\n- **Craiyon**: Previously known as DALL-E mini.\n\n- **DALL-E 2**: OpenAI’s image GenAI tool.\n\n- **DreamStudio**: Stable Diffusion’s image GenAI tool.\n\n- **Fotor**: Incorporates GenAI in a range of image-editing tools.\n\n- **Midjourney**: An independent image GenAI tool.\n\n- **NightCafe**: Interface to Stable Diffusion and DALL-E 2.\n\n- **Photosonic**: WriteSonic’s AI art generator.",
    "### Easy-to-Access Video GenAI\n\nExamples of easy-to-access video GenAI include the following:\n- **Elai**: Can convert presentations, websites, and text into videos.\n\n- **GliaCloud**: Can generate videos from news content, social media posts, live sporting events, and statistical data.\n\n- **Pictory**: Can automatically create short videos from long-form content.\n\n- **Runway**: Offers a range of video (and imaging) generation and editing tools.",
    "### Prompt-Engineering to Generate Desired Outputs\n\nWhile using GenAI can be as simple as typing in a question or other prompt, the reality is that it is still not straightforward for the user to get exactly the output that they want.\n\nFor example, the breakthrough AI image Théâtre D’opéra Spatial, which won a prize at the Colorado State Fair in the United States of America, took weeks of writing prompts and fine-tuning hundreds of images in order to generate the final submission.\n\nThe similar challenge of writing effective prompts for text GenAI has led to an increasing number of prompt-engineering jobs appearing on recruitment websites.\n\n‘Prompt-engineering’ refers to the processes and techniques for composing input to produce GenAI results.",
    "# Prompt-engineering and Recommendations\n\nPrompt-engineering is most successful when the prompt articulates a coherent chain of reasoning centered on a particular problem or a chain of thought in a logical order.\n\nSpecific recommendations include:\n\n- Use simple, clear and straightforward language that can be easily understood, avoiding complex or ambiguous wording.\n\n- Include examples to illustrate the desired response or format of generated completions.\n\n- Include context, which is crucial for generating relevant and meaningful completions.\n\n- Refine and iterate as necessary, experimenting with different variations.\n\n- Be ethical, avoiding prompts that may generate inappropriate, biased or harmful content.\n\nIt is also important to recognize immediately that GenAI outputs cannot be relied upon without critical evaluation.\n\nAs OpenAI writes about their most sophisticated GPT: In light of the quality of GenAI’s outputs, rigorous user tests and performance evaluations should be conducted before validating the tools for large-scale or high-stakes adoption.\n\nSuch exercises should be designed with a performance metric that is most relevant to the type of task for which users ask GenAI to provide outputs.\n\nFor example, for solving math problems, 'accuracy' could be used as the main metric to quantify how often a GenAI tool produces the correct answer; for responding to sensitive questions, the main metric to measure performance might be 'answer rate' (the frequency with which the GenAI directly answers a question); for code generation, the metric may be ‘the fraction of the generated codes that are directly executable’ (whether the generated code could be directly executed in a programming environment and pass the unit tests); and for visual reasoning, the metric could be ‘exact match’ (whether the generated visual objects exactly match the ground truth).\n\nIn summary, at a superficial level, GenAI is easy to use; however, more sophisticated outputs need skilled human input and must be critically evaluated before they are used.\n\nDespite its capabilities, GPT-4 has similar limitations as earlier GPT models.\n\nMost importantly, it still is not fully reliable (it ‘hallucinates’ facts and makes reasoning errors).\n\nGreat care should be taken when using language model outputs, particularly in high-stakes contexts, with the exact protocol (such as human review, grounding with additional context, or avoiding high-stakes uses altogether) matching the needs of a specific use case.",
    "# Emerging EdGPT and its Implications\n\nGiven that GenAI models can serve as the basis or starting point for developing more specialized or domain-specific models, some researchers have suggested that GPTs should be renamed ‘foundation models’.\n\nIn education, developers and researchers have started to fine-tune a foundation model to develop ‘EdGPT’.\n\nEdGPT models are trained with specific data to serve educational purposes.\n\nIn other words, EdGPT aims to refine the model derived from massive amounts of general training data with smaller amounts of high-quality, domain-specific education data.\n\nThis potentially gives EdGPT more scope to support the achievement of the transformations.\n\nFor example, EdGPT models targeting curriculum co-design may allow educators and learners to generate appropriate educational materials such as lesson plans, quizzes and interactive activities that closely align with an effective pedagogical approach and specific curricular objectives and levels of challenge for particular learners.\n\nSimilarly, in the context of a 1:1 language skills coach, a foundation model refined with texts appropriate for a particular language might be used to generate exemplar sentences, paragraphs or conversations for practice.\n\nWhen learners interact with the model, it can respond with relevant and grammatically accurate text at the right level for them.\n\nTheoretically, the outputs of EdGPT models could also contain fewer general biases or otherwise objectionable content than standard GPT, but still might generate errors.\n\nIt is critical to note that, unless the underlying GenAI models and approach change significantly, EdGPT may still generate errors and demonstrate other limitations.\n\nAccordingly, it is still important that the main users of EdGPT, especially teachers and learners, need to take a critical perspective on any outputs.\n\nCurrently, the refining of foundation models for more targeted use of GPT in education is at an early stage.\n\nExisting examples include EduChat, a foundation model developed by East China Normal University to provide services for teaching and learning, and whose codes, data and parameters are shared as open source.\n\nAnother example is MathGPT being developed by the TAL Education Group – a LLM that focuses on mathematics-related problem-solving and lecturing for users worldwide.\n\nHowever, before significant progress is possible, it is essential that efforts are put into refining foundation models not only by adding subject knowledge and de-biasing, but also by adding knowledge about relevant learning methods, and how this can be reflected in the design of algorithms and models.\n\nThe challenge is to determine the extent to which EdGPT models can go beyond subject knowledge to also target student-centred pedagogy and positive teacher-student interactions.\n\nThe further challenge is to determine the extent to which learner and teacher data may ethically be collected and used in order to inform an EdGPT.\n\nFinally, there is also a need for robust research to ensure that EdGPT does not undermine students’ human rights nor disempower teachers.",
    "## Worsening Digital Poverty\n\nAs noted earlier, GenAI relies upon huge amounts of data and massive computing power in addition to its iterative innovations in AI architectures and training methods, which are mostly only available to the largest international technology companies and a few economies (mostly the United States, People’s Republic of China, and to a lesser extent Europe).\n\nThis means that the possibility to create and control GenAI is out of reach for most companies and countries, especially those in the Global South.\n\nAs access to data becomes increasingly essential for the economic development of countries and for the digital opportunities of individuals, those countries and people who do not have access to or cannot afford enough data are left in a situation of ‘data poverty’.\n\nThe situation is similar for access to computing power.\n\nThe rapid pervasion of GenAI in technologically advanced countries and regions has accelerated exponentially the generation and processing of data, and has simultaneously intensified the concentration of AI wealth in the Global North.\n\nAs an immediate consequence, the data-poor regions have been further excluded and put at long-term risk of being colonized by the standards embedded in the GPT models.\n\nThe current ChatGPT models are trained on data from online users which reflect the values and norms of the Global North, making them inappropriate for locally relevant AI algorithms in data-poor communities in many parts of the Global South or in more disadvantaged communities in the Global North.",
    "## Outpacing National Regulatory Adaptation\n\nDominant GenAI providers have also been criticized for not allowing their systems to be subject to rigorous independent academic review.\n\nThe foundational technologies of a company’s GenAI tend to be protected as corporate intellectual property.\n\nMeanwhile, many of the companies that are starting to use GenAI are finding it increasingly challenging to maintain the security of their systems.\n\nMoreover, despite calls for regulation from the AI industry itself, the drafting of legislation on the creation and use of all AI, including GenAI, often lags behind the rapid pace of development.\n\nThis partly explains the challenges experienced by national or local agencies in understanding and governing the legal and ethical issues.\n\nWhile GenAI may augment human capacities in completing certain tasks, there is limited democratic control of the companies that are promoting GenAI.\n\nThis raises the question of regulations, in particular, in respect of access to, and use of, domestic data including data on local institutions and individuals as well as data generated on the countries’ territory.\n\nAppropriate legislation is needed so that local governmental agencies may gain some control over the surging waves of GenAI to ensure its governance as a public good.",
    "## Use of Content Without Consent\n\nAs noted earlier, GenAI models are built from large amounts of data (e.g.\n\ntext, sounds, code and images) often scraped from the internet and usually without any owner’s permission.\n\nMany image GenAI systems and some code GenAI systems have consequently been accused of violating intellectual property rights.\n\nAt the time of writing, there are several ongoing international legal cases that relate to this issue.\n\nFurthermore, some have pointed out that GPTs may contravene laws such as the European Union’s (2016) General Data Protection Regulation or GDPR, especially people’s right to be forgotten, as it is currently impossible to remove someone’s data (or the results of that data) from a GPT model once it has been trained.",
    "## Unexplainable Models Used to Generate Outputs\n\nIt has long been recognized that artificial neural networks (ANNs) are usually ‘black boxes’; that is, that their inner workings are not open to inspection.\n\nAs a result, ANNs are not ‘transparent’ or ‘explainable’, and it is not possible to ascertain how their outputs were determined.\n\nWhile the overall approach, including the algorithms used, is generally explainable, the particular models and their parameters, including the model’s weights, are not inspectable, which is why a specific output that is generated cannot be explained.\n\nThere are billions of parameters/weights in a model like GPT-4 and it is the weights collectively that hold the learned patterns that the model uses to generate its outputs.\n\nAs parameters/weights are not transparent in ANNs, one cannot explain the precise way a specific output is created by these models.\n\nGenAI’s lack of transparency and explainability is increasingly problematic as GenAI becomes ever more complex, often producing unexpected or undesired results.\n\nIn addition, GenAI models inherit and perpetuate biases present in their training data which, given the non-transparent nature of the models, are hard to detect and address.\n\nFinally, this opacity is also a key cause of trust issues around GenAI.\n\nIf users don’t understand how a GenAI system arrived at a specific output, they are less likely to be willing to adopt it or use it.",
    "## AI-generated Content Polluting the Internet\n\nBecause GPT training data is typically drawn from the internet, which all too frequently includes discriminatory and other unacceptable language, developers have had to implement what they call ‘guardrails’ to prevent GPT output from being offensive and/or unethical.\n\nHowever, due to the absence of strict regulations and effective monitoring mechanisms, biased materials generated by GenAI are increasingly spreading throughout the internet, polluting one of the main sources of content or knowledge for most learners across the world.\n\nThis is especially important because the material generated by GenAI can appear to be quite accurate and convincing, when often it contains errors and biased ideas.\n\nThis poses a high risk for young learners who do not have solid prior knowledge of the topic in question.\n\nIt also poses a recursive risk for future GPT models that will be trained on text scraped from the Internet that GPT models have themselves created which also include their biases and errors.",
    "## Lack of Understanding of the Real World\n\nText GPTs are sometimes pejoratively referred to as ‘stochastic parrots’ because, as has been noted earlier, while they can produce text that appears convincing, that text often contains errors and can include harmful statements.\n\nThis all occurs because GPTs only repeat language patterns found in their training data (usually text drawn from the internet), starting with random (or ‘stochastic’) patterns, and without understanding their meaning – just as a parrot can mimic sounds without actually comprehending what it is saying.\n\nThe disconnect between GenAI models ‘appearing’ to understand the text that they use and generate, and the ‘reality’ that they do not understand the language and the real world can lead teachers and students to place a level of trust in the output that it does not warrant.\n\nThis poses serious risks for future education.\n\nIndeed, GenAI is not informed by observations of the real world or other key aspects of the scientific method, nor is it aligned with human or social values.\n\nFor these reasons, it cannot generate genuinely novel content about the real world, objects and their relations, people and social relations, human-object relations, or human-tech relations.\n\nWhether the apparently novel content generated by GenAI models can be recognized as scientific knowledge is contested.\n\nAs already noted, GPTs can frequently produce inaccurate or unreliable text.\n\nIn fact, it is well known that GPTs make up some things that do not exist in real life.\n\nSome call this ‘hallucination’, although others criticize the use of such an anthropomorphic and therefore misleading term.\n\nThis is acknowledged by the companies producing GenAI.\n\nThe bottom of the ChatGPT public interface, for instance, states: ‘ChatGPT may produce inaccurate information about people, places, or facts’.\n\nIt has also been suggested by a few advocates that GenAI represents a significant step in the journey towards artificial general intelligence (AGI), a term suggesting a class of AI that is more intelligent than humans.\n\nHowever, this has long been critiqued, with the argument that AI will never progress towards AGI at least until it in some way brings together, in symbiosis, both knowledge-based AI (also known as symbolic or rule-based AI) and data-based AI (also known as machine learning).\n\nThe AGI or sentience claims also distract us from more careful consideration of current harms being perpetrated with AI, such as hidden discrimination against already discriminated- against groups.",
    "## Reducing the Diversity of Opinions and Further Marginalizing Already Marginalized Voices\n\nChatGPT and similar such tools tend to output only standard answers that assume the values of the owners/creators of the data used to train the models.\n\nIndeed, if a sequence of words appears frequently in the training data – as is the case with common and uncontroversial topics and mainstream or dominant beliefs – it is likely to be repeated by the GPT in its output.\n\nThis risks constraining and undermining the development of plural opinions and plural expressions of ideas.\n\nData-poor populations, including marginalized communities in the Global North, have minimal or limited digital presence online.\n\nTheir voices are consequently not being heard and their concerns are not represented in the data being used to train GPTs, and so rarely appear in the outputs.\n\nFor these reasons, given the pre-training methodology based on data from internet web pages and social media conversations, GPT models can further marginalize already disadvantaged people.",
    "## Generating Deeper Deepfakes\n\nIn addition to the controversies common to all GenAI, GAN GenAI can be used to alter or manipulate existing images or videos to generate fake ones that are difficult to distinguish from real ones.\n\nGenAI is making it increasingly easy to create these ‘deepfakes’ and so-called ‘fake news’.\n\nIn other words, GenAI is making it easier for certain actors to commit unethical, immoral and criminal acts, such as spreading disinformation, promoting hate speech and incorporating the faces of people, without their knowledge or consent, into entirely fake and sometimes compromising films.",
    "# Regulating the Use of Generative AI in Education\n\nIn order to address the controversies around generative AI and to harness the potential benefits of GenAI in education, it first needs to be regulated.\n\nRegulation of GenAI for educational purposes requires a number of steps and policy measures based on a human-centred approach to ensure its ethical, safe, equitable and meaningful use.",
    "## A Human-centred Approach to AI\n\nUNESCO’s 2021 Recommendation on the Ethics of Artificial Intelligence provides the requisite normative framework to start addressing the multiple controversies around generative AI, including those that pertain to education and research.\n\nIt is based on a human-centred approach to AI which advocates that the use of AI should be at the service of the development of human capabilities for inclusive, just and sustainable futures.\n\nSuch an approach must be guided by human rights principles and the need to protect human dignity and the cultural diversity that defines the knowledge commons.\n\nIn terms of governance, a human-centred approach requires proper regulation that can ensure human agency, transparency and public accountability.\n\nThe 2019 Beijing Consensus on Artificial Intelligence (AI) and Education further elaborates what a human-centred approach implies for the use of AI in the context of education.\n\nThe Consensus affirms that the use of AI technologies in education should enhance human capacities for sustainable development and effective human-machine collaboration in life, learning and work.\n\nIt also calls for further actions to ensure equitable access to AI to support marginalized people and address inequalities while promoting linguistic and cultural diversities.\n\nThe Consensus suggests adopting whole-of-government, intersectoral and multistakeholder approaches to the planning of policies on AI in education.\n\nAI and education: Guidance for policy-makers further refines what a human-centred approach means when examining the benefits and risks of AI in education and the role of education as a means of developing AI competencies.\n\nIt proposes concrete recommendations for the formulation of policies to steer the use of AI to (i) enable inclusive access to learning programmes, especially for vulnerable groups such as learners with disabilities; (ii) support personalized and open learning options; (iii) improve data-based provisions and management to expand access and improve quality in learning; (iv) monitor learning processes and alert teachers to failure risks; and (v) develop understanding and skills for the ethical and meaningful use of AI.",
    "## Steps to Regulate GenAI in Education\n\nPrior to the release of ChatGPT, governments had been developing or adapting frameworks for regulating the collection and use of data and the adoption of AI systems across sectors including in education, which provided a legislative and policy context for the regulation of newly emergent AI applications.\n\nIn the aftermath of the release of multiple competitive GenAI models starting in November 2022, governments have been adopting different policy responses – from banning GenAI to assessing needs for adapting existing frameworks to urgently formulating new regulations.\n\nGovernmental strategies for regulating and facilitating the creative use of GenAI were mapped and reviewed in April 2023.\n\nThe review suggests a series of seven steps that governmental agencies can take to regulate generative AI and reassert public control in order to leverage its potentials across sectors, including in education.",
    "**Step 1: Endorse international or regional general data protection regulations or develop national ones**\n\nThe training of GenAI models has involved collecting and processing online data from citizens across many countries.\n\nThe use, by GenAI models, of data and content without consent is further challenging the issue of data protection.\n\nGeneral data protection regulations, with the EU’s GDPR enacted in 2018 as one of the forerunner examples, provide the necessary legal framework to regulate the collection and processing of personal data by the suppliers of GenAI.\n\nAccording to the Data Protection and Privacy Legislation Worldline portal of the United Nations Conference on Trade and Development, 137 out of 194 countries have established legislation to safeguard data protection and privacy.\n\nThe extent to which these frameworks are being implemented in those countries, however, remains unclear.\n\nIt is therefore ever more critical to ensure that these are properly implemented, including regular monitoring of the operations of GenAI systems.\n\nIt is also urgent for countries that do not yet have general data protection laws to develop them.",
    "**Step 2: Adopt/revise and fund whole-of- government strategies on AI**\n\nRegulating generative AI must be part and parcel of broader national AI strategies that can ensure safe and equitable use of AI across development sectors, including in education.\n\nThe formulation, endorsement, funding and implementation of national AI strategies requires a whole-of-government approach.\n\nOnly such an approach can ensure the coordination of intersectoral actions required for integrated responses to emerging challenges.\n\nBy early 2023, some 67 countries had developed or planned national strategies on AI, with 61 of them taking the form of a standalone AI strategy, and 7 being chapters on AI integrated within broader national ICT or digitalization strategies.\n\nUnderstandably, given its novelty, none of these national strategies had yet covered generative AI as a specific issue at the time of writing.\n\nIt is critical that countries revise existing national AI strategies, or develop them, ensuring provisions to regulate the ethical use of AI across sectors including in education.",
    "**Step 3: Solidify and implement specific regulations on the ethics of AI**\n\nIn order to address the ethical dimensions posed by the use of AI, specific regulations are required.\n\nThe UNESCO 2023 review of existing national AI strategies indicates that the identification of such ethical issues and the formulation of guiding principles is only common to some 40 national AI strategies.\n\nAnd even here, the ethical principles will need to be translated into enforceable laws or regulations.\n\nThis is seldom the case.\n\nIndeed, only around 20 countries had defined any clear regulations on the ethics of AI including as they relate to education, either as part of national AI strategies or otherwise.\n\nInterestingly, while education is highlighted as a policy domain across some 45 national AI strategies, references to education are articulated more in terms of AI skills and talent development required to support national competitiveness, and less in terms of ethical issues.\n\nCountries that do not yet have regulations on ethics of AI must urgently articulate and implement them.",
    "**Step 4: Adjust or enforce existing copyright laws to regulate AI-generated content**\n\nThe increasingly pervasive use of GenAI has introduced new challenges for copyright, both concerning the copyrighted content or work that models are trained on, as well as the status of the ‘non-human’ knowledge outputs they produce.\n\nAt present, only China, EU countries and the United States have adjusted copyright laws to account for the implications of generative AI.\n\nThe US Copyright Office, for instance, has ruled that the output of GenAI systems such as ChatGPT are not protectable under US copyright law, arguing that ‘copyright can protect only material that is the product of human creativity’.\n\nMeanwhile in the EU, the proposed EU AI Act requires AI tool developers to disclose the copyrighted materials they used in building their systems.\n\nChina, through its regulation on GenAI released in July 2023, requires the labeling of outputs of GenAI as AI-generated content, and only recognizes them as outputs of digital synthesis.\n\nRegulating the use of copyrighted materials in the training of GenAI models and defining the copyright status of GenAI outputs are emerging as new accountabilities of copyright laws.\n\nIt is urgent that existing laws be adjusted to account for this.",
    "**Step 5: Elaborate regulatory frameworks on generative AI**\n\nThe rapid pace of development of AI technologies is forcing national/local governance agencies to speed up their renewal of regulations.\n\nAs of July 2023, only one country, China, had released specific official regulations on GenAI.\n\nThe Provisional Regulations on Governing the Service of Generative AI released on 13 July 2023 requires providers of GenAI systems to label AI-generated content, images and videos properly and lawfully in accordance with its existing Regulation on Deep Synthesis in the Framework of Online Information Services.\n\nMore of such national GenAI-specific frameworks need to be developed based upon an assessment of the gaps in existing local regulations and laws.",
    "**Step 6: Build capacity for proper use of GenAI in education and research**\n\nSchools and other educational institutions need to develop capacities to understand the potential benefits and risks of AI, including GenAI, for education.\n\nIt is only based on such understanding that they can validate the adoption of AI tools.\n\nMoreover, teachers and researchers need to be supported to strengthen their capacities for the proper use of GenAI, including through training and continuous coaching.\n\nA number of countries have launched such capacity-building programmes, including Singapore, which has been offering a dedicated platform for the AI capacity development of educational institutions through its AI Government Cloud Cluster which includes a dedicated repository of GPT models.",
    "**Step 7: Reflect on the long-term implications of GenAI for education and research**\n\nThe impact of current versions of GenAI is just beginning to unfold, and their effects on education are yet to be fully explored and understood.\n\nMeanwhile, stronger versions of GenAI and other classes of AI continue to be developed and deployed.\n\nCrucial questions remain, however, around the implications of GenAI for knowledge creation, transmission and validation – for teaching and learning, for curriculum design and assessment, and for research and copyright.\n\nMost countries are at the early stage of the adoption of GenAI in education, even as the longer-term impacts have yet to be understood.\n\nTo ensure a human-centred use of AI, open public debate and policy dialogues on the long-term implications should urgently be conducted.\n\nInclusive debate involving government, the private sector and other partners, should serve to provide insights and inputs for the iterative renewal of regulations and policies.",
    "## Regulations on GenAI: Key Elements\n\nAll countries need to properly regulate GenAI in order to ensure it benefits development in education and other contexts.\n\nThis section proposes actions around key elements that can be taken by: (1) governmental regulatory agencies, (2) providers of AI-enabled tools, (3) institutional users, and (4) individual users.\n\nWhile many of the elements in the framework are of a transnational nature, all should also be considered in light of the local context, that is, the specific country’s educational systems and general regulatory frameworks already in place.",
    "## Alignment of legislation\n\nAlign the framework with the relevant legislative and regulatory contexts of each country – with, for example, general data protection laws, regulations on internet security, laws on the security of data produced from or used to serve citizens, and other relevant legislation and usual practices.\n\nAssess the appropriateness of existing regulations and any necessary adaptations in response to new issues raised by GenAI.",
    "## Balance between the regulation of GenAI and the promotion of AI innovation\n\nPromote intersectoral cooperation among companies, organizations, and education and research institutions, as well as relevant public agencies to jointly develop trustworthy models; encourage the building of open-source ecosystems to promote the sharing of super-computing resources and high-quality pre-training datasets; and foster the practical application of GenAI across sectors and the creation of high-quality content for the public good.",
    "## Assessment and classification of the potential risks of AI\n\nEstablish principles and a process for the assessment and categorization of the efficacy, safety, and security of GenAI services, before they are deployed and throughout the system’s life cycle.\n\nConsider categorization mechanisms based on the levels of risk that GenAI may imply for citizens.\n\nClassify them into strict regulations (i.e., banning AI-enabled applications or systems with unacceptable risks), special regulations for high-risk applications, and general regulations on applications that are not listed as high risk.\n\nSee the EU’s draft AI Act for an example of this approach.",
    "## Definition and enforcement of age limit for the use of GenAI\n\nMost GenAI applications are primarily designed for adult users.\n\nThese applications often entail substantial risks for children, including exposure to inappropriate content as well as the potential for manipulation.\n\nIn light of these risks and given the considerable uncertainty that continues to surround iterative GenAI applications, age restrictions are strongly recommended for general-purpose AI technologies in order to protect children’s rights and well-being.\n\nCurrently, the terms of use for ChatGPT require that users must be at least 13 years old, and users under 18 must have their parent or legal guardian's permission to use the services.\n\nThese age restrictions or thresholds are derived from the Children’s Online Privacy Protection Act of the United States of America (Federal Trade Commission, 1998).\n\nPassed in 1998 before widespread social media use and well before the creation of easy-to-use and powerful GenAI applications such as ChatGPT, the US law specifies that organizations or individual social media providers are not allowed to provide services for children under the age of 13 without parental permission.\n\nMany commentators understand this threshold to be too young and have advocated for legislation to raise the age to 16.\n\nThe GDPR of the European Union (2016) specifies that users must be at least 16 years old to use the services of social media without parental permissions.\n\nThe emergence of various GenAI chatbots demand that countries carefully consider – and publicly deliberate – the appropriate age threshold for independent conversations with GenAI platforms.\n\nThe minimum threshold should be 13 years of age.\n\nCountries will also need to decide if self-reporting age remains an appropriate means of age verification.\n\nCountries will need to mandate the accountabilities of GenAI providers for age verification and accountabilities of parents or guardians for monitoring the independent conversations of underage children.",
    "## National data ownership and the risk of data poverty\n\nTake legislative measures to protect national data ownership and regulate providers of GenAI that operate within its borders.\n\nFor datasets generated by citizens that are being used for commercial purposes, establish regulations to promote mutually beneficial cooperation so that this category of data shall not be drained from the country to be exploited exclusively by the big tech companies.",
    "# Providers of GenAI Tools\n\nProviders of GenAI include organizations and individuals who are responsible for developing and making available GenAI tools, and/or are using GenAI technologies to provide services, including through programmable application programming interfaces (APIs).\n\nMost of the influential providers of GenAI tools are extremely well-funded companies.\n\nIt should be made clear to GenAI providers that they are accountable for ethics by design, including for implementing the ethical principles stipulated in the regulations.\n\nThe following ten categories of accountabilities should be covered:",
    "## Trustworthy data and models\n\nGenAI providers should be required to evidence the trustworthiness and ethics of the data sources and methods used by their models and outputs.\n\nThey must be mandated to adopt data and foundation models with proven legal sources, and abide by the relevant intellectual property laws (e.g., if the data are protected by intellectual property rights).\n\nIn addition, when the models need to use personal information, the collection of said information should take place only with the informed and explicit consent of the owners.",
    "## Non-discriminatory content generation\n\nProviders of GenAI must prohibit the design and deployment of GenAI systems that generate biased or discriminatory content based on race, nationality, gender, or other protected characteristics.\n\nThey should ensure that robust ‘guardrails’ are in place to prevent GenAI from producing offensive, biased, or false content, while ensuring that the humans involved in informing the guardrails are protected and not exploited.",
    "## Explainability and transparency of GenAI models\n\nProviders should submit to public governance agencies their explanations of the sources, scale, and types of data used by the models; their rules for labeling data in pre-training; the methods or algorithms that their models use to generate content or responses; and the services that their GenAI tools are providing.\n\nWhen necessary, they should offer support to help governance agencies understand the technology and data.\n\nGenAI’s propensity to generate content with errors and contestable responses should be made transparent for users.",
    "## Acknowledging the limitations and preventing predictable risks\n\nProviders of GenAI should clearly advertise the limitations of the methods used by the systems and their outputs.\n\nThey need to develop technologies to ensure that the input data, methods, and outputs do no predictable harm to users, together with protocols to mitigate unpredictable harms when they occur.\n\nThey must also provide guidance to help users understand GenAI-generated content based on ethical principles, and to prevent their over-reliance on and addiction to the generated content.",
    "## Monitoring and reporting of unlawful use\n\nProviders shall cooperate with public governance agencies to facilitate the monitoring and reporting of unlawful use.\n\nThis includes when people use GenAI products in ways that are illegal or violate ethical or social values such as promoting disinformation or hate speech, generating spam, or composing malware.",
    "## Validating proportionality and protecting users’ well-being\n\nImplement national classification mechanisms or build an institutional policy for categorizing and validating GenAI systems and applications.\n\nEnsure that the GenAI systems adopted by the institution are in line with locally validated ethical frameworks and do no predictable harm to the institutions’ target users, especially children and vulnerable groups.",
    "# Individual Users\n\nIndividual users potentially include all people globally who have access to the Internet and at least one type of GenAI tool.\n\nThe term ‘individual users,’ as employed here, mainly refers to individual teachers, researchers, and learners in formal educational institutions or those participating in non-formal programs of study.",
    "# Towards a Policy Framework for the Use of Generative AI in Education and Research\n\nRegulating GenAI to harness the potential benefits for education and research requires the development of appropriate policies.\n\nThe 2023 survey data cited above indicate that only a handful of countries have adopted specific policies or plans for the use of AI in education.\n\nThe preceding section outlined a vision, the steps required, and the key elements and actions that can be taken by various stakeholders.\n\nThis section provides measures that can be taken to develop coherent, comprehensive policy frameworks to regulate the use of GenAI in education and research.\n\nA starting point for this is the 2022 AI and education: guidance for policy-makers (UNESCO, 2022b).\n\nIt proposes a comprehensive set of recommendations to guide governments in the development and implementation of sector-wide policies on AI and education with a focus on promoting quality education, social equity, and inclusion.\n\nMost of the recommendations remain applicable and can be further adapted to guide the formulation of specific policies on GenAI in education.\n\nThe following eight specific measures for the planning of policies on GenAI in education and research are proposed here to complement this existing guidance.",
    "## Promote inclusion, equity, linguistic and cultural diversity\n\nThe critical importance of inclusion must be recognized and addressed throughout the life cycle of GenAI.\n\nMore specifically, GenAI tools will not help address the fundamental challenges in education or the achievement of SDG 4 commitments unless such tools are made inclusively accessible (irrespective of gender, ethnicity, special educational needs, socio-economic status, geographic location, displacement status and so on), and if they do not by design advance equity, linguistic diversities and cultural pluralism.\n\nTo achieve this, the following three policy measures are recommended:\n\n1.\n\nIdentify those who do not have or cannot afford internet connectivity or data, and take action to promote universal connectivity and digital competencies in order to reduce the barriers to equitable and inclusive access to AI applications.\n\nEstablish sustainable funding mechanisms for the development and provision of AI-enabled tools for learners who have disabilities or special needs.\n\nPromote the use of GenAI to support lifelong learners of all ages, locations, and backgrounds.\n\n2.\n\nDevelop criteria for the validation of GenAI systems to ensure that there is no gender bias, discrimination against marginalized groups, or hate speech embedded in data or algorithms.\n\n3.\n\nDevelop and implement inclusive specifications for GenAI systems and implement institutional measures to protect linguistic and cultural diversities when deploying GenAI in education and research at scale.\n\nRelevant specifications should require providers of GenAI to include data in multiple languages, especially local or indigenous languages, in the training of GPT models to improve GenAI’s ability to respond to and generate multilingual text.\n\nSpecifications and institutional measures should strictly prevent AI providers from any intentional or unintentional removal of minority languages or discrimination against speakers of indigenous languages, and require providers to stop systems promoting dominant languages or cultural norms.",
    "## Protect human agency\n\nAs GenAI becomes increasingly sophisticated, a key danger is its potential to undermine human agency.\n\nAs more individuals use GenAI to support their writing or other creative activities, they might unintentionally come to rely upon it.\n\nThis can compromise the development of intellectual skills.\n\nWhile GenAI may be used to challenge and extend human thinking, it should not be allowed to usurp human thinking.\n\nThe protection and enhancement of human agency should always be core considerations when designing and adopting GenAI from the following seven perspectives:\n\n1.\n\nInform learners about the types of data that GenAI may collect from them, how these data are used, and the impact it may have on their education and wider lives.\n\n2.\n\nProtect learners’ intrinsic motivation to grow and learn as individuals.\n\nReinforce human autonomy over their own approaches to research, teaching, and learning in the context of using increasingly sophisticated GenAI systems.\n\n3.\n\nPrevent the use of GenAI where it would deprive learners of opportunities to develop cognitive abilities and social skills through observations of the real world, empirical practices such as experiments, discussions with other humans, and independent logical reasoning.\n\n4.\n\nEnsure sufficient social interaction and appropriate exposure to creative output produced by humans and prevent learners becoming addicted to or dependent on GenAI.\n\n5.\n\nUse GenAI tools to minimize the pressure of homework and exams, rather than to exacerbate it.\n\n6.\n\nConsult researchers, teachers, and learners about their views on GenAI and use the feedback to decide whether and how specific GenAI tools should be deployed at an institutional scale.\n\nEncourage learners, teachers, and researchers to critique and question the methodologies behind the AI systems, the accuracy of the output content, and the norms or pedagogies that they may impose.\n\n7.\n\nPrevent ceding human accountability to GenAI systems when making high-stakes decisions.",
    "## Monitor and validate GenAI systems for education\n\nAs noted, the development and deployment of GenAI should be ethical by design.\n\nSubsequently, once the GenAI is in use, and throughout its life cycle, it needs to be carefully monitored and validated – for its ethical risks, its pedagogical appropriateness and rigor, and its impact on students, teachers, and classroom/school relationships.\n\nIn this respect, the following five actions are recommended:\n\n1.\n\nBuild validation mechanisms to test whether GenAI systems used in education and research are free of biases, especially gender biases, and whether they are trained on data representative of diversity (in terms of gender, disability, social and economic status, ethnic and cultural background, and geographic location).\n\n2.\n\nAddress the complex issue of informed consent, particularly in contexts where children or other vulnerable learners are not capable of giving genuinely informed consent.\n\n3.\n\nAudit whether outputs of GenAI include deepfake images, fake (inaccurate or false) news or hate speech.\n\nIf the GenAI is found to be generating inappropriate content, institutions and educators should be willing and able to take swift and robust action to mitigate or eliminate the problem.\n\n4.\n\nExercise strict ethical validation of GenAI applications before they are officially adopted in educational or research institutions (i.e., adopt an ethics-by-design approach).\n\n5.\n\nBefore making decisions on institutional adoption, ensure that the GenAI applications in question do no predictable harm to students, are educationally effective and valid for the ages and abilities of the target learners, and are aligned with sound pedagogical principles (i.e., based on the relevant domains of knowledge and the expected learning outcomes and development of values).",
    "## Develop AI competencies including GenAI-related skills for learners\n\nThe development of AI competencies among learners is key to the safe, ethical, and meaningful use of AI in education and beyond.\n\nHowever, according to UNESCO data, only some 15 countries had developed and implemented, or were in the process of developing, government-endorsed AI curricula in schools in early 2022 (UNESCO, 2022c).\n\nThe latest developments of GenAI have further reinforced the urgent need for everyone to achieve an appropriate level of literacy in both the human and technological dimensions of AI, understanding how it works in broad terms, as well as the specific impact of GenAI.\n\nIn order to do so, the following five actions are now urgently needed:\n\n1.\n\nCommit to the provision of government-sanctioned AI curricula for school education, in technical and vocational education and training, as well as for lifelong learning.\n\nAI curricula should cover the impact of AI on our lives, including the ethical issues it raises, as well as an age-appropriate understanding of algorithms and data, and skills for the proper and creative use of AI tools including GenAI applications.\n\n2.\n\nSupport higher education and research institutions to enhance programs to develop local AI talent.\n\n3.\n\nPromote gender equality in developing advanced AI competencies and create a gender-balanced pool of professionals.\n\n4.\n\nDevelop intersectoral forecasts of the national and global job shifts caused by the latest GenAI automation, and enhance future-proof skills at all levels of education and lifelong learning systems based on prospective shifts in demand.\n\n5.\n\nProvide special programs for older workers and citizens who may need to learn new skills and adapt to new environments.",
    "## Build capacity for teachers and researchers to make proper use of GenAI\n\nAccording to 2023 survey data on the governmental use of AI for education (UNESCO, 2023c), only some seven countries (China, Finland, Georgia, Qatar, Spain, Thailand, and Türkiye) reported that they had developed or were developing frameworks or training programs on AI for teachers.\n\nOnly the Ministry of Education of Singapore reported building an online repository centered on the use of ChatGPT in teaching and learning.\n\nThis clearly shows that teachers in most countries do not have access to well-structured training on the use of AI in education, not least on the use of GenAI.\n\nTo prepare teachers for the responsible and effective use of GenAI, countries need to take the following four actions:\n\n1.\n\nFormulate or adjust guidance based on local tests to help researchers and teachers to navigate widely available GenAI tools and steer the design of new domain-specific AI applications.\n\n2.\n\nProtect the rights of teachers and researchers and the value of their practices when using GenAI.\n\nMore specifically, analyze teachers’ unique roles in facilitating higher-order thinking, organizing human interaction, and fostering human values.\n\n3.\n\nDefine the value orientation, knowledge, and skills that teachers need in order to understand and use GenAI systems effectively and ethically.\n\nEnable teachers to create specific GenAI-based tools to facilitate learning in the classroom and in their own professional development.\n\n4.\n\nDynamically review the competencies needed by teachers to understand and use AI for teaching, learning, and for their professional development, and integrate emerging sets of values, understanding, and skills on AI into the competency frameworks and programs for training in-service and pre-service teachers.",
    "## Promote plural opinions and plural expressions of ideas\n\nAs noted earlier, GenAI understands neither the prompt nor the response.\n\nInstead, its responses are based on probabilities of language patterns found in the data (from the internet) that it ingested when its model was trained.\n\nTo address some of the fundamental problems of its outputs, new methods are currently being researched such as connecting GenAI with knowledge databases and reasoning engines.\n\nNonetheless, because of how it works, its source materials and the tacit perspectives of its developers, GenAI, by definition, reproduces dominant worldviews in its outputs and undermines minority and plural opinions.\n\nAccordingly, if human civilizations are to flourish, it is essential that we recognize that GenAI can never be an authoritative source of knowledge on whatever topic it engages with.\n\nAs a result, users need to view GenAI’s outputs critically.\n\nIn particular:\n\n1.\n\nUnderstand the role of GenAI as a fast but frequently unreliable source of information.\n\nWhile some plugins and LLM-based tools mentioned earlier are designed to support the need to access validated and up-to-date information, there is little robust evidence as yet that these are effective.\n\n2.\n\nEncourage learners and researchers to critique the responses provided by GenAI.\n\nRecognize that GenAI typically only repeats established or standard opinions, thus undermining plural and minority opinions and plural expressions of ideas.\n\n3.\n\nProvide learners with sufficient opportunities to learn from trial and error, empirical experiments, and observations of the real world.",
    "## Test locally relevant application models and build a cumulative evidence base\n\nGenAI models are thus far dominated by information from the Global North and under-representing voices from the Global South and indigenous communities.\n\nOnly by means of determined efforts, for example, harnessing synthetic data (Marwala, 2023), will GenAI tools be made sensitive to the context and needs of local communities, particularly those from the Global South.\n\nTo explore approaches relevant to local needs, while collaborating more widely, the following eight actions are recommended:\n\n1.\n\nEnsure the design and adoption of GenAI are strategically planned rather than facilitating a passive and non-critical procurement process.\n\n2.\n\nIncentivize the designers of GenAI to target open-ended, exploratory, and diverse learning options.\n\n3.\n\nTest and scale-up evidence-based use cases of applying AI in education and research in accordance with educational priorities, rather than novelty, myth, or hype.\n\n4.\n\nGuide the use of GenAI to trigger innovation in research, including through leveraging computing capabilities, large-scale data, and GenAI outputs to inform and inspire the improvement of research methodologies.\n\n5. Review the social and ethical implications of incorporating GenAI into research processes.\n\n6.\n\nEstablish specific criteria based on evidenced pedagogical research and methodologies and build an evidence base for the effectiveness of GenAI in terms of supporting the provision of inclusive learning opportunities, meeting learning and research objectives, and promoting linguistic and cultural diversities.\n\n7.\n\nTake iterative steps to strengthen evidence on the social and ethical impact of GenAI.\n\n8.\n\nAnalyze the environmental costs of leveraging AI technologies at scale (e.g.\n\nthe energy and resources required for training GPT models), and develop sustainable targets to be met by AI providers in a bid to avoid adding to climate change.",
    "## Review long-term implications in an intersectoral and interdisciplinary manner\n\nIntersectoral and interdisciplinary approaches are essential for the effective and ethical use of GenAI in education and research.\n\nOnly by drawing on a range of expertise, while bringing together multiple stakeholders, will key challenges be identified promptly and addressed effectively to minimize long-term negative implications while leveraging ongoing and cumulative benefits.\n\nTherefore, these three actions are recommended:\n\n1.\n\nCollaborate with AI providers, educators, researchers, and representatives of parents and students to plan system-wide adjustments in curriculum frameworks and assessment methodologies, to fully leverage the potential and mitigate the risks of GenAI for education and research.\n\n2.\n\nBring together intersectoral and interdisciplinary expertise including educators, researchers, learning scientists, AI engineers, and representatives of other stakeholders to examine the long-term implications of GenAI for learning and knowledge production, research and copyright, curriculum and assessment, and human collaboration and social dynamics.\n\n3.\n\nProvide timely advice to inform the iterative updates of regulations and policies.",
    "# Facilitating Creative Use of GenAI in Education and Research\n\nWhen ChatGPT was first launched, educators across the world expressed their concerns about its potential to generate essays and how it might help students to cheat.\n\nMore recently, many people and organizations including some of the world’s leading universities have argued that ‘the genie is out of the bottle’ and tools like ChatGPT are here to stay and may be used productively in educational settings.\n\nMeanwhile, the internet is now awash with suggestions for the use of GenAI in education and research.\n\nThese include using it to inspire new ideas, generate multi-perspective examples, develop lesson plans and presentations, summarize existing materials, and stimulate image creation.\n\nAlthough new ideas appear on the internet almost every day, researchers and educators are still working out exactly what GenAI means for teaching, learning, and research.\n\nIn particular, the people behind many of the proposed uses may not have properly considered ethical principles, while others are driven by the technological potentials.",
    "## Institutional Strategies to Facilitate Responsible and Creative Use of GenAI\n\nAs stated earlier, educational and research institutions should develop, implement, and validate appropriate strategies and ethical frameworks to guide the responsible and ethical use of GenAI systems and applications to meet the needs of teaching, learning, and research.\n\nThis can be achieved through the following four strategies:\n- **Institutional Implementation of Ethical Principles:** Ensure that researchers, teachers, and learners use GenAI tools responsibly and ethically, and critically approach the accuracy and validity of the outputs.\n\n- **Guidance and Training:** Provide guidance and training to researchers, teachers, and learners about GenAI tools to ensure that they understand the ethical issues such as biases in data labeling and algorithms, and that they comply with the appropriate regulations on data privacy and intellectual property.\n\n- **Building GenAI Prompt-Engineering Capacities:** In addition to subject-specific knowledge, researchers and teachers will also need expertise in engineering and critically evaluating the prompts generated by GenAI.\n\nGiven that the challenges raised by GenAI are complex, researchers and teachers must receive high-quality training and support to do this.\n\n- **Detecting GenAI-Based Plagiarism in Written Assignments:** GenAI might allow students to pass off text that they did not write as their own work, a new type of ‘plagiarism’.\n\nGenAI providers are required to label their outputs with ‘generated by AI’ watermarks, while tools are being developed to identify material that has been produced by AI.\n\nHowever, there is little evidence that these measures or tools are effective.\n\nThe immediate institutional strategy is to uphold academic integrity and reinforce accountability through rigorous detection by humans.\n\nThe long-term strategy is for institutions and educators to rethink the design of written assignments so that they are not used to assess tasks that GenAI tools can do better than human learners.\n\nInstead, they should address what humans can do that GenAI and other AI tools cannot do, including applying human values such as compassion and creativity to complex real-world challenges.",
    "## A ‘Human-Centred and Pedagogically Appropriate Interaction’ Approach\n\nResearchers and educators should prioritize human agency and responsible, pedagogically appropriate interaction between humans and AI tools when deciding on whether and how to use GenAI.\n\nThis includes the following five considerations:\n- The use of the tool(s) should contribute to humans’ needs and make learning or research more effective than a no-tech or other alternative approach.\n\n- Educators’ and learners’ use of the tool(s) should be based on their intrinsic motivation.\n\n- The process of using the tool(s) should be controlled by the human educators, learners, or researchers.\n\n- The choice and organization of the tool(s) and the content they generate should be proportionate, based on the learners’ age range, the expected results, and the type of target knowledge (e.g.\n\nfactual, conceptual, procedural, or metacognitive) or target problem (e.g.\n\nwell-structured or ill-structured).\n\n- The usage processes should ensure humans’ interactive engagement with GenAI and higher-order thinking, as well as human accountability for decisions related to the accuracy of AI-generated content, teaching or research strategies, and their impact on human behaviors.",
    "## Co-Designing the Use of GenAI in Education and Research\n\nThe use of GenAI in education and research should be neither imposed in a top-down approach nor driven by commercial hyperbole.\n\nInstead, its safe and effective use should be co-designed by teachers, learners, and researchers.\n\nIt also needs a robust process of piloting and evaluation to examine the effectiveness and the long-term impact of different uses.\n\nTo facilitate the recommended co-design, this Guidance proposes a framework composed of the following six perspectives to consolidate pedagogically appropriate interactions and the prioritization of human agency:\n- Appropriate domains of knowledge or problems\n- Expected outcomes\n- Appropriate GenAI tools and comparative advantages\n- Requirements for users\n- Required human pedagogical methods and example prompts\n- Ethical risks\n\nThis section provides examples of how a process of co-design in the use of GenAI can inform research practices, assist in teaching, provide coaching for the self-paced acquisition of foundational skills, facilitate higher-order thinking, and support learners with special needs.\n\nThese examples represent only the tip of the iceberg of the increasing number of domains in which GenAI may have potential.",
    "## Generative AI for Research\n\nGenAI models have demonstrated their potential to expand views on research outlines and to enrich data exploration as well as literature reviews (see Table 3).\n\nWhile a wider range of use cases may emerge, novel research is needed to define the potential domain of research problems and expected outcomes, to demonstrate the efficacy and accuracy, and to ensure that human agency in understanding the real world through research will not be undermined by the use of AI tools.",
    "## Generative AI to Facilitate Teaching\n\nThe use of both general GenAI platforms and specific educational GenAI tools should be designed to enhance teachers’ understanding of their subject as well as their knowledge on teaching methodologies, including through teacher-AI co-designing of lesson plans, course packages, or entire curricula.\n\nThe GenAI-assisted conversational teachers’ assistants or ‘generative twins of teaching assistants’ that are pre-trained based on data from experienced teachers and libraries, have been tested in some educational institutions and may hold unknown potential as well as uncharted ethical risk.\n\nThe practical application processes and further iterations of these models still need to be carefully audited through the framework recommended in this Guidance and safeguarded by human supervision as exemplified in Table 4.",
    "## Generative AI as a 1:1 Coach for Self-Paced Acquisition of Foundational Skills\n\nWhile higher-order thinking and creativity have been drawing increasing attention when defining learning outcomes, there is still no doubting the importance of foundational skills in children’s psychological development and competency progression.\n\nAmong a large spectrum of abilities, these foundational skills include listening, pronouncing, and writing a mother tongue or foreign language, as well as basic numeracy, art, and coding.\n\n‘Drill and practice’ should not be considered as an obsolete pedagogical method; instead, it should be reinvigorated and upgraded with GenAI technologies to foster learners’ self-paced rehearsal of foundational skills.\n\nIf guided by ethical and pedagogical principles, GenAI tools have the potential to become 1:1 coaches for such self-paced practice, as illustrated in Table 5.",
    "## Generative AI to Facilitate Inquiry or Project-Based Learning\n\nIf not used purposefully to facilitate higher-order thinking or creativity, GenAI tools tend to encourage plagiarism or shallow ‘stochastic parroting’ outputs.\n\nHowever, given that GenAI models have been trained based on large-scale data, they have potential for acting as an opponent in Socratic dialogues or as a research assistant in project-based learning.\n\nYet these potentials can only be leveraged through instructional/learning design processes that aim to trigger higher-order thinking as exemplified in Table 6.",
    "## Generative AI to Support Learners with Special Needs\n\nTheoretically, GenAI models have the potential to help learners with hearing or visual impairments.\n\nThe emerging practices include GenAI-enabled subtitles or captions for deaf and hard-of-hearing learners, and GenAI-generated audio description for visually impaired learners.\n\nGenAI models can also convert text to speech and speech to text to enable people with visual, hearing, or speech impairments to access content, ask questions, and communicate with their peers.\n\nHowever, this function has not yet been leveraged at scale.\n\nAccording to the survey mentioned earlier, conducted by UNESCO in 2023 on governments’ use of AI in education, only four countries (China, Jordan, Malaysia, and Qatar) reported that their governmental agencies had validated and recommended AI-assisted tools to support inclusive access for learners who have disabilities (UNESCO, 2023c).\n\nThere is also a trend toward iterations of GenAI models being trained to support learners to use their languages, including minority and indigenous languages, to learn and communicate.\n\nFor example, PaLM 2, Google’s next-generation LLM, is trained on parallel data covering hundreds of languages in the form of source and target text pairs.\n\nThe inclusion of parallel multilingual data is designed to further improve the model’s ability to understand and generate multilingual text (Google, 2023b).\n\nBy providing real-time translations, paraphrasing, and automatic correction, GenAI tools have the potential to help learners who use minority languages to communicate ideas and enhance their collaboration with peers from different linguistic backgrounds.\n\nHowever, this will not happen naturally at scale.\n\nOnly with purposeful design can this potential be leveraged to amplify the voices of marginalized groups.\n\nFinally, it has also been suggested that GenAI systems have the potential to carry out conversation-based diagnoses, identifying psychological or social-emotional problems as well as learning difficulties.\n\nHowever, there remains little evidence that this approach is either effective or safe, and any diagnoses would require interpretation by skilled professionals.",
    "### Uncharted Ethical Issues\n\nThe increasingly sophisticated GenAI tools will raise additional ethical concerns that need to be examined in detail.\n\nFurther to Sections 2 and 3, deeper and more forward-looking analyses are needed to reveal and address uncharted ethical issues from at least the following five perspectives:\n- **Access and Equity:** GenAI systems in education may exacerbate existing disparities in access to technology and educational resources, further deepening inequities.\n\n- **Human Connection:** GenAI systems in education may reduce human-to-human interaction and the critical social-emotional aspects of learning.\n\n- **Human Intellectual Development:** GenAI systems in education may limit learners’ autonomy and agency by providing predetermined solutions or narrowing the range of possible learning experiences.\n\nTheir long-term impact on young learners’ intellectual development needs to be investigated.\n\n- **Psychological Impact:** GenAI systems that mimic human interactions may have unknown psychological effects on learners, raising concerns about their cognitive development and emotional well-being, and about the potential for manipulation.\n\n- **Hidden Bias and Discrimination:** As more sophisticated GenAI systems are being developed and applied in education, they are likely to generate new biases and forms of discrimination based on the training data and methods used by the models, which can result in unknown and potentially harmful outputs.",
    "### Copyright and Intellectual Property\n\nThe emergence of GenAI is rapidly changing the way in which scientific, artistic, and literary works are created, distributed, and consumed.\n\nUnauthorized copying, distribution, or use of copyrighted works without permission from the copyright holder violates their exclusive rights and can lead to legal consequences.\n\nFor example, the training of GenAI models has been accused of infringing copyright.\n\nIn one of the recent cases, the AI-generated song featuring ‘Drake’ and ‘The Weeknd’ reached millions of listeners before being taken offline due to a copyright dispute (Coscarelli, 2023).\n\nWhile the emerging regulatory frameworks intend to require GenAI providers to recognize and protect the intellectual property of the owners of the content used by the model, it is becoming increasingly challenging to determine the ownership and originality of the overwhelming amount of generated works.\n\nThis lack of traceability not only raises concerns about protecting the rights of creators and ensuring fair compensation for their intellectual contributions, but also introduces challenges into educational contexts about how the output of GenAI tools may responsibly be used.\n\nThis may have profound implications for the research system.",
    "### Sources of Content and Learning\n\nGenAI tools are changing the way teaching and learning content can be generated and provided.\n\nIn the future, content generated through human-AI conversations may become one of the main sources of knowledge production.\n\nThis is likely to further undermine learners’ direct engagement with educational content based on resources, textbooks, and curricula created and validated by humans.\n\nThe authoritative appearance of GenAI text may mislead young learners who do not have sufficient prior knowledge to be able to recognize inaccuracies or to question it effectively.\n\nWhether learners’ engagement with unvalidated content should be recognized as ‘learning’ is also contestable.\n\nThe resultant concentration on aggregated second-hand information may also reduce learners’ opportunities for constructing knowledge through proven methods such as directly perceiving and experiencing the real world, learning from trial and error, performing empirical experiments, and developing common sense.\n\nIt may also threaten the social construction of knowledge and the fostering of social values through collaborative classroom practices.",
    "### Homogenized Responses Versus Diverse and Creative Outputs\n\nGenAI narrows plural narratives as the outputs generated tend to represent and reinforce dominant viewpoints.\n\nThe resulting homogenization of knowledge limits pluralistic and creative thinking.\n\nThe increased dependency of teachers and students on GenAI tools to seek suggestions may lead to the standardization and conformity of responses, weakening the value of independent thought and self-directed inquiry.\n\nThe potential homogenization of expression in written pieces and artwork can limit learners’ imagination, creativity, and alternative perspectives of expressions.\n\nGenAI providers and educators need to consider the extent to which EdGPT might be developed and used to foster creativity, collaboration, critical thinking, and other higher-order thinking skills.",
    "### Rethinking Assessment and Learning Outcomes\n\nThe implications of GenAI for assessment go far beyond the immediate concerns about learners cheating on written assignments.\n\nWe must contend with the fact that GenAI can produce relatively well-organized papers and essays and impressive works of art, and can pass some knowledge-based exams in certain subject areas.\n\nWe, therefore, need to rethink what exactly should be learned and to what ends, and how learning is to be assessed and validated.\n\nCritical discussion by educators, policymakers, learners, and other stakeholders need to consider the following four categories of learning outcomes:\n- **Values:** The values required to ensure the human-centered design and use of technology are central to the rethinking of learning outcomes and their assessment in the digital era.\n\nIn revisiting the purpose of education, the values that inform the way in which technology relates to education should be made explicit.\n\nIt is through this normative lens that learning outcomes and their assessment and validation need to be iteratively updated to respond to the increasingly pervasive use of technology, including AI, in society.\n\n- **Foundational Knowledge and Skills:** Even in the domains of competencies where GenAI tools can do better than humans, learners will still need sound foundational knowledge and skills.\n\nFoundational literacy, numeracy, and basic scientific literacy skills will remain key for education in the future.\n\nThe scope and nature of these foundational skills will need to be regularly revisited to reflect the increasingly AI-rich environments we live in.\n\n- **Higher-Order Thinking Skills:** Learning outcomes will need to include skills required to support higher-order thinking and problem-solving based on human-AI collaboration and the use of GenAI-generated outputs.\n\nThese may include understanding the roles of factual and conceptual knowledge in grounding higher-order thinking, and the critical evaluation of AI-generated content.\n\n- **Vocational Skills Needed to Work with AI:** In the domains where AI can do better than humans and is automating task units, human learners need to nurture new skills that enable them to develop, operate, and work with GenAI tools.\n\nThe redesign of learning outcomes and educational assessment will need to reflect the vocational skills required for the new jobs created by AI.",
    "### Thinking Processes\n\nThe most fundamental perspective of the long-term implications of GenAI for education and research is still about the complementary relationship between human agency and machines.\n\nOne of the key questions is whether humans can possibly cede basic levels of thinking and skill-acquisition processes to AI and rather concentrate on higher-order thinking skills based on the outputs provided by AI.\n\nWriting, for example, is often associated with the structuring of thinking.\n\nWith GenAI, rather than starting from scratch to plan the aims, scope, and outline of a set of ideas, humans can now start with a well-structured outline provided by GenAI.\n\nSome experts have characterized the use of GenAI to generate text in this way as ‘writing without thinking’ (Chayka, 2023).\n\nAs these new GenAI-assisted practices become more widely adopted, established methods for the acquisition and assessment of writing skills will need to adapt.\n\nOne option in the future is that the learning of writing may focus on building skills in planning and composing prompts, critical evaluation of the GenAI outputs, and higher-order thinking, as well as on co-writing based on GenAI’s outlines.",
    "## Concluding Remarks\n\nFrom the perspective of a human-centered approach, AI tools should be designed to extend or augment human intellectual abilities and social skills, and not undermine them, conflict with them, or usurp them.\n\nIt has long been expected that AI tools can be further integrated into part and parcel of the tools available to humans to support analysis and action for more inclusive and sustainable futures.\n\nFor AI to be a trustable part and parcel of human-machine collaboration – at individual, institutional, and system levels – the human-centered approach informed by the 2021 UNESCO Recommendation on the Ethics of AI is to be further specified and implemented according to the specific characteristics of emerging technologies such as GenAI.\n\nOnly in this way can we ensure that GenAI becomes a trustworthy tool for researchers, teachers, and learners.\n\nWhile GenAI should be used to serve education and research, we all need to be cognizant that GenAI might also change the established systems and their foundations in these domains.\n\nThe transformation of education and research to be triggered by GenAI, if any, should be rigorously reviewed and steered by a human-centered approach.\n\nOnly by doing so can we ensure that the potentials of AI in particular, and all other categories of technologies used in education more broadly, enhance human capabilities to build inclusive digital futures for all.",
    "# References\n\n- Anders, B.\n\nA.\n\n2023.\n\nIs using ChatGPT...\n- Bass, D. and Metz, R. 2023.\n\nOpenAI’s Sam Altman...\n- Bender, E. M., Gebru, T., McMillan-Major, A. and Shmitchell, S. 2021...\n- Bommasani, R. et al.\n\n2021.\n\nOn the Opportunities...\n- Bove, T. 2023.\n\nBig tech is making big AI promises...\n- Chayka, K. 2023.\n\nMy A.I.\n\nWriting Report...\n- Chen, L., Zaharia, M., and Zou, J.\n\n2023...\n- Coscarelli, J.\n\n2023.\n\nAn A.I.\n\nHit of Fake ‘Drake’...\n- Cyberspace Administration of China.\n\n2023a...\n- Cyberspace Administration of China.\n\n2023b...\n- Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Baabdullah, A. M., Koohang, A., Raghavan, V...\n- E2Analyst.\n\n2023.\n\nGPT-4: Everything you want to know...\n- European Commission.\n\n2021.\n\nLaying down harmonised rules...\n- European Union.\n\n2016.\n\nRegulation (EU) 2016/679...\n- Federal Trade Commission.\n\n1998.\n\nChildren’s Online Privacy...\n- Giannini, S. 2023.\n\nGenerative AI and the Future...\n- Google.\n\n2023a.\n\nRecommendations for Regulating AI...\n- Lin, B.\n\n2023.\n\nAI Is Generating Security Risks...\n- Marcus, G. 2022.\n\nHoping for the Best as AI Evolves...\n- Marwala, T. 2023.\n\nAlgorithm Bias...\n- Metz, C. 2021. Who Is Making Sure the A.I.\n\nMachines...\n- Murphy Kelly, S. 2023.\n\nMicrosoft is bringing...\n- Nazaretsky, T., Cukurova, M. and Alexandron, G. 2022a...\n- Nazaretsky, T., Ariely, M., Cukurova, M. and Alexandron...\n- Ocampo, Y.\n\n2023.\n\nSingapore Unveils AI Government...\n- OpenAI.\n\n2018.\n\nAI and compute...\n- OpenAI.\n\n2023.\n\nEducator considerations for ChatGPT...",
    "# References\n\nPT.\n\nSan Francisco, OpenAI.\n\nAvailable at:  (Accessed 23 June 2023.)\n\nPopli, N. 2023.\n\nThe AI Job That Pays Up to $335K—and You Don’t Need a Computer Engineering Background.\n\nNew York, TIME USA.\n\nAvailable at:  (Accessed 23 June 2023.)\n\nRoose, K. 2022.\n\nAn A.I.-Generated Picture Won an Art Prize.\n\nArtists Aren’t Happy.\n\nNew York, The New York Times.\n\nAvailable at:  (Accessed 23 June 2023.)\n\nRussell Group, 2023.\n\nRussell Group principles on the use of generative AI tools in education.\n\nCambridge, Russell Group.\n\nAvailable at:  (Accessed 25 August 2023.)\n\nStanford University.\n\n2019.\n\nArtificial Intelligence Index Report.\n\nStanford, Stanford University.\n\nAvailable at:  (Accessed 23 June 2023.)\n\n---.\n\n2023.\n\nArtificial Intelligence Index Report.\n\nStanford, Stanford University.\n\nAvailable at:  (Accessed 23 June 2023.)\n\nThe Verge.\n\n2023a.\n\nOpenAI co-founder on company’s past approach to openly sharing research: ‘We were wrong’.\n\nWashington DC, Vox Media.\n\nAvailable at:  (Accessed 1 August 2023.)\n\n---.\n\n2023b.\n\nOpenAI CEO Sam Altman on GPT-4: ‘people are begging to be disappointed and they will be’.\n\nWashington DC, Vox Media.\n\nAvailable at:  (Accessed 1 August 2023.)\n\nTlili, A., Shehata, B., Agyemang Adarkwah, M., Bozkurt, A., Hickey, D. T., Huang, R. and Agyemang, B.\n\nWhat if the devil is my guardian angel: ChatGPT as a case study of using chatbots in education.\n\nSmart Learning Environments, Vol.\n\n10, No.\n\n15.\n\nBerlin, Springer.\n\nAvailable at:  (Accessed 23 June 2023.)\n\nUNESCO.\n\n2019.\n\nBeijing Consensus on Artificial Intelligence and Education.\n\nParis, UNESCO.\n\nAvailable at:  (Accessed 3 July 2023.)\n\n---.\n\n2022a.\n\nRecommendation on the Ethics of Artificial Intelligence.\n\nParis, UNESCO.\n\nAvailable at:  (Accessed 3 July 2023.)\n\n---.\n\n2022b.\n\nAI and education: guidance for policy-makers.\n\nParis, UNESCO.\n\nAvailable at:  (Accessed 23 June 2023.)\n\n---.\n\n2022c.\n\nK-12 AI curricula: a mapping of government-endorsed AI curricula.\n\nParis, UNESCO.\n\nAvailable at:  (Accessed 20 July 2023.)\n\n---.\n\n2022d.\n\nGuidelines for ICT in education policies and masterplans.\n\nParis, UNESCO.\n\nAvailable at:  (Accessed 31 July 2023.)\n\n---.\n\n2023a.\n\nArtificial Intelligence: UNESCO calls on all Governments to implement Global Ethical Framework without delay.\n\nParis, UNESCO.\n\nAvailable at:  (Accessed 3 July 2023.)\n\n---.\n\n2023b.\n\nMapping and analysis of governmental strategies for regulating and facilitating the creative use of GenAI.\n\nUnpublished.\n\n---.\n\n2023c.\n\nSurvey for the governmental use of AI as a public good for education.\n\nUnpublished (Submitted to UNESCO).\n\n---.\n\n2023.\n\nTechnology in Education: A tool on whose terms?\n\nParis, Global Education Monitoring Report Team.\n\nAvailable at:  (Accessed 25 August 2023.)\n\n---.\n\n2023.\n\nChatGPT and Artificial Intelligence in Higher Education: Quick start guide.\n\nCaracas, UNESCO International Institute for Higher Education in Latin America and the Caribbean.\n\nAvailable at:  (Accessed 25 August 2023.)\n\nUS Copyright Office.\n\n2023.\n\nCopyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence.\n\nFederal Register, Vol.\n\n88, No.\n\n51.\n\nWashington DC, United States (U.S.) Copyright Office, Library of Congress, pp.\n\n16190-16194.\n\nAvailable at:  (Accessed 3 July 2023.)\n\n---",
    "# Endnotes\n\nGenAI models became available to researchers and other interested parties far earlier than ChatGPT.\n\nFor example, in 2015 Google released what they called ‘DeepDream’ (\n\nSee \n\nFor an explanation of AI techniques and technologies and their relationship, see UNESCO, 2022b, pp.\n\n8-10.\n\nNote that, because GenAI is still relatively new, different companies often use these terms in different ways, and sometimes use different words to mean the same thing.\n\nThere is concern that the data used to train future iterations of OpenAI GPT will include substantial amounts of text generated by previous versions of GPT.\n\nThis self-referential loop might contaminate the training data and thus compromise the capabilities of future GPT models.\n\nNB OpenAI, the company that developed the GPTs in this table, has not publicly released detailed information about GPT-4 (The Verge, 2023a).\n\nIn fact, the number of parameters has been debunked by OpenAI’s CEO (The Verge, 2023b).\n\nHowever, the figures included here have been reported by a number of outlets (for example, see E2Analyst, 2023).\n\nIn any case, the main takeaway is that GPT-4 is built on a massively larger dataset and uses a massively larger number of parameters than GPT-3.\n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nGuidance for generative AI in education and research\n\nEndnotes\n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee \n\nSee  and \n\nSee \n\nSee \n\nThere are a few exceptions, such as Hugging Face, a group that is dedicated to open-source AI development.\n\nSee, for example, calls from Google (2023a) and OpenAI (Bass and Metz, 2023).\n\nFor one project to regulate AI see the European Commission’s draft AI Act (2021).\n\nThe review was based on data collected from a UNESCO survey distributed to its 193 Member States on the governmental use of AI in education (UNESCO, 2023c), the OECD AI Policy Observatory, and Stanford University’s AI Index Report (Stanford University, 2023), and first-hand information elicited from a group of international experts.\n\nSee \n\nFrom the mapping, as of April 2023, the following countries had published national strategies on AI: Argentina, Australia, Austria, Belgium, Benin, Brazil, Canada, Bulgaria, Chile, China, Columbia, Cyprus, Czechia, Denmark, Egypt, Estonia, Finland, France, Germany, Hungary, Iceland, India, Indonesia, Ireland, Italy, Japan, Jordan, Latvia, Lithuania, Luxembourg, Malaysia, Malta, Mauritius, Mexico, Netherlands (Kingdom of the), Norway, New Zealand, Oman, Peru, Poland, Portugal, Philippines, Qatar, Republic of Korea, Romania, Russian Federation, Saudi Arabia, Serbia, Singapore, Slovenia, Spain, Sweden, Thailand, Türkiye, Tunisia, United Arab Emirates, United Kingdom, United States, Uruguay and Viet Nam.\n\nAdditionally, some countries have incorporated AI strategies within broader ICT or digital strategies, including Algeria, Botswana, Kazakhstan, Kenya, Sierra Leone, Slovakia, Switzerland and Uganda.\n\nAccording to a rapid review of all national AI strategies (UNESCO, 2023b), over 40 strategies have dedicated sections on the issue of ethics.\n\nAccording to a rapid review of all national AI strategies (UNESCO, 2023b), around 45 strategies have dedicated sections on the issue of education.\n\nSee \n\nIn some countries, a teacher will have a teaching assistant (TA) whose role is to spend time answering the questions of individual students covering the course material.\n\nGenAI might be used to develop a generative twin of a TA, which can be supportive to the students and other teachers, but may also cause some negative issues (e.g.\n\naround social relationships in the classroom).",
    "# Guidance for Generative AI in Education and Research\n\nThis Guidance aims to support the planning of appropriate regulations, policies and human capacity development programmes to ensure that generative artificial intelligence (GenAI) becomes a tool that genuinely benefits and empowers teachers, learners and researchers.\n\nIt explains the AI techniques used by GenAI and maps out a list of GPT models that are made publicly available, especially those under open-source licences.\n\nIt also opens a discussion on the emergence of EdGPT – GenAI models that are trained with specific data to serve educational purposes.\n\nFurthermore, it summarizes some of the key controversies around GenAI, from worsening digital poverty to the homogenization of opinions, and from deeper deepfakes to issues of copyright.\n\nBased on a humanistic vision, the Guidance proposes key steps for the regulation of GenAI tools, including mandating the protection of data privacy and setting an age limit for independent conversations with GenAI platforms.\n\nTo guide the proper use of the tools in education and research, this Guidance proposes a human-agent and age-appropriate approach to the ethical validation and pedagogical design processes.",
    "### By:\n\nSeaton Huang, Helen Toner, Zac Haluza, Rogier Creemers, Graham Webster  \nPublished April 12, 2023\n\nThis translation is by (in randomized order), Seaton Huang, Helen Toner, Zac Haluza, and Rogier Creemers, and was edited by Graham Webster.\n\nDuring editing, an alternative translation from China Law Translate was consulted.\n\nFor analysis of this draft, please see our DigiChina Forum, compiling analysis from a group of invited specialists.",
    "### Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)\n\n**Article 1:** In order to stimulate the healthy development and standardized application of generative artificial intelligence (AI), on the basis of the Cybersecurity Law of the Peopleʼs Republic of China, the Data Security Law of the Peopleʼs Republic of China, the Personal Information Protection Law of the Peopleʼs Republic of China, and other such laws and administrative regulations, these Measures are formulated.\n\n**Article 2:** These Measures apply to the research, development, and use of products with generative AI functions, and to the provision of services to the public within the [mainland] territory of the Peopleʼs Republic of China.\n\nGenerative AI, as mentioned in these Measures, refers to technologies generating text, image, audio, video, code, or other such content based on algorithms, models, or rules.\n\n**Article 3:** The State supports indigenous innovation, broad application, and international cooperation in foundational technologies such as AI algorithms and frameworks, and encourages the prioritized use of secure and reliable software, tools, computing, and data resources.\n\n**Article 4:** The provision of generative AI products or services shall abide by the requirements of laws and regulations, respect social virtue and good public customs, and conform to the following requirements:\n\n- Content generated through the use of generative AI shall reflect the Socialist Core Values, and may not contain: subversion of state power; overturning of the socialist system; incitement of separatism; harm to national unity; propagation of terrorism or extremism; propagation of ethnic hatred or ethnic discrimination; violent, obscene, or sexual information; false information; as well as content that may upset economic order or social order.\n\n- In processes such as algorithm design, selecting training data, model generation and model optimization, service provision, etc., adopt measures to prevent the emergence of discrimination on the basis of race, ethnicity, religious belief, nationality, region, sex, age, or profession.\n\n- Respect intellectual property rights and commercial ethics; advantages in algorithms, data, platforms, etc., may not be used to engage in unfair competition.\n\n- Content generated through the use of generative AI shall be true and accurate, and measures are to be adopted to prevent the generation of false information.\n\n- Respect the lawful rights and interests of others; prevent harm to the physical and mental health of others, infringement of their likeness rights, reputation rights and personal privacy, as well as infringement of intellectual property rights.\n\nIt is prohibited to illegally obtain, divulge or use personal information and private [information], as well as commercial secrets.\n\n**Article 5:** Organizations or individuals that use generative AI to provide services such as chat, text, image, or audio generation (hereinafter referred to as “providers”); including providing programmable interfaces [i.e., APIs] and other means which support others to themselves generate text, images, audio, etc.\n\n; bear responsibility as the producer of the content generated by the product.\n\nWhere personal information is involved, they bear legal responsibility as personal information handlers and are to fulfill personal information protection obligations.\n\n**Article 6:** Before using generative AI products to provide services to the public, a security assessment must be submitted to the state cyberspace and information department [i.e., the Cyberspace Administration of China] in accordance with the Provisions on the Security Assessment of Internet Information Services With Public Opinion Properties or Social Mobilization Capacity, and the procedures of algorithm filing, modification, and cancellation of filing must be carried out in accordance with the Internet Information Service Algorithmic Recommendation Management Provisions.\n\n**Article 7:** Providers shall bear responsibility for the legality of the sources of generative AI product pre-training data and optimization training data.\n\nData used for generative AI product pre-training and optimization training shall satisfy the following requirements:\n\n- Conforming to the requirements of the Cybersecurity Law of the Peopleʼs Republic of China and other such laws and regulations;\n- Not containing content infringing intellectual property rights;\n- Where data includes personal information, the consent of the personal information subject shall be obtained, or other procedures conforming with the provisions of laws and administrative regulations followed;\n- Be able to ensure the dataʼs veracity, accuracy, objectivity, and diversity;\n- Other supervision requirements of the state cybersecurity and informatization department concerning generative AI functions and services.",
    "**Article 8:** When human annotation is used in the development of generative AI products, providers shall formulate clear, specific, and practicable annotation rules conforming to the requirements of these Measures; necessary training of annotation personnel shall be conducted; and the validity of annotation content shall be spot checked.\n\n**Article 9:** When providing generative AI services, users shall be required to provide real identity information in accordance with the provisions of the Cybersecurity Law of the Peopleʼs Republic of China.\n\n**Article 10:** Providers shall explicitly disclose the user groups, occasions, and uses for their services, and adopt appropriate measures to prevent users from excessive reliance on or addiction to generated content.\n\n**Article 11:** In the process of providing services, providers have the duty to protect information input by users and usage records.\n\nThey may not illegally preserve input information from which it is possible to deduce the identity of users, they may not conduct profiling on the basis of information input by users and their usage details, and they may not provide information input by users to others.\n\nWhere laws or regulations provide otherwise, those provisions are to be followed.\n\n**Article 12:** Providers may not engage in content generation that is discriminatory based on a userʼs race, nationality, sex, etc.\n\n**Article 13:** Providers shall establish mechanisms for receiving and handling user complaints and promptly handle individual requests concerning revision, deletion, or masking of their personal information; and when they discover or learn that generated text, images, audio, video, etc., infringe other personsʼ likeness rights, reputation rights, personal privacy, or commercial secrets, or do not conform to the demands of these Measures, they shall adopt measures to cease generation and prevent the expansion of the harm.\n\n**Article 14:** Providers shall, throughout the lifecycle, provide secure, stable, and sustained services, and ensure usersʼ normal usage.\n\n**Article 15:** When generated content that does not conform to the requirements of these Measures is discovered during operations or reported by users, aside from adopting content filtering and other such measures, repeat generation is to be prevented through such methods as optimization training within three months.\n\n**Article 16:** Providers shall mark generated images, videos, and other content in accordance with the Internet Information Service Deep Synthesis Management Provisions.\n\n**Article 17:** Providers shall, in accordance with the requirements of the state cybersecurity and informatization department and relevant responsible departments, provide necessary information that could influence users trust or choices, including descriptions of the source, scale, type, quality, etc., of pre-training and optimization training data; rules for human annotation; the scale and type of human-annotated data; and foundational algorithms and technological systems.\n\n**Article 18:** Providers shall guide users to scientifically understand and rationally use content generated by generative AI; not to use generated content to damage othersʼ image, reputation, or other lawful rights and interests; and not to engage in commercial hype or improper marketing.\n\nWhen users discover generated content that does not meet the requirements of these measures, they have the right to report this to cybersecurity and informatization departments or relevant responsible departments.\n\n**Article 19:** If a provider finds that a user has used generative AI products to violate laws or regulations; violate business ethics or social virtue, including engaging in online hype, malicious posting and commenting, creating spam, or writing malicious software; or engage in improper business marketing; etc.\n\n; service shall be suspended or terminated.\n\n**Article 20:** If a provider violates the provisions of the Measures, the cybersecurity and informatization department and relevant responsible departments are to impose penalties in accordance with the provisions of Cybersecurity Law of the Peopleʼs Republic of China, the Data Security Law of the Peopleʼs Republic of China, the Personal Information Protection Law of the Peopleʼs Republic of China, and other such laws and administrative regulations.\n\nWhere there are no provisions of law or administrative regulation, the cybersecurity and informatization department and relevant responsible departments are to, in accordance with their duties, issue warnings, circulate criticisms, and order corrections within a set period of time.\n\nWhere corrections are refused or circumstances are grave, they are to order suspension or termination of their use of generative AI provider services, and a penalty more than 10,000 yuan and less than 100,000 yuan is to be imposed.",
    "Where behavior constitutes a violation of public security management, public security management penalties are to be imposed in accordance with the law.\n\nWhere a crime is constituted, criminal responsibility shall be pursued in accordance with the law.\n\n**Article 21:** These measures are effective beginning [day] [month], 2023.",
    "## Chinese-language Original\n\n[Source] [Archived copy]\n\n生成式人工智能服务管理办法\n（征求意见稿）\n第一条 为促进生成式人工智能健康发展和规范应用，根据《中华人民共和国网络安全法》《中华人民共和国数据安全法》《中华人民共和国个人信息保护法》等法律、行政法规，制定本办法。\n第二条 研发、利用生成式人工智能产品，面向中华人民共和国境内公众提供服务的，适用本办法。\n本办法所称生成式人工智能，是指基于算法、模型、规则生成文本、图片、声音、视频、代码等内容的技术。\n第三条 国家支持人工智能算法、框架等基础技术的自主创新、推广应用、国际合作，鼓励优先采用安全可信的软件、工具、计算和数据资源。\n第四条 提供生成式人工智能产品或服务应当遵守法律法规的要求，尊重社会公德、公序良俗，符合以下要求：\n\n- （一）利用生成式人工智能生成的内容应当体现社会主义核心价值观，不得含有颠覆国家政权、推翻社会主义制度，煽动分裂国家、破坏国家统一，宣扬恐怖主义、极端主义，宣扬民族仇恨、民族歧视，暴力、淫秽色情信息，虚假信息，以及可能扰乱经济秩序和社会秩序的内容。\n\n- （二）在算法设计、训练数据选择、模型生成和优化、提供服务等过程中，采取措施防止出现种族、民族、信仰、国别、地域、性别、年龄、职业等歧视。\n\n- （三）尊重知识产权、商业道德，不得利用算法、数据、平台等优势实施不公平竞争。\n\n- （四）利用生成式人工智能生成的内容应当真实准确，采取措施防止生成虚假信息。\n\n- （五）尊重他人合法利益，防止伤害他人身心健康，损害肖像权、名誉权和个人隐私，侵犯知识产权。禁止非法获取、披露、利用个人信息和隐私、商业秘密。\n\n第五条 利用生成式人工智能产品提供聊天和文本、图像、声音生成等服务的组织和个人（以下称“提供者”），包括通过提供可编程接口等方式支持他人自行生成文本、图像、声音等，承担该产品生成内容生产者的责任；涉及个人信息的，承担个人信息处理者的法定责任，履行个人信息保护义务。\n第六条 利用生成式人工智能产品向公众提供服务前，应当按照《具有舆论属性或社会动员能力的互联网信息服务安全评估规定》向国家网信部门申报安全评估，并按照《互联网信息服务算法推荐管理规定》履行算法备案和变更、注销备案手续。\n第七条 提供者应当对生成式人工智能产品的预训练数据、优化训练数据来源的合法性负责。用于生成式人工智能产品的预训练、优化训练数据，应满足以下要求：\n\n- （一）符合《中华人民共和国网络安全法》等法律法规的要求；\n- （二）不含有侵犯知识产权的内容；\n- （三）数据包含个人信息的，应当征得个人信息主体同意或者符合法律、行政法规规定的其他情形；\n- （四）能够保证数据的真实性、准确性、客观性、多样性；\n- （五）国家网信部门关于生成式人工智能服务的其他监管要求。\n\n第八条 生成式人工智能产品研制中采用人工标注时，提供者应当制定符合本办法要求，清晰、具体、可操作的标注规则，对标注人员进行必要培训，抽样核验标注内容的正确性。\n第九条 提供生成式人工智能服务应当按照《中华人民共和国网络安全法》规定，要求用户提供真实身份信息。\n第十条 提供者应当明确并公开其服务的适用人群、场合、用途，采取适当措施防范用户过分依赖或沉迷生成内容。\n第十一条 提供者在提供服务过程中，对用户的输入信息和使用记录承担保护义务。不得非法留存能够推断出用户身份的输入信息，不得根据用户输入信息和使用情况进行画像，不得向他人提供用户输入信息。法律法规另有规定的，从其规定。\n第十二条 提供者不得根据用户的种族、国别、性别等进行带有歧视性的内容生成。\n第十三条 提供者应当建立用户投诉接收处理机制，及时处置个人关于更正、删除、屏蔽其个人信息的请求；发现、知悉生成的文本、图片、声音、视频等侵害他人肖像权、名誉权、个人隐私、商业秘密，或者不符合本办法要求时，应当采取措施，停止生成，防止危害持续。\n第十四条 提供者应当在生命周期内，提供安全、稳健、持续的服务，保障用户正常使用。\n第十五条 对于运行中发现、用户举报的不符合本办法要求的生成内容，除采取内容过滤等措施外，应在3个月内通过模型优化训练等方式防止再次生成。\n第十六条 提供者应当按照《互联网信息服务深度合成管理规定》对生成的图片、视频等内容进行标识。\n第十七条 提供者应当根据国家网信部门和有关主管部门的要求，提供可以影响用户信任、选择的必要信息，包括预训练和优化训练数据的来源、规模、类型、质量等描述，人工标注规则，人工标注数据的规模和类型，基础算法和技术体系等。\n第十八条 提供者应当指导用户科学认识和理性使用生成式人工智能生成的内容，不利用生成内容损害他人形象、名誉以及其他合法权益，不进行商业炒作、不正当营销。\n用户发现生成内容不符合本办法要求时，有权向网信部门或者有关主管部门举报。\n\n第十九条 提供者发现用户利用生成式人工智能产品过程中违反法律法规，违背商业道德、社会公德行为时，包括从事网络炒作、恶意发帖跟评、制造垃圾邮件、编写恶意软件，实施不正当的商业营销等，应当暂停或者终止服务。\n第二十条 提供者违反本办法规定的，由网信部门和有关主管部门按照《中华人民共和国网络安全法》《中华人民共和国数据安全法》《中华人民共和国个人信息保护法》等法律、行政法规的规定予以处罚。\n法律、行政法规没有规定的，由网信部门和有关主管部门依据职责给予警告、通报批评，责令限期改正；拒不改正或者情节严重的，责令暂停或者终止其利用生成式人工智能提供服务，并处一万元以上十万元以下罚款。构成违反治安管理行为的，依法给予治安管理处罚；构成犯罪的，依法追究刑事责任。\n第二十一条 本办法自2023年 月 日起实施。",
    "## P9_TA(2023)0236\n\nArtificial Intelligence Act\n\nAmendments adopted by the European Parliament on 14 June 2023 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD))1  \n(Ordinary legislative procedure: first reading)\n\n1 The matter was referred back for interinstitutional negotiations to the committee responsible, pursuant to Rule 59(4), fourth subparagraph (A9-0188/2023).",
    "**Proposal for a regulation Recital 1**\n\n- **Text proposed by the Commission**  \nThe purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values.\n\nThis Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI-based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation.\n\n- **Amendment**  \nThe purpose of this Regulation is to promote the uptake of human-centric and trustworthy artificial intelligence and to ensure a high level of protection of health, safety, fundamental rights, democracy and rule of law and the environment from harmful effects of artificial intelligence systems in the Union while supporting innovation and improving the functioning of the internal market.\n\nThis Regulation lays down a uniform legal framework in particular for the development, the placing on the market, the putting into service and the use of artificial intelligence in conformity with Union values and ensures the free movement of AI-based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of Artificial Intelligence systems (AI systems), unless explicitly authorised by this Regulation.\n\nCertain AI systems can also have an impact on democracy and rule of law and the environment.\n\nThese concerns are specifically addressed in the critical sectors and use cases listed in the annexes to this Regulation.",
    "**Proposal for a regulation Recital 1 a (new)**\n\n- **Text proposed by the Commission**  \n(1a) This Regulation should preserve the values of the Union facilitating the distribution of artificial intelligence benefits across society, protecting individuals, companies, democracy and rule of law and the environment from risks while boosting innovation and employment and making the Union a leader in the field.",
    "**Proposal for a regulation Recital 2**\n\n- **Text proposed by the Commission**  \nArtificial intelligence systems (AI systems) can be easily deployed in multiple sectors of the economy and society, including cross-border, and circulate throughout the Union.\n\nCertain Member States have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental rights obligations.\n\nDiffering national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems.\n\nA consistent and high level of protection throughout the Union should therefore be ensured, while divergences hampering the free circulation of AI systems and related products and services within the internal market should be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU).\n\nTo the extent that this Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning restrictions of the use of AI systems for ‘real-time’ remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this Regulation, in as far as those specific rules are concerned, on Article 16 of TFEU.\n\nIn light of those specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board.\n\n- **Amendment**  \nAI systems can be easily deployed in multiple sectors of the economy and society, including cross-border, and circulate throughout the Union.\n\nCertain Member States have already explored the adoption of national rules to ensure that artificial intelligence is trustworthy and safe and is developed and used in compliance with fundamental rights obligations.\n\nDiffering national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems.\n\nA consistent and high level of protection throughout the Union should therefore be ensured in order to achieve trustworthy AI, while divergences hampering the free circulation, innovation, deployment and uptake of AI systems and related products and services within the internal market should be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU).",
    "**Proposal for a regulation Recital 2 a (new)**\n\n- **Text proposed by the Commission**  \n(2a) As artificial intelligence often relies on the processing of large volumes of data, and many AI systems and applications on the processing of personal data, it is appropriate to base this Regulation on Article 16 TFEU, which enshrines the right to the protection of natural persons with regard to the processing of personal data and provides for the adoption of rules on the protection of individuals with regard to the processing of personal data.",
    "**Proposal for a regulation Recital 2 b (new)**\n\n- **Text proposed by the Commission**  \n(2b) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive 2016/680.\n\nDirective 2002/58/EC additionally protects private life and the confidentiality of communications, including providing conditions for any personal and non-personal data storing in and access from terminal equipment.\n\nThose legal acts provide the basis for sustainable and responsible data processing, including where datasets include a mix of personal and nonpersonal data.\n\nThis Regulation does not seek to affect the application of existing Union law governing the processing of personal data, including the tasks and powers of the independent supervisory authorities competent to monitor compliance with those instruments.\n\nThis Regulation does not affect the fundamental rights to private life and the protection of personal data as provided for by Union law on data protection and privacy and enshrined in the Charter of Fundamental Rights of the European Union (the ‘Charter’).",
    "**Proposal for a regulation Recital 2 c (new)**\n\n- **Text proposed by the Commission**  \n(2c) Artificial intelligence systems in the Union are subject to relevant product safety legislation that provides a framework protecting consumers against dangerous products in general and such legislation should continue to apply.\n\nThis Regulation is also without prejudice to the rules laid down by other Union legal acts related to consumer protection and product safety, including Regulation (EU) 2017/2394, Regulation (EU) 2019/1020 and Directive 2001/95/EC on general product safety and Directive 2013/11/EU.",
    "**Proposal for a regulation Recital 2 d (new)**\n\n- **Text proposed by the Commission**  \n(2d) In accordance with Article 114(2) TFEU, this Regulation complements and should not undermine the rights and interests of employed persons.\n\nThis Regulation should therefore not affect Union law on social policy and national labour law and practice, that is any legal and contractual provision concerning employment conditions, working conditions, including health and safety at work and the relationship between employers and workers, including information, consultation and participation.\n\nThis Regulation should not affect the exercise of fundamental rights as recognised in the Member States and at Union level, including the right or freedom to strike or to take other action covered by the specific industrial relations systems in Member States, in accordance with national law and/or practice.\n\nNor should it affect concertation practices, the right to negotiate, to conclude and enforce collective agreement or to take collective action in accordance with national law and/or practice.\n\nIt should in any event not prevent the Commission from proposing specific legislation on the rights and freedoms of workers affected by AI systems.",
    "**Proposal for a regulation Recital 2 f (new)**\n\n- **Text proposed by the Commission**  \n(2f) This Regulation should help in supporting research and innovation and should not undermine research and development activity and respect freedom of scientific research.\n\nIt is therefore necessary to exclude from its scope AI systems specifically developed for the sole purpose of scientific research and development and to ensure that the Regulation does not otherwise affect scientific research and development activity on AI systems.\n\nUnder all circumstances, any research and development activity should be carried out in accordance with the Charter, Union law as well as the national law;",
    "**Proposal for a regulation Recital 3**\n\n- **Text proposed by the Commission**  \nArtificial intelligence is a fast evolving family of technologies that can contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities.\n\nBy improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes, for example in healthcare, farming, education and training, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, and climate change mitigation and adaptation.\n\n- **Amendment**  \nArtificial intelligence is a fast evolving family of technologies that can and already contributes to a wide array of economic, environmental and societal benefits across the entire spectrum of industries and social activities if developed in accordance with relevant general principles in line with the Charter and the values on which the Union is founded.\n\nBy improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes, for example in healthcare, farming, food safety, education and training, media, sports, culture, infrastructure management, energy, transport and logistics, crisis management, public services, security, justice, resource and energy efficiency, environmental monitoring, the conservation and restoration of biodiversity and ecosystems and climate change mitigation and adaptation.",
    "**Proposal for a regulation Recital 3 a (new)**\n\n- **Text proposed by the Commission**  \n(3a) To contribute to reaching the carbon neutrality targets, European companies should seek to utilise all available technological advancements that can assist in realising this goal.\n\nArtificial Intelligence is a technology that has the potential of being used to process the ever-growing amount of data created during industrial, environmental, health and other processes.\n\nTo facilitate investments in AI-based analysis and optimisation tools, this Regulation should provide a predictable and proportionate environment for low-risk industrial solutions.",
    "**Proposal for a regulation Recital 4**\n\n- **Text proposed by the Commission**  \nAt the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law.\n\nSuch harm might be material or immaterial.\n\n- **Amendment**  \nAt the same time, depending on the circumstances regarding its specific application and use, as well as the level of technological development, artificial intelligence may generate risks and cause harm to public or private interests and fundamental rights of natural persons that are protected by Union law.\n\nSuch harm might be material or immaterial, including physical, psychological, societal or economic harm.",
    "**Proposal for a regulation Recital 4 a (new)**\n\n- **Text proposed by the Commission**  \n(4a) Given the major impact that artificial intelligence can have on society and the need to build trust, it is vital for artificial intelligence and its regulatory framework to be developed according to Union values enshrined in Article 2 TEU, the fundamental rights and freedoms enshrined in the Treaties, the Charter, and international human rights law.\n\nAs a prerequisite, artificial intelligence should be a human-centric technology.\n\nIt should not substitute human autonomy or assume the loss of individual freedom and should primarily serve the needs of the society and the common good.\n\nSafeguards should be provided to ensure the development and use of ethically embedded artificial intelligence that respects Union values and the Charter.",
    "**Proposal for a regulation Recital 5**\n\n- **Text proposed by the Commission**  \nA Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law.\n\nTo achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services.\n\nBy laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council, and it ensures the protection of ethical principles, as specifically requested by the European Parliament.\n\n- **Amendment**  \nA Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety, protection of fundamental rights, democracy and rule of law and the environment, as recognised and protected by Union law.\n\nTo achieve that objective, rules regulating the placing on the market, the putting into service and the use of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services.\n\nThese rules should be clear and robust in protecting fundamental rights, supportive of new innovative solutions, and enabling to a European ecosystem of public and private actors creating AI systems in line with Union values.\n\nBy laying down those rules as well as measures in support of innovation with a particular focus on SMEs and start-ups, this Regulation supports the objective of promoting the AI made in Europe, of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council, and it ensures the protection of ethical principles, as specifically requested by the European Parliament.",
    "**Proposal for a regulation Recital 5 a (new)**\n\n- **Text proposed by the Commission**  \n(5a) Furthermore, in order to foster the development of AI systems in line with Union values, the Union needs to address the main gaps and barriers blocking the potential of the digital transformation including the shortage of digitally skilled workers, cybersecurity concerns, lack of investment and access to investment, and existing and potential gaps between large companies, SMEs and start-ups.\n\nSpecial attention should be paid to ensuring that the benefits of AI and innovation in new technologies are felt across all regions of the Union and that sufficient investment and resources are provided especially to those regions that may be lagging behind in some digital indicators.",
    "**Proposal for a regulation Recital 6**\n\n- **Text proposed by the Commission**  \nThe notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments.\n\nThe definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of human-defined objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension.\n\nAI systems can be designed to operate with varying levels of autonomy and be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded).\n\nThe definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to-date in the light of market and technological developments through the adoption of delegated acts by the Commission to amend that list.\n\n- **Amendment**  \nThe notion of AI system in this Regulation should be clearly defined and closely aligned with the work of international organisations working on artificial intelligence to ensure legal certainty, harmonization and wide acceptance, while providing the flexibility to accommodate the rapid technological developments in this field.\n\nMoreover, it should be based on key characteristics of artificial intelligence, such as its learning, reasoning or modelling capabilities, so as to distinguish it from simpler software systems or programming approaches.\n\nAI systems are designed to operate with varying levels of autonomy, meaning that they have at least some degree of independence of actions from human controls and of capabilities to operate without human intervention.\n\nThe term “machine-based” refers to the fact that AI systems run on machines.\n\nThe reference to explicit or implicit objectives underscores that AI systems can operate according to explicit human-defined objectives or to implicit objectives.\n\nThe objectives of the AI system may be different from the intended purpose of the AI system in a specific context.\n\nThe reference to predictions includes content, which is considered in this Regulation a form of prediction as one of the possible outputs produced by an AI system.\n\nFor the purposes of this Regulation, environments should be understood as the contexts in which the AI systems operate, whereas outputs generated by the AI system, meaning predictions, recommendations or decisions, respond to the objectives of the system, on the basis of inputs from said environment.\n\nSuch output further influences said environment, even by merely introducing new information to it.",
    "**Proposal for a regulation Recital 6 a (new)**\n\n- **Text proposed by the Commission**  \n(6a) AI systems often have machine learning capacities that allow them to adapt and perform new tasks autonomously.\n\nMachine learning refers to the computational process of optimizing the parameters of a model from data, which is a mathematical construct generating an output based on input data.\n\nMachine learning approaches include, for instance, supervised, unsupervised and reinforcement learning, using a variety of methods including deep learning with neural networks.\n\nThis Regulation is aimed at addressing new potential risks that may arise by delegating control to AI systems, in particular to those AI systems that can evolve after deployment.\n\nThe function and outputs of many of these AI systems are based on abstract mathematical relationships that are difficult for humans to understand, monitor and trace back to specific inputs.\n\nThese complex and opaque characteristics (black box element) impact accountability and explainability.\n\nComparably simpler techniques such as knowledge-based approaches, Bayesian estimation or decision-trees may also lead to legal gaps that need to be addressed by this Regulation, in particular when they are used in combination with machine learning approaches in hybrid systems.",
    "**Proposal for a regulation Recital 6 b (new)**\n\n- **Text proposed by the Commission**  \n(6b) AI systems can be used as standalone software systems, integrated into a physical product (embedded), used to serve the functionality of a physical product without being integrated therein (non-embedded) or used as an AI component of a larger system.\n\nIf this larger system would not function without the AI component in question, then the entire larger system should be considered as one single AI system under this Regulation.",
    "**Proposal for a regulation Recital 7**\n\n- **Text proposed by the Commission**  \nThe notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679 of the European Parliament and of the Council, Article 3(18) of Regulation (EU) 2018/1725 of the European Parliament and of the Council and Article 3(13) of Directive (EU) 2016/680 of the European Parliament and of the Council.\n\n- **Amendment**  \nThe notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679 of the European Parliament and of the Council.\n\nBiometrics-based data are additional data resulting from specific technical processing relating to physical, physiological or behavioural signals of a natural person, such as facial expressions, movements, pulse frequency, voice, key strikes or gait, which may or may not allow or confirm the unique identification of a natural person.\n\n35.\n\nRegulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1).",
    "**Proposal for a regulation Recital 7 a (new)**\n\n- **Text proposed by the Commission**  \n(7a) The notion of biometric identification as used in this Regulation should be defined as the automated recognition of physical, physiological, behavioural, and psychological human features such as the face, eye movement, facial expressions, body shape, voice, speech, gait, posture, heart rate, blood pressure, odour, keystrokes, psychological reactions (anger, distress, grief, etc.)\n\nfor the purpose of establishing an individual’s identity by comparing biometric data of that individual to stored biometric data of individuals in a database (one-to-many identification), irrespective of whether the individual has given its consent or not.",
    "**Proposal for a regulation Recital 7 b (new)**\n\n- **Text proposed by the Commission**  \n(7b) The notion of biometric categorisation as used in this Regulation should be defined as assigning natural persons to specific categories or inferring their characteristics and attributes such as gender, sex, age, hair colour, eye colour, tattoos, ethnic or social origin, health, mental or physical ability, behavioural or personality, traits language, religion, or membership of a national minority or sexual or political orientation on the basis of their biometric or biometric-based data, or which can be inferred from such data.",
    "**Proposal for a regulation Recital 8**\n\n- **Text proposed by the Commission**  \nThe notion of remote biometric identification system as used in this Regulation should be defined functionally, as an AI system intended for the identification of natural persons at a distance through the comparison of a person’s biometric data with the biometric data contained in a reference database, and without prior knowledge whether the targeted person will be present and can be identified, irrespective of the particular technology, processes or types of biometric data used.",
    "# Remote Biometric Identification Systems\n\nConsidering their different characteristics and manners in which they are used, as well as the different risks involved, a distinction should be made between ‘real-time’ and ‘post’ remote biometric identification systems.\n\nIn the case of ‘real-time’ systems, the capturing of the biometric data, the comparison and the identification occur all instantaneously, near-instantaneously or in any event without a significant delay.\n\nIn this regard, there should be no scope for circumventing the rules of this Regulation on the ‘real-time’ use of the AI systems in question by providing for minor delays.\n\n‘Real-time’ systems involve the use of ‘live’ or ‘near-‘live’ material, such as video footage, generated by a camera or other device with similar functionality.\n\nIn the case of ‘post’ systems, in contrast, the biometric data have already been captured and the comparison and identification occur only after a significant delay.\n\nThis involves material, such as pictures or video footage generated by closed circuit television cameras or private devices, which has been generated before the use of the system in respect of the natural persons concerned.\n\nThe notion of remote biometric identification system as used in this Regulation should be defined functionally, as an AI system intended for the identification of natural persons at a distance through the comparison of a person’s biometric data with the biometric data contained in a reference database, and without prior knowledge whether the targeted person will be present and can be identified, irrespective of the particular technology, processes or types of biometric data used, excluding verification systems which merely compare the biometric data of an individual to their previously provided biometric data (one-to-one).\n\nConsidering their different characteristics and manners in which they are used, as well as the different risks involved, a distinction should be made between ‘real-time’ and ‘post’ remote biometric identification systems.\n\nIn the case of ‘real-time’ systems, the capturing of the biometric data, the comparison and the identification occur all instantaneously, near-instantaneously or in any event without a significant delay.\n\nIn this regard, there should be no scope for circumventing the rules of this Regulation on the ‘real-time’ use of the AI systems in question by providing for minor delays.\n\n‘Real-time’ systems involve the use of ‘live’ or ‘near-‘live’ material, such as video footage, generated by a camera or other device with similar functionality.\n\nIn the case of ‘post’ systems, in contrast, the biometric data have already been captured and the comparison and identification occur only after a significant delay.\n\nThis involves material, such as pictures or video footage generated by closed circuit television cameras or private devices, which has been generated before the use of the system in respect of the natural persons concerned.\n\nGiven that the notion of biometric identification is independent from the individual’s consent, this definition applies even when warning notices are placed in the location that is under surveillance of the remote biometric identification system, and is not de facto annulled by pre-enrolment.",
    "**Proposal for a regulation Recital 8 a (new)**\n\n(8a) The identification of natural persons at a distance is understood to distinguish remote biometric identification systems from close proximity individual verification systems using biometric identification means, whose sole purpose is to confirm whether or not a specific natural person presenting themselves for identification is permitted, such as in order to gain access to a service, a device, or premises.",
    "**Proposal for a regulation Recital 9**\n\nFor the purposes of this Regulation the notion of publicly accessible space should be understood as referring to any physical place that is accessible to the public, irrespective of whether the place in question is privately or publicly owned.\n\nTherefore, the notion does not cover places that are private in nature and normally not freely accessible for third parties, including law enforcement authorities, unless those parties have been specifically invited or authorised, such as homes, private clubs, offices, warehouses and factories.\n\nOnline spaces are not covered either, as they are not physical spaces.\n\nHowever, the mere fact that certain conditions for accessing a particular space may apply, such as admission tickets or age restrictions, does not mean that the space is not publicly accessible within the meaning of this Regulation.\n\nConsequently, in addition to public spaces such as streets, relevant parts of government buildings and most transport infrastructure, spaces such as cinemas, theatres, shops and shopping centres are normally also publicly accessible.\n\nWhether a given space is accessible to the public should however be determined on a case-by-case basis, having regard to the specificities of the individual situation at hand.",
    "**Proposal for a regulation Recital 9 a (new)**\n\n(9a) It is important to note that AI systems should make best efforts to respect general principles establishing a high-level framework that promotes a coherent human-centric approach to ethical and trustworthy AI in line with the Charter of Fundamental Rights of the European Union and the values on which the Union is founded, including the protection of fundamental rights, human agency and oversight, technical robustness and safety, privacy and data governance, transparency, non-discrimination and fairness and societal and environmental well-being.",
    "**Proposal for a regulation Recital 9 b (new)**\n\n(9b) ‘AI literacy’ refers to skills, knowledge and understanding that allows providers, users and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause and thereby promote its democratic control.\n\nAI literacy should not be limited to learning about tools and technologies, but should also aim to equip providers and users with the notions and skills required to ensure compliance with and enforcement of this Regulation.\n\nIt is therefore necessary that the Commission, the Member States as well as providers and users of AI systems, in cooperation with all relevant stakeholders, promote the development of a sufficient level of AI literacy, in all sectors of society, for people of all ages, including women and girls, and that progress in that regard is closely followed.",
    "**Proposal for a regulation Recital 10**\n\nIn order to ensure a level playing field and an effective protection of rights and freedoms of individuals across the Union, the rules established by this Regulation should apply to providers of AI systems in a non-discriminatory manner, irrespective of whether they are established within the Union or in a third country, and to users of AI systems established within the Union.\n\nIn order for the Union to be true to its fundamental values, AI systems intended to be used for practices that are considered unacceptable by this Regulation, should equally be deemed to be unacceptable outside the Union because of their particularly harmful effect to fundamental rights as enshrined in the Charter.\n\nTherefore it is appropriate to prohibit the export of such AI systems to third countries by providers residing in the Union.",
    "**Proposal for a regulation Recital 11**\n\nIn light of their digital nature, certain AI systems should fall within the scope of this Regulation even when they are neither placed on the market, nor put into service, nor used in the Union.\n\nThis is the case for example of an operator established in the Union that contracts certain services to an operator established outside the Union in relation to an activity to be performed by an AI system that would qualify as high-risk and whose effects impact natural persons located in the Union.\n\nIn those circumstances, the AI system used by the operator outside the Union could process data lawfully collected in and transferred from the Union, and provide to the contracting operator in the Union the output of that AI system resulting from that processing, without that AI system being placed on the market, put into service or used in the Union.\n\nTo prevent the circumvention of this Regulation and to ensure an effective protection of natural persons located in the Union, this Regulation should also apply to providers and users of AI systems that are established in a third country, to the extent the output produced by those systems is intended to be used in the Union.\n\nNonetheless, to take into account existing arrangements and special needs for cooperation with foreign partners with whom information and evidence is exchanged, this Regulation should not apply to public authorities of a third country and international organisations when acting in the framework of international agreements concluded at national or European level for law enforcement and judicial cooperation with the Union or with its Member States.\n\nSuch agreements have been concluded bilaterally between Member States and third countries or between the European Union, Europol and other EU agencies and third countries and international organisations.\n\nThis exception should nevertheless be limited to trusted countries and international organisation that share Union values.",
    "**Proposal for a regulation Recital 12**\n\nThis Regulation should also apply to Union institutions, offices, bodies and agencies when acting as a provider or deployer of an AI system.\n\nAI systems exclusively developed or used for military purposes should be excluded from the scope of this Regulation where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the Treaty on the European Union (TEU).\n\nThis Regulation should be without prejudice to the provisions regarding the liability of intermediary service providers set out in Directive 2000/31/EC of the European Parliament and of the Council [as amended by the Digital Services Act].",
    "**Proposal for a regulation Recital 12 a (new)**\n\n(12a) Software and data that are openly shared and where users can freely access, use, modify and redistribute them or modified versions thereof, can contribute to research and innovation in the market.\n\nResearch by the Commission also shows that free and open-source software can contribute between EUR 65 billion to EUR 95 billion to the European Union’s GDP and that it can provide significant growth opportunities for the European economy.\n\nUsers are allowed to run, copy, distribute, study, change and improve software and data, including models by way of free and open-source licences.\n\nTo foster the development and deployment of AI, especially by SMEs, start-ups, academic research but also by individuals, this Regulation should not apply to such free and open-source AI components except to the extent that they are placed on the market or put into service by a provider as part of a high-risk AI system or of an AI system that falls under Title II or IV of this Regulation.",
    "**Proposal for a regulation Recital 12 b (new)**\n\n(12b) Neither the collaborative development of free and open-source AI components nor making them available on open repositories should constitute a placing on the market or putting into service.\n\nA commercial activity, within the understanding of making available on the market, might however be characterised by charging a price, with the exception of transactions between micro enterprises, for a free and open-source AI component but also by charging a price for technical support services, by providing a software platform through which the provider monetises other services, or by the use of personal data for reasons other than exclusively for improving the security, compatibility or interoperability of the software.",
    "**Proposal for a regulation Recital 12 c (new)**\n\n(12c) The developers of free and open-source AI components should not be mandated under this Regulation to comply with requirements targeting the AI value chain and, in particular, not towards the provider that has used that free and open-source AI component.\n\nDevelopers of free and open-source AI components should however be encouraged to implement widely adopted documentation practices, such as model and data cards, as a way to accelerate information sharing along the AI value chain, allowing the promotion of trustworthy AI systems in the Union.",
    "**Proposal for a regulation Recital 13**\n\nIn order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common normative standards for all high-risk AI systems should be established.\n\nThose standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union’s international trade commitments.",
    "**Proposal for a regulation Recital 14**\n\nIn order to introduce a proportionate and effective set of binding rules for AI systems, a clearly defined risk-based approach should be followed.\n\nThat approach should tailor the type and content of such rules to the intensity and scope of the risks that AI systems can generate.\n\nIt is therefore necessary to prohibit certain unacceptable artificial intelligence practices, to lay down requirements for high-risk AI systems and obligations for the relevant operators, and to lay down transparency obligations for certain AI systems.",
    "**Proposal for a regulation Recital 15**\n\nAside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices.\n\nSuch practices are particularly harmful and abusive and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy and the rights of the child.",
    "**Proposal for a regulation Recital 16**\n\nThe placing on the market, putting into service or use of certain AI systems with the objective to or the effect of materially distorting human behaviour, whereby physical or psychological harms are likely to occur, should be forbidden.\n\nThis limitation should be understood to include neuro-technologies assisted by AI systems that are used to monitor, use, or influence neural data gathered through brain-computer interfaces insofar as they are materially distorting the behaviour of a natural person in a manner that causes or is likely to cause that person or another person significant harm.\n\nSuch AI systems deploy subliminal components individuals cannot perceive or exploit vulnerabilities of individuals and specific groups of persons due to their known or predicted personality traits, age, physical or mental incapacities, social or economic situation.\n\nThey do so with the intention to or the effect of materially distorting the behaviour of a person and in a manner that causes or is likely to cause significant harm to that or another person or groups of persons, including harms that may be accumulated over time.\n\nThe intention to distort the behaviour may not be presumed if the distortion results from factors external to the AI system which are outside of the control of the provider or the user, such as factors that may not be reasonably foreseen and mitigated by the provider or the deployer of the AI system.\n\nIn any case, it is not necessary for the provider or the deployer to have the intention to cause the significant harm, as long as such harm results from the manipulative or exploitative AI-enabled practices.\n\nThe prohibitions for such AI practices is complementary to the provisions contained in Directive 2005/29/EC, according to which unfair commercial practices are prohibited, irrespective of whether they carried out having recourse to AI systems or otherwise.\n\nIn such setting, lawful commercial practices, for example in the field of advertising, that are in compliance with Union law should not in themselves be regarded as violating prohibition.\n\nResearch for legitimate purposes in relation to such AI systems should not be stifled by the prohibition, if such research does not amount to use of the AI system in human-machine relations that exposes natural persons to harm and such research is carried out in accordance with recognised ethical standards for scientific research and on the basis of specific informed consent of the individuals that are exposed to them or, where applicable, of their legal guardian.",
    "**Proposal for a regulation Recital 16 a (new)**\n\n(16a) AI systems that categorise natural persons by assigning them to specific categories, according to known or inferred sensitive or protected characteristics are particularly intrusive, violate human dignity and hold great risk of discrimination.\n\nSuch characteristics include gender, gender identity, race, ethnic origin, migration or citizenship status, political orientation, sexual orientation, religion, disability or any other grounds on which discrimination is prohibited under Article 21 of the Charter of Fundamental Rights of the European Union, as well as under Article 9 of Regulation (EU)2016/769.\n\nSuch systems should therefore be prohibited.",
    "**Proposal for a regulation Recital 17**\n\nAI systems providing social scoring of natural persons for general purpose may lead to discriminatory outcomes and the exclusion of certain groups.\n\nThey violate the right to dignity and non-discrimination and the values of equality and justice.\n\nSuch AI systems evaluate or classify natural persons or groups based on multiple data points and time occurrences related to their social behaviour in multiple contexts or known, inferred or predicted personal or personality characteristics.\n\nThe social score obtained from such AI systems may lead to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour.\n\nSuch AI systems should be therefore prohibited.",
    "**Proposal for a regulation Recital 18**\n\nThe use of AI systems for ‘real-time’ remote biometric identification of natural persons in publicly accessible spaces is particularly intrusive to the rights and freedoms of the concerned persons, and can ultimately affect the private life of a large part of the population, evoke a feeling of constant surveillance and indirectly dissuade the exercise of the rights to freedom of movement, assembly, and expression.",
    "# Exercise of Freedom and Fundamental Rights\n\nThe exercise of the freedom of assembly and other fundamental rights is impacted by the use of AI systems in 'real-time,' as these systems carry heightened risks for the rights and freedoms of persons concerned by law enforcement activities.\n\nThis constant surveillance gives a position of uncontrollable power to parties deploying biometric identification in publicly accessible spaces.\n\nIt indirectly dissuades the exercise of fundamental rights central to the Rule of Law.\n\nTechnical inaccuracies in AI systems intended for remote biometric identification can lead to biased results and discriminatory effects.\n\nThis is significant concerning age, ethnicity, sex, or disabilities.\n\nThe use of these systems in publicly accessible places should be prohibited.\n\nAI systems used for analyzing recorded footage through ‘post’ remote biometric identification should also be prohibited unless pre-judicial authorization is obtained, connecting it to a specific serious crime and subject to stringent authorization.",
    "### Proposal for Regulation Recital 19\n\nThe use of those systems for law enforcement should be prohibited, except in specific situations where it is strictly necessary for substantial public interest outweighing the risks.\n\nThese situations involve searching for potential crime victims, specific threats to life or safety, and persecution of perpetrators of serious criminal offenses provided they are punishable by at least three years of detention.",
    "### Proposal for Regulation Recital 21\n\nEach use of a real-time remote biometric identification system in publicly accessible spaces for law enforcement should be authorized by a judicial or administrative authority.\n\nExceptions are allowed for urgent situations where pre-authorization is impossible, subject to strict conditions and authorizations expeditiously sought post-use.",
    "### Text Proposed by the Commission\n\nAs regards AI systems that are safety components of products, or which are themselves products, falling within the scope of certain Union harmonisation legislation, it is appropriate to classify them as high-risk under this Regulation if the product in question undergoes the conformity assessment procedure with a third-party conformity assessment body pursuant to that relevant Union harmonisation legislation.\n\nIn particular, such products are machinery, toys, lifts, equipment and protective systems intended for use in potentially explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.",
    "### Amendment\n\nAs regards AI systems that are safety components of products, or which are themselves products, falling within the scope of certain Union harmonisation law listed in Annex II, it is appropriate to classify them as high-risk under this Regulation if the product in question undergoes the conformity assessment procedure in order to ensure compliance with essential safety requirements with a third-party conformity assessment body pursuant to that relevant Union harmonisation law.\n\nIn particular, such products are machinery, toys, lifts, equipment and protective systems intended for use in potentially explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.\n\n---",
    "### Text Proposed by the Commission\n\nThe classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered ‘high-risk’ under the criteria established in the relevant Union harmonisation legislation that applies to the product.\n\nThis is notably the case for Regulation (EU) 2017/745 of the European Parliament and of the Council47 and Regulation (EU) 2017/746 of the European Parliament and of the Council48, where a third-party conformity assessment is provided for medium-risk and high-risk products.",
    "### Amendment\n\nThe classification of an AI system as high-risk pursuant to this Regulation should not mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered ‘high-risk’ under the criteria established in the relevant Union harmonisation law that applies to the product.\n\nThis is notably the case for Regulation (EU) 2017/745 of the European Parliament and of the Council47 and Regulation (EU) 2017/746 of the European Parliament and of the Council48, where a third-party conformity assessment is provided for medium-risk and high-risk products.\n\n---",
    "### Text Proposed by the Commission\n\nAs regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in the Regulation.\n\nThe identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems.",
    "### Amendment\n\nAs regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products and that are listed in one of the areas and use cases in Annex III, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a significant risk of harm to the health and safety or the fundamental rights of persons and, where the AI system is used as a safety component of a critical infrastructure, to the environment.\n\nSuch significant risk of harm should be identified by assessing on the one hand the effect of such risk with respect to its level of severity, intensity, probability of occurrence and duration combined altogether and on the other hand whether the risk can affect an individual, a plurality of persons or a particular group of persons.\n\nSuch combination could for instance result in a high severity but low probability to affect a natural person, or a high probability to affect a group of persons with a low intensity over a long period of time, depending on the context.\n\nThe identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems.\n\n---",
    "### Text Proposed by the Commission\n\n(32a) Providers whose AI systems fall under one of the areas and use cases listed in Annex III that consider their system does not pose a significant risk of harm to the health, safety, fundamental rights or the environment should inform the national supervisory authorities by submitting a reasoned notification.\n\nThis could take the form of a one-page summary of the relevant information on the AI system in question, including its intended purpose and why it would not pose a significant risk of harm to the health, safety, fundamental rights or the environment.\n\nThe Commission should specify criteria to enable companies to assess whether their system would pose such risks, as well as develop an easy to use and standardised template for the notification.\n\nProviders should submit the notification as early as possible and in any case prior to the placing of the AI system on the market or its putting into service, ideally at the development stage, and they should be free to place it on the market at any given time after the notification.\n\nHowever, if the authority estimates the AI system in question was misclassified, it should object to the notification within a period of three months.\n\nThe objection should be substantiated and duly explain why the AI system has been misclassified.\n\nThe provider should retain the right to appeal by providing further arguments.\n\nIf after the three months there has been no objection to the notification, national supervisory authorities could still intervene if the AI system presents a risk at national level, as for any other AI system on the market.\n\nNational supervisory authorities should submit annual reports to the AI Office detailing the notifications received and the decisions taken.\n\n---",
    "### Text Proposed by the Commission\n\nTechnical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects.\n\nThis is particularly relevant when it comes to age, ethnicity, sex or disabilities.\n\nTherefore, ‘real-time’ and ‘post’ remote biometric identification systems should be classified as high-risk.\n\nIn view of the risks that they pose, both types of remote biometric identification systems should be subject to specific requirements on logging capabilities and human oversight.",
    "### Text Proposed by the Commission\n\n(33a) As biometric data constitute a special category of sensitive personal data in accordance with Regulation 2016/679, it is appropriate to classify as high-risk several critical use-cases of biometric and biometrics-based systems.\n\nAI systems intended to be used for biometric identification of natural persons and AI systems intended to be used to make inferences about personal characteristics of natural persons on the basis of biometric or biometrics-based data, including emotion recognition systems, with the exception of those which are prohibited under this Regulation should therefore be classified as high-risk.\n\nThis should not include AI systems intended to be used for biometric verification, which includes authentication, whose sole purpose is to confirm that a specific natural person is the person he or she claims to be and to confirm the identity of a natural person for the sole purpose of having access to a service, a device or premises (one-to-one verification).\n\nBiometric and biometrics-based systems which are provided for under Union law to enable cybersecurity and personal data protection measures should not be considered as posing a significant risk of harm to the health, safety and fundamental rights.\n\n---",
    "### Text Proposed by the Commission\n\nAs regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities.",
    "### Amendment\n\nAs regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of the supply of water, gas, heating electricity and critical digital infrastructure, since their failure or malfunctioning may infringe the security and integrity of such critical infrastructure or put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities.\n\nSafety components of critical infrastructure, including critical digital infrastructure, are systems used to directly protect the physical integrity of critical infrastructure or health and safety of persons and property.\n\nFailure or malfunctioning of such components might directly lead to risks to the physical integrity of critical infrastructure and thus to risks to the health and safety of persons and property.\n\nComponents intended to be used solely for cybersecurity purposes should not qualify as safety components.\n\nExamples of such safety components may include systems for monitoring water pressure or fire alarm controlling systems in cloud computing centres.\n\n---",
    "### Text Proposed by the Commission\n\nAI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education should be considered high-risk, since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood.\n\nWhen improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination.",
    "### Amendment\n\nDeployment of AI systems in education is important in order to help modernise entire education systems, to increase educational quality, both offline and online and to accelerate digital education, thus also making it available to a broader audience.\n\nAI systems used in education or vocational training, notably for determining access or materially influence decisions on admission or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education or to assess the appropriate level of education for an individual and materially influence the level of education and training that individuals will receive or be able to access or to monitor and detect prohibited behavior of students during tests should be classified as high-risk AI systems, since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood.\n\nWhen improperly designed and used, such systems can be particularly intrusive and may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation.\n\n---",
    "### Text Proposed by the Commission\n\nAI systems used in employment, workers management and access to self-employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk, since those systems may appreciably impact future career prospects and livelihoods of these persons.\n\nRelevant work-related contractual relationships should involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021.\n\nSuch persons should in principle not be considered users within the meaning of this Regulation.\n\nThroughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation.\n\nAI systems used to monitor the performance and behavior of these persons may also impact their rights to data protection and privacy.",
    "### Amendment\n\nAI systems used in employment, workers management and access to self-employment, notably for the recruitment and selection of persons, for making decisions or materially influence decisions on initiation, promotion and termination and for personalized task allocation based on individual behavior, personal traits or biometric data, monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk, since those systems may appreciably impact future career prospects, livelihoods of these persons and workers’ rights.\n\nRelevant work-related contractual relationships should meaningfully involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021.\n\nThroughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation.\n\nAI systems used to monitor the performance and behavior of these persons may also undermine the essence of their fundamental rights to data protection and privacy.\n\nThis Regulation applies without prejudice to Union and Member State competences to provide for more specific rules for the use of AI systems in the employment context.\n\n---",
    "### Text Proposed by the Commission\n\nAnother area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one’s standard of living.\n\nIn particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services.\n\nAI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, disabilities, age, sexual orientation, or create new forms of discriminatory impacts.\n\nConsidering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small-scale providers for their own use.\n\nNatural persons applying for or receiving public assistance benefits and services from public authorities are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities.\n\nIf AI systems are used for determining whether such benefits and services should be denied, reduced, revoked or reclaimed by authorities, they may have a significant impact on persons’ livelihood and may infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignity or an effective remedy.\n\nThose systems should therefore be classified as high-risk.",
    "### Amendment\n\nAnother area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services, including healthcare services, and essential services, including but not limited to housing, electricity, heating/cooling and internet, and benefits necessary for people to fully participate in society or to improve one’s standard of living.\n\nIn particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services.\n\nAI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, gender, disabilities, age, sexual orientation, or create new forms of discriminatory impacts.\n\nHowever, AI systems provided for by Union law for the purpose of detecting fraud in the offering of financial services should not be considered as high-risk under this Regulation.\n\nNatural persons applying for or receiving public assistance benefits and services from public authorities, including healthcare services and essential services, including but not limited to housing, electricity, heating/cooling and internet, are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities.\n\nIf AI systems are used for determining whether such benefits and services should be denied, reduced, revoked or reclaimed by authorities, they may have a significant impact on persons’ livelihood and may infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignity or an effective remedy.\n\nSimilarly, AI systems intended to be used to make decisions or materially influence decisions on the eligibility of natural persons for health and life insurance may also have a significant impact on persons’ livelihood and may infringe their fundamental rights such as by limiting access to healthcare or by perpetuating discrimination based on personal characteristics.\n\nThose systems should therefore be classified as high-risk.\n\nNonetheless, this Regulation should not hamper the development and use of innovative approaches in the public administration, which would stand to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal and natural persons.\n\nFinally, AI systems used to evaluate and classify emergency calls by natural persons or to dispatch or establish priority in the dispatching of emergency first response services should also be classified as high-risk since they make decisions in very critical situations for the life and health of persons and their property.\n\n---",
    "### Text Proposed by the Commission\n\n(37a) Given the role and responsibility of police and judicial authorities, and the impact of decisions they take for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, some specific use-cases of AI applications in law enforcement has to be classified as high-risk, in particular in instances where there is the potential to significantly affect the lives or the fundamental rights of individuals.\n\n---",
    "### Text Proposed by the Commission\n\nActions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter.\n\nIn particular, if the AI system is not trained with high quality data, does not meet adequate requirements in terms of its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner.\n\nFurthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented.\n\nIt is therefore appropriate to classify as high-risk a number of AI systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress.\n\nIn view of the nature of the activities in question and the risks relating thereto, those high-risk AI systems should include in particular AI systems intended to be used by law enforcement authorities for individual risk assessments, polygraphs and similar tools or to detect the emotional state of a natural person, to detect ‘deep fakes’, for the evaluation of the reliability of evidence in criminal proceedings, for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of natural persons, or assessing personality traits and characteristics or past criminal behavior of natural persons.",
    "### Amendment\n\nActions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter.\n\nIn particular, if the AI system is not trained with high quality data, does not meet adequate requirements in terms of its performance, its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner.\n\nFurthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented.\n\nIt is therefore appropriate to classify as high-risk a number of AI systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress.\n\nIn view of the nature of the activities in question and the risks relating thereto, those high-risk AI systems",
    "# High-Risk AI Systems in Law Enforcement and Administrative Proceedings\n\nAI systems used by law enforcement authorities include tools such as polygraphs and similar devices, which are used for evaluating evidence reliability in criminal proceedings and for profiling in crime detection and investigation.\n\nHowever, AI systems used in administrative proceedings by tax and customs authorities are not classified as high-risk when used by law enforcement.\n\nThe use of AI tools by law enforcement and judicial authorities should not contribute to inequality or exclusion.\n\nCare must be taken to ensure AI tools do not infringe on defendants' rights, particularly concerning the difficulty of assessing the functioning of these systems and challenging their output in court.",
    "# Amendment 70: AI Systems in Migration and Border Control\n\nAI systems utilized in migration, asylum, and border control are critical as they impact vulnerable individuals.\n\nThese systems must ensure accuracy, non-discrimination, and transparency to respect fundamental rights such as free movement and data protection.\n\nAI systems used for assessing risks posed by individuals entering the EU, or verifying document authenticity, are considered high-risk.\n\nAI systems should not be used to bypass obligations under the Refugee Convention or to infringe on non-refoulement principles.",
    "# Amendment 71: AI Systems in Justice and Democratic Processes\n\nAI systems employed in the administration of justice can significantly impact democracy and individual freedoms.\n\nThese systems should not replace judicial independence, with decision-making remaining a human task.\n\nAI systems assisting judicial authorities or used in alternative dispute resolution are classified as high-risk, excluding ancillary activities such as document anonymization.",
    "# Amendment 74: Legal Compliance and High-Risk AI\n\nClassification as a high-risk AI system under this regulation does not imply legality under other EU laws pertaining to data protection and privacy.\n\nCompliance with applicable requirements from the Charter and relevant laws is essential.\n\nThis regulation is not a legal ground for processing personal data, especially sensitive data categories.",
    "# Amendment 77: Quality of Data for High-Risk AI\n\nHigh-quality data is crucial for AI system performance.\n\nAppropriate data governance ensures AI meets intended purposes without discrimination.\n\nDatasets must be relevant, representative, and unbiased to prevent discrimination, especially against vulnerable groups.\n\nConditions must adhere to relevant data protection regulations for processing sensitive data categories.",
    "# Functioning of the Internal Market\n\nIt may be necessary to establish recommendations, guidelines, and targets for sustainability within the internal market.\n\nThe Commission is entitled to develop a methodology to contribute towards Key Performance Indicators (KPIs) and reference for the Sustainable Development Goals (SDGs).\n\nThe goal should be to enable fair comparison between AI implementation choices, providing incentives to promote more efficient AI technologies addressing energy and resource concerns.\n\nThis regulation should establish a baseline collection of data reported on emissions from development, training, and deployment.",
    "## Proposal for Regulation Recital 49\n\n- Proposed: High-risk AI systems should maintain consistent performance throughout their lifecycle, meeting appropriate accuracy, robustness, and cybersecurity levels.\n\nPerformance metrics' expected levels should be communicated clearly, mitigating risks and negative impacts.\n\nThe European Artificial Intelligence Office should coordinate on benchmarking with standardization organizations.",
    "## Directives and Regulations\n\nAmending Council Directives 89/686/EEC and 93/15/EEC and Directives 94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and 2009/105/EC of the European Parliament and of the Council and repealing Council Decision 87/95/EEC and Decision No.\n\nRegulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending Council Directives 89/686/EEC and 93/15/EEC and Directives 94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and 2009/105/EC of the European Parliament and of the Council and repealing Council Decision 87/95/EEC and Decision No.\n\n1673/2006/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12).",
    "### Recital 61 a (new)\n\nText proposed by the Commission - Amendment\n\n(61a) In order to facilitate compliance, the first standardisation requests should be issued by the Commission two months after the entry into force of this Regulation at the latest.\n\nThis should serve to improve legal certainty, thereby promoting investment and innovation in AI, as well as competitiveness and growth of the Union market, while enhancing multistakeholder governance representing all relevant European stakeholders such as the AI Office, European standardisation organisations and bodies or experts groups established under relevant sectorial Union law as well as industry, SMEs, start-ups, civil society, researchers and social partners, and should ultimately facilitate global cooperation on standardisation in the field of AI in a manner consistent with Union values.\n\nWhen preparing the standardisation request, the Commission should consult the AI Office and the AI advisory Forum in order to collect relevant expertise.",
    "### Recital 61 c (new)\n\nText proposed by the Commission - Amendment\n\n(61c) The Commission should be able to adopt common specifications under certain conditions, when no relevant harmonised standard exists or to address specific fundamental rights concerns.\n\nThrough the whole drafting process, the Commission should regularly consult the AI Office and its advisory forum, the European standardisation organisations and bodies or expert groups established under relevant sectorial Union law as well as relevant stakeholders, such as industry, SMEs, start-ups, civil society, researchers and social partners.",
    "### Recital 61 d (new)\n\nText proposed by the Commission - Amendment\n\n(61d) When adopting common specifications, the Commission should strive for regulatory alignment of AI with likeminded global partners, which is key to fostering innovation and cross-border partnerships within the field of AI, as coordination with likeminded partners in international standardisation bodies is of great importance.",
    "### Recital 62\n\nText proposed by the Commission - Amendment\n\nIn order to ensure a high level of trustworthiness of high-risk AI systems, those systems should be subject to a conformity assessment prior to their placing on the market or putting into service.\n\nTo increase the trust in the value chain and to give certainty to businesses about the performance of their systems, third-parties that supply AI components may voluntarily apply for a third-party conformity assessment.",
    "### Recital 64\n\nText proposed by the Commission - Amendment\n\n(64) Given the more extensive experience of professional pre-market certifiers in the field of product safety and the different nature of risks involved, it is appropriate to limit, at least in an initial phase of application of this Regulation, the scope of application of third-party conformity assessment for high-risk AI systems other than those related to products.\n\nTherefore, the conformity assessment of such systems should be carried out as a general rule by the provider under its own responsibility, with the only exception of AI systems intended to be used for the remote biometric identification of persons, for which the involvement of a notified body in the conformity assessment should be foreseen, to the extent they are not prohibited.\n\nGiven the complexity of high-risk AI systems and the risks that are associated to them, it is essential to develop a more adequate capacity for the application of third party conformity assessment for high-risk AI systems.\n\nHowever, given the current experience of professional pre-market certifiers in the field of product safety and the different nature of risks involved, it is appropriate to limit, at least in an initial phase of application of this Regulation, the scope of application of third-party conformity assessment for high-risk AI systems other than those related to products.\n\nTherefore, the conformity assessment of such systems should be carried out as a general rule by the provider under its own responsibility, with the only exception of AI systems intended to be used for the remote biometric identification of persons, or AI systems intended to be used to make inferences about personal characteristics of natural persons on the basis of biometric or biometrics-based data, including emotion recognition systems, for which the involvement of a notified body in the conformity assessment should be foreseen, to the extent they are not prohibited.",
    "### Recital 65\n\nText proposed by the Commission - Amendment\n\nIn order to carry out third-party conformity assessment for AI systems intended to be used for the remote biometric identification of persons, notified bodies should be designated under this Regulation by the national competent authorities, provided they are compliant with a set of requirements, notably on independence, competence and absence of conflicts of interests.\n\nIn order to carry out third-party conformity assessments when so required, notified bodies should be designated under this Regulation by the national competent authorities, provided they are compliant with a set of requirements, notably on independence, competence, absence of conflicts of interests and minimum cybersecurity requirements.\n\nMember States should encourage the designation of a sufficient number of conformity assessment bodies, in order to make the certification feasible in a timely manner.\n\nThe procedures of assessment, designation, notification and monitoring of conformity assessment bodies should be implemented as uniformly as possible in Member States, with a view to removing administrative border barriers and ensuring that the potential of the internal market is realised.",
    "### Recital 65 a (new)\n\nText proposed by the Commission - Amendment\n\n(65a) In line with Union commitments under the World Trade Organization Agreement on Technical Barriers to Trade, it is adequate to maximise the acceptance of test results produced by competent conformity assessment bodies, independent of the territory in which they are established, where necessary to demonstrate conformity with the applicable requirements of the Regulation.\n\nThe Commission should actively explore possible international instruments for that purpose and in particular pursue the possible establishment of mutual recognition agreements with countries which are on a comparable level of technical development, and have compatible approach concerning AI and conformity assessment.",
    "### Recital 66\n\nText proposed by the Commission - Amendment\n\nIn line with the commonly established notion of substantial modification for products regulated by Union harmonisation legislation, it is appropriate that an AI system undergoes a new conformity assessment whenever a change occurs which may affect the compliance of the system with this Regulation or when the intended purpose of the system changes.\n\nIn addition, as regards AI systems which continue to ‘learn’ after being placed on the market or put into service (i.e.\n\nthey automatically adapt how functions are carried out), it is necessary to provide rules establishing that changes to the algorithm and its performance that have been pre-determined by the provider and assessed at the moment of the conformity assessment should not constitute a substantial modification.\n\nIn line with the commonly established notion of substantial modification for products regulated by Union harmonisation legislation, it is appropriate that a high-risk AI system undergoes a new conformity assessment whenever an unplanned change occurs which goes beyond controlled or predetermined changes by the provider including continuous learning and which may create a new unacceptable risk and significantly affect the compliance of the high-risk AI system with this Regulation or when the intended purpose of the system changes.\n\nIn addition, as regards AI systems which continue to ‘learn’ after being placed on the market or put into service (i.e.\n\nthey automatically adapt how functions are carried out), it is necessary to provide rules establishing that changes to the algorithm and its performance that have been pre-determined by the provider and assessed at the moment of the conformity assessment should not constitute a substantial modification.\n\nThe same should apply to updates of the AI system for security reasons in general and to protect against evolving threats of manipulation of the system, provided that they do not amount to a substantial modification.",
    "### Recital 67\n\nText proposed by the Commission - Amendment\n\nHigh-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely within the internal market.\n\nMember States should not create unjustified obstacles to the placing on the market or putting into service of high-risk AI systems that comply with the requirements laid down in this Regulation and bear the CE marking.\n\nHigh-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely within the internal market.\n\nFor physical high-risk AI systems, a physical CE marking should be affixed, and may be complemented by a digital CE marking.\n\nFor digital-only high-risk AI systems, a digital CE marking should be used.\n\nMember States should not create unjustified obstacles to the placing on the market or putting into service of high-risk AI systems that comply with the requirements laid down in this Regulation and bear the CE marking.",
    "### Recital 68\n\nText proposed by the Commission - Amendment\n\nUnder certain conditions, rapid availability of innovative technologies may be crucial for health and safety of persons and for society as a whole.\n\nIt is thus appropriate that under exceptional reasons of public security or protection of life and health of natural persons and the protection of industrial and commercial property, Member States could authorise the placing on the market or putting into service of AI systems which have not undergone a conformity assessment.\n\nUnder certain conditions, rapid availability of innovative technologies may be crucial for health and safety of persons, the environment and climate change, and for society as a whole.\n\nIt is thus appropriate that under exceptional reasons of protection of life and health of natural persons, environmental protection, and the protection of critical infrastructure, Member States could authorise the placing on the market or putting into service of AI systems which have not undergone a conformity assessment.",
    "### Recital 69\n\nText proposed by the Commission - Amendment\n\nIn order to facilitate the work of the Commission and the Member States in the artificial intelligence field as well as to increase the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation, should be required to register their high-risk AI system in a EU database, to be established and managed by the Commission.\n\nThe Commission should be the controller of that database, in accordance with Regulation (EU) 2018/1725 of the European Parliament and of the Council.\n\nIn order to ensure the full functionality of the database, when deployed, the procedure for setting the database should include the elaboration of functional specifications by the Commission and an independent audit report.\n\nIn order to facilitate the work of the Commission and the Member States in the artificial intelligence field as well as to increase the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation should be required to register their high-risk AI system and foundation models in a EU database, to be established and managed by the Commission.\n\nThis database should be freely and publicly accessible, easily understandable, and machine-readable.\n\nThe database should also be user-friendly and easily navigable, with search functionalities at minimum allowing the general public to search the database for specific high-risk systems, locations, categories of risk under Annex IV and keywords.\n\nDeployers who are public authorities or Union institutions, bodies, offices and agencies or deployers acting on their behalf and deployers who are undertakings designated as a gatekeeper under Regulation (EU)2022/1925 should also register in the EU database before putting into service or using a high-risk AI system for the first time and following each substantial modification.\n\nOther deployers should be entitled to do so voluntarily.\n\nAny substantial modification of high-risk AI systems shall also be registered in the EU database.\n\nThe Commission should be the controller of that database, in accordance with Regulation (EU) 2018/1725 of the European Parliament and of the Council.\n\nIn order to ensure the full functionality of the database, when deployed, the procedure for setting the database should include the elaboration of functional specifications by the Commission and an independent audit report.\n\nThe Commission should take into account cybersecurity and hazard-related risks when carrying out its tasks as data controller on the EU database.\n\nIn order to maximise the availability and use of the database by the public, the database, including the information made available through it, should comply with requirements under the Directive 2019/882.",
    "### Recital 71\n\nText proposed by the Commission - Amendment\n\nArtificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures.\n\nTo ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service.\n\nArtificial intelligence is a rapidly developing family of technologies that requires regulatory oversight and a safe and controlled space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures.\n\nTo ensure a legal framework that promotes innovation, is future-proof, and resilient to disruption, Member States should establish at least one artificial intelligence regulatory sandbox to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service.\n\nIt is indeed desirable for the establishment of regulatory sandboxes, whose establishment is currently left at the discretion of Member States, as a next step to be made mandatory with established criteria.\n\nThat mandatory sandbox could also be established jointly with one or several other Member States, as long as that sandbox would cover the respective national level of the involved Member States.\n\nAdditional sandboxes may also be established at different levels, including cross Member States, in order to facilitate cross-border cooperation and synergies.\n\nWith the exception of the mandatory sandbox at the national level, Member States should also be able to establish virtual or hybrid sandboxes.\n\nAll regulatory sandboxes should be able to accommodate both physical and virtual products.\n\nEstablishing authorities should also ensure that the regulatory sandboxes have the adequate financial and human resources for their functioning.",
    "### Recital 72\n\nText proposed by the Commission - Amendment\n\nThe objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation; to enhance legal certainty for innovators and the competent authorities’ oversight and understanding of the opportunities, emerging risks and the impacts of AI use, and to accelerate access to markets, including by removing barriers for small and medium enterprises (SMEs) and start-ups.\n\nTo ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes.\n\nThis Regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation (EU) 2018/1725, and without prejudice to Article 4(2) of Directive (EU) 2016/680.\n\nParticipants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to mitigate any high-risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox.\n\nThe conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under Article 83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680.\n\nThe objectives of the regulatory sandboxes should be: for the establishing authorities to increase their understanding of technical developments, improve supervisory methods and provide guidance to AI systems developers and providers to achieve regulatory compliance with this Regulation or where relevant, other applicable Union and Member States legislation, as well as with the Charter of Fundamental Rights; for the prospective providers to allow and facilitate the testing and development of innovative solutions related to AI systems in the pre-marketing phase to enhance legal certainty, to allow for more regulatory learning by establishing authorities in a controlled environment to develop better guidance and to identify possible future improvements of the legal framework through the ordinary legislative procedure.\n\nAny significant risks identified during the development and testing of such AI systems should result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place.\n\nTo ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes.\n\nMember States should ensure that regulatory sandboxes are widely available throughout the Union, while the participation should remain voluntary.\n\nIt is especially important to ensure that SMEs and startups can easily access these sandboxes, are actively involved, and participate in the development and testing of innovative AI systems, in order to be able to contribute with their knowhow and experience.",
    "### Recital 72 a (new)\n\nText proposed by the Commission - Amendment\n\n(72a) This Regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox only under specified conditions in line with Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation (EU) 2018/1725, and without prejudice to Article 4(2) of Directive (EU) 2016/680.\n\nProspective providers in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to mitigate any high-risks to safety, health, and the environment, and fundamental rights that may arise during the development and experimentation in the sandbox.\n\nThe conduct of the prospective providers in the sandbox should be taken into account when competent authorities decide over the temporary or permanent suspension of their participation in the sandbox or whether to impose an administrative fine under Article 83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680.",
    "### Recital 72 b (new)\n\nText proposed by the Commission - Amendment\n\n(72b) To ensure that Artificial Intelligence leads to socially and environmentally beneficial outcomes, Member States should support and promote research and development of AI in support of socially and environmentally beneficial outcomes by allocating sufficient resources, including public and Union funding, and giving priority access to regulatory sandboxes to projects led by civil society.\n\nSuch projects should be based on the principle of interdisciplinary cooperation between AI developers, experts on inequality and non-discrimination, accessibility, consumer, environmental, and digital rights, as well as academics.",
    "### Recital 73\n\nText proposed by the Commission - Amendment\n\nIn order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account.\n\nTo this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication.\n\nMoreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees.\n\nTranslation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale.\n\nMember States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users.\n\nIn order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account.\n\nTo this objective, Member States should develop initiatives, which are targeted at those operators, including on AI literacy, awareness raising, and information communication.\n\nMember States shall utilise existing channels and where appropriate, establish new dedicated channels for communication with SMEs, start-ups, user and other innovators to provide guidance and respond to queries about the implementation of this Regulation.\n\nSuch existing channels could include but are not limited to ENISA’s Computer Security Incident Response Teams, National Data Protection Agencies, the AI-on demand platform, the European Digital Innovation Hubs, and other relevant instruments funded by EU programs as well as the Testing and Experimentation Facilities established by the Commission and the Member States at national or Union level.\n\nWhere appropriate, these channels shall work together to create synergies and ensure homogeneity in their guidance to start-ups, SMEs, and users.\n\nMoreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees.\n\nThe Commission shall regularly assess the certification and compliance costs for SMEs and start-ups, including through transparent consultations with SMEs, start-ups, and users, and shall work with Member States to lower such costs.\n\nFor example, translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale.\n\nMember States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users.\n\nMedium-sized enterprises which recently changed from the small to medium-size category within the meaning of the Annex to Recommendation 2003/361/EC (Article 16) shall have access to these initiatives and guidance for a period of time deemed appropriate by the Member States, as these new medium-sized enterprises may sometimes lack the legal resources and training necessary to ensure proper understanding and compliance with provisions.",
    "### Recital 74\n\nText proposed by the Commission - Amendment\n\nIn order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers and notified bodies with their obligations under this Regulation, the AI-on demand platform, the European Digital Innovation Hubs, and the Testing and Experimentation Facilities established by the Commission and the Member States at national or EU level should possibly contribute to the implementation of this Regulation.\n\nWithin their respective mission and fields of competence, they may provide in particular technical and scientific support to providers and notified bodies.\n\n(74) In order to minimise the r",
    "**Text proposed by the Commission**\n\n(76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established.\n\nThe Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence.",
    "**Amendment**\n\nIn order to avoid fragmentation, to ensure the optimal functioning of the Single market, to ensure effective and harmonised implementation of this Regulation, to achieve a high level of trustworthiness and of protection of health and safety, fundamental rights, the environment, democracy and the rule of law across the Union with regards to AI systems, to actively support national supervisory authorities, Union institutions, bodies, offices and agencies in matters pertaining to this Regulation, and to increase the uptake of artificial intelligence throughout the Union, a European Union Artificial Intelligence Office should be established.\n\nThe AI Office should have legal personality, should act in full independence, should be responsible for a number of advisory and coordination tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation and should be adequately funded and staffed.\n\nMember States should provide the strategic direction and control of the AI Office through the management board of the AI Office, alongside the Commission, the EDPS, the FRA, and ENISA.\n\nAn executive director should be responsible for managing the activities of the secretariat of the AI Office and for representing the AI Office.\n\nStakeholders should formally participate in the work of the AI Office through an advisory forum that should ensure varied and balanced stakeholder representation and should advise the AI Office on matters pertaining to this Regulation.\n\nIn case the establishment of the AI Office proves not to be sufficient to ensure a fully consistent application of this Regulation at Union level as well as efficient cross-border enforcement measures, the creation of an AI agency should be considered.",
    "**Text proposed by the Commission**\n\nMember States hold a key role in the application and enforcement of this Regulation.\n\nIn this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation.\n\nIn order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels, in each Member State one national authority should be designated as national supervisory authority.",
    "**Amendment**\n\nEach Member State should designate a national supervisory authority for the purpose of supervising the application and implementation of this Regulation.\n\nIt should also represent its Member State at the management board of the AI Office.\n\nIn order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels.\n\nEach national supervisory authority should act with complete independence in performing its tasks and exercising its powers in accordance with this Regulation.",
    "**Text proposed by the Commission**\n\n_(77a)_ The national supervisory authorities should monitor the application of the provisions pursuant to this Regulation and contribute to its consistent application throughout the Union.\n\nFor that purpose, the national supervisory authorities should cooperate with each other, with the relevant national competent authorities, the Commission, and with the AI Office.",
    "**Text proposed by the Commission**\n\n_(77b)_ The member or the staff of each national supervisory authority should, in accordance with Union or national law, be subject to a duty of professional secrecy both during and after their term of office, with regard to any confidential information which has come to their knowledge in the course of the performance of their tasks or exercise of their powers.\n\nDuring their term of office, that duty of professional secrecy should in particular apply to trade secrets and to reporting by natural persons of infringements of this Regulation.",
    "**Text proposed by the Commission**\n\nIn order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner, all providers should have a post-market monitoring system in place.\n\nThis system is also key to ensure that the possible risks emerging from AI systems which continue to ‘learn’ after being placed on the market or put into service can be more efficiently and timely addressed.\n\nIn this context, providers should also be required to have a system in place to report to the relevant authorities any serious incidents or any breaches to national and Union law protecting fundamental rights resulting from the use of their AI systems.",
    "**Amendment**\n\nIn order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner, all providers should have a post-market monitoring system in place.\n\nThis system is also key to ensure that the possible risks emerging from AI systems which continue to ‘learn’ or evolve after being placed on the market or put into service can be more efficiently and timely addressed.\n\nIn this context, providers should also be required to have a system in place to report to the relevant authorities any serious incidents or any breaches to national and Union law, including those protecting fundamental rights and consumer rights resulting from the use of their AI systems and take appropriate corrective actions.\n\nDeployers should also report to the relevant authorities, any serious incidents or breaches to national and Union law resulting from the use of their AI system when they become aware of such serious incidents or breaches.",
    "**Text proposed by the Commission**\n\nIn order to ensure an appropriate and effective enforcement of the requirements and obligations set out by this Regulation, which is Union harmonisation legislation, the system of market surveillance and compliance of products established by Regulation (EU) 2019/1020 should apply in its entirety.\n\nWhere necessary for their mandate, national public authorities or bodies, which supervise the application of Union law protecting fundamental rights, including equality bodies, should also have access to any documentation created under this Regulation.",
    "**Amendment**\n\n(79) In order to ensure an appropriate and effective enforcement of the requirements and obligations set out by this Regulation, which is Union harmonisation legislation, the system of market surveillance and compliance of products established by Regulation (EU) 2019/1020 should apply in its entirety.\n\nFor the purpose of this Regulation, national supervisory authorities should act as market surveillance authorities for AI systems covered by this Regulation except for AI systems covered by Annex II of this Regulation.\n\nFor AI systems covered by legal acts listed in the Annex II, the competent authorities under those legal acts should remain the lead authority.\n\nNational supervisory authorities and competent authorities in the legal acts listed in Annex II should work together whenever necessary.\n\nWhen appropriate, the competent authorities in the legal acts listed in Annex II should send competent staff to the national supervisory authority in order to assist in the performance of its tasks.\n\nFor the purpose of this Regulation, national supervisory authorities should have the same powers and obligations as market surveillance authorities under Regulation (EU) 2019/1020.\n\nWhere necessary for their mandate, national public authorities or bodies, which supervise the application of Union law protecting fundamental rights, including equality bodies, should also have access to any documentation created under this Regulation.\n\nAfter having exhausted all other reasonable ways to assess/verify the conformity and upon a reasoned request, the national supervisory authority should be granted access to the training, validation and testing datasets, the trained and training model of the high-risk AI system, including its relevant model parameters and their execution/run environment.\n\nIn cases of simpler software systems falling under this Regulation that are not based on trained models, and where all other ways to verify conformity have been exhausted, the national supervisory authority may exceptionally have access to the source code, upon a reasoned request.\n\nWhere the national supervisory authority has been granted access to the training, validation, and testing datasets in accordance with this Regulation, such access should be achieved through appropriate technical means and tools, including on site access and in exceptional circumstances, remote access.\n\nThe national supervisory authority should treat any information, including source code, software, and data as applicable, obtained as confidential information and respect relevant Union law on the protection of intellectual property and trade secrets.\n\nThe national supervisory authority should delete any information obtained upon the completion of the investigation.",
    "**Text proposed by the Commission**\n\nUnion legislation on financial services includes internal governance and risk management rules and requirements which are applicable to regulated financial institutions in the course of provision of those services, including when they make use of AI systems.\n\nIn order to ensure coherent application and enforcement of the obligations under this Regulation and relevant rules and requirements of the Union financial services legislation, the authorities responsible for the supervision and enforcement of the financial services legislation, including where applicable the European Central Bank, should be designated as competent authorities for the purpose of supervising the implementation of this Regulation, including for market surveillance activities, as regards AI systems provided or used by regulated and supervised financial institutions.\n\nTo further enhance the consistency between this Regulation and the rules applicable to credit institutions regulated under Directive 2013/36/EU of the European Parliament and of the Council, it is also appropriate to integrate the conformity assessment procedure and some of the providers’ procedural obligations in relation to risk management, post-marketing monitoring and documentation into the existing obligations and procedures under Directive 2013/36/EU.\n\nIn order to avoid overlaps, limited derogations should also be envisaged in relation to the quality management system of providers and the monitoring obligation placed on users of high-risk AI systems to the extent that these apply to credit institutions regulated by Directive 2013/36/EU.",
    "**Amendment**\n\n(80) Union law on financial services includes internal governance and risk management rules and requirements which are applicable to regulated financial institutions in the course of provision of those services, including when they make use of AI systems.\n\nIn order to ensure coherent application and enforcement of the obligations under this Regulation and relevant rules and requirements of the Union financial services law, the competent authorities responsible for the supervision and enforcement of the financial services law, including where applicable the European Central Bank, should be designated as competent authorities for the purpose of supervising the implementation of this Regulation, including for market surveillance activities, as regards AI systems provided or used by regulated and supervised financial institutions.\n\nTo further enhance the consistency between this Regulation and the rules applicable to credit institutions regulated under Directive 2013/36/EU of the European Parliament and of the Council, it is also appropriate to integrate the conformity assessment procedure and some of the providers’ procedural obligations in relation to risk management, post-marketing monitoring and documentation into the existing obligations and procedures under Directive 2013/36/EU.\n\nIn order to avoid overlaps, limited derogations should also be envisaged in relation to the quality management system of providers and the monitoring obligation placed on deployers of high-risk AI systems to the extent that these apply to credit institutions regulated by Directive 2013/36/EU.",
    "**Text proposed by the Commission**\n\n_(80a)_ Given the objectives of this Regulation, namely to ensure an equivalent level of protection of health, safety and fundamental rights of natural persons, to ensure the protection of the rule of law and democracy, and taking into account that the mitigation of the risks of AI system against such rights may not be sufficiently achieved at national level or may be subject to diverging interpretation which could ultimately lead to an uneven level of protection of natural persons and create market fragmentation, the national supervisory authorities should be empowered to conduct joint investigations or rely on the union safeguard procedure provided for in this Regulation for effective enforcement.\n\nJoint investigations should be initiated where the national supervisory authority has sufficient reasons to believe that an infringement of this Regulation amounts to a widespread infringement or a widespread infringement with a Union dimension, or where the AI system or foundation model presents a risk which affects or is likely to affect at least 45 million individuals in more than one Member State.",
    "**Text proposed by the Commission**\n\n(82) It is important that AI systems related to products that are not high-risk in accordance with this Regulation and thus are not required to comply with the requirements set out herein are nevertheless safe when placed on the market or put into service.\n\nTo contribute to this objective, the Directive 2001/95/EC of the European Parliament and of the Council57 would apply as a safety net.",
    "**Amendment**\n\nIt is important that AI systems related to products that are not high-risk in accordance with this Regulation and thus are not required to comply with the requirements set out for high-risk AI systems are nevertheless safe when placed on the market or put into service.\n\nTo contribute to this objective, the Directive 2001/95/EC of the European Parliament and of the Council57 would apply as a safety net.\n\n_57 Directive 2001/95/EC of the European Parliament and of the Council of 3 December 2001 on general product safety (OJ L 11, 15.1.2002, p. 4)._",
    "**Amendment**\n\nIn order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should aim for transparency and openness while respecting the confidentiality of information and data obtained in carrying out their tasks by putting in place technical and organisational measures to protect the security and confidentiality of the information obtained carrying out their activities including for intellectual property rights and public and national security interests.\n\nWhere the activities of the Commission, national competent authorities and notified bodies pursuant to this Regulation result in a breach of intellectual property rights, Member States should provide for adequate measures and remedies to ensure the enforcement of intellectual property rights in application of Directive 2004/48/EC.",
    "**Text proposed by the Commission**\n\nMember States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for their infringement.\n\nFor certain specific infringements, Member States should take into account the margins and criteria set out in this Regulation.\n\nThe European Data Protection Supervisor should have the power to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation.",
    "**Amendment**\n\nCompliance with this Regulation should be enforceable by means of the imposition of fines by the national supervisory authority when carrying out proceedings under the procedure laid down in this Regulation.\n\nMember States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for their infringement.\n\nIn order to strengthen and harmonise administrative penalties for infringement of this Regulation, the upper limits for setting the administrative fines for certain specific infringements should be laid down.\n\nWhen assessing the amount of the fines, national competent authorities should, in each individual case, take into account all relevant circumstances of the specific situation, with due regard to the nature, gravity and duration of the infringement and of its consequences and to the provider’s size, in particular if the provider is a SME or a start-up.\n\nThe European Data Protection Supervisor should have the power to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation.\n\nThe penalties and litigation costs under this Regulation should not be subject to contractual clauses or any other arrangements.",
    "**Text proposed by the Commission**\n\n_(84a)_ As the rights and freedoms of natural and legal persons and groups of natural persons can be seriously undermined by AI systems, it is essential that natural and legal persons or groups of natural persons have meaningful access to reporting and redress mechanisms and to be entitled to access proportionate and effective remedies.\n\nThey should be able to report infringements of this Regulation to their national supervisory authority and have the right to lodge a complaint against the providers or deployers of AI systems.\n\nWhere applicable, deployers should provide internal complaints mechanisms to be used by natural and legal persons or groups of natural persons.\n\nWithout prejudice to any other administrative or non-judicial remedy, natural and legal persons and groups of natural persons should also have the right to an effective judicial remedy with regard to a legally binding decision of a national supervisory authority concerning them or, where the national supervisory authority does not handle a complaint, does not inform the complainant of the progress or preliminary outcome of the complaint lodged or does not comply with its obligation to reach a final decision, with regard to the complaint.",
    "**Text proposed by the Commission**\n\n_(84b)_ Affected persons should always be informed that they are subject to the use of a high-risk AI system, when deployers use a high-risk AI system to assist in decision-making or make decisions related to natural persons.\n\nThis information can provide a basis for affected persons to exercise their right to an explanation under this Regulation.\n\nWhen deployers provide an explanation to affected persons under this Regulation, they should take into account the level of expertise and knowledge of the average consumer or individual.",
    "**Text proposed by the Commission**\n\nIn order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems, the Union harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply.\n\nIt is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making58.\n\nIn particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts.",
    "**Amendment**\n\n(85) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the Union harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply.\n\nIt is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making58.\n\nThese consultations should involve the participation of a balanced selection of stakeholders, including consumer organisations, civil society, associations representing affected persons, businesses representatives from different sectors and sizes, as well as researchers and scientists.\n\nIn particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts.\n\n_58 OJ L 123, 12.5.2016, p. 1._",
    "**Text proposed by the Commission**\n\n_(85a)_ Given the rapid technological developments and the required technical expertise in conducting the assessment of high-risk AI systems, the Commission should regularly review the implementation of this Regulation, in particular the prohibited AI systems, the transparency obligations and the list of high-risk areas and use cases, at least every year, while consulting the AI office and the relevant stakeholders.",
    "**Text proposed by the Commission**\n\n_(87a)_ As reliable information on the resource and energy use, waste production and other environmental impact of AI systems and related ICT technology, including software, hardware and in particular data centres, is limited, the Commission should introduce of an adequate methodology to measure the environmental impact and effectiveness of this Regulation in light of the Union environmental and climate objectives.",
    "**Text proposed by the Commission**\n\nThe purpose of this Regulation is to promote the uptake of human-centric and trustworthy artificial intelligence and to ensure a high level of protection of health, safety, fundamental rights, democracy and the rule of law, and the environment from harmful effects of artificial intelligence systems in the Union while supporting innovation.",
    "### Article 1 – paragraph 1 – point d\n\n- **Text proposed by the Commission**  \n  Harmonised transparency rules for AI systems intended to interact with natural persons, emotion recognition systems, and biometric categorisation systems, and AI systems used to generate or manipulate image, audio or video content;\n- **Amendment**  \n  Harmonised transparency rules for certain AI systems;",
    "### Article 1 – paragraph 1 – point e a (new)\n\n- **Text proposed by the Commission**  \n  Amendment\n- **Amendment**  \n  (ea) Measures to support innovation, with a particular focus on SMEs and start-ups, including on setting up regulatory sandboxes and targeted measures to reduce the regulatory burden on SMEs and start-ups;",
    "### Article 2 – paragraph 1 – point c\n\n- **Text proposed by the Commission**  \n  Providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union;\n- **Amendment**  \n  (c) Providers and deployers of AI systems that have their place of establishment or who are located in a third country, where either Member State law applies by virtue of public international law, or the output produced by the system is intended to be used in the Union;",
    "### Article 2 – paragraph 1 – point c a (new)\n\n- **Text proposed by the Commission**  \n  Amendment\n- **Amendment**  \n  (ca) Providers placing on the market or putting into service AI systems referred to in Article 5 outside the Union where the provider or distributor of such systems is located within the Union;",
    "### Article 2 – paragraph 1 – point c b (new)\n\n- **Text proposed by the Commission**  \n  Amendment\n- **Amendment**  \n  (cb) Importers and distributors of AI systems as well as authorised representatives of providers of AI systems, where such importers, distributors or authorised representatives have their establishment or are located in the Union;",
    "### Article 2 – paragraph 1 – point c c (new)\n\n- **Text proposed by the Commission**  \n  Amendment\n- **Amendment**  \n  (cc) Affected persons as defined in Article 3(8a) that are located in the Union and whose health, safety or fundamental rights are adversely impacted by the use of an AI system that is placed on the market or put into service within the Union.",
    "### Article 2 – paragraph 2 – introductory part\n\n- **Text proposed by the Commission**  \n  For high-risk AI systems that are safety components of products or systems, or which are themselves products or systems, falling within the scope of the following acts, only Article 84 of this Regulation shall apply:\n- **Amendment**  \n  For high-risk AI systems that are safety components of products or systems, or which are themselves products or systems and that fall, within the scope of harmonisation legislation listed in Annex II - Section B, only Article 84 of this Regulation shall apply;",
    "## Further Amendments\n\nThe subsequent amendments (151-194) follow the format established above, detailing proposed amendments to specific articles, paragraphs, and points within the regulation, including new proposals for definitions, market rules, and more, affecting a wide range of aspects such as the treatment of SMEs, AI components, data rules, and enforcement measures.\n\nEmphasis is placed on clarifying definitions (e.g., AI systems, biometric data), introducing new categories (e.g., foundation models, significant risk), and specifying rules for compliance, safety, and enforcement, reflecting changes in governance structures and data protection alignment.\n\nProposals also target the operation of AI systems within and outside the Union, the rights of individuals affected by AI decisions, market dynamics, safety protocols, and transparency of AI systems.\n\n```",
    "# Definitions from Regulation\n\nmeans any form of automated processing of personal data as defined in point (4) of Article 4 of Regulation (EU) 2016/679; or in the case of law enforcement authorities – in point 4 of Article 3 of Directive (EU) 2016/680 or, in the case of Union institutions, bodies, offices or agencies, in point 5 Article 3 of Regulation (EU) 2018/1725;",
    "**Text proposed by the Commission**\n\n(44d) \"deep fake\" means manipulated or synthetic audio, image or video content that would falsely appear to be authentic or truthful, and which features depictions of persons appearing to say or do things they did not say or do, produced using AI techniques, including machine learning and deep learning;",
    "**Text proposed by the Commission**\n\n(44e) ‘widespread infringement’ means any act or omission contrary to Union law that protects the interest of individuals: which has harmed or is likely to harm the collective interests of individuals residing in at least two Member States other than the Member State, in which: the act or omission originated or took place; the provider concerned, or, where applicable, its authorised representative is established; or, the deployer is established, when the infringement is committed by the deployer; which protects the interests of individuals, that have caused, cause or are likely to cause harm to the collective interests of individuals and that have common features, including the same unlawful practice, the same interest being infringed and that are occurring concurrently, committed by the same operator, in at least three Member States;",
    "**Text proposed by the Commission**\n\n(44g) ‘regulatory sandbox’ means a controlled environment established by a public authority that facilitates the safe development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan under regulatory supervision;",
    "**Text proposed by the Commission**\n\n(44h) ‘critical infrastructure’ means an asset, a facility, equipment, a network or a system, or a part of an asset, a facility, equipment, a network or a system, which is necessary for the provision of an essential service within the meaning of Article 2(4) of Directive (EU) 2022/2557;",
    "**Text proposed by the Commission**\n\nArticle 4 deleted Amendments to Annex I The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend the list of techniques and approaches listed in Annex I, in order to update that list to market and technological developments on the basis of characteristics that are similar to the techniques and approaches listed therein.",
    "**Text proposed by the Commission**\n\nArticle 4 a General principles applicable to all AI systems All operators falling under this Regulation shall make their best efforts to develop and use AI systems or foundation models in accordance with the following general principles establishing a high-level framework that promotes a coherent human-centric European approach to ethical and trustworthy Artificial Intelligence, which is fully in line with the Charter as well as the values on which the Union is founded: ‘human agency and oversight’ means that AI systems shall be developed and used as a tool that serves people, respects human dignity and personal autonomy, and that is functioning in a way that can be appropriately controlled and overseen by humans; ‘technical robustness and safety’ means that AI systems shall be developed and used in a way to minimize unintended and unexpected harm as well as being robust in case of unintended problems and being resilient against attempts to alter the use or performance of the AI system so as to allow unlawful use by malicious third parties; ‘privacy and data governance’ means that AI systems shall be developed and used in compliance with existing privacy and data protection rules, while processing data that meets high standards in terms of quality and integrity; ‘transparency’ means that AI systems shall be developed and used in a way that allows appropriate traceability and explainability, while making humans aware that they communicate or interact with an AI system as well as duly informing users of the capabilities and limitations of that AI system and affected persons about their rights;.\n\n‘diversity, non-discrimination and fairness’ means that AI systems shall be developed and used in a way that includes diverse actors and promotes equal access, gender equality and cultural diversity, while avoiding discriminatory impacts and unfair biases that are prohibited by Union or national law; ‘social and environmental well-being’ means that AI systems shall be developed and used in a sustainable and environmentally friendly manner as well as in a way to benefit all human beings, while monitoring and assessing the long-term impacts on the individual, society and democracy.\n\nParagraph 1 is without prejudice to obligations set up by existing Union and national law.\n\nFor high-risk AI systems, the general principles are translated into and complied with by providers or deployers by means of the requirements set out in Articles 8 to 15, and the relevant obligations laid down in Chapter 3 of Title III of this Regulation.\n\nFor foundation models, the general principles are translated into and complied with by providers by means of the requirements set out in Articles 28 to 28b.\n\nFor all AI systems, the application of the principles referred to in paragraph 1 can be achieved, as applicable, through the provisions of Article 28, Article 52, or the application of harmonised standards, technical specifications, and codes of conduct as referred to in Article 69,without creating new obligations under this Regulation.\n\nThe Commission and the AI Office shall incorporate these guiding principles in standardisation requests as well as recommendations consisting in technical guidance to assist providers and deployers on how to develop and use AI systems.\n\nEuropean Standardisation Organisations shall take the general principles referred to in paragraph 1of this Article into account as outcome-based objectives when developing the appropriate harmonised standards for high risk AI systems as referred to in Article 40(2b).",
    "**Text proposed by the Commission**\n\nArticle 4 b AI literacy When implementing this Regulation, the Union and the Member States shall promote measures for the development of a sufficient level of AI literacy, across sectors and taking into account the different needs of groups of providers, deployers and affected persons concerned, including through education and training, skilling and reskilling programmes and while ensuring proper gender and age balance, in view of allowing a democratic control of AI systems Providers and deployers of AI systems shall take measures to ensure a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on which the AI systems are to be used.\n\nSuch literacy measures shall consist, in particular, of the teaching of basic notions and skills about AI systems and their functioning, including the different types of products and uses, their risks and benefits.\n\nA sufficient level of AI literacy is one that contributes, as necessary, to the ability of providers and deployers to ensure compliance and enforcement of this Regulation.",
    "**Text proposed by the Commission**\n\n- the placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person’s consciousness in order to materially distort a person’s behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm; the placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person’s consciousness or purposefully manipulative or deceptive techniques, with the objective to or the effect of materially distorting a person’s or a group of persons’ behaviour by appreciably impairing the person’s ability to make an informed decision, thereby causing the person to take a decision that that person would not have otherwise taken in a manner that causes or is likely to cause that person, another person or group of persons significant harm; The prohibition of AI system that deploys subliminal techniques referred to in the first sub-paragraph shall not apply to AI systems intended to be used for approved therapeutical purposes on the basis of specific informed consent of the individuals that are exposed to them or, where applicable, of their legal guardian;",
    "**Text proposed by the Commission**\n\n- the placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm; the placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a person or a specific group of persons, including characteristics of such person’s or a such group’s known or predicted personality traits or social or economic situation age, physical or mental ability with the objective or to the effect of materially distorting the behaviour of that person or a person pertaining to that group in a manner that causes or is likely to cause that person or another person significant harm;;",
    "**Text proposed by the Commission**\n\n(b a) the placing on the market, putting into service or use of biometric categorisation systems that categorise natural persons according to sensitive or protected attributes or characteristics or based on the inference of those attributes or characteristics.\n\nThis prohibition shall not apply to AI systems intended to be used for approved therapeutical purposes on the basis of specific informed consent of the individuals that are exposed to them or, where applicable, of their legal guardian.",
    "**Text proposed by the Commission**\n\n- the placing on the market, putting into service or use of AI systems by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behaviour or known or predicted personal or personality characteristics, with the social score leading to either or both of the following: the placing on the market, putting into service or use of AI systems for the social scoring evaluation or classification of natural persons or groups thereof over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following:",
    "**Text proposed by the Commission**\n\n(i) detrimental or unfavourable treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected; (i) detrimental or unfavourable treatment of certain natural persons or whole groups thereof in social contexts that are unrelated to the contexts in which the data was originally generated or collected;",
    "**Text proposed by the Commission**\n\n- the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement, unless and in as far as such use is strictly necessary for one of the following objectives: the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces;",
    "**Text proposed by the Commission**\n\nthe detection, localisation, identification or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2(2) of Council Framework Decision 2002/584/JHA62 and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years, as determined by the law of that Member State.\n\ndeleted\n\n62 Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender procedures between Member States (OJ L 190, 18.7.2002, p. 1).",
    "**Text proposed by the Commission**\n\n(d a) the placing on the market, putting into service or use of an AI system for making risk assessments of natural persons or groups thereof in order to assess the risk of a natural person for offending or reoffending or for predicting the occurrence or reoccurrence of an actual or potential criminal or administrative offence based on profiling of a natural person or on assessing personality traits and characteristics, including the person’s location, or past criminal behaviour of natural persons or groups of natural persons;",
    "**Text proposed by the Commission**\n\n(d d) the putting into service or use of AI systems for the analysis of recorded footage of publicly accessible spaces through ‘post’ remote biometric identification systems, unless they are subject to a pre-judicial authorisation in accordance with Union law and strictly necessary for the targeted search connected to a specific serious criminal offense as defined in Article 83(1) of TFEU that already took place for the purpose of law enforcement.",
    "**Text proposed by the Commission**\n\nThe use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall take into account the following elements: the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm caused in the absence of the use of the system; the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences.\n\nIn addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall comply with necessary and proportionate safeguards and conditions in relation to the use, in particular as deleted regards the temporal, geographic and personal limitations.",
    "**Text proposed by the Commission**\n\nAs regards paragraphs 1, point (d) and 2, each individual use for the purpose of law enforcement of a ‘real-time’ remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph 4.\n\nHowever, in a duly justified situation of urgency, the use of the system may be commenced without an authorisation and the authorisation may be requested only during or after the use.\n\nThe competent judicial or administrative authority shall only grant the authorisation where it is satisfied, based on objective evidence or clear indications presented to it, that the use of the ‘real-time’ remote biometric identification system at issue is necessary for and proportionate to achieving one of the objectives specified in paragraph 1, point (d), as identified in the request.\n\nIn deciding on the request, the competent judicial or administrative authority shall take into account the elements referred to in paragraph 2.",
    "**Text proposed by the Commission**\n\nA Member State may decide to provide for the possibility to fully or partially authorise the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement within the limits and under the conditions listed in paragraphs 1, point (d), 2 and 3.\n\nThat Member State shall lay down in its national law the necessary detailed rules for the request, issuance and exercise of, as well as supervision relating to, the authorisations referred to in paragraph 3.\n\nThose rules shall also specify in respect of which of the objectives listed in paragraph 1, point (d), including which of the criminal offences referred to in point (iii) thereof, the competent authorities may be authorised to use those systems for the purpose of law enforcement.\n\ndeleted",
    "**Text proposed by the Commission**\n\n(a) the AI system is intended to be used as a safety component of a product, or is itself a product, covered by the Union harmonisation legislation listed in Annex II; the AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation law listed in Annex II;",
    "**Text proposed by the Commission**\n\nthe product whose safety component is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment with a view to the placing on the market or putting into service of that product pursuant to the Union harmonisation legislation listed in Annex II.\n\nthe product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment related to risks for health and safety, with a view to the placing on the market or putting into service of that product pursuant to the Union harmonisation law listed in Annex II;",
    "**Text proposed by the Commission**\n\n2.\n\nIn addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall also be considered high-risk.\n\n2.\n\nIn addition to the high-risk AI systems referred to in paragraph 1, AI systems falling under one or more of the critical areas and use cases referred to in Annex III shall be considered high-risk if they pose a significant risk of harm to the health, safety or fundamental rights of natural persons.\n\nWhere an AI system falls under Annex III point 2, it shall be considered to be high-risk if it poses a significant risk of harm to the environment.\n\nThe Commission shall, six months prior to the entry into force of this Regulation, after consulting the AI Office and relevant stakeholders, provide guidelines clearly specifying the circumstances where the output of AI systems referred to in Annex III would pose a significant risk of harm to the health, safety or fundamental rights of natural persons or cases in which it would not.",
    "**Text proposed by the Commission**\n\n2 a.\n\nWhere providers falling under one or more of the critical areas and use cases referred to in Annex III consider that their AI system does not pose a significant risk as described in paragraph 2, they shall submit a reasoned notification to the national supervisory authority that they are not subject to the requirements of Title III Chapter 2 of this Regulation.\n\nWhere the AI system is intended to be used in two or more Member States, that notification shall be addressed to the AI Office.\n\nWithout prejudice to Article 65, the national supervisory authority shall review and reply to the notification, directly or via the AI Office, within three months if they deem the AI system to be misclassified.",
    "**Text proposed by the Commission**\n\n2 b.\n\nProviders that misclassify their AI system as not subject to the requirements of Title III Chapter 2 of this Regulation and place it on the market before the deadline for objection by national supervisory authorities shall be subject to fines pursuant to Article 71.",
    "**Text proposed by the Commission**\n\nThe Commission is empowered to adopt delegated acts in accordance with Article 73 to update the list in Annex III by adding high-risk AI systems where both of the following conditions are fulfilled: The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend Annex III by adding or modifying areas or use-cases of high-risk AI systems where these pose a significant risk of harm to health and safety, or an adverse impact on fundamental rights, to the environment, or to democracy and the rule of law, and that risk is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III.",
    "**Text proposed by the Commission**\n\nthe AI systems pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights, that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III.\n\ndeleted",
    "**Text proposed by the Commission**\n\nWhen assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria: 2.\n\nWhen assessing an AI system for the purposes of paragraph 1 and 1a the Commission shall take into account the following criteria:",
    "### Article 7 – Paragraph 2 – Point c\n\nText proposed by the Commission\n\nAmendment  \nthe extent to which the use of an AI system has already caused harm to the health and safety or adverse impact on the fundamental rights or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities;  \nthe extent to which the use of an AI system has already caused harm to health and safety, has had an adverse impact on fundamental rights, the environment, democracy and the rule of law or has given rise to significant concerns in relation to the likelihood of such harm or adverse impact, as demonstrated for example by reports or documented allegations submitted to national supervisory authorities, to the Commission, to the AI Office, to the EDPS, or to the European Union Agency for Fundamental Rights;",
    "### Article 7 – Paragraph 2 – Point d\n\nText proposed by the Commission\n\nAmendment  \nthe potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons;  \nthe potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons or to disproportionately affect a particular group of persons;",
    "### Article 7 – Paragraph 2 – Point e\n\nText proposed by the Commission\n\nAmendment  \nthe extent to which potentially harmed or adversely impacted persons are dependent on the outcome produced with an AI system, in particular because for practical or legal reasons it is not reasonably possible to opt-out from that outcome;  \nthe extent to which potentially harmed or adversely impacted persons are dependent on the output produced involving an AI system, and that output is purely accessory in respect of the relevant action or decision to be taken, in particular because for practical or legal reasons it is not reasonably possible to opt-out from that output;",
    "### Article 7 – Paragraph 2 – Point f\n\nText proposed by the Commission\n\nAmendment  \nthe extent to which potentially harmed or adversely impacted persons are in a vulnerable position in relation to the user of an AI system, in particular due to an imbalance of power, knowledge, economic or social circumstances, or age;  \nthe extent to which there is an imbalance of power, or the potentially harmed or adversely impacted persons are in a vulnerable position in relation to the user of an AI system, in particular due to status, authority, knowledge, economic or social circumstances, or age;",
    "### Article 7 – Paragraph 2 – Point g\n\nText proposed by the Commission\n\nAmendment  \nthe extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an impact on the health or safety of persons shall not be considered as easily reversible;  \nthe extent to which the outcome produced involving an AI system is easily reversible or remedied, whereby outcomes having an adverse impact on health, safety, fundamental rights of persons, the environment, or on democracy and rule of law shall not be considered as easily reversible;",
    "### Article 7 – Paragraph 2 – Point h –\n\nText proposed by the Commission\n\nAmendment  \neffective measures of redress in relation to the risks posed by an AI system, with the exclusion of claims for damages;  \nthe extent to which existing Union law provides for:\n\n- effective measures of redress in relation to the damage caused by an AI system, with the exclusion of claims for direct or indirect damages;\n- effective measures to prevent or substantially minimise those risks.",
    "### Article 7 – Paragraph 2 a (new)\n\nText proposed by the Commission\n\nAmendment  \n2 a.\n\nWhen assessing an AI system for the purposes of paragraphs 1 or 1a the Commission shall consult the AI Office and, where relevant, representatives of groups on which an AI system has an impact, industry, independent experts, the social partners, and civil society organisations.\n\nThe Commission shall also organise public consultations in this regard and shall make the results of those consultations and of the final assessment publicly available;",
    "### Article 7 – Paragraph 2 b (new)\n\nText proposed by the Commission\n\nAmendment  \n2 b.\n\nThe AI Office, national supervisory authorities or the European Parliament may request the Commission to reassess and recategorise the risk categorisation of an AI system in accordance with paragraphs 1 and 1a.\n\nThe Commission shall give reasons for its decision and make them public.",
    "### Article 8 – Paragraph 1 a (new)\n\nText proposed by the Commission\n\nAmendment  \n1 a.\n\nIn complying with the requirement established in this Chapter, due account shall be taken of guidelines developed as referred to in Article 82b, the generally acknowledged state of the art, including as reflected in the relevant harmonised standards and common specifications as referred to in articles 40 and 41 or those already set out in Union harmonisation law;",
    "### Article 8 – Paragraph 2\n\nText proposed by the Commission\n\nAmendment  \n2.\n\nThe intended purpose of the high-risk AI system and the risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements.\n\n2.\n\nThe intended purpose of the high-risk AI system, the reasonably foreseeable misuses and the risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements.",
    "### Article 8 – Paragraph 2 a (new)\n\nText proposed by the Commission\n\nAmendment  \n2 a.\n\nAs long as the requirements of Title III, Chapters 2 and 3 or Title VIII, Chapters 1, 2 and 3 for high-risk AI systems are addressed by Union harmonisation law listed in Annex II, Section A, the requirements or obligations of those Chapters of this Regulation shall be deemed to be fulfilled, as long as they include the AI component.\n\nRequirements of Chapters 2 and 3 of Title III or Title VIII, Chapters 1, 2 and 3 for high-risk AI systems not addressed by Union harmonisation law listed in Annex II Section A, shall be incorporated into that Union harmonisation law, where applicable.\n\nThe relevant conformity assessment shall be carried out as part of the procedures laid out under Union harmonisation law listed in Annex II, Section A.",
    "### Article 9 – Paragraph 1\n\nText proposed by the Commission\n\nAmendment  \nA risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems.\n\nA risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems, throughout the entire lifecycle of the AI system.\n\nThe risk management system can be integrated into, or a part of, already existing risk management procedures relating to the relevant Union sectoral law insofar as it fulfils the requirements of this article.",
    "### Article 9 – Paragraph 2 – Introductory part\n\nText proposed by the Commission\n\nAmendment  \nThe risk management system shall consist of a continuous iterative process run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic updating.\n\nIt shall comprise the following steps:  \nThe risk management system shall consist of a continuous iterative process run throughout the entire lifecycle of a high-risk AI system, requiring regular review and updating of the risk management process, to ensure its continuing effectiveness, and documentation of any significant decisions and actions taken subject to this Article.\n\nIt shall comprise the following steps:",
    "### Article 9 – Paragraph 2 – Point a\n\nText proposed by the Commission\n\nAmendment  \n(a) identification and analysis of the known and foreseeable risks associated with each high-risk AI system;  \nidentification, estimation and evaluation of the known and the reasonably foreseeable risks that the high-risk AI system can pose to the health or safety of natural persons, their fundamental rights including equal access and opportunities, democracy and rule of law or the environment when the high-risk AI system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse;",
    "### Article 9 – Paragraph 2 – Point c\n\nText proposed by the Commission\n\nAmendment  \nevaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61;  \nevaluation of emerging significant risks as described in point (a) and identified based on the analysis of data gathered from the post-market monitoring system referred to in Article 61;",
    "### Article 9 – Paragraph 2 – Point d\n\nText proposed by the Commission\n\nAmendment  \nadoption of suitable risk management measures in accordance with the provisions of the following paragraphs.\n\n(d) adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to points a and b of this paragraph in accordance with the provisions of the following paragraphs",
    "### Article 9 – Paragraph 3\n\nText proposed by the Commission\n\nAmendment  \nThe risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this Chapter 2.\n\nThey shall take into account the generally acknowledged state of the art, including as reflected in relevant harmonised standards or common specifications.\n\nThe risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this Chapter 2, with a view to mitigate risks effectively while ensuring an appropriate and proportionate implementation of the requirements.",
    "### Article 9 – Paragraph 4 – Introductory part\n\nText proposed by the Commission\n\nAmendment  \nThe risk management measures referred to in paragraph 2, point (d) shall be such that any residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged acceptable, provided that the high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse.\n\nThose residual risks shall be communicated to the user.\n\nThe risk management measures referred to in paragraph 2, point (d) shall be such that relevant residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is reasonably judged to be acceptable, provided that the high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse.\n\nThose residual risks and the reasoned judgements made shall be communicated to the deployer.\n\nIn identifying the most appropriate risk management measures, the following shall be ensured:",
    "### Article 9 – Paragraph 4 – Subparagraph 1 – Point a\n\nText proposed by the Commission\n\nAmendment  \n(a) elimination or reduction of risks as far as possible through adequate design and development;  \nelimination or reduction of identified risks as far as technically feasible through adequate design and development of the high-risk AI system, involving when relevant, experts and external stakeholders;",
    "### Article 9 – Paragraph 4 – Subparagraph 1 – Point b\n\nText proposed by the Commission\n\nAmendment  \nwhere appropriate, implementation of adequate mitigation and control measures in relation to risks that cannot be eliminated;  \nwhere appropriate, implementation of adequate mitigation and control measures addressing significant risks that cannot be eliminated;",
    "### Article 9 – Paragraph 4 – Subparagraph 1 – Point c\n\nText proposed by the Commission\n\nAmendment  \nprovision of adequate information pursuant to Article 13, in particular as regards the risks referred to in paragraph 2, point (b) of this Article, and, where appropriate, training to users.\n\n(c) provision of the required information pursuant to Article 13, and, where appropriate, training to deployers.",
    "### Article 9 – Paragraph 4 – Subparagraph 2\n\nText proposed by the Commission\n\nAmendment  \nIn eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the user and the environment in which the system is intended to be used.\n\nIn eliminating or reducing risks related to the use of the high-risk AI system, providers shall take into due consideration the technical knowledge, experience, education and training the deployer may need, including in relation to the presumable context of use.",
    "### Article 9 – Paragraph 5\n\nText proposed by the Commission\n\nAmendment  \nHigh-risk AI systems shall be tested for the purposes of identifying the most appropriate risk management measures.\n\nTesting shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this Chapter.\n\nHigh-risk AI systems shall be tested for the purposes of identifying the most appropriate and targeted risk management measures and weighing any such measures against the potential benefits and intended goals of the system.\n\nTesting shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this Chapter.",
    "### Article 9 – Paragraph 6\n\nText proposed by the Commission\n\nAmendment  \nTesting procedures shall be suitable to achieve the intended purpose of the AI system and do not need to go beyond what is necessary to achieve that purpose.\n\nTesting procedures shall be suitable to achieve the intended purpose of the AI system.",
    "### Article 9 – Paragraph 7\n\nText proposed by the Commission\n\nAmendment  \nThe testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the market or the putting into service.\n\nTesting shall be made against preliminarily defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system.\n\nThe testing of the high-risk AI systems shall be performed, prior to the placing on the market or the putting into service.\n\nTesting shall be made against prior defined metrics, and probabilistic thresholds that are appropriate to the intended purpose or reasonably foreseeable misuse of the high-risk AI system.",
    "### Article 9 – Paragraph 8\n\nText proposed by the Commission\n\nAmendment  \nWhen implementing the risk management system described in paragraphs 1 to 7, specific consideration shall be given to whether the high-risk AI system is likely to be accessed by or have an impact on children.\n\nWhen implementing the risk management system described in paragraphs 1 to 7, providers shall give specific consideration to whether the high-risk AI system is likely to adversely impact vulnerable groups of people or children.",
    "### Article 9 – Paragraph 9\n\nText proposed by the Commission\n\nAmendment  \nFor credit institutions regulated by Directive 2013/36/EU, the aspects described in paragraphs 1 to 8 shall be part of the risk management procedures established by those institutions pursuant to Article 74 of that Directive.\n\nFor providers and AI systems already covered by Union law that require them to establish a specific risk management, including credit institutions regulated by Directive 2013/36/EU, the aspects described in paragraphs 1 to 8 shall be part of or combined with the risk management procedures established by that Union law.",
    "### Article 10 – Paragraph 1\n\nText proposed by the Commission\n\nAmendment  \nHigh-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5.\n\nHigh-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 as far as this is technically feasible according to the specific market segment or scope of application.\n\nTechniques that do not require labelled input data such as unsupervised learning and reinforcement learning shall be developed on the basis of data sets such as for testing and verification that meet the quality criteria referred to in paragraphs 2 to 5.",
    "### Article 10 – Paragraph 2 – Introductory part\n\nText proposed by the Commission\n\nAmendment  \nTraining, validation and testing data sets shall be subject to appropriate data governance and management practices.\n\nThose practices shall concern in particular,  \nTraining, validation and testing data sets shall be subject to data governance appropriate for the context of use as well as the intended purpose of the AI system.\n\nThose measures shall concern in particular,",
    "### Article 10 – Paragraph 2 – Point d\n\nText proposed by the Commission\n\nAmendment  \nthe formulation of relevant assumptions, notably with respect to the information that the data are supposed to measure and represent;  \nthe formulation of assumptions, notably with respect to the information that the data are supposed to measure and represent;",
    "### Article 10 – Paragraph 2 – Point f\n\nText proposed by the Commission\n\nAmendment  \nexamination in view of possible biases;  \nexamination in view of possible biases that are likely to affect the health and safety of persons, negatively impact fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations (‘feedback loops’) and appropriate measures to detect, prevent and mitigate possible biases;",
    "### Article 10 – Paragraph 2 – Point g\n\nText proposed by the Commission\n\nAmendment  \nthe identification of any possible data gaps or shortcomings, and how those gaps and shortcomings can be addressed.\n\n(g) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed;",
    "### Article 10 – Paragraph 3\n\nText proposed by the Commission\n\nAmendment  \nTraining, validation and testing data sets shall be relevant, representative, free of errors and complete.\n\nThey shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used.\n\nThese characteristics of the data sets may be met at the level of individual data sets or a combination thereof.\n\nTraining datasets, and where they are used, validation and testing datasets, including the labels, shall be relevant, sufficiently representative, appropriately vetted for errors and be as complete as possible in view of the intended purpose.\n\nThey shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used.\n\nThese characteristics of the datasets shall be met at the level of individual datasets or a combination thereof.",
    "### Article 10 – Paragraph 4\n\nText proposed by the Commission\n\nAmendment  \nTraining, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used.\n\nDatasets shall take into account, to the extent required by the intended purpose or reasonably foreseeable misuses of the AI system, the characteristics or elements that are particular to the specific geographical, contextual behavioural or functional setting within which the high-risk AI system is intended to be used.",
    "### Article 10 – Paragraph 5\n\nText proposed by the Commission\n\nAmendment  \nTo the extent that it is strictly necessary for the purposes of ensuring bias monitoring, detection and correction in relation to the high-risk AI systems, the providers of such systems may process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons, including technical limitations on the re-use and use of state-of-the-art security and privacy-preserving measures, such as pseudonymisation, or encryption where anonymisation may significantly affect the purpose pursued.\n\n5.\n\nTo the extent that it is strictly necessary for the purposes of ensuring negative bias detection and correction in relation to the high-risk AI systems, the providers of such systems may exceptionally process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons, including technical limitations on the re-use and use of state-of-the-art security and privacy-preserving.\n\nIn particular, all the following conditions shall apply in order for this processing to occur:\n- the bias detection and correction cannot be effectively fulfilled by processing synthetic or anonymised data;\n- the data are pseudonymised;\n- the provider takes appropriate technical and organisational measures to ensure that the data processed for the purpose of this paragraph are secured, protected, subject to suitable safeguards and only authorised persons have access to those data with appropriate confidentiality obligations;\n- the data processed for the purpose of this paragraph are not to be transmitted, transferred or otherwise accessed by other parties;\n- the data processed for the purpose of this paragraph are protected by means of appropriate technical and organisational measures and deleted once the bias has been corrected or the personal data has reached the end of its retention period;\n- effective and appropriate measures are in place to ensure availability, security and resilience of processing systems and services against technical or physical incidents;\n- effective and appropriate measures are in place to ensure physical security of locations where the data are stored and processed, internal IT and IT security governance and management, certification of processes and products;  \nProviders having recourse to this provision shall draw up documentation explaining why the processing of special categories of personal data was necessary to detect and correct biases.",
    "### Article 10 – Paragraph 6 a (new)\n\nText proposed by the Commission\n\nAmendment  \n6 a.\n\nWhere the provider cannot comply with the obligations laid down in this Article because that provider does not have access to the data and the data is held exclusively by the deployer, the deployer may, on the basis of a contract, be made responsible for any infringement of this Article.",
    "### Article 11 – Paragraph 1 – Subparagraph 1\n\nText proposed by the Commission\n\nAmendment  \nThe technical documentation shall be drawn up in such a way to demonstrate that the high-risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with all the necessary information to assess the compliance of the AI system with those requirements.\n\nIt shall contain, at a minimum,",
    "## Technical Documentation\n\nThe elements set out in Annex IV.\n\nThe technical documentation shall be drawn up in such a way to demonstrate that the high-risk AI system complies with the requirements set out in this Chapter and provide national supervisory authorities and notified bodies with the necessary information to assess the compliance of the AI system with those requirements.\n\nIt shall contain, at a minimum, the elements set out in Annex IV or, in the case of SMEs and start-ups, any equivalent documentation meeting the same objectives, subject to approval of the competent national authority.",
    "### Proposal for a Regulation Article 11 – Paragraph 2\n\nText proposed by the Commission  \nAmendment\n\n2.\n\nWhere a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in Annex IV as well as the information required under those legal acts.\n\n2.\n\nWhere a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in paragraph 1 as well as the information required under those legal acts.",
    "### Proposal for a Regulation Article 11 – Paragraph 3 a (new)\n\nText proposed by the Commission  \nAmendment\n\n3 a.\n\nProviders that are credit institutions regulated by Directive 2013/36/EU shall maintain the technical documentation as part of the documentation concerning internal governance, arrangements, processes, and mechanisms pursuant to Article 74 of that Directive.",
    "### Proposal for a Regulation Article 12 – Paragraph 1\n\nText proposed by the Commission  \nAmendment\n\nHigh-risk AI systems shall be designed and developed with capabilities enabling the automatic recording of events (‘logs’) while the high-risk AI systems is operating.\n\nThose logging capabilities shall conform to recognised standards or common specifications.\n\nHigh-risk AI systems shall be designed and developed with capabilities enabling the automatic recording of events (‘logs’) while the high-risk AI systems is operating.\n\nThose logging capabilities shall conform to the state of the art and recognised standards or common specifications.",
    "### Proposal for a Regulation Article 12 – Paragraph 2\n\nText proposed by the Commission  \nAmendment\n\nThe logging capabilities shall ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to the intended purpose of the system.\n\nIn order to ensure a level of traceability of the AI system’s functioning throughout its entire lifetime that is appropriate to the intended purpose of the system, the logging capabilities shall facilitate the monitoring of operations as referred to in Article 29(4) as well as the post market monitoring referred to in Article 61.\n\nIn particular, they shall enable the recording of events relevant for the identification of situations that may: result in the AI system presenting a risk within the meaning of Article 65(1); or lead to a substantial modification of the AI system.",
    "### Proposal for a Regulation Article 12 – Paragraph 2 a (new)\n\nText proposed by the Commission  \nAmendment\n\n2 a.\n\nHigh-risk AI systems shall be designed and developed with the logging capabilities enabling the recording of energy consumption, the measurement or calculation of resource use, and environmental impact of the high-risk AI system during all phases of the system’s lifecycle.",
    "### Proposal for a Regulation Article 12 – Paragraph 3\n\nText proposed by the Commission  \nAmendment\n\nIn particular, logging capabilities shall enable the monitoring of the operation of the high-risk AI system with respect to the occurrence of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or lead to a substantial modification, and facilitate the post-market monitoring referred to in Article 61.  \ndeleted",
    "### Proposal for a Regulation Article 13 – Paragraph 1\n\nText proposed by the Commission  \nAmendment\n\nHigh-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system’s output and use it appropriately.\n\nAn appropriate type and degree of transparency shall be ensured, with a view to achieving compliance with the relevant obligations of the user and of the provider set out in Chapter 3 of this Title.\n\nHigh-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable providers and users to reasonably understand the system’s functioning.\n\nAppropriate transparency shall be ensured in accordance with the intended purpose of the AI system, with a view to achieving compliance with the relevant obligations of the provider and user set out in Chapter 3 of this Title.\n\nTransparency shall thereby mean that, at the time the high-risk AI system is placed on the market, all technical means available in accordance with the generally acknowledged state of the art are used to ensure that the AI system’s output is interpretable by the provider and the user.\n\nThe user shall be enabled to understand and use the AI system appropriately by generally knowing how the AI system works and what data it processes, allowing the user to explain the decisions taken by the AI system to the affected person pursuant to Article 68(c).",
    "### Proposal for a Regulation Article 13 – Paragraph 2\n\nText proposed by the Commission  \nAmendment\n\nHigh-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct, and clear information that is relevant, accessible, and comprehensible to users.\n\nHigh-risk AI systems shall be accompanied by intelligible instructions for use in an appropriate digital format or made otherwise available in a durable medium that includes concise, correct, clear, and to the extent possible complete information that helps operating and maintaining the AI system as well as supporting informed decision-making by users and is reasonably relevant, accessible and comprehensible to users.",
    "### Proposal for a Regulation Article 13 – Paragraph 3 – Point a\n\nText proposed by the Commission  \nAmendment\n\n(a) the identity and the contact details of the provider and, where applicable, of its authorised representative;  \nthe identity and the contact details of the provider and, where applicable, of its authorised representatives;",
    "### Proposal for a Regulation Article 13 – Paragraph 3 – Point a a (new)\n\nText proposed by the Commission  \nAmendment\n\n(aa) where it is not the same as the provider, the identity and the contact details of the entity that carried out the conformity assessment and, where applicable, of its authorised representative:",
    "### Proposal for a Regulation Article 13 – Paragraph 3 – Point b – Introductory Part\n\nText proposed by the Commission  \nAmendment\n\nthe characteristics, capabilities, and limitations of performance of the high-risk AI system, including:  \n(b) the characteristics, capabilities, and limitations of performance of the high-risk AI system, including, where appropriate:",
    "### Proposal for a Regulation Article 13 – Paragraph 3 – Point b – Point ii\n\nText proposed by the Commission  \nAmendment\n\n(ii) the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness, and cybersecurity;  \nthe level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any clearly known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity;",
    "### Proposal for a Regulation Article 13 – Paragraph 3 – Point b – Point iii\n\nText proposed by the Commission  \nAmendment\n\nany known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights;  \n(iii) any clearly known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety, fundamental rights or the environment, including, where appropriate, illustrative examples of such limitations and of scenarios for which the system should not be used;",
    "### Proposal for a Regulation Article 13 – Paragraph 3 – Point b – Point v\n\nText proposed by the Commission  \nAmendment\n\n(v) when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation, and testing data sets used, taking into account the intended purpose of the AI system.\n\n(v) relevant information about user actions that may influence system performance, including type or quality of input data, or any other relevant information in terms of the training, validation, and testing data sets used, taking into account the intended purpose of the AI system.",
    "### Proposal for a Regulation Article 13 – Paragraph 3 – Point e\n\nText proposed by the Commission  \nAmendment\n\n(e) the expected lifetime of the high-risk AI system and any necessary maintenance and care measures to ensure the proper functioning of that AI system, including with regards to software updates.\n\n(e) any necessary maintenance and care measures to ensure the proper functioning of that AI system, including with regards to software updates, through its expected lifetime.",
    "### Proposal for a Regulation Article 14 – Paragraph 1\n\nText proposed by the Commission  \nAmendment\n\nHigh-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use.\n\nHigh-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they are effectively overseen by natural persons as proportionate to the risks associated with those systems.\n\nNatural persons in charge of ensuring human oversight shall have a sufficient level of AI literacy in accordance with Article 4b and the necessary support and authority to exercise that function, during the period in which the AI system is in use and to allow for thorough investigation after an incident.",
    "### Proposal for a Regulation Article 14 – Paragraph 2\n\nText proposed by the Commission  \nAmendment\n\nHuman oversight shall aim at preventing or minimising the risks to health, safety, or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter.\n\nHuman oversight shall aim at preventing or minimising the risks to health, safety, fundamental rights or the environment that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter and where decisions based solely on automated processing by AI systems produce legal or otherwise significant effects on the persons or groups of persons on which the system is to be used.",
    "### Proposal for a Regulation Article 14 – Paragraph 3 – Introductory Part\n\nText proposed by the Commission  \nAmendment\n\nHuman oversight shall be ensured through either one or all of the following measures:  \nHuman oversight shall take into account the specific risks, the level of automation, and context of the AI system and shall be ensured through either one or all of the following types of measures:",
    "### Proposal for a Regulation Article 14 – Paragraph 4 – Introductory Part\n\nText proposed by the Commission  \nAmendment\n\nThe measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances:  \nFor the purpose of implementing paragraphs 1 to 3, the high-risk AI system shall be provided to the user in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate to the circumstances:",
    "### Proposal for a Regulation Article 14 – Paragraph 4 – Point a\n\nText proposed by the Commission  \nAmendment\n\n(a) fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions, and unexpected performance can be detected and addressed as soon as possible;  \n(a) be aware of and sufficiently understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions, and unexpected performance can be detected and addressed as soon as possible;",
    "### Proposal for a Regulation Article 14 – Paragraph 4 – Point e\n\nText proposed by the Commission  \nAmendment\n\n(e) be able to intervene on the operation of the high-risk AI system or interrupt the system through a “stop” button or a similar procedure.\n\n(e) be able to intervene on the operation of the high-risk AI system or interrupt, the system through a “stop” button or a similar procedure that allows the system to come to a halt in a safe state, except if the human interference increases the risks or would negatively impact the performance in consideration of generally acknowledged state-of-the-art.",
    "### Proposal for a Regulation Article 14 – Paragraph 5\n\nText proposed by the Commission  \nAmendment\n\nFor high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons.\n\n5.\n\nFor high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons with the necessary competence, training, and authority.",
    "### Proposal for a Regulation Article 15 – Paragraph 1\n\nText proposed by the Commission  \nAmendment\n\nHigh-risk AI systems shall be designed and developed in such a way that they achieve, in the light of their intended purpose, an appropriate level of accuracy, robustness, and cybersecurity, and perform consistently in those respects throughout their lifecycle.\n\nHigh-risk AI systems shall be designed and developed following the principle of security by design and by default.\n\nIn the light of their intended purpose, they should achieve an appropriate level of accuracy, robustness, safety, and cybersecurity, and perform consistently in those respects throughout their lifecycle.\n\nCompliance with these requirements shall include the implementation of state-of-the-art measures, according to the specific market segment or scope of application.",
    "### Proposal for a Regulation Article 15 – Paragraph 1 a (new)\n\nText proposed by the Commission  \nAmendment\n\n1 a.\n\nTo address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 of this Article, the AI Office shall bring together national and international metrology and benchmarking authorities and provide non-binding guidance on the matter as set out in Article 56, paragraph 2, point (a).",
    "### Proposal for a Regulation Article 15 – Paragraph 1 b (new)\n\nText proposed by the Commission  \nAmendment\n\n1b.\n\nTo address any emerging issues across the internal market with regard to cybersecurity, the European Union Agency for Cybersecurity (ENISA) shall be involved alongside the European Artificial Intelligence Board as set out Article 56, paragraph 2, point (b).",
    "### Paragraph 1 Point e\n\n- Technical specifications, including standards, to be applied and, where the relevant harmonised standards are not applied in full, or do not cover all of the relevant requirements, the means to be used to ensure that the high-risk AI system complies with the requirements set out in Chapter 2 of this Title.",
    "### Paragraph 1 Point f\n\n- Systems and procedures for data management, including data acquisition data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the data that is performed before and for the purposes of the placing on the market or putting into service of high-risk AI systems.",
    "### Paragraph 1\n\n- Providers of high-risk AI systems shall keep the logs automatically generated by their high-risk AI systems, to the extent such logs are under their control.\n\nWithout prejudice to applicable Union or national law, the logs shall be kept for a period of at least 6 months.\n\nThe retention period shall be in accordance with industry standards and appropriate to the intended purpose of high-risk AI system.",
    "### Paragraph 1\n\n- Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system which they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it or to recall it, as appropriate.",
    "### Paragraph 1a (new)\n\n- The providers shall also inform the authorised representative, if one was appointed in accordance with Article 25, and the notified body if the high-risk AI system had to undergo a third-party conformity assessment in accordance with Article 43.\n\nWhere applicable, they shall also investigate the causes in collaboration with the deployer.",
    "### Paragraph 1\n\n- Where the high-risk AI system presents a risk within the meaning of Article 65(1) and the provider of the system becomes aware of that risk, that provider shall immediately inform the national supervisory authorities of the Member States in which it made the system available and, where applicable, the notified body that issued a certificate for the high-risk AI system, in particular the nature of the non-compliance and of any relevant corrective actions taken.",
    "### Paragraph 1a (new)\n\n- In the cases referred to in the first paragraph, providers of the high-risk AI system shall immediately inform:\n  - the distributors;\n  - the importers;\n  - the national competent authorities of the Member States in which they made the AI system available or put it into service; and\n  - where possible, the deployers.",
    "### Paragraph 1\n\n- Providers and where applicable, deployers of high-risk AI systems shall, upon a reasoned request by a national competent authority or where applicable, by the AI Office or the Commission, provide them with all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title, in an official Union language determined by the Member State concerned.",
    "### Paragraph 1a (new)\n\n- Upon a reasoned request by a national competent authority or, where applicable, by the Commission, providers and, where applicable, deployers shall also give the requesting national competent authority or the Commission, as applicable, access to the logs automatically generated by the high-risk AI system, to the extent such logs are under their control.",
    "### Paragraph 2\n\n- The authorised representative shall perform the tasks specified in the mandate received from the provider.\n\nIt shall provide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of the institution of the Union determined by the national competent authority.\n\nFor the purpose of this Regulation, the mandate shall empower the authorised representative to carry out the following tasks:\n  - Ensure that the EU declaration of conformity and the technical documentation have been drawn up and that an appropriate conformity assessment procedure has been carried out by the provider.",
    "### Paragraph 2b (new)\n\n- The authorised representative shall terminate the mandate if it considers or has reason to consider that the provider acts contrary to its obligations under this Regulation.\n\nIn such a case, it shall also immediately inform the national supervisory authority of the Member State in which it is established, as well as, where applicable, the relevant notified body, about the termination of the mandate and the reasons thereof.",
    "### Paragraph 1\n\n- Before placing a high-risk AI system on the market, importers of such system shall ensure that such a system is in conformity with this Regulation by ensuring that:\n  - The relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of that AI system\n  - The provider has drawn up the technical documentation in accordance with Article 11 and Annex IV;\n  - Where applicable, the provider has appointed an authorised representative in accordance with Article 25(1).",
    "### Paragraph 2\n\n- Where an importer considers or has reason to consider that a high-risk AI system is not in conformity with this Regulation, or is counterfeit, or accompanied by falsified documentation it shall not place that system on the market until that AI system has been brought into conformity.\n\nWhere the high-risk AI system presents a risk within the meaning of Article 65(1), the importer shall inform the provider of the AI system and the market surveillance authorities to that effect.",
    "### Paragraph 5\n\n- Importers shall provide national competent authorities, upon a reasoned request, with all the necessary information and documentation to demonstrate the conformity of a high-risk AI system with the requirements set out in Chapter 2 of this Title in a language which can be easily understood by them, including access to the logs automatically generated by the high-risk AI system to the extent such logs are under the control of the provider in accordance with Article 20.",
    "### Paragraph 1\n\n- Before making a high-risk AI system available on the market, distributors shall verify that the high-risk AI system bears the required CE conformity marking, that it is accompanied by the required documentation and instruction of use, and that the provider and the importer of the system, as applicable, have complied with their obligations set out in this Regulation in Articles 16 and 26 respectively.",
    "### Paragraph 2\n\n- Where a distributor considers or has reason to consider, on the basis of the information in its possession that a high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title, it shall not make the high-risk AI system available on the market until that system has been brought into conformity with those requirements.\n\nFurthermore, where the system presents a risk within the meaning of Article 65(1), the distributor shall inform the provider or the importer of the system, the relevant national competent authority, as applicable, to that effect.",
    "### Paragraph 4\n\n- A distributor that considers or has reason to consider, on the basis of the information in its possession, that a high-risk AI system which it has made available on the market is not in conformity with the requirements set out in Chapter 2 of this Title shall take the corrective actions necessary to bring that system into conformity with those requirements, to withdraw it or recall it or shall ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions.\n\nWhere the high-risk AI system presents a risk within the meaning of Article 65(1), the distributor shall immediately inform the provider or importer of the system and the national competent authorities of the Member States in which it has made the product available to that effect, giving details, in particular, of the non-compliance and of any corrective actions taken.",
    "### Paragraph 5\n\n- Upon a reasoned request from a national competent authority, distributors of high-risk AI systems shall provide that authority with all the information and documentation in their possession or available to them, in accordance with the obligations of distributors as outlined in paragraph 1, that are necessary to demonstrate the conformity of a high-risk system with the requirements set out in Chapter 2 of this Title.",
    "### Paragraph 1\n\n- Any distributor, importer, deployer or other third-party shall be considered a provider for the purposes of this Regulation and shall be subject to the obligations of the provider under Article 16, in any of the following circumstances:\n  - They put their name or trademark on a high-risk AI system already placed on the market or put into service.\n\n- They make a substantial modification to a high-risk AI system that has already been placed on the market or has already been placed into service.\n\n---\n\nNote: This Markdown conversion includes all the text from the provided document in a structured format.",
    "# Amendment 394 Proposal for a Regulation Article 28 – Paragraph 1 – Point ba (new)\n\n**Text proposed by the Commission**  \n(b a) they make a substantial modification to an AI system, including a general-purpose AI system, which has not been classified as high-risk and has already been placed on the market or put into service in such manner that the AI system becomes a high-risk AI system in accordance with Article 6.",
    "# Amendment 395 Proposal for a Regulation Article 28 – Paragraph 2\n\n**Text proposed by the Commission**  \nWhere the circumstances referred to in paragraph 1, point (b) or (c), occur, the provider that initially placed the high-risk AI system on the market or put it into service shall no longer be considered a provider for the purposes of this Regulation.\n\n2.\n\nWhere the circumstances referred to in paragraph 1, point (a) to (ba) occur, the provider that initially placed the AI system on the market or put it into service shall no longer be considered a provider of that specific AI system for the purposes of this Regulation.\n\nThis former provider shall provide the new provider with the technical documentation and all other relevant and reasonably expected information capabilities of the AI system, technical access or other assistance based on the generally acknowledged state of the art that are required for the fulfillment of the obligations set out in this Regulation.\n\nThis paragraph shall also apply to providers of foundation models as defined in Article 3 when the foundation model is directly integrated into a high-risk AI system.",
    "# Amendment 396 Proposal for a Regulation Article 28 – Paragraph 2a (new)\n\n**Text proposed by the Commission**  \n2 a.\n\nThe provider of a high-risk AI system and the third party that supplies tools, services, components, or processes that are used or integrated into the high-risk AI system shall, by written agreement, specify the information, capabilities, technical access, and/or other assistance, based on the generally acknowledged state of the art, that the third party is required to provide in order to enable the provider of the high-risk AI system to fully comply with the obligations under this Regulation.\n\nThe Commission shall develop and recommend non-binding model contractual terms between providers of high-risk AI systems and third parties that supply tools, services, components, or processes that are used or integrated into high-risk AI systems in order to assist both parties in drafting and negotiating contracts with balanced contractual rights and obligations, consistent with each party’s level of control.\n\nWhen developing non-binding model contractual terms, the Commission shall take into account possible contractual requirements applicable in specific sectors or business cases.\n\nThe non-binding contractual terms shall be published and be available free of charge in an easily usable electronic format on the AI Office’s website.",
    "# Amendment 397 Proposal for a Regulation Article 28 – Paragraph 2b (new)\n\n**Text proposed by the Commission**  \n2 b.\n\nFor the purposes of this Article, trade secrets shall be preserved and shall only be disclosed provided that all specific necessary measures pursuant to Directive (EU) 2016/943 are taken in advance to preserve their confidentiality, in particular with respect to third parties.\n\nWhere necessary, appropriate technical and organizational arrangements can be agreed to protect intellectual property rights or trade secrets.",
    "# Amendment 398 Proposal for a Regulation Article 28a (new)\n\n**Text proposed by the Commission**  \nArticle 28a\n\nUnfair contractual terms unilaterally imposed on an SME or startup.\n\nA contractual term concerning the supply of tools, services, components, or processes that are used or integrated into a high-risk AI system or the remedies for the breach or the termination of related obligations which has been unilaterally imposed by an enterprise on an SME or startup shall not be binding on the latter enterprise if it is unfair.\n\nA contractual term is not to be considered unfair where it arises from applicable Union law.\n\nA contractual term is unfair if it is of such a nature that it objectively impairs the ability of the party upon whom the term has been unilaterally imposed to protect its legitimate commercial interest in the information in question or its use grossly deviates from good commercial practice in the supply of tools, services, components, or processes that are used or integrated into a high-risk AI system, contrary to good faith and fair dealing or creates a significant imbalance between the rights and the obligations of the parties in the contract.\n\nA contractual term is also unfair if it has the effect of shifting penalties referred to in Article 71 or associated litigation costs across parties to the contract, as referred to in Article 71(8).\n\nA contractual term is unfair for the purposes of this Article if its object or effect is to:\n\n- exclude or limit the liability of the party that unilaterally imposed the term for intentional acts or gross negligence;\n- exclude the remedies available to the party upon whom the term has been unilaterally imposed in the case of non-performance of contractual obligations or the liability of the party that unilaterally imposed the term in the case of a breach of those obligations;\n- give the party that unilaterally imposed the term the exclusive right to determine whether the technical documentation, information supplied are in conformity with the contract or to interpret any term of the contract.\n\nA contractual term shall be considered to be unilaterally imposed within the meaning of this Article if it has been supplied by one contracting party and the other contracting party has not been able to influence its content despite an attempt to negotiate it.\n\nThe contracting party that supplied a contractual term shall bear the burden of proving that that term has not been unilaterally imposed.\n\nWhere the unfair contractual term is severable from the remaining terms of the contract, those remaining terms shall remain binding.\n\nThe party that supplied the contested term shall not argue that the term is an unfair term.\n\nThis Article shall apply to all new contracts entered into force after ... [date of entry into force of this Regulation].\n\nBusinesses shall review existing contractual obligations that are subject to this Regulation by …[three years after the date of entry into force of this Regulation].\n\nGiven the rapidity in which innovations occur in the markets, the list of unfair contractual terms within Article 28a shall be reviewed regularly by the Commission and be updated to new business practices if necessary.",
    "# Amendment 399 Proposal for a Regulation Article 28b (new)\n\n**Text proposed by the Commission**  \nArticle 28b Obligations of the provider of a foundation model\n\nA provider of a foundation model shall, prior to making it available on the market or putting it into service, ensure that it is compliant with the requirements set out in this Article, regardless of whether it is provided as a standalone model or embedded in an AI system or a product, or provided under free and open-source licences, as a service, as well as other distribution channels.\n\nFor the purpose of paragraph 1, the provider of a foundation model shall:\n\n- demonstrate through appropriate design, testing, and analysis the identification, the reduction, and mitigation of reasonably foreseeable risks to health, safety, fundamental rights, the environment, democracy, and the rule of law prior and throughout development with appropriate methods such as with the involvement of independent experts, as well as the documentation of remaining non-mitigable risks after development;\n- process and incorporate only datasets that are subject to appropriate data governance measures for foundation models, in particular measures to examine the suitability of the data sources and possible biases and appropriate mitigation;\n- design and develop the foundation model in order to achieve throughout its lifecycle appropriate levels of performance, predictability, interpretability, corrigibility, safety, and cybersecurity assessed through appropriate methods such as model evaluation with the involvement of independent experts, documented analysis, and extensive testing during conceptualization, design, and development;\n- design and develop the foundation model, making use of applicable standards to reduce energy use, resource use, and waste, as well as to increase energy efficiency, and the overall efficiency of the system, without prejudice to relevant existing Union and national law.\n\nThis obligation shall not apply before the standards referred to in Article 40 are published.\n\nFoundation models shall be designed with capabilities enabling the measurement and logging of the consumption of energy and resources, and, where technically feasible, other environmental impact the deployment and use of the systems may have over their entire lifecycle;\n- draw up extensive technical documentation and intelligible instructions for use, in order to enable the downstream providers to comply with their obligations pursuant to Articles 16 and 28(1);\n- establish a quality management system to ensure and document compliance with this Article, with the possibility to experiment in fulfilling this requirement;\n- register that foundation model in the EU database referred to in Article 60, in accordance with the instructions outlined in Annex VIII point C.\n\nWhen fulfilling those requirements, the generally acknowledged state of the art shall be taken into account, including as reflected in relevant harmonized standards or common specifications, as well as the latest assessment and measurement methods, reflected in particular in benchmarking guidance and capabilities referred to in Article 58a.\n\nProviders of foundation models shall, for a period ending 10 years after their foundation models have been placed on the market or put into service, keep the technical documentation referred to in paragraph 2(e) at the disposal of the national competent authorities.\n\nProviders of foundation models used in AI systems specifically intended to generate, with varying levels of autonomy, content such as complex text, images, audio, or video (“generative AI”) and providers who specialize a foundation model into a generative AI system, shall in addition comply with the transparency obligations outlined in Article 52 (1), train, and where applicable, design and develop the foundation model in such a way as to ensure adequate safeguards against the generation of content in breach of Union law in line with the generally-acknowledged state of the art, and without prejudice to fundamental rights, including the freedom of expression, without prejudice to Union or national or Union legislation on copyright, document and make publicly available a sufficiently detailed summary of the use of training data protected under copyright law.",
    "# Amendment 400 Proposal for a Regulation Article 29 – Paragraph 1\n\n**Text proposed by the Commission**  \nUsers of high-risk AI systems shall use such systems in accordance with the instructions of use accompanying the systems, pursuant to paragraphs 2 and 5.\n\nDeployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions of use accompanying the systems, pursuant to paragraphs 2 and 5 of this Article.",
    "# Amendment 401 Proposal for a Regulation Article 29 – Paragraph 1a (new)\n\n**Text proposed by the Commission**  \n1 a.\n\nTo the extent deployers exercise control over the high-risk AI system, they shall\n\n- implement human oversight according to the requirements laid down in this Regulation\n- ensure that the natural persons assigned to ensure human oversight of the high-risk AI systems are competent, properly qualified and trained, and have the necessary resources in order to ensure the effective supervision of the AI system in accordance with Article 14\n- ensure that relevant and appropriate robustness and cybersecurity measures are regularly monitored for effectiveness and are regularly adjusted or updated.",
    "# Amendment 402 Proposal for a Regulation Article 29 – Paragraph 2\n\n**Text proposed by the Commission**  \nThe obligations in paragraph 1 are without prejudice to other user obligations under Union or national law and to the user’s discretion in organizing its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.\n\nThe obligations in paragraph 1 and 1a, are without prejudice to other deployer obligations under Union or national law and to the deployer’s discretion in organizing its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.",
    "# Amendment 403 Proposal for a Regulation Article 29 – Paragraph 3\n\n**Text proposed by the Commission**  \nWithout prejudice to paragraph 1, to the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system.\n\nWithout prejudice to paragraph 1 and 1a, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system.",
    "# Amendment 404 Proposal for a Regulation Article 29 – Paragraph 4 – Introductory Part\n\n**Text proposed by the Commission**  \nUsers shall monitor the operation of the high-risk AI system on the basis of the instructions of use.\n\nWhen they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall inform the provider or distributor and suspend the use of the system.\n\nThey shall also inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system.\n\nIn case the user is not able to reach the provider, Article 62 shall apply mutatis mutandis.\n\nDeployers shall monitor the operation of the high-risk AI system on the basis of the instructions of use and when relevant, inform providers in accordance with Article 61.\n\nWhen they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall, without undue delay, inform the provider or distributor and relevant national supervisory authorities and suspend the use of the system.\n\nThey shall also immediately inform first the provider, and then the importer or distributor and relevant national supervisory authorities when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system.\n\nIf the deployer is not able to reach the provider, Article 62 shall apply mutatis mutandis.",
    "# Amendment 405 Proposal for a Regulation Article 29 – Paragraph 4 – Subparagraph 1\n\n**Text proposed by the Commission**  \nFor users that are credit institutions regulated by Directive 2013/36/EU, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes, and mechanisms pursuant to Article 74 of that Directive.\n\nFor deployers that are credit institutions regulated by Directive 2013/36/EU, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes, and mechanisms pursuant to Article 74 of that Directive.",
    "# Amendment 406 Proposal for a Regulation Article 29 – Paragraph 5 – Introductory Part\n\n**Text proposed by the Commission**  \nUsers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control.\n\nThe logs shall be kept for a period that is appropriate in the light of the intended purpose of the high-risk AI system and applicable legal obligations under Union or national law.\n\nDeployers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent that such logs are under their control and are required for ensuring and demonstrating compliance with this Regulation, for ex-post audits of any reasonably foreseeable malfunction, incidents, or misuses of the system, or for ensuring and monitoring for the proper functioning of the system throughout its lifecycle.\n\nWithout prejudice to applicable Union or national law, the logs shall be kept for a period of at least six months.\n\nThe retention period shall be in accordance with industry standards and appropriate to the intended purpose of the high-risk AI system.",
    "# Amendment 407 Proposal for a Regulation Article 29 – Paragraph 5 – Subparagraph 1\n\n**Text proposed by the Commission**  \nUsers that are credit institutions regulated by Directive 2013/36/EU shall maintain the logs as part of the documentation concerning internal governance arrangements, processes, and mechanisms pursuant to Article 74 of that Directive.\n\nDeployers that are credit institutions regulated by Directive 2013/36/EU shall maintain the logs as part of the documentation concerning internal governance arrangements, processes, and mechanisms pursuant to Article 74 of that Directive.",
    "# Amendment 408 Proposal for a Regulation Article 29 – Paragraph 5a (new)\n\n**Text proposed by the Commission**  \n5 a.\n\nPrior to putting into service or use a high-risk AI system at the workplace, deployers shall consult workers' representatives with a view to reaching an agreement in accordance with Directive 2002/14/EC and inform the affected employees that they will be subject to the system.",
    "# Amendment 409 Proposal for a Regulation Article 29 – Paragraph 5b (new)\n\n**Text proposed by the Commission**  \n5 b. Deployers of high-risk AI systems that are public authorities or Union institutions, bodies, offices, and agencies or undertakings referred to in Article 51(1a)(b) shall comply with the registration obligations referred to in Article 51.",
    "# Amendment 410 Proposal for a Regulation Article 29 – Paragraph 6\n\n**Text proposed by the Commission**  \nUsers of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, where applicable.\n\nWhere applicable, deployers of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, a summary of which shall be published, having regard to the specific use and the specific context in which the AI system is intended to operate.\n\nDeployers may revert in part to those data protection impact assessments for fulfilling some of the obligations set out in this article, insofar as the data protection impact assessment fulfills those obligations.",
    "# Amendment 411 Proposal for a Regulation Article 29 – Paragraph 6a (new)\n\n**Text proposed by the Commission**  \n6 a.\n\nWithout prejudice to Article 52, deployers of high-risk AI systems referred to in Annex III, which make decisions or assist in making decisions related to natural persons, shall inform the natural persons that they are subject to the use of the high-risk AI system.\n\nThis information shall include the intended purpose and the type of decisions it makes.\n\nThe deployer shall also inform the natural person about its right to an explanation referred to in Article 68c.",
    "# Amendment 413 Proposal for a Regulation Article 29a (new)\n\n**Text proposed by the Commission**  \nArticle 29a Fundamental rights impact assessment for high-risk AI systems\n\nPrior to putting a high-risk AI system as defined in Article 6(2) into use, with the exception of AI systems intended to be used in area 2 of Annex III, deployers shall conduct an assessment of the systems’ impact in the specific context of use.\n\nThis assessment shall include, at a minimum, the following elements:\n\n- a clear outline of the intended purpose for which the system will be used;\n- a clear outline of the intended geographic and temporal scope of the system’s use;\n- categories of natural persons and groups likely to be affected by the use of the system;\n- verification that the use of the system is compliant with relevant Union and national law on fundamental rights;\n- the reasonably foreseeable impact on fundamental rights of putting the high-risk AI system into use;\n- specific risks of harm likely to impact marginalized persons or vulnerable groups;\n- the reasonably foreseeable adverse impact of the use of the system on the environment;\n- a detailed plan as to how the harms and the negative impact on fundamental rights identified will be mitigated.\n\n- the governance system the deployer will put in place, including human oversight, complaint-handling, and redress.\n\nIf a detailed plan to mitigate the risks outlined in the course of the assessment outlined in paragraph 1 cannot be identified, the deployer shall refrain from putting the high-risk AI system into use and inform the provider and the National supervisory authority without undue delay.\n\nNational supervisory authorities, pursuant to Articles 65 and 67, shall take this information into account when investigating systems which present a risk at the national level.\n\nThe obligation outlined under paragraph 1 applies for the first use of the high-risk AI system.\n\nThe deployer may, in similar cases, draw back on previously conducted fundamental rights impact assessment or existing assessment carried out by providers.\n\nIf, during the use of the high-risk AI system, the deployer considers that the criteria listed in paragraph 1 are no longer met, it shall conduct a new fundamental rights impact assessment.\n\nIn the course of the impact assessment, the deployer, with the exception of SMEs, shall notify the national supervisory authority and relevant stakeholders and shall, to the best extent possible, involve representatives of the persons or groups of persons that are likely to be affected by the high-risk AI system, as identified in paragraph 1, including but not limited to: equality bodies, consumer protection agencies, social partners, and data protection agencies, with a view to receiving input into the impact assessment.\n\nThe deployer shall allow a period of six weeks for bodies to respond.\n\nSMEs may voluntarily apply the provisions laid down in this paragraph.\n\nIn the case referred to in Article 47(1), public authorities may be exempted from these obligations.\n\nThe deployer that is a public authority or an undertaking referred to in Article 51(1a) (b) shall publish a summary of the results of the impact assessment as part of the registration of use pursuant to their obligation under Article 51(2).\n\nWhere the deployer is already required to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamental rights impact assessment referred to in paragraph 1 shall be conducted in conjunction with the data protection impact assessment.\n\nThe data protection impact assessment shall be published as an addendum.",
    "# Amendment 414 Proposal for a Regulation Article 30 – Paragraph 1\n\n**Text proposed by the Commission**  \n1.\n\nEach Member State shall designate or establish a notifying authority responsible for setting up and carrying out the necessary procedures for the assessment, designation, and notification of conformity assessment bodies and for their monitoring.\n\n1.\n\nEach Member State shall designate or establish a notifying authority responsible for setting up and carrying out the necessary procedures for the assessment, designation, and notification of conformity assessment bodies and for their monitoring.\n\nThose procedures shall be developed in cooperation between the notifying authorities of all Member States.",
    "# Amendment 415 Proposal for a Regulation Article 30 – Paragraph 7\n\n**Text proposed by the Commission**  \nNotifying authorities shall have a sufficient number of competent personnel at their disposal for the proper performance of their tasks.\n\nNotifying authorities shall have a sufficient number of competent personnel at their disposal for the proper performance of their tasks.\n\nWhere applicable, competent personnel shall have the necessary expertise, such as a degree in an appropriate legal field, in the supervision of fundamental rights enshrined in the Charter of Fundamental Rights of the European Union.",
    "# Amendment 416 Proposal for a Regulation Article 30 – Paragraph 8\n\n**Text proposed by the Commission**  \nNotifying authorities shall make sure Notifying authorities shall make sure that conformity assessments are carried out in a proportionate manner, avoiding unnecessary burdens for providers, and that notified bodies perform their activities taking due account of the size of an undertaking, the sector in which it operates, its structure, and the degree of complexity of the AI system in question.\n\nParticular attention shall be paid to minimizing administrative burdens and compliance costs for micro and small enterprises as defined in the Annex to Commission Recommendation 2003/361/EC.",
    "# Amendment 417 Proposal for a Regulation Article 32 – Paragraph 1\n\n**Text proposed by the Commission**  \nNotifying authorities may notify only conformity assessment bodies which have satisfied the requirements laid down in Article 33.\n\nNotifying authorities shall notify only conformity assessment bodies which have satisfied the requirements laid down in Article 33.",
    "# Amendment 418 Proposal for a Regulation Article 32 – Paragraph 2\n\n**Text proposed by the Commission**  \nNotifying authorities shall notify the Commission and the other Member States using the electronic notification tool developed and managed by the Commission.\n\nNotifying authorities shall notify the Commission and the other Member States using the electronic notification tool developed and managed by the Commission of each conformity assessment body referred to in paragraph 1.",
    "# Amendment 419 Proposal for a Regulation Article 32 – Paragraph 3\n\n**Text proposed by the Commission**  \nThe notification shall include full details of the conformity assessment activities, the conformity assessment module or modules, and the artificial intelligence technologies concerned.\n\nThe notification referred to in paragraph 2 shall include full details of the conformity assessment activities, the conformity assessment module or modules, and the artificial intelligence technologies concerned, as well as the relevant attestation of competence.",
    "# Conformity Assessment Body Actions\n\nOnly where no objections are raised by the Commission or the other Member States within one month of a notification.\n\nThe conformity assessment body concerned may perform the activities of a notified body only where no objections are raised by the Commission or the other Member States within two weeks of the validation of the notification where it includes an accreditation certificate referred to in Article 31(2), or within two months of the notification where it includes documentary evidence referred to in Article 31(3).",
    "### Article 32 – Paragraph 4 a (new)\n\n**Text proposed by the Commission**  \n4 a.\n\nWhere objections are raised, the Commission shall without delay enter into consultation with the relevant Member States and the conformity assessment body.\n\nIn view thereof, the Commission shall decide whether the authorisation is justified or not.\n\nThe Commission shall address its decision to the Member State concerned and the relevant conformity assessment body.",
    "### Article 33 – Paragraph 2\n\n**Text proposed by the Commission**  \nNotified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks.\n\n**Amendment**  \n2.\n\nNotified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks as well as the minimum cybersecurity requirements set out for public administration entities identified as operators of essential services pursuant to Directive (EU) 2022/2555.",
    "### Article 33 – Paragraph 4\n\n**Text proposed by the Commission**  \nNotified bodies shall be independent of the provider of a high-risk AI system in relation to which it performs conformity assessment activities.\n\nNotified bodies shall also be independent of any other operator having an economic interest in the high-risk AI system that is assessed, as well as of any competitors of the provider.\n\n**Amendment**  \n4.\n\nNotified bodies shall be independent of the provider of a high-risk AI system in relation to which it performs conformity assessment activities.\n\nNotified bodies shall also be independent of any other operator having an economic interest in the high-risk AI system that is assessed, as well as of any competitors of the provider.\n\nThis shall not preclude the use of assessed AI systems that are necessary for the operations of the conformity assessment body or the use of such systems for personal purposes.",
    "### Article 33 – Paragraph 4 a (new)\n\n**Text proposed by the Commission**  \n4 a.\n\nA conformity assessment pursuant to paragraph 1 shall be performed by employees of notified bodies who have not provided any other other service related to the matter assessed than the conformity assessment to the provider of a high-risk AI system nor to any legal person connected to that provider in the 12 months’ period before the assessment and have committed to not providing them with such services in the 12 month period following the completion of the assessment.",
    "### Article 33 – Paragraph 6\n\n**Text proposed by the Commission**  \nNotified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.\n\nThe staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out.\n\n**Amendment**  \n6.\n\nNotified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.\n\nThe staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out.\n\nAny information and documentation obtained by notified bodies pursuant to the provisions of this Article shall be treated in compliance with the confidentiality obligations set out in Article 70.",
    "### Article 34 – Paragraph 3\n\n**Text proposed by the Commission**  \nActivities may be subcontracted or carried out by a subsidiary only with the agreement of the provider.\n\n**Amendment**  \nActivities may be subcontracted or carried out by a subsidiary only with the agreement of the provider.\n\nNotified bodies shall make a list of their subsidiaries publicly available.",
    "### Article 34 – Paragraph 4\n\n**Text proposed by the Commission**  \nNotified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation.\n\n**Amendment**  \n4.\n\nNotified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the verification of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation.",
    "### Article 36 – Paragraph 1\n\n**Text proposed by the Commission**  \nWhere a notifying authority has suspicions or has been informed that a notified body no longer meets the requirements laid down in Article 33, or that it is failing to fulfil its obligations, that authority shall without delay investigate the matter with the utmost diligence.\n\nIn that context, it shall inform the notified body concerned about the objections raised and give it the possibility to make its views known.\n\nIf the notifying authority comes to the conclusion that the notified body investigation no longer meets the requirements laid down in Article 33 or that it is failing to fulfil its obligations, it shall restrict, suspend or withdraw the notification as appropriate, depending on the seriousness of the failure.\n\nIt shall also immediately inform the Commission and the other Member States accordingly.\n\n**Amendment**  \nWhere a notifying authority has suspicions or has been informed that a notified body no longer meets the requirements laid down in Article 33, or that it is failing to fulfil its obligations, that authority shall without delay investigate the matter with the utmost diligence.\n\nIn that context, it shall inform the notified body concerned about the objections raised and give it the possibility to make its views known.\n\nIf the notifying authority comes to the conclusion that the notified body no longer meets the requirements laid down in Article 33 or that it is failing to fulfil its obligations, it shall restrict, suspend or withdraw the notification as appropriate, depending on the seriousness of the failure.\n\nIt shall also immediately inform the Commission and the other Member States accordingly.",
    "### Article 36 – Paragraph 2\n\n**Text proposed by the Commission**  \nIn the event of restriction, suspension or withdrawal of notification, or where the notified body has ceased its activity, the notifying authority shall take appropriate steps to ensure that the files of that notified body are either taken over by another notified body or kept available for the responsible notifying authorities at their request.\n\n**Amendment**  \n2.\n\nIn the event of restriction, suspension or withdrawal of notification, or where the notified body has ceased its activity, the notifying authority shall take appropriate steps to ensure that the files of that notified body are either taken over by another notified body or kept available for the responsible notifying authorities, and market surveillance authority at their request.",
    "### Article 37 – Paragraph 1\n\n**Text proposed by the Commission**  \nThe Commission shall, where necessary, investigate all cases where there are reasons to doubt whether a notified body complies with the requirements laid down in Article 33.\n\n**Amendment**  \nThe Commission shall, where necessary, investigate all cases where there are reasons to doubt the competence of a notified body or the continued fulfilment by a notified body of the applicable requirements and responsibilities.",
    "### Article 37 – Paragraph 2\n\n**Text proposed by the Commission**  \nThe Notifying authority shall provide the Commission, on request, with all relevant information relating to the notification of the notified body concerned.\n\n**Amendment**  \nThe Notifying authority shall provide the Commission, on request, with all relevant information relating to the notification or the maintenance of the competence of the notified body concerned.",
    "### Article 37 – Paragraph 3\n\n**Text proposed by the Commission**  \nThe Commission shall ensure that all confidential information obtained in the course of its investigations pursuant to this Article is treated confidentially.\n\n**Amendment**  \nThe Commission shall ensure that all sensitive information obtained in the course of its investigations pursuant to this Article is treated confidentially.",
    "### Article 37 – Paragraph 4\n\n**Text proposed by the Commission**  \nWhere the Commission ascertains that a notified body does not meet or no longer meets the requirements laid down in Article 33, it shall adopt a reasoned decision requesting the notifying Member State to take the necessary corrective measures, including withdrawal of notification if necessary.\n\nThat implementing act shall be adopted in accordance with the examination procedure referred to in Article 74(2).\n\n**Amendment**  \n4.\n\nWhere the Commission ascertains that a notified body does not meet or no longer meets the requirements for its notification, it shall inform the notifying Member State accordingly and request it to take the necessary corrective measures, including suspension or withdrawal of the notification if necessary.\n\nWhere the Member State fails to take the necessary corrective measures, the Commission may, by means of an implementing act, suspend, restrict or withdraw the designation.\n\nThat implementing act shall be adopted in accordance with the examination procedure referred to in Article 74(2).",
    "### Article 40 – Paragraph 1\n\n**Text proposed by the Commission**  \nHigh-risk AI systems which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those standards cover those requirements.\n\n**Amendment**  \nHigh-risk AI systems and foundation models which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union in accordance with Regulation (EU) 1025/2012 shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title or Article 28b, to the extent those standards cover those requirements.",
    "### Article 40 – Paragraph 1 a (new)\n\n**Text proposed by the Commission**  \nThe Commission shall issue standardisation requests covering all requirements of this Regulation, in accordance with Article 10 of Regulation EU (No)1025/2012 by... [two months after the date of entry into force of this Regulation].\n\nWhen preparing standardisation request, the Commission shall consult the AI Office and the Advisory Forum.",
    "### Article 40 – Paragraph 1 b (new)\n\n**Text proposed by the Commission**  \nWhen issuing a standardisation request to European standardisation organisations, the Commission shall specify that standards have to be consistent, including with the sectorial law listed in Annex II, and aimed at ensuring that AI systems or foundation models placed on the market or put into service in the Union meet the relevant requirements laid down in this Regulation.",
    "### Article 40 – Paragraph 1 c (new)\n\n**Text proposed by the Commission**  \nThe actors involved in the standardisation process shall take into account the general principles for trustworthy AI set out in Article 4(a), seek to promote investment and innovation in AI as well as competitiveness and growth of the Union market, and contribute to strengthening global cooperation on standardisation and taking into account existing international standards in the field of AI that are consistent with Union values, fundamental rights and interests, and ensure a balanced representation of interests and effective participation of all relevant stakeholders in accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.",
    "### Article 41 – Paragraph 1\n\n**Text proposed by the Commission**  \nWhere harmonised standards referred to in Article 40 do not exist or where the Commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific safety or fundamental right concerns, the Commission may, by means of implementing acts, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title.\n\nThose implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2).\n\n**Amendment**  \nDeleted",
    "### Article 41 – Paragraph 1 a (new)\n\n**Text proposed by the Commission**  \n1 a.\n\nThe Commission may, by means of implementing act adopted in accordance with the examination procedure referred to in Article 74(2) and after consulting the AI Office and the AI Advisory Forum, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title or Article 28b wherein all of the following conditions are fulfilled:\n- there is no reference to harmonised standards already published in the Official Journal of the European Union related to the essential requirement(s), unless the harmonised standard in question is an existing standard that must be revised;\n- the Commission has requested one or more European standardisation organisations to draft a harmonised standard for the essential requirement(s) set out in Chapter 2;\n- the request referred to in point (b) has not been accepted by any of the European standardisation organisations; or there are undue delays in the establishment of an appropriate harmonised standard; or the standard provided does not satisfy the requirements of the relevant Union law, or does not comply with the request of the Commission.",
    "### Article 41 – Paragraph 2\n\n**Text proposed by the Commission**  \nThe Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups established under relevant sectorial Union law.\n\n**Amendment**  \nThe Commission shall, throughout the whole process of drafting the common specifications referred to in paragraphs 1a and 1b, regularly consult the AI Office and the Advisory Forum, the European standardisation organisations and bodies or expert groups established under relevant sectorial Union law as well as other relevant stakeholders.\n\nThe Commission shall fulfil the objectives referred to in Article 40 (1c) and duly justify why it decided to resort to common specifications.\n\nWhere the Commission intends to adopt common specifications pursuant to paragraph 1a of this Article, it shall also clearly identify the specific fundamental rights concern to be addressed.\n\nWhen adopting common specifications pursuant to paragraphs 1a and 1b of this Article, the Commission shall take into account the opinion issued by the AI Office referred to in Article 56e(b) of this Regulation.\n\nWhere the Commission decides not to follow the opinion of the AI Office, it shall provide a reasoned explanation to the AI Office.",
    "### Article 41 – Paragraph 3\n\n**Text proposed by the Commission**  \nHigh-risk AI systems which are in conformity with the common specifications referred to in paragraph 1 shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those common specifications cover those requirements.\n\n**Amendment**  \nHigh-risk AI systems which are in conformity with the common specifications referred to in paragraphs 1a and 1b shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those common specifications cover those requirements.",
    "### Article 41 – Paragraph 3 a (new)\n\n**Text proposed by the Commission**  \n3 a.\n\nWhere a harmonised standard is adopted by a European standardisation organisation and proposed to the Commission for the publication of its reference in the Official Journal of the European Union, the Commission shall assess the harmonised standard in accordance with Regulation (EU) No 1025/2012.\n\nWhen reference of a harmonised standard is published in the Official Journal of the European Union, the Commission shall repeal acts referred to in paragraph 1 and 1b, or parts thereof which cover the same requirements set out in Chapter 2 of this Title.",
    "### Article 41 – Paragraph 4\n\n**Text proposed by the Commission**  \nWhere providers do not comply with the common specifications referred to in paragraph 1, they shall duly justify that they have adopted technical solutions that are at least equivalent thereto.\n\n**Amendment**  \n4.\n\nWhere providers of high-risk AI systems do not comply with the common specifications referred to in paragraph 1, they shall duly justify that they have adopted technical solutions that meet the requirements referred to in Chapter II to a level at least equivalent thereto.",
    "### Article 42 – Paragraph 1\n\n**Text proposed by the Commission**  \n1.\n\nTaking into account their intended purpose, high-risk AI systems that have been trained and tested on data concerning the specific geographical, behavioural and functional setting within which they are intended to be used shall be presumed to be in compliance with the requirement set out in Article 10(4).\n\n**Amendment**  \n1.\n\nTaking into account their intended purpose, high-risk AI systems that have been trained and tested on data concerning the specific geographical, behavioural contextual and functional setting within which they are intended to be used shall be presumed to be in compliance with the respective requirements set out in Article 10(4).",
    "### Article 43 – Paragraph 1 – Introductory Part\n\n**Text proposed by the Commission**  \n1.\n\nFor high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall follow one of the following procedures:\n\n**Amendment**  \n1.\n\nFor high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall opt for one of the following procedures:",
    "### Article 43 – Paragraph 1 – Point b\n\n**Text proposed by the Commission**  \nthe conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII.\n\n**Amendment**  \n(b) the conformity assessment procedure based on assessment of the quality management system and of the technical documentation, with the involvement of a notified body, referred to in Annex VII;",
    "### Article 43 – Paragraph 1 – Subparagraph 1\n\n**Text proposed by the Commission**  \nWhere, in demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider has not applied or has applied only in part harmonised standards referred to in Article 40, or where such harmonised standards do not exist and common specifications referred to in Article 41 are not available, the provider shall follow the conformity assessment procedure set out in Annex VII.\n\n**Amendment**  \nIn demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider shall follow the conformity assessment procedure set out in Annex VII in the following cases:\n\n- where harmonised standards referred to in Article 40, the reference number of which has been published in the Official Journal of the European Union, covering all relevant safety requirements for the AI system, do not exist and common specifications referred to in Article 41 are not available;\n- where the technical specifications referred to in point (a) exist but the provider has not applied them or has applied them only in part;\n- where one or more of the technical specifications referred to in point (a) has been published with a restriction and only on the part of the standard that was restricted;\n- when the provider considers that the nature, design, construction or purpose of the AI system necessitate third party verification, regardless of its risk level.",
    "### Article 43 – Paragraph 1 – Subparagraph 2\n\n**Text proposed by the Commission**  \nFor the purpose of the conformity assessment procedure referred to in Annex VII, the provider may choose any of the notified bodies.\n\nHowever, when the system is intended to be put into service by law enforcement, immigration or asylum authorities as well as EU institutions, bodies or agencies, the market surveillance authority referred to in Article 63(5) or (6), as applicable, shall act as a notified body.\n\n**Amendment**  \nFor the purpose of carrying out the conformity assessment procedure referred to in Annex VII, the provider may choose any of the notified bodies.\n\nHowever, when the system is intended to be put into service by law enforcement, immigration or asylum authorities as well as EU institutions, bodies or agencies, the market surveillance authority referred to in Article 63(5) or (6), as applicable, shall act as a notified body.",
    "### Article 43 – Paragraph 4 – Introductory Part\n\n**Text proposed by the Commission**  \n4.\n\nHigh-risk AI systems shall undergo a new conformity assessment procedure whenever they are substantially modified, regardless of whether the modified system is intended to be further distributed or continues to be used by the current user.\n\n**Amendment**  \nHigh-risk AI systems that have already been subject to a conformity assessment procedure shall undergo a new conformity assessment procedure whenever they are substantially modified, regardless of whether the modified system is intended to be further distributed or continues to be used by the current deployer.",
    "### Article 43 – Paragraph 5\n\n**Text proposed by the Commission**  \nThe Commission is empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating Annexes VI and Annex VII in order to introduce elements of the conformity assessment procedures that become necessary in light of technical progress.\n\n**Amendment**  \nThe Commission is empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating Annexes VI and Annex VII in order to introduce elements of the conformity assessment procedures that become necessary in light of technical progress.\n\nWhen preparing such delegated acts, the Commission shall consult the AI Office and the stakeholders affected.",
    "### Article 43 – Paragraph 6\n\n**Text proposed by the Commission**  \nThe Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof.\n\nThe Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the availability\n\n**Amendment**  \n6.\n\nThe Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof.\n\nThe Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the",
    "**Proposal for a regulation Article 44 – paragraph 1**\n\nText proposed by the Commission:\n\nCertificates issued by notified bodies in accordance with Annex VII shall be drawn-up in an official Union language determined by the Member State in which the notified body is established or in an official Union language otherwise acceptable to the notified body.\n\nAmendment:\n\nCertificates issued by notified bodies in accordance with Annex VII shall be drawn-up in one or several official Union languages determined by the Member State in which the notified body is established or in one or several official Union languages otherwise acceptable to the notified body;",
    "**Proposal for a regulation Article 44 – paragraph 2**\n\nText proposed by the Commission:\n\nCertificates shall be valid for the period they indicate, which shall not exceed five years.\n\nOn application by the provider, the validity of a certificate may be extended for further periods, each not exceeding five years, based on a re-assessment in accordance with the applicable conformity assessment procedures.\n\nAmendment:\n\nCertificates shall be valid for the period they indicate, which shall not exceed four years.\n\nOn application by the provider, the validity of a certificate may be extended for further periods, each not exceeding four years, based on a re-assessment in accordance with the applicable conformity assessment procedures;",
    "**Proposal for a regulation Article 44 – paragraph 3**\n\nText proposed by the Commission:\n\nWhere a notified body finds that an AI system no longer meets the requirements set out in Chapter 2 of this Title, it shall, taking account of the principle of proportionality, suspend or withdraw the certificate issued or impose any restrictions on it, unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body.\n\nThe notified body shall give reasons for its decision.\n\nAmendment:\n\nWhere a notified body finds that an AI system no longer meets the requirements set out in Chapter 2 of this Title, it shall suspend or withdraw the certificate issued or impose any restrictions on it, unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body.\n\nThe notified body shall give reasons for its decision;",
    "**Proposal for a regulation Article 45 – paragraph 1**\n\nText proposed by the Commission:\n\nMember States shall ensure that an appeal procedure against decisions of the notified bodies is available to parties having a legitimate interest in that decision.\n\nAmendment:\n\nMember States shall ensure that an appeal procedure against decisions of the notified bodies, including on issued conformity certificates is available to parties having a legitimate interest in that decision;",
    "**Proposal for a regulation Article 46 – paragraph 3**\n\nText proposed by the Commission:\n\nEach notified body shall provide the other notified bodies carrying out similar conformity assessment activities covering the same artificial intelligence technologies with relevant information on issues relating to negative and, on request, positive conformity assessment results.\n\nAmendment:\n\nEach notified body shall provide the other notified bodies carrying out similar conformity assessment activities with relevant information on issues relating to negative and, on request, positive conformity assessment results;",
    "**Proposal for a regulation Article 47 – paragraph 1**\n\nText proposed by the Commission:\n\nBy way of derogation from Article 43, any market surveillance authority may authorize the placing on the market or putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets.\n\nThat authorization shall be for a limited period of time, while the necessary conformity assessment procedures are being carried out, and shall terminate once those procedures have been completed.\n\nThe completion of those procedures shall be undertaken without undue delay.\n\nAmendment:\n\nBy way of derogation from Article 43, any national supervisory authority may request a judicial authority to authorize the placing on the market or putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of the protection of life and health of persons, environmental protection and the protection of critical infrastructure.\n\nThat authorization shall be for a limited period of time, while the necessary conformity assessment procedures are being carried out, and shall terminate once those procedures have been completed.\n\nThe completion of those procedures shall be undertaken without undue delay;",
    "**Proposal for a regulation Article 47 – paragraph 2**\n\nText proposed by the Commission:\n\nThe authorization referred to in paragraph 1 shall be issued only if the market surveillance authority concludes that the high-risk AI system complies with the requirements of Chapter 2 of this Title.\n\nThe market surveillance authority shall inform the Commission and the other Member States of any authorization issued pursuant to paragraph 1.\n\nAmendment:\n\nThe authorization referred to in paragraph 1 shall be issued only if the national supervisory authority and judicial authority conclude that the high-risk AI system complies with the requirements of Chapter 2 of this Title.\n\nThe national supervisory authority shall inform the Commission, the AI office, and the other Member States of any request made and any subsequent authorization issued pursuant to paragraph 1;",
    "**Proposal for a regulation Article 47 – paragraph 3**\n\nText proposed by the Commission:\n\nWhere, within 15 calendar days of receipt of the information referred to in paragraph 2, no objection has been raised by either a Member State or the Commission in respect of an authorization issued by a market surveillance authority of a Member State in accordance with paragraph 1, that authorization shall be deemed justified.\n\nAmendment:\n\nWhere, within 15 calendar days of receipt of the information referred to in paragraph 2, no objection has been raised by either a Member State or the Commission in respect to the request of the national supervisory authority for an authorization issued by a national supervisory authority of a Member State in accordance with paragraph 1, that authorization shall be deemed justified;",
    "**Proposal for a regulation Article 47 – paragraph 4**\n\nText proposed by the Commission:\n\nWhere, within 15 calendar days of receipt of the notification referred to in paragraph 2, objections are raised by a Member State against an authorization issued by a market surveillance authority of another Member State, or where the Commission considers the authorization to be contrary to Union law or the conclusion of the Member States regarding the compliance of the system as referred to in paragraph 2 to be unfounded, the Commission shall without delay enter into consultation with the relevant Member State; the operator(s) concerned shall be consulted and have the possibility to present their views.\n\nIn view thereof, the Commission shall decide whether the authorization is justified or not.\n\nThe Commission shall address its decision to the Member State concerned and the relevant operator or operators.\n\nAmendment:\n\nWhere, within 15 calendar days of receipt of the notification referred to in paragraph 2, objections are raised by a Member State against a request issued by a national supervisory authority of another Member State, or where the Commission considers the authorization to be contrary to Union law or the conclusion of the Member States regarding the compliance of the system as referred to in paragraph 2 to be unfounded, the Commission shall without delay enter into consultation with the relevant Member State and the AI Office; the operator(s) concerned shall be consulted and have the possibility to present their views.\n\nIn view thereof, the Commission shall decide whether the authorization is justified or not.\n\nThe Commission shall address its decision to the Member State concerned and the relevant operator(s);",
    "**Proposal for a regulation Article 47 – paragraph 5**\n\nText proposed by the Commission:\n\nIf the authorization is considered unjustified, this shall be withdrawn by the market surveillance authority of the Member State concerned.\n\nAmendment:\n\nIf the authorization is considered unjustified, this shall be withdrawn by the national supervisory authority of the Member State concerned;",
    "**Proposal for a regulation Article 48 – paragraph 1**\n\nText proposed by the Commission:\n\nThe provider shall draw up a written EU declaration of conformity for each AI system and keep it at the disposal of the national competent authorities for 10 years after the AI system has been placed on the market or put into service.\n\nThe EU declaration of conformity shall identify the AI system for which it has been drawn up.\n\nA copy of the EU declaration of conformity shall be given to the relevant national competent authorities upon request.\n\nAmendment:\n\nThe provider shall draw up a written machine readable, physical or electronic EU declaration of conformity for each high-risk AI system and keep it at the disposal of the national supervisory authority and the national competent authorities for 10 years after the AI high-risk system has been placed on the market or put into service.\n\nA copy of the EU declaration of conformity shall be submitted to the national supervisory authority and the relevant national competent authorities upon request;",
    "**Proposal for a regulation Article 48 – paragraph 2**\n\nText proposed by the Commission:\n\nThe EU declaration of conformity shall state that the high-risk AI system in question meets the requirements set out in Chapter 2 of this Title.\n\nThe EU declaration of conformity shall contain the information set out in Annex V and shall be translated into an official Union language or languages required by the Member State(s) in which the high-risk AI system is made available.\n\nAmendment:\n\nThe EU declaration of conformity shall state that the high-risk AI system in question meets the requirements set out in Chapter 2 of this Title.\n\nThe EU declaration of conformity shall contain the information set out in Annex V and shall be translated into an official Union language or languages required by the Member State(s) in which the high-risk AI system is placed on the market or made available;",
    "**Proposal for a regulation Article 48 – paragraph 3**\n\nText proposed by the Commission:\n\nWhere high-risk AI systems are subject to other Union harmonization legislation which also requires an EU declaration of conformity, a single EU declaration of conformity shall be drawn up in respect of all Union legislations applicable to the high-risk AI system.\n\nThe declaration shall contain all the information required for identification of the Union harmonization legislation to which the declaration relates.\n\nAmendment:\n\nWhere high-risk AI systems are subject to other Union harmonization legislation which also requires an EU declaration of conformity, a single EU declaration of conformity may be drawn up in respect of all Union legislations applicable to the high-risk AI system.\n\nThe declaration shall contain all the information required for identification of the Union harmonization legislation to which the declaration relates;",
    "**Proposal for a regulation Article 48 – paragraph 5**\n\nText proposed by the Commission:\n\nThe Commission shall be empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating the content of the EU declaration of conformity set out in Annex V in order to introduce elements that become necessary in light of technical progress.\n\nAmendment:\n\nAfter consulting the AI Office, the Commission shall be empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating the content of the EU declaration of conformity set out in Annex V in order to introduce elements that become necessary in light of technical progress;",
    "**Proposal for a regulation Article 49 – paragraph 1**\n\nText proposed by the Commission:\n\nThe CE marking shall be affixed visibly, legibly, and indelibly for high-risk AI systems.\n\nWhere that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.\n\nAmendment:\n\nThe physical CE marking shall be affixed visibly, legibly, and indelibly for high-risk AI systems before the high-risk AI system is placed on the market.\n\nWhere that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.\n\nIt may be followed by a pictogram or any other marking indicating a special risk of use;",
    "**Proposal for a regulation Article 49 – paragraph 1 a (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nFor digital-only high-risk AI systems, a digital CE marking shall be used, only if it can be easily accessed via the interface from which the AI system is accessed or via an easily accessible machine-readable code or other electronic means.",
    "**Proposal for a regulation Article 49 – paragraph 3**\n\nText proposed by the Commission:\n\nWhere applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43.\n\nThe identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking.\n\nAmendment:\n\nWhere applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43.\n\nThe identification number of the notified body shall be affixed by the body itself or, under its instructions, by the provider’s authorised representative.\n\nThe identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking;",
    "**Proposal for a regulation Article 49 – paragraph 3 a (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nWhere high-risk AI systems are subject to other Union law which also provides for the affixing of the CE marking, the CE marking shall indicate that the high-risk AI system also fulfils the requirements of that other law.",
    "**Proposal for a regulation Article 50 – paragraph 1 – introductory part**\n\nText proposed by the Commission:\n\nThe provider shall, for a period ending 10 years after the AI system has been placed on the market or put into service, keep at the disposal of the national competent authorities:\n\nAmendment:\n\nThe provider shall, for a period ending 10 years, after the AI system has been placed on the market or put into service keep at the disposal of the national supervisory authority and the national competent authorities;",
    "**Proposal for a regulation Article 51 – paragraph 1**\n\nText proposed by the Commission:\n\nBefore placing on the market or putting into service a high-risk AI system referred to in Article 6(2), the provider or, where applicable, the authorised representative shall register that system in the EU database referred to in Article 60.\n\nAmendment:\n\nBefore placing on the market or putting into service a high-risk AI system referred to in Article 6(2), the provider or, where applicable, the authorised representative shall register that system in the EU database referred to in Article 60, in accordance with Article 60(2);",
    "**Proposal for a regulation Article 51 – paragraph 1 a (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nBefore putting into service or using a high-risk AI system in accordance with Article 6(2), the following categories of deployers shall register the use of that AI system in the EU database referred to in Article 60:\n- deployers who are public authorities or Union institutions, bodies, offices or agencies or deployers acting on their behalf;\n- deployers who are undertakings designated as a gatekeeper under Regulation (EU) 2022/1925.",
    "**Proposal for a regulation Article 51 – paragraph 1 b (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nDeployers who do not fall under subparagraph 1a.\n\nshall be entitled to voluntarily register the use of a high-risk AI system referred to in Article 6(2) in the EU database referred to in Article 60.",
    "**Proposal for a regulation Article 52 – paragraph 1**\n\nText proposed by the Commission:\n\nProviders shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use.\n\nThis obligation shall not apply to AI systems authorized by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence.\n\nAmendment:\n\nProviders shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that the AI system, the provider itself, or the user informs the natural person exposed to an AI system that they are interacting with an AI system in a timely, clear, and intelligible manner, unless this is obvious from the circumstances and the context of use.\n\nWhere appropriate and relevant, this information shall also include which functions are AI-enabled, if there is human oversight, and who is responsible for the decision-making process, as well as the existing rights and processes that, according to Union and national law, allow natural persons or their representatives to object against the application of such systems to them and to seek judicial redress against decisions taken by or harm caused by AI systems, including their right to seek an explanation.\n\nThis obligation shall not apply to AI systems authorized by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence.",
    "**Proposal for a regulation Article 52 – paragraph 2**\n\nText proposed by the Commission:\n\nUsers of an emotion recognition system or a biometric categorization system shall inform of the operation of the system the natural persons exposed thereto.\n\nThis obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences.\n\nAmendment:\n\nUsers of an emotion recognition system or a biometric categorization system which is not prohibited pursuant to Article 5 shall inform in a timely, clear, and intelligible manner of the operation of the system the natural persons exposed thereto and obtain their consent prior to the processing of their biometric and other personal data in accordance with Regulation (EU) 2016/679, Regulation (EU) 2016/1725, and Directive (EU) 2016/280, as applicable.\n\nThis obligation shall not apply to AI systems used for biometric categorization, which are permitted by law to detect, prevent, and investigate criminal offences.",
    "**Proposal for a regulation Article 52 – paragraph 3 – subparagraph 1**\n\nText proposed by the Commission:\n\nUsers of an AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful (‘deep fake’), shall disclose that the content has been artificially generated or manipulated.\n\nAmendment:\n\nUsers of an AI system that generates or manipulates text, audio, or visual content that would falsely appear to be authentic or truthful and which features depictions of people appearing to say or do things they did not say or do, without their consent (‘deep fake’), shall disclose in an appropriate, timely, clear, and visible manner that the content has been artificially generated or manipulated, as well as, whenever possible, the name of the natural or legal person that generated or manipulated it.\n\nDisclosure shall mean labeling the content in a way that informs that the content is inauthentic and that is clearly visible for the recipient of that content.\n\nTo label the content, users shall take into account the generally acknowledged state of the art and relevant harmonized standards and specifications.",
    "**Proposal for a regulation Article 52 – paragraph 3 – subparagraph 2**\n\nText proposed by the Commission:\n\nHowever, the first subparagraph shall not apply where the use is authorized by law to detect, prevent, investigate, and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties.\n\nAmendment:\n\nParagraph 3 shall not apply where the use of an AI system that generates or manipulates text, audio, or visual content is authorized by law or if it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties.\n\nWhere the content forms part of an evidently creative, satirical, artistic or fictional cinematographic, video games visuals and analogous work or program, transparency obligations set out in paragraph 3 are limited to disclosing of the existence of such generated or manipulated content in an appropriate clear and visible manner that does not hamper the display of the work and disclosing the applicable copyrights, where relevant.\n\nIt shall also not prevent law enforcement authorities from using AI systems intended to detect deep fakes and prevent, investigate, and prosecute criminal offences linked with their use",
    "**Proposal for a regulation Article 52 – paragraph 3 b (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nThe information referred to in paragraphs 1 to 3 shall be provided to the natural persons at the latest at the time of the first interaction or exposure.\n\nIt shall be accessible to vulnerable persons, such as persons with disabilities or children, complete, where relevant and appropriate, with intervention or flagging procedures for the exposed natural person taking into account the generally acknowledged state of the art and relevant harmonized standards and common specifications.",
    "**Proposal for a regulation Article 53 – paragraph 1**\n\nText proposed by the Commission:\n\nAI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan.\n\nThis shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox.\n\nAmendment:\n\nMember States shall establish at least one AI regulatory sandbox at national level, which shall be operational at the latest on the day of the entry into application of this Regulation.\n\nThis sandbox can also be established jointly with one or several other Member States;",
    "**Proposal for a regulation Article 53 – paragraph 1 d (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nAI regulatory sandboxes shall, in accordance with criteria set out in Article 53a, provide for a controlled environment that fosters innovation and facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan agreed between the prospective providers and the establishing authority;",
    "**Proposal for a regulation Article 53 – paragraph 1 e (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nThe establishment of AI regulatory sandboxes shall aim to contribute to the following objectives:\n- for the competent authorities to provide guidance to AI systems prospective providers to achieve regulatory compliance with this Regulation or where relevant other applicable Union and Member States legislation;\n- for the prospective providers to allow and facilitate the testing and development of innovative solutions related to AI systems;\n- regulatory learning in a controlled environment.",
    "**Proposal for a regulation Article 53 – paragraph 1 f (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nEstablishing authorities shall provide guidance and supervision within the sandbox with a view to identifying risks, in particular to fundamental rights, democracy, and rule of law, health and safety, and the environment, test and demonstrate mitigation measures for identified risks, and their effectiveness and ensure compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation.",
    "**Proposal for a regulation Article 53 – paragraph 1 f (new)**\n\nText proposed by the Commission: \n\nAmendment:\n\nEstablishing authorities shall provide sandbox prospective providers who develop high-risk AI systems with guidance and supervision on how to fulfill the requirements set out in this Regulation, so that the AI systems may exit the sandbox being in presumption of conformity with the specific requirements of this Regulation that were assessed within the sandbox.",
    "## Proposal for a regulation Article 53 – paragraph 2\n\nText proposed by the Commission Amendment\n\nMember States shall ensure that to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data, the national data protection authorities and those other national authorities are associated with the operation of the AI regulatory sandbox.\n\nEstablishing authorities shall ensure that, to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to personal data, the national data protection authorities, or in cases referred to in paragraph 1b the EDPS, and those other national authorities are associated with the operation of the AI regulatory sandbox and involved in the supervision of those aspects to the full extent of their respective tasks and powers.",
    "## Proposal for a regulation Article 53 – paragraph 3\n\nText proposed by the Commission Amendment\n\nThe AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities.\n\nAny significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place.\n\nThe AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities, including at regional or local level.\n\nAny significant risks to fundamental rights, democracy and rule of law, health and safety, or the environment identified during the development and testing of such AI systems shall result in immediate and adequate mitigation.\n\nCompetent authorities shall have the power to temporarily or permanently suspend the testing process, or participation in the sandbox if no effective mitigation is possible and inform the AI office of such decision.",
    "## Proposal for a regulation Article 53 – paragraph 4\n\nText proposed by the Commission Amendment\n\nParticipants in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result from the experimentation taking place in the sandbox.\n\nProspective providers in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result of the experimentation taking place in the sandbox.\n\nHowever, provided that the prospective provider(s) respect the specific plan referred to in paragraph 1c and the terms and conditions for their participation and follow in good faith the guidance given by the establishing authorities, no administrative fines shall be imposed by the authorities for infringements of this Regulation.",
    "## Proposal for a regulation Article 53 – paragraph 5\n\nText proposed by the Commission Amendment\n\nMember States’ competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board.\n\nThey shall submit annual reports to the Board and the Commission on the results from the implementation of those schemes, including good practices, lessons learnt, and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox.\n\nEstablishing authorities shall coordinate their activities and cooperate within the framework of the AI office.",
    "## Proposal for a regulation Article 53 – paragraph 5 a (new)\n\nText proposed by the Commission Amendment\n\n5 a.\n\nEstablishing authorities shall inform the AI Office of the establishment of a sandbox and may ask for support and guidance.\n\nA list of planned and existing sandboxes shall be made publicly available by the AI office and kept up to date in order to encourage more interaction in the regulatory sandboxes and transnational cooperation.",
    "## Proposal for a regulation Article 53 – paragraph 5 b (new)\n\nText proposed by the Commission Amendment\n\n5 b.\n\nEstablishing authorities shall submit to the AI office and, unless the Commission is the sole establishing authority, to the Commission, annual reports, starting one year after the establishment of the sandbox and then every year until its termination and a final report.\n\nThose reports shall provide information on the progress and results of the implementation of those sandboxes, including best practices, incidents, lessons learnt and recommendations on their setup and, where relevant, on the application and possible revision of this Regulation and other Union law supervised within the sandbox.\n\nThose annual reports or abstracts thereof shall be made available to the public, online.",
    "## Proposal for a regulation Article 53 – paragraph 6\n\nText proposed by the Commission Amendment\n\nThe modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox, and the rights and obligations of the participants shall be set out in implementing acts.\n\nThose implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2).\n\nThe Commission shall develop a single and dedicated interface containing all relevant information related to sandboxes, together with a single contact point at Union level to interact with the regulatory sandboxes and to allow stakeholders to raise enquiries with competent authorities, and to seek non-binding guidance on the conformity of innovative products, services, business models embedding AI technologies.\n\nThe Commission shall proactively coordinate with national, regional, and also local authorities, where relevant.",
    "## Proposal for a regulation Article 53 – paragraph 6 a (new)\n\nText proposed by the Commission Amendment\n\n6 a.\n\nFor the purpose of paragraph 1 and 1a, the Commission shall play a complementary role, enabling Member States to build on their expertise and, on the other hand, assisting and providing technical understanding and resources to those Member States that seek guidance on the set-up and running of these regulatory sandboxes.",
    "## Proposal for a regulation Article 53 a (new)\n\nText proposed by the Commission Amendment\n\nArticle 53 a Modalities and functioning of AI regulatory sandboxes In order to avoid fragmentation across the Union, the Commission, in consultation with the AI office, shall adopt a delegated act detailing the modalities for the establishment, development, implementation, functioning, and supervision of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox, and the rights and obligations of the participants based on the provisions set out in this Article.\n\nThe Commission is empowered to adopt delegated acts in accordance with the procedure referred to in Article 73, no later than 12 months following the entry into force of this Regulation and shall ensure that regulatory sandboxes are open to any applying prospective provider of an AI system who fulfils eligibility and selection criteria.\n\nThe criteria for accessing to the regulatory sandbox are transparent and fair and establishing authorities inform applicants of their decision within 3 months of the application; regulatory sandboxes allow broad and equal access and keep up with demand for participation; access to the AI regulatory sandboxes is free of charge for SMEs and start-ups without prejudice to exceptional costs that establishing authorities may recover in a fair and proportionate manner; regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified bodies and standardisation organisations (SMEs, start-ups, enterprises, innovators, testing and experimentation facilities, research and experimentation labs and digital innovation hubs, centers of excellence, individual researchers), in order to allow and facilitate cooperation with the public and private sector; they allow prospective providers to fulfill, in a controlled environment, the conformity assessment obligations of this Regulation or the voluntary application of the codes of conduct referred to in Article 69; procedures, processes and administrative requirements for application, selection, participation and exiting the sandbox are simple, easily intelligible, clearly communicated in order to facilitate the participation of SMEs and start-ups with limited legal and administrative capacities and are streamlined across the Union, in order to avoid fragmentation and that participation in a regulatory sandbox established by a Member State, by the Commission, or by the EDPS is mutually and uniformly recognised and carries the same legal effects across the Union; participation in the AI regulatory sandbox is limited to a period that is appropriate to the complexity and scale of the project.\n\nthe sandboxes shall facilitate the development of tools and infrastructure for testing, benchmarking, assessing and explaining dimensions of AI systems relevant to sandboxes, such as accuracy, robustness, and cybersecurity as well as minimisation of risks to fundamental rights, environment and the society at large.\n\nProspective providers in the sandboxes, in particular SMEs and start-ups, shall be facilitated access to pre-deployment services such as guidance on the implementation of this Regulation, to other value-adding services such as help with standardisation documents and certification and consultation, and to other Digital Single Market initiatives such as Testing & Experimentation Facilities, Digital Hubs, Centres of Excellence, and EU benchmarking capabilities.",
    "## Proposal for a regulation Article 54 – title\n\nText proposed by the Commission Amendment\n\nFurther processing of personal data for developing certain AI systems in the public interest in the AI regulatory sandbox Further processing of data for developing certain AI systems in the public interest in the AI regulatory sandbox",
    "## Proposal for a regulation Article 54 – paragraph 1 – introductory part\n\nText proposed by the Commission Amendment\n\nIn the AI regulatory sandbox personal data lawfully collected for other purposes shall be processed for the purposes of developing and testing certain innovative AI systems in the sandbox under the following conditions: In the AI regulatory sandbox personal data lawfully collected for other purposes may be processed solely for the purposes of developing and testing certain AI systems in the sandbox when all of the following conditions are met:",
    "## Proposal for a regulation Article 54 – paragraph 1 – point a – introductory part\n\nText proposed by the Commission Amendment\n\n(a) the innovative AI systems shall be developed for safeguarding substantial public interest in one or more of the following areas: (a) AI systems shall be developed for safeguarding substantial public interest in one or more of the following areas:\n\n(ii) public safety and public health, including disease detection, diagnosis prevention, control, and treatment; (iii) a high level of protection and improvement of the quality of the environment, protection of biodiversity, pollution as well as climate change mitigation and adaptation; (iii a) safety and resilience of transport systems, critical infrastructure, and networks.",
    "## Proposal for a regulation Article 54 – paragraph 1 – point a – point i\n\nText proposed by the Commission Amendment\n\n(i) the prevention, investigation, detection or prosecution of criminal offenses or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security, under the control and responsibility of the competent authorities.\n\nThe processing shall be based on Member State or Union law; deleted",
    "## Proposal for a regulation Article 54 – paragraph 1 – point c\n\nText proposed by the Commission Amendment\n\n(c) there are effective monitoring mechanisms to identify if any high risks to the fundamental rights of the data subjects may arise during the sandbox experimentation as well as response mechanism to promptly mitigate those risks and, where necessary, stop the processing; there are effective monitoring mechanisms to identify if any high risks to the rights and freedoms of the data subjects, as referred to in Article 35 of Regulation (EU) 2016/679 and in Article 35 of Regulation (EU) 2018/1725 may arise during the sandbox experimentation as well as response mechanism to promptly mitigate those risks and, where necessary, stop the processing.",
    "## Proposal for a regulation Article 54 – paragraph 1 – point d\n\nText proposed by the Commission Amendment\n\nany personal data to be processed in the context of the sandbox are in a functionally separate, isolated, and protected data processing environment under the control of the participants and only authorised persons have access to that data; (d) any personal data to be processed in the context of the sandbox are in a functionally separate, isolated, and protected data processing environment under the control of the prospective provider and only authorised persons have access to those data.",
    "## Proposal for a regulation Article 54 – paragraph 1 – point f\n\nText proposed by the Commission Amendment\n\n(f) any processing of personal data in the context of the sandbox do not lead to measures or decisions affecting the data subjects; any processing of personal data in the context of the sandbox do not lead to measures or decisions affecting the data subjects nor affect the application of their rights laid down in Union law on the protection of personal data.",
    "## Proposal for a regulation Article 54 – paragraph 1 – point g\n\nText proposed by the Commission Amendment\n\nany personal data processed in the context of the sandbox are deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention period; any personal data processed in the context of the sandbox are protected by means of appropriate technical and organisational measures and deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention period.",
    "## Proposal for a regulation Article 54 – paragraph 1 – point h\n\nText proposed by the Commission Amendment\n\nthe logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation in the sandbox and 1 year after its termination, solely for the purpose of and only as long as necessary for fulfilling accountability and documentation obligations under this Article or other application Union or Member States legislation; (h) the logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation in the sandbox;",
    "## Proposal for a regulation Article 54 – paragraph 1 – point j\n\nText proposed by the Commission Amendment\n\n(j) a short summary of the AI project developed in the sandbox, its objectives and expected results published on the website of the competent authorities.\n\nA short summary of the AI system developed in the sandbox, its objectives, hypotheses, and expected results, published on the website of the competent authorities;",
    "## Proposal for a regulation Article 54 a (new)\n\nText proposed by the Commission Amendment\n\nArticle 54 a Promotion of AI research and development in support of socially and environmentally beneficial outcomes Member States shall promote research and development of AI solutions which support socially and environmentally beneficial outcomes, including but not limited to development of AI-based solutions to increase accessibility for persons with disabilities, tackle socio-economic inequalities, and meet sustainability and environmental targets, by: providing relevant projects with priority access to the AI regulatory sandboxes to the extent that they fulfil the eligibility conditions; earmarking public funding, including from relevant EU funds, for AI research and development in support of socially and environmentally beneficial outcomes; organising specific awareness raising activities about the application of this Regulation, the availability of and application procedures for dedicated funding, tailored to the needs of those projects; where appropriate, establishing accessible dedicated channels, including within the sandboxes, for communication with projects to provide guidance and respond to queries about the implementation of this Regulation.\n\nMember States shall support civil society and social stakeholders to lead or participate in such projects.",
    "## Proposal for a regulation Article 55 – paragraph 1 – point a\n\nText proposed by the Commission Amendment\n\nprovide small-scale providers and start-ups with priority access to the AI regulatory sandboxes to the extent that they fulfil the eligibility conditions; provide SMEs and start-ups, established in the Union, with priority access to the AI regulatory sandboxes, to the extent that they fulfil the eligibility conditions;",
    "## Proposal for a regulation Article 55 – paragraph 1 – point b\n\nText proposed by the Commission Amendment\n\norganise specific awareness raising activities about the application of this Regulation tailored to the needs of the small-scale providers and users; organise specific awareness raising and enhanced digital skills development activities on the application of this Regulation tailored to the needs of SMEs, start-ups, and users;",
    "## Proposal for a regulation Article 55 – paragraph 1 – point c\n\nText proposed by the Commission Amendment\n\nwhere appropriate, establish a dedicated channel for communication with small-scale providers and users and other innovators to provide guidance and respond to queries about the implementation of this Regulation.\n\n(c) utilise existing dedicated channels and where appropriate, establish new dedicated channels for communication with SMEs, start-ups, users, and other innovators to provide guidance and respond to queries about the implementation of this Regulation;",
    "## Proposal for a regulation Article 55 – paragraph 2\n\nText proposed by the Commission Amendment\n\nThe specific interests and needs of the small-scale providers shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to their size and market size.\n\n2.\n\nThe specific interests and needs of the SMEs, start-ups, and users shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to development stage, their size, market size, and market demand.\n\nThe Commission shall regularly assess the certification and compliance costs for SMEs and start-ups, including through transparent consultations with SMEs, start-ups, and users and shall work with Member States to lower such costs where possible.\n\nThe Commission shall report on these findings to the European Parliament and to the Council as part of the report on the evaluation and review of this Regulation provided for in Article 84(2).",
    "## Proposal for regulation Article 56 – paragraph 1\n\nText proposed by the Commission Amendment\n\nA ‘European Artificial Intelligence Board’ (the ‘Board’) is established.\n\nThe ‘European Artificial Intelligence Office’ (the ‘AI Office’) is hereby established.\n\nThe AI Office shall be an independent body of the Union.\n\nIt shall have legal personality.",
    "## Proposal for regulation Article 56 – paragraph 2 – introductory part\n\nText proposed by the Commission Amendment\n\nThe Board shall provide advice and assistance to the Commission in order to: 2.\n\nThe AI Office shall have a secretariat and shall be adequately funded and staffed for the purpose of performing its tasks pursuant to this Regulation.",
    "## Proposal for regulation Article 56 b (new)\n\nText proposed by the Commission Amendment\n\nArticle 56 b Tasks of the AI Office The AI Office shall carry out the following tasks: support, advise and cooperate with Member States, national supervisory authorities, the Commission, and other Union institutions, bodies, offices and agencies with regard to the implementation of this Regulation; monitor and ensure the effective and consistent application of this Regulation, without prejudice to the tasks of national supervisory authorities; contribute to the coordination among national supervisory authorities responsible for the application of this Regulation, serve as a mediator in discussions about serious disagreements that may arise between competent authorities regarding the application of the Regulation coordinate joint investigations, pursuant to Article 66a; contribute to the effective cooperation with the competent authorities of third countries and with international organisations, collect and share Member States’ expertise and best practices and to assist Member States national supervisory authorities and the Commission in developing the organisational and technical expertise required for the implementation of this Regulation, including by means of facilitating the creation and maintenance of a Union pool of experts examine, on its own initiative or upon the request of its management board or the Commission, questions relating to the implementation of this Regulation and to issue opinions, recommendations or written contributions including with regard to: (i) technical specifications or existing standards; (ii) the Commission’s guidelines codes of conduct and the application thereof, in close cooperation with industry and other relevant stakeholders; the possible revision of the Regulation, the preparation of the delegated acts, and possible alignments of this Regulation with the legal acts listed in Annex II; trends, such as European global competitiveness in artificial intelligence, the uptake of artificial intelligence in the Union, the development of digital skills, and emerging systemic threats relating to artificial intelligence guidance on how this Regulation applies to the ever-evolving typology of AI value chains, in particular on the resulting implications in terms of accountability of all the entities involved issue: an annual report that includes an evaluation of the implementation of this Regulation, a review of serious incident reports as referred to in Article 62 and the functioning of the database referred to in Article 60 and recommendations to the Commission on the categorization of prohibited practices, high-risk AI systems referred to in Annex III, the codes of conduct referred to in Article 69, and the application of the general principles outlined in Article 4a.\n\nassist authorities in the establishment and development of regulatory sandboxes and to facilitate cooperation among regulatory sandboxes; organise meetings with Union agencies and governance bodies whose tasks are related to artificial intelligence and the implementation of this Regulation; organise quarterly consultations with the advisory forum, and, where appropriate, public consultations with other stakeholders, and to make the results of those consultations public on its website; promote public awareness and understanding of the benefits, risks, safeguards, and rights and obligations in relation to the use of AI systems; facilitate the development of common criteria and a shared understanding among market operators and competent authorities of the relevant concepts provided for in this Regulation; provide monitoring of foundation models and to organise a regular dialogue with the developers of foundation models with regard to their compliance as well as AI systems that make use of such AI models provide interpretive guidance on how the AI Act applies to the ever-evolving typology of AI value chains, and what the resulting implications in terms of accountability of all the entities involved will be under the different scenarios based on the generally acknowledged state of the art, including as reflected in relevant harmonised standards; provide particular oversight and monitoring and institutionalise regular dialogue with the providers of foundation models about the compliance of foundation models as well as AI systems that make use of such AI models with Article 28b of this Regulation, and about industry best practices for self-governance.",
    "Any such meeting shall be open to national supervisory authorities, notified bodies, and market surveillance authorities to attend and contribute issue and periodically update guidelines on the thresholds that qualify training a foundation model as a large training run, record and monitor known instances of large training runs, and issue an annual report on the state of play in the development, proliferation, and use of foundation models alongside policy options to address risks and opportunities specific to foundation models promote AI literacy pursuant to Article 4b.",
    "## Proposal for regulation Article 56 c (new)\n\nText proposed by the Commission Amendment\n\nArticle 56 c Accountability, independence, and transparency 1.\n\nThe AI Office shall: be accountable to the European Parliament and to the Council in accordance with this Regulation; act independently when carrying out its tasks or exercising its powers; and ensure a high level of transparency concerning its activities and develop good administrative practices in that regard.\n\nRegulation (EC) No 1049/2001 shall apply to documents held by the AI Office.",
    "#### Article - 57 a Composition of the Management Board\n\nThe management board shall be composed of the following members:\n- one representative of each Member State’s national supervisory authority;\n- one representative from the Commission;\n- one representative from the European Data Protection Supervisor (EDPS);\n- one representative from the European Union Agency for Cybersecurity (ENISA);\n- one representative from the Fundamental Rights Agency (FRA).\n\nEach representative of a national supervisory authority shall have one vote.\n\nThe representatives of the Commission, the EDPS, the ENISA, and the FRA shall not have voting rights.\n\nEach member shall have a substitute.\n\nThe appointment of members and substitute members of the management board shall take into account the need to gender balance.\n\nThe members of the management board and their substitute members shall be made public.\n\nThe members and substitutes members of the management board shall not hold conflicting positions or commercial interests with regard to any topic related to the application of this Regulation.\n\nThe rules for the meetings and voting of the management board and the appointment and removal of the Executive Director shall be laid down in the rules of procedure referred to in Article – 57 b, point (a).",
    "#### Article - 57 b Functions of the Management Board\n\nThe management board shall have the following tasks:\n- to make strategic decisions on the activities of the AI Office and to adopt its rules of procedure by a two-thirds majority of its members;\n- to implement its rules of procedure;\n- to adopt the AI Office’s single programming document as well as its annual public report and transmit both to the European Parliament, to the Council, to the Commission, and to the Court of Auditors;\n- to adopt the AI Office’s budget;\n- to appoint the executive director and, where relevant, to extend or curtail the executive director’s term of office or remove him or her from office;\n- to decide on the establishment of the AI Office’s internal structures and, where necessary, the modification of those internal structures necessary for the fulfillment of the AI Office tasks.",
    "#### Article - 57 c Chair of the Management Board\n\nThe management board shall elect a Chair and two deputy Chairs from among its voting members, by simple majority.\n\nThe term of office of the Chair and of the deputy Chairs shall be four years.\n\nThe terms of the Chair and of the deputy Chairs are renewable once.",
    "### Proposal for a Regulation Article 57 – paragraph 1\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor.\n\nOther national authorities may be invited to the meetings, where the issues discussed are of relevance for them.\n\nThe activities of the secretariat shall be managed by an executive director.\n\nThe executive director shall be accountable to the management board.\n\nWithout prejudice to the respective powers of the management board and the Union institutions, the executive director shall neither seek nor take instructions from any government or from any other body.",
    "### Proposal for a Regulation Article 57 – paragraph 2\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Board shall adopt its rules of procedure by a simple majority of its members, following the consent of the Commission.\n\nThe rules of procedure shall also contain the operational aspects related to the execution of the Board’s tasks as listed in Article 58.\n\nThe Board may establish sub-groups as appropriate for the purpose of examining specific questions.\n\nThe executive director shall attend hearings on any matter linked to the AI Office's activities and shall report on the performance of the executive director’s duties when invited to do so by the European Parliament or the Council.",
    "### Proposal for a Regulation Article 57 – paragraph 3\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Board shall be chaired by the Commission.\n\nThe Commission shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure.\n\nThe Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation.\n\nThe executive director shall represent the AI Office, including in international fora for cooperation with regard to artificial intelligence.",
    "### Proposal for a Regulation Article 57 – paragraph 4\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent.\n\nTo that end, the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies, and advisory groups.\n\nThe secretariat shall provide the management board and the advisory forum with the analytical, administrative, and logistical support necessary to fulfil the tasks of the AI Office, including by:\n- Implementing the decisions, programmes, and activities adopted by the management board;\n- preparing each year the draft single programming document, the draft budget, the annual activity report on the AI Office, the draft opinions, and the draft positions of the AI Office, and submit them to the management board;\n- Coordinating with international fora for cooperation on artificial intelligence.",
    "### Proposal for a Regulation Article 58 – paragraph 1 – introductory part\n\n**Text proposed by the Commission**  \nAmendment\n\nWhen providing advice and assistance to the Commission in the context of Article 56(2), the Board shall in particular: The advisory forum shall provide the AI Office with stakeholder input in matters relating to this Regulation, in particular with regard to the tasks set out in Article 56b point (l).",
    "### Proposal for a Regulation Article 58 – paragraph 2 (new)\n\n**Text proposed by the Commission**  \nAmendment\n\nThe membership of the advisory forum shall represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, the social partners, and academia.\n\nThe membership of the advisory forum shall be balanced with regard to commercial and non-commercial interests and, within the category of commercial interests, with regards to SMEs and other undertakings.",
    "### Proposal for a Regulation Article 58 – paragraph 3 (new)\n\n**Text proposed by the Commission**  \nAmendment\n\nThe management board shall appoint the members of the advisory forum in accordance with the selection procedure established in the AI Office’s rules of procedure and taking into account the need for transparency and in accordance with the criteria set out in paragraph 2.",
    "### Proposal for a Regulation Article 58 – paragraph 5 (new)\n\n**Text proposed by the Commission**  \nAmendment\n\nThe European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute (ETSI) shall be permanent members of the Advisory Forum.\n\nThe Joint Research Centre shall be a permanent member, without voting rights.",
    "### Proposal for a Regulation Article 58 – paragraph 6 (new)\n\n**Text proposed by the Commission**  \nAmendment\n\nThe advisory forum shall draw up its rules of procedure.\n\nIt shall elect two co-Chairs from among its members, in accordance with criteria set out in paragraph 2.\n\nThe term of office of the co-Chairs shall be two years, renewable once.",
    "### Proposal for a Regulation Article 58 – paragraph 7 (new)\n\n**Text proposed by the Commission**  \nAmendment\n\nThe advisory forum shall hold meetings at least four times a year.\n\nThe advisory forum may invite experts and other stakeholders to its meetings.\n\nThe executive director may attend, ex officio, the meetings of the advisory forum.",
    "#### Article 58 a Benchmarking\n\nThe European authorities on benchmarking referred to in Article 15 (1a) and the AI Office shall, in close cooperation with international partners, jointly develop cost-effective guidance and capabilities to measure and benchmark aspects of AI systems and AI components, and in particular of foundation models relevant to the compliance and enforcement of this Regulation based on the generally acknowledged state of the art, including as reflected in relevant harmonized standards.",
    "### Proposal for a Regulation Article 59 – paragraph 1\n\n**Text proposed by the Commission**  \nAmendment\n\nNational competent authorities shall be established or designated by each Member State for the purpose of ensuring the application and implementation of this Regulation.\n\nNational competent authorities shall be organized so as to safeguard the objectivity and impartiality of their activities and tasks.\n\nEach Member State shall designate one national supervisory authority, which shall be organized so as to safeguard the objectivity and impartiality of its activities and tasks by ...[three months after the date of entry into force of this Regulation].",
    "### Proposal for a Regulation Article 59 – paragraph 2\n\n**Text proposed by the Commission**  \nAmendment\n\nEach Member State shall designate a national supervisory authority among the national competent authorities.\n\nThe national supervisory authority shall act as notifying authority and market surveillance authority unless a Member State has organizational and administrative reasons to designate more than one authority.\n\nThe national supervisory authority shall ensure the application and implementation of this Regulation.\n\nWith regard to high-risk AI systems, related to products to which legal acts listed in Annex II apply, the competent authorities designated under those legal acts shall continue to lead the administrative procedures.\n\nHowever, to the extent a case involves aspects exclusively covered by this Regulation, those competent authorities shall be bound by the measures related to those aspects issued by the national supervisory authority designated under this Regulation.\n\nThe national supervisory authority shall act as market surveillance authority.",
    "### Proposal for a Regulation Article 59 – paragraph 3\n\n**Text proposed by the Commission**  \nAmendment\n\nMember States shall inform the Commission of their designation or designations and, where applicable, the reasons for designating more than one authority.\n\nMember States shall make publicly available and communicate to the AI Office and the Commission the national supervisory authority and information on how it can be contacted, by… [three months after the date of entry into force of this Regulation].\n\nThe national supervisory authority shall act as a single point of contact for this Regulation and should be contactable through electronic communications means.",
    "### Proposal for a Regulation Article 59 – paragraph 4\n\n**Text proposed by the Commission**  \nAmendment\n\nMember States shall ensure that national competent authorities are provided with adequate financial and human resources to fulfill their tasks under this Regulation.\n\nIn particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements.\n\nMember States shall ensure that the national supervisory authority is provided with adequate technical, financial and human resources, and infrastructure to fulfill their tasks effectively under this Regulation.\n\nIn particular, the national supervisory authority shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, personal data protection, cybersecurity, competition law, fundamental rights, health and safety risks, and knowledge of existing standards and legal requirements.\n\nMember States shall assess and, if deemed necessary, update competence and resource requirements referred to in this paragraph on an annual basis.",
    "### Proposal for a Regulation Article 59 – paragraph 4 a (new)\n\n**Text proposed by the Commission**  \nAmendment\n\n4 a.\n\nEach national supervisory authority shall exercise their powers and carry out their duties independently, impartially, and without bias.\n\nThe members of each national supervisory authority, in the performance of their tasks and exercise of their powers under this Regulation, shall neither seek nor take instructions from any body and shall refrain from any action incompatible with their duties.",
    "### Proposal for a Regulation Article 59 – paragraph 5\n\n**Text proposed by the Commission**  \nAmendment\n\nMember States shall report to the Commission on an annual basis on the status of the financial and human resources of the national competent authorities with an assessment of their adequacy.\n\nThe Commission shall transmit that information to the Board for discussion and possible recommendations.\n\nMember States shall report to the Commission on an annual basis on the status of the financial and human resources of the national supervisory authority with an assessment of their adequacy.\n\nThe Commission shall transmit that information to the AI Office for discussion and possible recommendations.",
    "### Proposal for a Regulation Article 59 – paragraph 7\n\n**Text proposed by the Commission**  \nAmendment\n\nNational competent authorities may provide guidance and advice on the implementation of this Regulation, including to small-scale providers.\n\nWhenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union legislation, the competent national authorities under that Union legislation shall be consulted, as appropriate.\n\nMember States may also establish one central contact point for communication with operators.\n\nNational supervisory authorities may provide guidance and advice on the implementation of this Regulation, including to SMEs and start-ups, taking into account the AI Office or the Commission’s guidance and advice.\n\nWhenever the national supervisory authority intends to provide guidance and advice with regard to an AI system in areas covered by other Union law, the guidance shall be drafted in consultation with the competent national authorities under that Union law, as appropriate.",
    "### Proposal for a Regulation Article 59 – paragraph 8\n\n**Text proposed by the Commission**  \nAmendment\n\nWhen Union institutions, agencies, and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision.\n\nWhen Union institutions, agencies, and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision and coordination.",
    "#### Article 59 a Cooperation Mechanism Between National Supervisory Authorities in Cases Involving Two or More Member States\n\nEach national supervisory authority shall perform its tasks and powers conferred on in accordance with this Regulation on the territory of its own Member State.\n\nIn the event of a case involving two or more national supervisory authorities, the national supervisory authority of the Member State where the infringement took place shall be considered the lead supervisory authority.\n\nIn the cases referred to in paragraph 2, the relevant supervisory authorities shall cooperate and exchange all relevant information in due time.\n\nNational supervisory authorities shall cooperate in order to reach a consensus.",
    "### Proposal for a Regulation Article 60 – paragraph 1\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Commission shall, in collaboration with the Member States, set up and maintain an EU database containing information referred to in paragraph 2 concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Article 51.\n\nThe Commission shall, in collaboration with the Member States, set up and maintain a public EU database containing information referred to in paragraphs 2 and 2a concerning high-risk AI systems referred to in Article 6 (2) which are registered in accordance with Article 51.",
    "### Proposal for a Regulation Article 60 – paragraph 2\n\n**Text proposed by the Commission**  \nAmendment\n\nThe data listed in Annex VIII shall be entered into the EU database by the providers.\n\nThe Commission shall provide them with technical and administrative support.\n\nThe data listed in Annex VIII, Section A, shall be entered into the EU database by the providers.",
    "### Proposal for a Regulation Article 60 – paragraph 2 a (new)\n\n**Text proposed by the Commission**  \nAmendment\n\n2 a.\n\nThe data listed in Annex VIII, Section B, shall be entered into the EU database by the deployers who are or who act on behalf of public authorities or Union institutions, bodies, offices, or agencies, and by deployers who are undertakings referred to in Article 51(1a) and (1b).",
    "### Proposal for a Regulation Article 60 – paragraph 3\n\n**Text proposed by the Commission**  \nAmendment\n\nInformation contained in the EU database shall be accessible to the public.\n\nInformation contained in the EU database shall be freely available to the public, user-friendly, and accessible, easily navigable and machine-readable containing structured digital data based on a standardized protocol.",
    "### Proposal for a Regulation Article 60 – paragraph 4\n\n**Text proposed by the Commission**  \nAmendment\n\nThe EU database shall contain personal data only insofar as necessary for collecting and processing information in accordance with this Regulation.\n\nThat information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider.\n\nThe EU database shall contain personal data only insofar as necessary for collecting and processing information in accordance with this Regulation.\n\nThat information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider or the deployer which is a public authority or Union institution, body, office, or agency or a deployer acting on their behalf or a deployer which is an undertaking referred to in Article 51(1a)(b) and (1b).",
    "### Proposal for a Regulation Article 60 – paragraph 5\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Commission shall be the controller of the EU database.\n\nIt shall also ensure to providers adequate technical and administrative support.\n\n5.\n\nThe Commission shall be the controller of the EU database.\n\nIt shall also ensure to providers and deployers adequate technical and administrative support.\n\nThe database shall comply with the accessibility requirements of Annex I to Directive (EU) 2019/882.",
    "### Proposal for a Regulation Article 61 – paragraph 2\n\n**Text proposed by the Commission**  \nAmendment\n\nThe post-market monitoring system shall actively and systematically collect, document, and analyze relevant data provided by users or collected through other sources on the performance of high-risk AI systems throughout their lifetime, and allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Title III, Chapter 2.\n\nThe post-market monitoring system shall actively and systematically collect, document, and analyze relevant data provided by deployers or collected through other sources on the performance of high-risk AI systems throughout their lifetime, and allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Title III, Chapter 2.\n\nWhere relevant, post-market monitoring shall include an analysis of the interaction with other AI system environments, including other devices and software taking into account the rules applicable from areas such as data protection, intellectual property rights, and competition law.",
    "### Proposal for a Regulation Article 61 – paragraph 3\n\n**Text proposed by the Commission**  \nAmendment\n\nThe post-market monitoring system shall be based on a post-market monitoring plan.\n\nThe post-market monitoring plan shall be part of the technical documentation referred to in Annex IV.\n\nThe Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan.\n\n3.\n\nThe post-market monitoring system shall be based on a post-market monitoring plan.\n\nThe post-market monitoring plan shall be part of the technical documentation referred to in Annex IV.\n\nThe Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan by [twelve months after the date of entry into force of this Regulation].",
    "### Proposal for a Regulation Article 62 – paragraph 1 – introductory part\n\n**Text proposed by the Commission**  \nAmendment\n\nProviders of high-risk AI systems placed on the Union market shall report any serious incident or any malfunctioning of those systems which constitutes a breach of obligations under Union law intended to protect fundamental rights to the market surveillance authorities of the Member States where that incident or breach occurred.\n\nProviders and, where deployers have identified a serious incident, deployers of high-risk AI systems placed on the Union market shall report any serious incident of those systems which constitutes a breach of obligations under Union law intended to protect fundamental rights to the national supervisory authority of the Member States where that incident or breach occurred.",
    "### Proposal for a Regulation Article 62 – paragraph 1 – subparagraph 1\n\n**Text proposed by the Commission**  \nAmendment\n\nSuch notification shall be made immediately after the provider has established a causal link between the AI system and the incident or malfunctioning or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the provider becomes aware of the serious incident or of the malfunctioning.\n\nSuch notification shall be made without undue delay after the provider, or, where applicable the deployer, has established a causal link between the AI system and the incident or the reasonable likelihood of such a link, and, in any event, not later than 72 hours after the provider or, where applicable, the deployer becomes aware of the serious incident.",
    "### Proposal for a Regulation Article 62 – paragraph 1 a (new)\n\n**Text proposed by the Commission**  \nAmendment\n\n1 a.\n\nUpon establishing a causal link between the AI system and the serious incident or the reasonable likelihood of such a link, providers shall take appropriate corrective actions pursuant to Article 21.",
    "### Proposal for a Regulation Article 62 – paragraph 2\n\n**Text proposed by the Commission**  \nAmendment\n\nUpon receiving a notification related to a breach of obligations under Union law intended to protect fundamental rights, the market surveillance authority shall inform the national public authorities or bodies referred to in Article 64(3).\n\nThe Commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1.\n\nThat guidance shall be issued 12 months after the entry into force of this Regulation, at the latest.\n\nUpon receiving a notification related to a breach of obligations under Union law intended to protect fundamental rights, the national supervisory authority shall inform the national public authorities or bodies referred to in Article 64(3).\n\nThe Commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1.\n\nThat guidance shall be issued by [the entry into force of this Regulation] and shall be assessed regularly.",
    "## 2 a.\n\nThe national supervisory authority shall take appropriate measures within 7 days from the date it received the notification referred to in paragraph 1.\n\nWhere the infringement takes place or is likely to take place in other Member States, the national supervisory authority shall notify the AI Office and the relevant national supervisory authorities of these Member States.\n\n---",
    "### Proposal for a regulation Article 62 – paragraph 3\n\n**Text proposed by the Commission**  \nFor high-risk AI systems referred to in point 5(b) of Annex III which are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013/36/EU and for high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU) 2017/746, the notification of serious incidents or malfunctioning shall be limited to those that constitute a breach of obligations under Union law intended to protect fundamental rights.\n\n**Amendment**  \n3.\n\nFor high-risk AI systems referred to in Annex III that are placed on the market or put into service by providers that are subject to Union legislative instruments laying down reporting obligations equivalent to those set out in this Regulation, the notification of serious incidents constituting a breach of fundamental rights under Union law shall be transferred to the national supervisory authority.\n\n---",
    "### Proposal for a regulation Article 63 – paragraph 1 – introductory part\n\n**Text proposed by the Commission**  \nRegulation (EU) 2019/1020 shall apply to AI systems covered by this Regulation.\n\nHowever, for the purpose of the effective enforcement of this Regulation:\n\n**Amendment**  \nRegulation (EU) 2019/1020 shall apply to AI systems and foundation models covered by this Regulation.\n\nHowever, for the purpose of the effective enforcement of this Regulation:\n\n---",
    "### Proposal for a regulation Article 63 – paragraph 1 – point b a (new)\n\n**Text proposed by the Commission**  \nAmendment  \n(b a) the national supervisory authorities shall act as market surveillance authorities under this Regulation and have the same powers and obligations as market surveillance authorities under Regulation (EU) 2019/1020.\n\n---",
    "### Proposal for a regulation Article 63 – paragraph 2\n\n**Text proposed by the Commission**  \nThe national supervisory authority shall report to the Commission on a regular basis the outcomes of relevant market surveillance activities.\n\nThe national supervisory authority shall report, without delay, to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules.\n\n**Amendment**  \nThe national supervisory authority shall report to the Commission and the AI Office annually the outcomes of relevant market surveillance activities.\n\nThe national supervisory authority shall report, without delay, to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules.\n\n---",
    "### Proposal for a regulation Article 63 – paragraph 3 a (new)\n\n**Text proposed by the Commission**  \nAmendment  \n3 a.\n\nFor the purpose of ensuring the effective enforcement of this Regulation, national supervisory authorities may:  \ncarry out unannounced on-site and remote inspections of high-risk AI systems;  \nacquire samples related to high-risk AI systems, including through remote inspections, to reverse-engineer the AI systems and to acquire evidence to identify non-compliance.\n\n---",
    "### Proposal for a regulation Article 63 – paragraph 5\n\n**Text proposed by the Commission**  \nFor AI systems listed in point 1(a) in so far as the systems are used for law enforcement purposes, points 6 and 7 of Annex III, Member States shall designate as market surveillance authorities for the purposes of this Regulation either the competent data protection supervisory authorities under Directive (EU) 2016/680, or Regulation 2016/679 or the national competent authorities supervising the activities of the law enforcement, immigration or asylum authorities putting into service or using those systems.\n\n**Amendment**  \n5.\n\nFor AI systems that are used for law enforcement purposes, Member States shall designate as market surveillance authorities for the purposes of this Regulation the competent data protection supervisory authorities under Directive (EU) 2016/680.\n\n---",
    "### Proposal for a regulation Article 63 – paragraph 7\n\n**Text proposed by the Commission**  \n7.\n\nMember States shall facilitate the coordination between market surveillance authorities designated under this Regulation and other relevant national authorities or bodies which supervise the application of Union harmonisation legislation listed in Annex II or other Union legislation that might be relevant for the high-risk AI systems referred to in Annex III.\n\n**Amendment**  \n7.\n\nNational supervisory authorities designated under this Regulation shall coordinate with other relevant national authorities or bodies which supervise the application of Union harmonisation law listed in Annex II or other Union law that might be relevant for the high-risk AI systems referred to in Annex III.\n\n---",
    "### Proposal for a regulation Article 64 – paragraph 1\n\n**Text proposed by the Commission**  \nAccess to data and documentation in the context of their activities, the market surveillance authorities shall be granted full access to the training, validation and testing datasets used by the provider, including through application programming interfaces (‘API’) or other appropriate technical means and tools enabling remote access.\n\n**Amendment**  \nIn the context of their activities, and upon their reasoned request the national supervisory authority shall be granted full access to the training, validation and testing datasets used by the provider, or, where relevant, the deployer, that are relevant and strictly necessary for the purpose of its request through appropriate technical means and tools.\n\n---",
    "### Proposal for a regulation Article 64 – paragraph 2\n\n**Text proposed by the Commission**  \nWhere necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2 and upon a reasoned request, the market surveillance authorities shall be granted access to the source code of the AI system.\n\n**Amendment**  \nWhere necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2, after all other reasonable ways to verify conformity including paragraph 1 have been exhausted and have proven to be insufficient, and upon a reasoned request, the national supervisory authority shall be granted access to the training and trained models of the AI system, including its relevant model parameters.\n\nAll information in line with Article 70 obtained shall be treated as confidential information and shall be subject to existing Union law on the protection of intellectual property and trade secrets and shall be deleted upon the completion of the investigation for which the information was requested.\n\n---",
    "### Proposal for a regulation Article 64 – paragraph 3\n\n**Text proposed by the Commission**  \nNational public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation when access to that documentation is necessary for the fulfilment of the competences under their mandate within the limits of their jurisdiction.\n\nThe relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request.\n\n**Amendment**  \nNational public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation when access to that documentation is necessary for the fulfilment of the competences under their mandate within the limits of their jurisdiction.\n\nThe relevant public authority or body shall inform the national supervisory authority of the Member State concerned of any such request.\n\n---",
    "### Proposal for a regulation Article 64 – paragraph 4\n\n**Text proposed by the Commission**  \nBy 3 months after the entering into force of this Regulation, each Member State shall identify the public authorities or bodies referred to in paragraph 3 and make a list publicly available on the website of the national supervisory authority.\n\nMember States shall notify the list to the Commission and all other Member States and keep the list up to date.\n\n**Amendment**  \nBy 3 months after the entering into force of this Regulation, each Member State shall identify the public authorities or bodies referred to in paragraph 3 and make a list publicly available on the website of the national supervisory authority.\n\nNational supervisory authorities shall notify the list to the Commission, the AI Office, and all other national supervisory authorities and keep the list up to date.\n\nThe Commission shall publish in a dedicated website the list of all the competent authorities designated by the Member States in accordance with this Article.\n\n---",
    "### Proposal for a regulation Article 64 – paragraph 5\n\n**Text proposed by the Commission**  \nWhere the documentation referred to in paragraph 3 is insufficient to ascertain whether a breach of obligations under Union law intended to protect fundamental rights has occurred, the public authority or body referred to paragraph 3 may make a reasoned request to the market surveillance authority to organise testing of the high-risk AI system through technical means.\n\nThe market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within reasonable time following the request.\n\n**Amendment**  \n5.\n\nWhere the documentation referred to in paragraph 3 is insufficient to ascertain whether a breach of obligations under Union law intended to protect fundamental rights has occurred, the public authority or body referred to in paragraph 3 may make a reasoned request to the national supervisory authority, to organise testing of the high-risk AI system through technical means.\n\nThe national supervisory authority shall organise the testing with the close involvement of the requesting public authority or body within reasonable time following the request.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 1\n\n**Text proposed by the Commission**  \nAI systems presenting a risk shall be understood as a product presenting a risk defined in Article 3, point 19 of Regulation (EU) 2019/1020 insofar as risks to the health or safety or to the protection of fundamental rights of persons are concerned.\n\n**Amendment**  \nAI systems presenting a risk shall be understood as an AI system having the potential to affect adversely health and safety, fundamental rights of persons in general, including in the workplace, protection of consumers, the environment, public security, or democracy or the rule of law and other public interests, that are protected by the applicable Union harmonisation law, to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the system are concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirements.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 2 – introductory part\n\n**Text proposed by the Commission**  \nWhere the market surveillance authority of a Member State has sufficient reasons to consider that an AI system presents a risk as referred to in paragraph 1, they shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation.\n\nWhen risks to the protection of fundamental rights are present, the market surveillance authority shall also inform the relevant national public authorities or bodies referred to in Article 64(3).\n\nThe relevant operators shall cooperate as necessary with the market surveillance authorities and the other national public authorities or bodies referred to in Article 64(3).\n\n**Amendment**  \nWhere the national supervisory authority of a Member State has sufficient reasons to consider that an AI system presents a risk as referred to in paragraph 1, it shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation.\n\nWhen risks to fundamental rights are present, the national supervisory authority shall also immediately inform and fully cooperate with the relevant national public authorities or bodies referred to in Article 64(3); Where there is sufficient reason to consider that an AI system exploits the vulnerabilities of vulnerable groups or violates their rights intentionally or unintentionally, the national supervisory authority shall have the duty to investigate the design goals, data inputs, model selection, implementation, and outcomes of the AI system.\n\nThe relevant operators shall cooperate as necessary with the national supervisory authority and the other national public authorities or bodies referred to in Article 64(3).\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 2 – subparagraph 1\n\n**Text proposed by the Commission**  \nWhere, in the course of that evaluation, the market surveillance authority finds that the AI system does not comply with the requirements and obligations laid down in this Regulation, it shall without delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe.\n\n**Amendment**  \nWhere, in the course of that evaluation, the national supervisory authority or, where relevant, the national public authority referred to in Article 64(3) finds that the AI system does not comply with the requirements and obligations laid down in this Regulation, it shall without delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe and in any event no later than fifteen working days or as provided for in the relevant Union harmonisation law as applicable.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 2 – subparagraph 2\n\n**Text proposed by the Commission**  \nThe market surveillance authority shall inform the relevant notified body accordingly.\n\nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the second subparagraph.\n\n**Amendment**  \nThe national supervisory authority shall inform the relevant notified body accordingly.\n\nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the second subparagraph.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 3\n\n**Text proposed by the Commission**  \nWhere the market surveillance authority considers that non-compliance is not restricted to its national territory, it shall inform the Commission and the other Member States of the results of the evaluation and of the actions which it has required the operator to take.\n\n**Amendment**  \n3.\n\nWhere the national supervisory authority considers that non-compliance is not restricted to its national territory, it shall inform the Commission, the AI Office and the national supervisory authority of the other Member States without undue delay of the results of the evaluation and of the actions which it has required the operator to take.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 5\n\n**Text proposed by the Commission**  \n5.\n\nWhere the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market, to withdraw the product from that market or to recall it.\n\nThat authority shall inform the Commission and the other Member States, without delay, of those measures.\n\n**Amendment**  \nWhere the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the national supervisory authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market or put into service, to withdraw the AI system from that market or to recall it.\n\nThat authority shall immediately inform the Commission, the AI Office and the national supervisory authority of the other Member States of those measures.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 6 – introductory part\n\n**Text proposed by the Commission**  \nThe information referred to in paragraph 5 shall include all available details, in particular the data necessary for the identification of the non-compliant AI system, the origin of the AI system, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator.\n\nIn particular, the market surveillance authorities shall indicate whether the non-compliance is due to one or more of the following:\n\n**Amendment**  \nThe information referred to in paragraph 5 shall include all available details, in particular the data necessary for the identification of the non-compliant AI system, the origin of the AI system and the supply chain, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator.\n\nIn particular, the national supervisory authority shall indicate whether the non-compliance is due to one or more of the following:\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 6 – point a\n\n**Text proposed by the Commission**  \n(a) a failure of the AI system to meet requirements set out in Title III, Chapter 2;\n\n**Amendment**  \n(a) a failure of the high-risk AI system to meet requirements set out in this Regulation;\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 7\n\n**Text proposed by the Commission**  \nThe market surveillance authorities of the Member States other than the market surveillance authority of the Member State initiating the procedure shall without delay inform the Commission and the other Member States of any measures adopted and of any additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of disagreement with the notified national measure, of their objections.\n\n**Amendment**  \nThe national supervisory authorities of the Member States other than the national supervisory authority of the Member State initiating the procedure shall without delay inform the Commission, the AI Office and the other Member States of any measures adopted and of any additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of disagreement with the notified national measure, of their objections.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 8\n\n**Text proposed by the Commission**  \nWhere, within three months of receipt of the information referred to in paragraph 5, no objection has been raised by either a Member State or the Commission in respect of a provisional measure taken by a Member State, that measure shall be deemed justified.\n\nThis is without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation (EU) 2019/1020.\n\n**Amendment**  \nWhere, within three months of receipt of the information referred to in paragraph 5, no objection has been raised by either a national supervisory authority of a Member State or the Commission in respect of a provisional measure taken by a national supervisory authority of another Member State, that measure shall be deemed justified.\n\nThis is without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation (EU) 2019/1020.\n\nThe period referred to in the first sentence of this paragraph shall be reduced to thirty days in the event of non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 9\n\n**Text proposed by the Commission**  \nThe market surveillance authorities of all Member States shall ensure that appropriate restrictive measures are taken in respect of the product concerned, such as withdrawal of the product from their market, without delay.\n\n**Amendment**  \n9.\n\nThe national supervisory authorities of all Member States shall ensure that appropriate restrictive measures are taken in respect of the AI system concerned, such as withdrawal of the AI system from their market, without delay.\n\n---",
    "### Proposal for a regulation Article 65 – paragraph 9 a (new)\n\n**Text proposed by the Commission**  \nAmendment  \n9 a.\n\nNational supervisory authorities shall annually report to the AI Office about the use of prohibited practices that occurred during that year and about the measures taken to eliminate or mitigate the risks in accordance with this Article.\n\n---",
    "### Proposal for a regulation Article 66 – paragraph 1\n\n**Text proposed by the Commission**  \nWhere, within three months of receipt of the notification referred to in Article 65(5), objections are raised by a Member State against a measure taken by another Member State, or where the Commission considers the measure to be contrary to Union law, the Commission shall without delay enter into consultation with the relevant Member State and operator or operators and shall evaluate the national measure.\n\nOn the basis of the results of that evaluation, the Commission shall decide whether the national measure is justified or not within 9 months from the notification referred to in Article 65(5) and notify such decision to the Member State concerned.\n\n**Amendment**  \nWhere, within three months of receipt of the notification referred to in Article 65(5), or 30 days in the case of non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5, objections are raised by the national supervisory authority of a Member State against a measure taken by another national supervisory authority, or where the Commission considers the measure to be contrary to Union law, the Commission shall without delay enter into consultation with the national supervisory authority of the relevant Member State and operator or operators and shall evaluate the national measure.\n\nOn the basis of the results of that evaluation, the Commission shall decide whether the national measure is justified or not within three months, or 60 days in the case of non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5, starting from the notification referred to in Article 65(5) and notify such decision to the national supervisory authority of the Member State concerned.\n\nThe Commission shall also inform all other national supervisory authorities of such decision.\n\n---",
    "### Proposal for a regulation Article 66 – paragraph 2\n\n**Text proposed by the Commission**  \nIf the national measure is considered justified, all Member States shall take the measures necessary to ensure that the non- compliant AI system is withdrawn from their market, and shall inform the Commission accordingly.\n\nIf the national measure is considered unjustified, the Member State concerned shall withdraw the measure.\n\n**Amendment**  \n2.\n\nIf the national measure is considered justified, all national supervisory authorities designated under this Regulation shall take the measures necessary to ensure that the non-compliant AI system is withdrawn from their market without delay, and shall inform the Commission and the AI Office accordingly.\n\nIf the national measure is considered unjustified, the national supervisory authority of the Member State concerned shall withdraw the measure.\n\n---",
    "### Proposal for a regulation Article 66 a (new)\n\n**Text proposed by the Commission**  \nAmendment  \nArticle 66 a Joint investigations  \nWhere a national supervisory authority has reasons to suspect that the infringement by a provider or a deployer of a high-risk AI system or foundation model to this Regulation amounts to a widespread infringement with a Union dimension, or affects or is likely to affect at least 45 million individuals in more than one Member State, that national supervisory authority shall inform the AI Office and may request the national supervisory authorities of the Member States where such infringement took place to start a joint investigation.\n\nThe AI Office shall provide central coordination to the joint investigation.\n\nInvestigation powers shall remain within the competence of the national supervisory authorities.\n\n---",
    "### Proposal for a regulation Article 67 – paragraph 1\n\n**Text proposed by the Commission**  \nWhere, having performed an evaluation under Article 65, the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to compliance with obligations under Union or national law intended to protect fundamental rights or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk, to withdraw the AI system from the market or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe.\n\n**Amendment**  \nWhere, having performed an evaluation under Article 65, in full cooperation with the relevant national public authority referred to in Article 64(3), the national supervisory authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a serious risk to the health or safety of persons, to compliance with obligations under Union or national law intended to protect fundamental rights, or the environment or the democracy and rule of law or to...",
    "## Proposal for a regulation Article 67 – paragraph 2\n\nText proposed by the Commission  \nAmendment\n\nThe provider or other relevant operators shall ensure that corrective action is taken in respect of all the AI systems concerned that they have made available on the market throughout the Union within the timeline prescribed by the market surveillance authority of the Member State referred to in paragraph 1.\n\nThe provider or other relevant operators shall ensure that corrective action is taken in respect of all the AI systems concerned that they have made available on the market throughout the Union within the timeline prescribed by the national supervisory authority of the Member State referred to in paragraph 1.",
    "## Proposal for a regulation\n\nArticle 67 – paragraph 2 a (new)\n\nText proposed by the Commission  \nAmendment\n\n2 a.\n\nWhere the provider or other relevant operators fail to take corrective action as referred to in paragraph 2 and the AI system continues to present a risk as referred to in paragraph 1, the national supervisory authority may require the relevant operator to withdraw the AI system from the market or to recall it within a reasonable period, commensurate with the nature of the risk.",
    "## Proposal for a regulation Article 67 – paragraph 3\n\nText proposed by the Commission  \nAmendment\n\nThe Member State shall immediately inform the Commission and the other Member States.\n\nThat information shall include all available details, in particular the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature and duration of the national measures taken.\n\nThe national supervisory authority shall immediately inform the Commission, the AI Office and the other national supervisory authorities.\n\nThat information shall include all available details, in particular the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature and duration of the national measures taken.",
    "## Proposal for a regulation Article 67 – paragraph 4\n\nText proposed by the Commission  \nAmendment\n\nThe Commission shall without delay enter into consultation with the Member States and the relevant operator and shall evaluate the national measures taken.\n\nOn the basis of the results of that evaluation, the Commission shall decide whether the measure is justified or not and, where necessary, propose appropriate measures.\n\nThe Commission, in consultation with the AI Office shall without delay enter into consultation with the national supervisory authorities concerned and the relevant operator and shall evaluate the national measures taken.\n\nOn the basis of the results of that evaluation, the AI Office shall decide whether the measure is justified or not and, where necessary, propose appropriate measures.",
    "## Proposal for a regulation Article 67 – paragraph 5\n\nText proposed by the Commission  \nAmendment\n\nThe Commission shall address its decision to the Member States.\n\n5.\n\nThe Commission, in consultation with the AI Office shall immediately communicate its decision to the national supervisory authorities of the Member States concerned and to the relevant operators.\n\nIt shall also inform the decision to all other national supervisory authorities.",
    "## Proposal for a regulation Article 68 – paragraph 1 – introductory part\n\nText proposed by the Commission  \nAmendment\n\nWhere the market surveillance authority of a Member State makes one of the following findings, it shall require the relevant provider to put an end to the non-compliance concerned:  \nWhere the national supervisory authority of a Member State makes one of the following findings, it shall require the relevant provider to put an end to the non-compliance concerned:",
    "## Proposal for a regulation Article 68 – paragraph 2\n\nText proposed by the Commission  \nAmendment\n\nWhere the non-compliance referred to in paragraph 1 persists, the Member State concerned shall take all appropriate measures to restrict or prohibit the high-risk AI system being made available on the market or ensure that it is recalled or withdrawn from the market.\n\n2.\n\nWhere the non-compliance referred to in paragraph 1 persists, the national supervisory authority of the Member State concerned shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or ensure that it is recalled or withdrawn from the market without delay.\n\nThe national supervisory authority of the Member State concerned shall immediately inform the AI Office of the non-compliance and the measures taken.",
    "## Proposal for a regulation Article 68 a (new)\n\nText proposed by the Commission  \nAmendment\n\nArticle 68 a  \nRight to lodge a complaint with a national supervisory authority\n\nWithout prejudice to any other administrative or judicial remedy, every natural persons or groups of natural persons shall have the right to lodge a complaint with a national supervisory authority, in particular in the Member State of his or her habitual residence, place of work or place of the alleged infringement if they consider that the AI system relating to him or her infringes this Regulation.\n\nThe national supervisory authority with which the complaint has been lodged shall inform the complainant on the progress and the outcome of the complaint including the possibility of a judicial remedy pursuant to Article 78.",
    "## Proposal for a regulation Article 68 b (new)\n\nText proposed by the Commission  \nAmendment\n\nArticle 68 b  \nRight to an effective judicial remedy against a national supervisory authority\n\nWithout prejudice to any other administrative or non-judicial remedy, each natural or legal person shall have the right to an effective judicial remedy against a legally binding decision of a national supervisory authority concerning them.\n\nWithout prejudice to any other administrative or non-judicial remedy, each natural or legal person shall have the right to a an effective judicial remedy where the national supervisory authority which is competent pursuant to Articles 59 does not handle a complaint or does not inform the data subject within three months on the progress or outcome of the complaint lodged pursuant to Article 68a.\n\nProceedings against a national supervisory authority shall be brought before the courts of the Member State where the national supervisory authority is established.\n\nWhere proceedings are brought against a decision of a national supervisory authority which was preceded by an opinion or a decision of the Commission in the union safeguard procedure, the supervisory authority shall forward that opinion or decision to the court.",
    "## Proposal for a regulation Article 68 c (new)\n\nText proposed by the Commission  \nAmendment\n\nArticle 68 c  \nA right to explanation of individual decision-making\n\nAny affected person subject to a decision which is taken by the deployer on the basis of the output from an high-risk AI system which produces legal effects or similarly significantly affects him or her in a way that they consider to adversely impact their health, safety, fundamental rights, socio-economic well-being or any other of the rights deriving from the obligations laid down in this Regulation, shall have the right to request from the deployer clear and meaningful explanation pursuant to Article 13(1) on the role of the AI system in the decision-making procedure, the main parameters of the decision taken and the related input data.\n\nParagraph 1 shall not apply to the use of AI systems for which exceptions from, or restrictions to, the obligation under paragraph 1 follow from Union or national law are provided in so far as such exception or restrictions respect the essence of the fundamental rights and freedoms and is a necessary and proportionate measure in a democratic society.\n\nThis Article shall apply without prejudice to Articles 13, 14, 15, and 22 of the Regulation 2016/679.",
    "## Proposal for a regulation Article 68 d (new)\n\nText proposed by the Commission  \nAmendment\n\nArticle 68 d  \nAmendment to Directive (EU) 2020/1828\n\nIn Annex I to Directive (EU) 2020/1828 of the European Parliament and of the Council 1a, the following point is added:  \n“(67a) Regulation xxxx/xxxx of the European Parliament and of the Council [laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts (OJ L ...)]”.\n\n1a Directive (EU) 2020/1828 of the European Parliament and of the Council of 25 November 2020 on representative actions for the protection of the collective interests of consumers and repealing Directive 2009/22/EC (OJ L 409, 4.12.2020, p. 1).",
    "## Proposal for a regulation Article 68 e (new)\n\nText proposed by the Commission  \nAmendment\n\nArticle 68 e  \nReporting of breaches and protection of reporting persons\n\nDirective (EU) 2019/1937 of the European Parliament and of the Council shall apply to the reporting of breaches of this Regulation and the protection of persons reporting such breaches.",
    "## Proposal for a regulation Article 69 – paragraph 1\n\nText proposed by the Commission  \nAmendment\n\nThe Commission and the Member States shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to AI systems other than high-risk AI systems of the requirements set out in Title III, Chapter 2 on the basis of technical specifications and solutions that are appropriate means of ensuring compliance with such requirements in light of the intended purpose of the systems.\n\nThe Commission, the AI Office and the Member States shall encourage and facilitate the drawing up of codes of conduct intended, including where they are drawn up in order to demonstrate how AI systems respect the principles set out in Article 4a and can thereby be considered trustworthy, to foster the voluntary application to AI systems other than high-risk AI systems of the requirements set out in Title III, Chapter 2 on the basis of technical specifications and solutions that are appropriate means of ensuring compliance with such requirements in light of the intended purpose of the systems.",
    "## Proposal for a regulation Article 69 – paragraph 2\n\nText proposed by the Commission  \nAmendment\n\nThe Commission and the Board shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to AI systems of requirements related for example to environmental sustainability, accessibility for persons with a disability, stakeholders participation in the design and development of the AI systems and diversity of development teams on the basis of clear objectives and key performance indicators to measure the achievement of those objectives.\n\nCodes of conduct intended to foster the voluntary compliance with the principles underpinning trustworthy AI systems, shall, in particular:\n\n- aim for a sufficient level of AI literacy among their staff and other persons dealing with the operation and use of AI systems in order to observe such principles;\n- assess to what extent their AI systems may affect vulnerable persons or groups of persons, including children, the elderly, migrants and persons with disabilities or whether measures could be put in place in order to increase accessibility, or otherwise support such persons or groups of persons;\n- consider the way in which the use of their AI systems may have an impact or can increase diversity, gender balance and equality;\n- have regard to whether their AI systems can be used in a way that, directly or indirectly, may residually or significantly reinforce existing biases or inequalities;\n- reflect on the need and relevance of having in place diverse development teams in view of securing an inclusive design of their systems;\n- give careful consideration to whether their systems can have a negative societal impact, notably concerning political institutions and democratic processes;\n- evaluate how AI systems can contribute to environmental sustainability and in particular to the Union’s commitments under the European Green Deal and the European Declaration on Digital Rights and Principles.",
    "## Proposal for a regulation Article 69 – paragraph 3\n\nText proposed by the Commission  \nAmendment\n\nCodes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organisations.\n\nCodes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems.\n\n3.\n\nCodes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders, including scientific researchers, and their representative organisations, in particular trade unions, and consumer organisations.\n\nCodes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems.\n\nProviders adopting codes of conduct will designate at least one natural person responsible for internal monitoring.",
    "## Proposal for a regulation Article 69 – paragraph 4\n\nText proposed by the Commission  \nAmendment\n\nThe Commission and the Board shall take into account the specific interests and needs of the small-scale providers and start-ups when encouraging and facilitating the drawing up of codes of conduct.\n\n4.\n\nThe Commission and the AI Office shall take into account the specific interests and needs of SMEs and start-ups when encouraging and facilitating the drawing up of codes of conduct.",
    "## Proposal for a regulation Article 70 – introductory part\n\nText proposed by the Commission  \nAmendment\n\nNational competent authorities and notified bodies involved in the application of this Regulation shall respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect, in particular:  \nThe Commission, national competent authorities and notified bodies, the AI Office and any other natural or legal person involved in the application of this Regulation shall respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect, in particular:",
    "## Proposal for a regulation Article 70 – paragraph 1\n\nText proposed by the Commission  \nAmendment\n\n(a) intellectual property rights, and confidential business information or trade secrets of a natural or legal person, including source code, except the cases referred to in Article 5 of Directive 2016/943 on the protection of undisclosed know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure apply.\n\n(a) intellectual property rights, and confidential business information or trade secrets of a natural or legal person, in accordance with the provisions of Directives 2004/48/EC and 2016/943/EC, including source code, except the cases referred to in Article 5 of Directive 2016/943 on the protection of undisclosed know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure apply;",
    "## Proposal for a regulation Article 70 – paragraph 1 a (new)\n\nText proposed by the Commission  \nAmendment\n\n1 a.\n\nThe authorities involved in the application of this Regulation pursuant to paragraph 1 shall minimise the quantity of data requested for disclosure to the data that is strictly necessary for the perceived risk and the assessment of that risk.\n\nThey shall delete the data as soon as it is no longer needed for the purpose it was requested for.\n\nThey shall put in place adequate and effective cybersecurity, technical and organisational measures to protect the security and confidentiality of the information and data obtained in carrying out their tasks and activities;",
    "## Proposal for a regulation Article 70 – paragraph 2 – introductory part\n\nText proposed by the Commission  \nAmendment\n\nWithout prejudice to paragraph 1, information exchanged on a confidential basis between the national competent authorities and between national competent authorities and the Commission shall not be disclosed without the prior consultation of the originating national competent authority and the user when high-risk AI systems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, immigration or asylum authorities, when such disclosure would jeopardise public and national security interests.\n\nWithout prejudice to paragraphs 1 and 1a, information exchanged on a confidential basis between the national competent authorities and between national competent authorities and the Commission shall not be disclosed without the prior consultation of the originating national competent authority and the deployer when high-risk AI systems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, immigration or asylum authorities, when such disclosure would jeopardise public or national security.",
    "## Proposal for a regulation Article 70 – paragraph 3\n\nText proposed by the Commission  \nAmendment\n\nParagraphs 1 and 2 shall not affect the rights and obligations of the Commission, Member States and notified bodies with regard to the exchange of information and the dissemination of warnings, nor the obligations of the parties concerned to provide information under criminal law of the Member States.\n\nParagraphs 1, 1a and 2 shall not affect the rights and obligations of the Commission, Member States and notified bodies with regard to the exchange of information and the dissemination of warnings, nor the obligations of the parties concerned to provide information under criminal law of the Member States;",
    "## Proposal for a regulation Article 70 – paragraph 4\n\nText proposed by the Commission  \nAmendment\n\nThe Commission and Member States may exchange, where necessary, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.\n\n4.\n\nThe Commission and Member States may exchange, where strictly necessary and in accordance with relevant provisions of international and trade agreements, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.",
    "## Proposal for a regulation Article 71 – paragraph 1\n\nText proposed by the Commission  \nAmendment\n\nIn compliance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties, including administrative fines, applicable to infringements of this Regulation and shall take all measures necessary to ensure that they are properly and effectively implemented.\n\nThe penalties provided for shall be effective, proportionate, and dissuasive.\n\nThey shall take into particular account the interests of small-scale providers and start-up and their economic viability.\n\nIn compliance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties, applicable to infringements of this Regulation by any operator, and shall take all measures necessary to ensure that they are properly and effectively implemented and aligned with the guidelines issued by the Commission and the AI Office pursuant to Article 82b.\n\nThe penalties provided for shall be effective, proportionate, and dissuasive.\n\nThey shall take into account the interests of SMEs and start-ups and their economic viability;",
    "## Proposal for a regulation Article 71 – paragraph 2\n\nText proposed by the Commission  \nAmendment\n\nThe Member States shall notify the Commission of those rules and of those measures and shall notify it, without delay, of any subsequent amendment affecting them.\n\nThe Member States shall notify the Commission and the Office by [ 12 months after the date of entry into force of this Regulation] of those rules and of those measures and shall notify them, without delay, of any subsequent amendment affecting them.",
    "## Proposal for a regulation Article 71 – paragraph 3 – introductory part\n\nText proposed by the Commission  \nAmendment\n\nThe following infringements shall be subject to administrative fines of up to 30 000 000 EUR or, if the offender is a company, up to 6 % of its total worldwide annual turnover for the preceding financial year, whichever is higher:  \nNon compliance with the prohibition of the artificial intelligence practices referred to in Article 5 shall be subject to administrative fines of up to 40 000 000 EUR or, if the offender is a company, up to 7 % of its total worldwide annual turnover for the preceding financial year, whichever is higher:",
    "## Proposal for a regulation Article 71 – paragraph 3 a (new)\n\nText proposed by the Commission  \nAmendment\n\n3 a. Non-compliance of the AI system with the requirements laid down in Article 10 and 13 shall be subject to administrative fines of up to EUR 20 000 000 or, if the offender is a company, up to 4% of its total worldwide annual turnover for the preceding financial year, whichever is the higher.",
    "## Proposal for a regulation Article 71 – paragraph 4\n\nText proposed by the Commission  \nAmendment\n\nThe non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10, shall be subject to administrative fines of up to 20 000 000 EUR or, if the offender is a company, up to 4 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.\n\nNon-compliance of the AI system or foundation model with any requirements or obligations under this Regulation, other than those laid down in Articles 5, 10 and 13, shall be subject to administrative fines of up to EUR 10 000 000 or, if the offender is a company, up to 2% of its total worldwide annual turnover for the preceding financial year, whichever is higher;",
    "## Proposal for a regulation Article 71 – paragraph 5\n\nText proposed by the Commission  \nAmendment\n\nThe supply of incorrect, incomplete or misleading information to notified bodies and national competent authorities in reply to a request shall be subject to administrative fines of up to 10 000 000 EUR or, if the offender is a company, up to 2 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.\n\nThe supply of incorrect, incomplete or misleading information to notified bodies and national competent authorities in reply to a request shall be subject to administrative fines of up to 5 000 000 EUR or, if the offender is a company, up to 1 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.",
    "## Proposal for a regulation Article 71 – paragraph 6 – introductory part\n\nText proposed by the Commission  \nAmendment\n\nWhen deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following:  \nFines may be imposed in addition to or instead of non-monetary measures such as orders or warnings.\n\nWhen deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following;",
    "## Proposal for a regulation Article 71 – paragraph 6 – point a\n\nText proposed by the Commission  \nAmendment\n\n(a) the nature, gravity and duration of the infringement and of its consequences;  \nthe nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AI system, as well as, where appropriate, the number of affected persons and the level of damage suffered by them;",
    "## Proposal for a regulation Article 71 – paragraph 6 – point b\n\nText proposed by the Commission  \nAmendment\n\nwhether administrative fines have been already applied by other market surveillance authorities to the same operator for the same infringement.\n\nwhether administrative fines have been already applied by other national supervisory authorities of one or more Member States to the same operator for the same infringement;",
    "**Article 71 – Paragraph 7**\n\n**Text proposed by the Commission**  \nAmendment\n\nEach Member State shall lay down rules on whether and to what extent administrative fines may be imposed on public authorities and bodies established in that Member State.\n\n7. each Member State shall lay down rules on administrative fines to be imposed on public authorities and bodies established in that Member State;",
    "**Article 72 – Paragraph 1 – Point a**\n\n**Text proposed by the Commission**  \nAmendment\n\n(a) the nature, gravity and duration of the infringement and of its consequences;  \nthe nature, gravity and duration of the infringement and of its consequences;, taking into account the purpose of the AI system concerned as well as the number of affected persons and the level of damage suffered by them, and any relevant previous infringement;",
    "**Article 72 – Paragraph 1 – Point b**\n\n**Text proposed by the Commission**  \nAmendment\n\nthe cooperation with the European Data Protection Supervisor in order to remedy the infringement and mitigate the possible adverse effects of the infringement, including compliance with any of the measures previously ordered by the European Data Protection Supervisor against the Union institution or agency or body concerned with regard to the same subject matter;  \n(b) the degree of cooperation with the European Data Protection Supervisor in order to remedy the infringement and mitigate the possible adverse effects of the infringement, including compliance with any of the measures previously ordered by the European Data Protection Supervisor against the Union institution or agency or body concerned with regard to the same subject matter;",
    "**Article 72 – Paragraph 2 – Introductory part**\n\n**Text proposed by the Commission**  \nAmendment\n\n2.\n\nThe following infringements shall be subject to administrative fines of up to 500 000 EUR:  \nNon compliance with the prohibition of the artificial intelligence practices referred to in Article 5 shall be subject to administrative fines of up to EUR 1 500 000.",
    "**Article 72 – Paragraph 3**\n\n**Text proposed by the Commission**  \nAmendment\n\nThe non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10, shall be subject to administrative fines of up to 250 000 EUR.\n\n3. the non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10, shall be subject to administrative fines of up to EUR 750 000.",
    "**Article 72 – Paragraph 6**\n\n**Text proposed by the Commission**  \nAmendment\n\n6.\n\nFunds collected by imposition of fines in this Article shall be the income of the general budget of the Union.\n\n6.\n\nFunds collected by imposition of fines in this Article shall contribute to the general budget of the Union.\n\nThe fines shall not affect the effective operation of the Union institution, body or agency fined.",
    "**Article 73 – Paragraph 2**\n\n**Text proposed by the Commission**  \nAmendment\n\n2.\n\nThe delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) shall be conferred on the Commission for an indeterminate period of time from [entering into force of the Regulation].\n\n2.\n\nThe power to adopt delegated acts referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) shall be conferred on the Commission for a period of five years from … [the date of entry into force of the Regulation].The Commission shall draw up a report in respect of the delegation of power not later than 9 months before the end of the five-year period.\n\nThe delegation of power shall be tacitly extended for periods of an identical duration, unless the European Parliament or the Council opposes such extension not later than three months before the end of each period.",
    "**Text proposed by the Commission**\n\n3 a.\n\nBefore adopting a delegated act, the Commission shall consult with the relevant institutions, the Office, the Advisory Forum and other relevant stakeholders in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making.\n\nOnce the Commission decides to draft a delegated act, it shall notify the European Parliament of this fact.\n\nThis notification does not place an obligation on the Commission to adopt the said act.",
    "**Text proposed by the Commission**\n\nArticle 82 a Better Regulation in taking into account the requirements of this Regulation pursuant to the Amendments in Articles 75, 76, 77, 78, 79, 80, 81, and 82, the Commission shall conduct an analysis and consult relevant stakeholders to determine potential gaps as well as overlaps between existing sectoral legislation and the provisions of this Regulation.",
    "**Text proposed by the Commission**\n\nArticle 82 b Guidelines from the Commission on the implementation of this Regulation The Commission shall develop, in consultation with the AI office, guidelines on the practical implementation of this Regulation, and in particular on: the application of the requirements referred to in Articles 8 - 15 and Article 28 to 28b; the prohibited practices referred to in Article 5; the practical implementation of the provisions related to substantial modification; the practical circumstances where the output of an AI system referred to in Annex III would pose a significant risk of harm to the health, safety or fundamental rights of natural persons as referred to in Article 6, paragraph 2, including examples in relation to high risk AI systems referred to in Annex III; the practical implementation of transparency obligations laid down in Article 52; the development of codes of conduct referred to in Article 69; the relationship of this Regulation with other relevant Union law, including as regards consistency in their enforcement.\n\nthe practical implementation of Article 12, Article 28b on environmental impact of foundation models and Annex IV 3(b), particularly the measurement and logging methods to enable calculations and reporting of the environmental impact of systems to comply with the obligations in this Regulation, including carbon footprint and energy efficiency, taking into account state-of-the-art methods and economies of scale.\n\nWhen issuing such guidelines, the Commission shall pay particular attention to the needs of SMEs including start-ups, local public authorities and sectors most likely to be affected by this Regulation.\n\nUpon request of the Member States or the AI Office, or on its own initiative, the Commission shall update already adopted guidelines when deemed necessary.",
    "**Article 83 – Paragraph 1 – Introductory part**\n\n**Text proposed by the Commission**  \nAmendment\n\nThis Regulation shall not apply to the AI systems which are components of the large-scale IT systems established by the legal acts listed in Annex IX that have been placed on the market or put into service before [12 months after the date of application of this Regulation referred to in Article 85(2)], unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the AI system or AI systems concerned.\n\nOperators of the AI systems which are components of the large-scale IT systems established by the legal acts listed in Annex IX that have been placed on the market or put into service prior to ... [the date of entry into force of this Regulation] shall take the necessary steps to comply with the requirements laid down in this Regulation by … [four years after the date of entry into force of this Regulation].",
    "**Article 83 – Paragraph 1 – Subparagraph 1**\n\n**Text proposed by the Commission**  \nAmendment\n\nThe requirements laid down in this Regulation shall be taken into account, where applicable, in the evaluation of each large-scale IT systems established by the legal acts listed in Annex IX to be undertaken as provided for in those respective acts.\n\nThe requirements laid down in this Regulation shall be taken into account in the evaluation of each large-scale IT systems established by the legal acts listed in Annex IX to be undertaken as provided for in those respective acts and whenever those legal acts are replaced or amended.",
    "# Amendment 688\n\n**Proposal for a regulation**  \n**Article 83 – Paragraph 2** \n\n**Text proposed by the Commission**  \nAmendment\n\nThis Regulation shall apply to the high-risk AI systems, other than the ones referred to in paragraph 1, that have been placed on the market or put into service before [date of application of this Regulation referred to in Article 85(2)], only if, from that date, those systems are subject to significant changes in their design or intended purpose.\n\n2.\n\nThis Regulation shall apply to operators of high-risk AI systems, other than the ones referred to in paragraph 1, that have been placed on the market or put into service before [date of application of this Regulation referred to in Article 85(2)], only if, from that date, those systems are subject to substantial modifications as defined in Article 3(23).\n\nIn the case of high-risk AI systems intended to be used by public authorities, providers and deployers of such systems shall take the necessary steps to comply with the requirements of the present Regulation [two years after the date of entry into force of this Regulation].",
    "**Article 84 – Paragraph 1**\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Commission shall assess the need for amendment of the list in Annex III once a year following the entry into force of this Regulation.\n\nAfter consulting the AI Office, the Commission shall assess the need for amendment of the list in Annex III, including the extension of existing area headings or addition of new area headings in that Annex the list of prohibited AI practices in Article 5, and the list of AI systems requiring additional transparency measures in Article 52 once a year following the entry into force of this Regulation and following a recommendation of the Office.\n\nthe Commission shall submit the findings of that assessment to the European Parliament and the Council.",
    "**Article 84 – Paragraph 2**\n\n**Text proposed by the Commission**  \nAmendment\n\nBy [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council.\n\nThe reports shall be made public.\n\n2.\n\nBy … [two years after the date of application of this Regulation referred to in Article 85(2)] and every two years thereafter, the Commission, together with the AI office, shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council.\n\nThe reports shall be made public.",
    "**Article 84 – Paragraph 3 – Point a**\n\n**Text proposed by the Commission**  \nAmendment\n\n(a) the status of the financial and human resources of the national competent authorities in order to effectively perform the tasks assigned to them under this Regulation;  \n(a) the status of the financial, technical and human resources of the national competent authorities in order to effectively perform the tasks assigned to them under this Regulation;",
    "**Text proposed by the Commission**\n\n3 a.\n\nBy ... [two years after the date of entry into application of this Regulation referred to in Article 85(2)] the Commission shall evaluate the functioning of the AI office, whether the office has been given sufficient powers and competences to fulfil its tasks and whether it would be relevant and needed for the proper implementation and enforcement of this Regulation to upgrade the Office and its enforcement competences and to increase its resources.\n\nThe Commission shall submit this evaluation report to the European Parliament and to the Council.",
    "**Article 84 – Paragraph 4**\n\n**Text proposed by the Commission**  \nAmendment\n\n4.\n\nWithin [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter, the Commission shall evaluate the impact and effectiveness of codes of conduct to foster the application of the requirements set out in Title III, Chapter 2 and possibly other additional requirements for AI systems other than high-risk AI systems.\n\nWithin … [one year after the date of application of this Regulation referred to in Article 85(2)] and every two years thereafter, the Commission shall evaluate the impact and effectiveness of codes of conduct to foster the application of the requirements set out in Title III, Chapter 2 and possibly other additional requirements for AI systems other than high-risk AI systems;",
    "**Article 84 – Paragraph 5**\n\n**Text proposed by the Commission**  \nAmendment\n\nFor the purpose of paragraphs 1 to 4 the Board, the Member States and national competent authorities shall provide the Commission with information on its request.\n\nFor the purpose of paragraphs 1 to 4 the AI Office, the Member States and national competent authorities shall provide the Commission with information on its request without undue delay.",
    "**Article 84 – Paragraph 6**\n\n**Text proposed by the Commission**  \nAmendment\n\nIn carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the Commission shall take into account the positions and findings of the Board, of the European Parliament, of the Council, and of other relevant bodies or sources.\n\nin carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the Commission shall take into account the positions and findings of the -AI Office of the European Parliament, of the Council, and of other relevant bodies or sources and shall consult relevant stakeholders.\n\nThe result of such consultation shall be attached to the report;",
    "**Article 84 – Paragraph 7**\n\n**Text proposed by the Commission**  \nAmendment\n\nThe Commission shall, if necessary, submit appropriate proposals to amend this Regulation, in particular taking into account developments in technology and in the light of the state of progress in the information society.\n\nthe Commission shall, if necessary, submit appropriate proposals to amend this Regulation, in particular taking into account developments in technology, the effect of AI systems on health and safety, fundamental rights, the environment, equality, and accessibility for persons with disabilities, democracy and rule of law and in the light of the state of progress in the information society.",
    "**Text proposed by the Commission**\n\n7 a.\n\nTo guide the evaluations and reviews referred to in paragraphs 1 to 4 of this Article, the Office shall undertake to develop an objective and participative methodology for the evaluation of risk level based on the criteria outlined in the relevant articles and inclusion of new systems in: the list in Annex III, including the extension of existing area headings or addition of new area headings in that Annex; the list of prohibited practices laid down in Article 5; and the list of AI systems requiring additional transparency measures pursuant to Article 52.",
    "**Text proposed by the Commission**\n\n7 b.\n\nAny amendment to this Regulation pursuant to paragraph 7 of this Article, or relevant future delegated or implementing acts, which concern sectoral legislation listed in Annex II Section B, shall take into account the regulatory specificities of each sector, and existing governance, conformity assessment and enforcement mechanisms and authorities established therein.",
    "**Text proposed by the Commission**\n\n7 c. By … [five years from the date of application of this Regulation], the Commission shall carry out an assessment of the enforcement of this Regulation and shall report it to the European Parliament, the Council and the European Economic and Social Committee, taking into account the first years of application of the Regulation.\n\nOn the basis of the findings that report shall, where appropriate, be accompanied by a proposal for amendment of this Regulation with regard to the structure of enforcement and the need for a Union agency to resolve any identified shortcomings.",
    "**Annex I**\n\n**Text proposed by the Commission**  \nAmendment\n\nARTIFICIAL INTELLIGENCE TECHNIQUES AND APPROACHES referred to in Article 3, point 1  \ndeleted\n\nMachine learning approaches, including supervised, unsupervised and reinforcement learning, using a wide variety of methods including deep learning; Logic- and knowledge-based approaches, including knowledge representation, inductive (logic) programming, knowledge bases, inference and deductive engines, (symbolic) reasoning and expert systems; Statistical approaches, Bayesian estimation, search and optimization methods.",
    "**Annex III – Paragraph 1 – Introductory part**\n\n**Text proposed by the Commission**  \nAmendment\n\nHigh-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:  \nThe AI systems specifically referred to under points 1 to 8a stand for critical use cases and are each considered to be high-risk AI systems pursuant to Article 6(2), provided that they fulfil the criteria set out in that Article:",
    "**Annex III – Paragraph 1 – Point 1 – Point a**\n\n**Text proposed by the Commission**  \nAmendment\n\n(a) AI systems intended to be used for the ‘real-time’ and ‘post’ remote biometric identification of natural persons;  \n(a) AI systems intended to be used for biometric identification of natural persons, with the exception of those mentioned in Article 5;",
    "**Text proposed by the Commission**\n\n(a a) AI systems intended to be used to make inferences about personal characteristics of natural persons on the basis of biometric or biometrics-based data, including emotion recognition systems, with the exception of those mentioned in Article 5;  \nPoint 1 shall not include AI systems intended to be used for biometric verification whose sole purpose is to confirm that a specific natural person is the person he or she claims to be.",
    "**Annex III – Paragraph 1 – Point 2 – Point a**\n\n**Text proposed by the Commission**  \nAmendment\n\n(a) AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity.\n\n(a) AI systems intended to be used as safety components in the management and operation of road, rail and air traffic unless they are regulated in harmonisation or sectoral law.",
    "**Annex III – Paragraph 1 – Point 3 – Point a**\n\n**Text proposed by the Commission**  \nAmendment\n\nAI systems intended to be used for the purpose of determining access or assigning natural persons to educational and vocational training institutions;  \nAI systems intended to be used for the purpose of determining access or materially influence decisions on admission or assigning natural persons to educational and vocational training institutions;",
    "**Annex III – Paragraph 1 – Point 3 – Point b**\n\n**Text proposed by the Commission**  \nAmendment\n\nAI systems intended to be used for the purpose of assessing students in educational and vocational training institutions and for assessing participants in tests commonly required for admission to educational institutions.\n\n(b) AI systems intended to be used for the purpose of assessing students in educational and vocational training institutions and for assessing participants in tests commonly required for admission to those institutions;\n```markdown",
    "## Annex III – paragraph 1 – point 3 – point b a (new)\n\nText proposed by the Commission Amendment\n(b a) AI systems intended to be used for the purpose of assessing the appropriate level of education for an individual and materially influencing the level of education and vocational training that individual will receive or will be able to access;",
    "## Annex III – paragraph 1 – point 4 – point a\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used for recruitment or selection of natural persons, notably for advertising vacancies, screening or filtering applications, evaluating candidates in the course of interviews or tests; AI systems intended to be used for recruitment or selection of natural persons, notably for placing targeted job advertisements screening or filtering applications, evaluating candidates in the course of interviews or tests;",
    "## Annex III – paragraph 1 – point 4 – point b\n\nText proposed by the Commission Amendment\n\nAI intended to be used for making decisions on promotion and termination of work-related contractual relationships, for task allocation and for monitoring and evaluating performance and behavior of persons in such relationships.\n\n(b) AI systems intended to be used to make or materially influence decisions affecting the initiation, promotion and termination of work-related contractual relationships, task allocation based on individual behaviour or personal traits or characteristics, or for monitoring and evaluating performance and behavior of persons in such relationships;",
    "## Annex III – paragraph 1 – point 5 – point a\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibility of natural persons for public assistance benefits and services, as well as to grant, reduce, revoke, or reclaim such benefits and services; AI systems intended to be used by or on behalf of public authorities to evaluate the eligibility of natural persons for public assistance benefits and services, including healthcare services and essential services, including but not limited to housing, electricity, heating/cooling and internet, as well as to grant, reduce, revoke, increase or reclaim such benefits and services;",
    "## Annex III – paragraph 1 – point 5 – point b\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score, with the exception of AI systems put into service by small scale providers for their own use; AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score , with the exception of AI systems used for the purpose of detecting financial fraud;",
    "## Annex III – paragraph 1 – point 5 – point c\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used to dispatch, or to establish priority in the dispatching of emergency first response services, including by firefighters and medical aid.\n\n(c) AI systems intended to evaluate and classify emergency calls by natural persons or to be used to dispatch, or to establish priority in the dispatching of emergency first response services, including by police and law enforcement, firefighters and medical aid, as well as of emergency healthcare patient triage systems;",
    "## Annex III – paragraph 1 – point 6 – point a\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by law enforcement authorities for making individual risk assessments of natural persons in order to assess the risk of a natural person for offending or reoffending or the risk for potential victims of criminal offences; deleted",
    "## Annex III – paragraph 1 – point 6 – point b\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by law enforcement authorities as polygraphs and similar tools or to detect the emotional state of a natural person; AI systems intended to be used by or on behalf of law enforcement authorities, or by Union agencies, offices or bodies in support of law enforcement authorities as polygraphs and similar tools, insofar as their use is permitted under relevant Union and national law;",
    "## Annex III – paragraph 1 – point 6 – point d\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by law enforcement authorities for evaluation of the reliability of evidence in the course of investigation or prosecution of criminal offences; AI systems intended to be used by or on behalf of law enforcement authorities, or by Union agencies, offices or bodies in support of law enforcement authorities to evaluate the reliability of evidence in the course of investigation or prosecution of criminal offences;",
    "## Annex III – paragraph 1 – point 6 – point e\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by law enforcement authorities for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups; deleted",
    "## Annex III – paragraph 1 – point 6 – point f\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by law enforcement authorities for profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of detection, investigation or prosecution of criminal offences; AI systems intended to be used by or on behalf of law enforcement authorities or by Union agencies, offices or bodies in support of law enforcement authorities for profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of detection, investigation or prosecution of criminal offences or, in the case of Union agencies, offices or bodies, as referred to in Article 3(5) of Regulation (EU) 2018/1725;",
    "## Annex III – paragraph 1 – point 6 – point g\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used for crime analytics regarding natural persons, allowing law enforcement authorities to search complex related and unrelated large data sets available in different data sources or in different data formats in order to identify unknown patterns or discover hidden relationships in the data.\n\n(g) AI systems intended to be used by or on behalf of law enforcement authorities or by Union agencies, offices or bodies in support of law enforcement authorities for crime analytics regarding natural persons, allowing law enforcement authorities to search complex related and unrelated large data sets available in different data sources or in different data formats in order to identify unknown patterns or discover hidden relationships in the data.",
    "## Annex III – paragraph 1 – point 7 – point a\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by competent public authorities as polygraphs and similar tools or to detect the emotional state of a natural person; AI systems intended to be used by or on behalf of competent public authorities or by Union agencies, offices or bodies as polygraphs and similar tools insofar as their use is permitted under relevant Union or national law",
    "## Annex III – paragraph 1 – point 7 – point b\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by competent public authorities to assess a risk, including a security risk, a risk of irregular immigration, or a health risk, posed by a natural person who intends to enter or has entered into the territory of a Member State; AI systems intended to be used by or on behalf of competent public authorities or by Union agencies, offices or bodies to assess a risk, including a security risk, a risk of irregular immigration, or a health risk, posed by a natural person who intends to enter or has entered into the territory of a Member State;",
    "## Annex III – paragraph 1 – point 7 – point c\n\nText proposed by the Commission Amendment\n\nAI systems intended to be used by competent public authorities for the verification of the authenticity of travel documents and supporting documentation of natural persons and detect non-authentic documents by checking their security features; AI systems intended to be used by or on behalf of competent public authorities or by Union agencies, offices or bodies for the verification of the authenticity of travel documents and supporting documentation of natural persons and detect non-authentic documents by checking their security features;",
    "## Annex III – paragraph 1 – point 7 – point d\n\nText proposed by the Commission Amendment\n\nAI systems intended to assist competent public authorities for the examination of applications for asylum, visa and residence permits and associated complaints with regard to the eligibility of (d) AI systems intended to be used by or on behalf of competent public authorities or by Union agencies, offices or bodies to assist competent public authorities for the examination and assessment of the veracity of evidence in relation to applications for asylum, visa and residence permits and associated complaints with regard to the eligibility of the natural persons applying for a status;",
    "## Annex III – paragraph 1 – point 7 – point d a (new)\n\nText proposed by the Commission Amendment\n(d a) AI systems intended to be used by or on behalf of competent public authorities or by Union agencies, offices or bodies in migration, asylum and border control management to monitor, surveil or process data in the context of border management activities, for the purpose of detecting, recognising or identifying natural persons;",
    "## Annex III – paragraph 1 – point 7 – point d b (new)\n\nText proposed by the Commission Amendment\n(d b) AI systems intended to be used by or on behalf of competent public authorities or by Union agencies, offices or bodies in migration, asylum and border control management for the forecasting or prediction of trends related to migration movement and border crossing;",
    "## Annex III – paragraph 1 – point 8 – point a\n\nText proposed by the Commission Amendment\n\n(a) AI systems intended to assist a judicial authority in researching and interpreting facts and the law and in (a) AI systems intended to be used by a judicial authority ot administrative body or on their behalf to assist a judicial authority or administrative body in researching and interpreting facts and the law and in applying the law to a concrete set of facts or used in a similar way in alternative dispute resolution.",
    "## Annex III – paragraph 1 – point 8 – point a a (new)\n\nText proposed by the Commission Amendment\n(a a) AI systems intended to be used for influencing the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of their vote in elections or referenda.\n\nThis does not include AI systems whose output natural persons are not directly exposed to, such as tools used to organise, optimise and structure political campaigns from an administrative and logistic point of view.",
    "## Annex III – paragraph 1 – point 8 – point a b (new)\n\nText proposed by the Commission Amendment\n(a b) AI systems intended to be used by social media platforms that have been designated as very large online platforms within the meaning of Article 33 of Regulation EU 2022/2065, in their recommender systems to recommend to the recipient of the service user-generated content available on the platform.",
    "## Annex IV – paragraph 1 – point 1 – point a\n\nText proposed by the Commission Amendment\n\nits intended purpose, the person/s developing the system the date and the version of the system; its intended purpose, the name of the provider and the version of the system reflecting its relation to previous and, where applicable, more recent, versions in the succession of revisions;",
    "## Annex IV – paragraph 1 – point 1 – point a a (new)\n\nText proposed by the Commission Amendment\n(a a) the nature of data likely or intended to be processed by the system and, in the case of personal data, the categories of natural persons and groups likely or intended to be affected;",
    "## Annex IV – paragraph 1 – point 1 – point b\n\nText proposed by the Commission Amendment\n\nhow the AI system interacts or can be used to interact with hardware or software that is not part of the AI system itself, where applicable; how the AI system can interact or can be used to interact with hardware or software, including other AI systems, that are not part of the AI system itself, where applicable;",
    "## Annex IV – paragraph 1 – point 1 – point c\n\nText proposed by the Commission Amendment\n\nthe versions of relevant software or firmware and any requirement related to version update; the versions of relevant software or firmware and, where applicable, information for the deployer on any requirement related to version update;",
    "## Annex IV – paragraph 1 – point 1 – point d\n\nText proposed by the Commission Amendment\n\nthe description of all forms in which the AI system is placed on the market or put into service; (d) the description of the various configurations and variants of the AI system which are intended to be placed on the market or put into service;",
    "## Annex IV – paragraph 1 – point 1 – point g\n\nText proposed by the Commission Amendment\n\n(g) instructions of use for the user and, where applicable installation instructions; (g) instructions of use for the deployer in accordance with Article 13(2) and (3) as well as 14(4)(e) and, where applicable installation instructions;",
    "## Annex IV – paragraph 1 – point 2 – point b\n\nText proposed by the Commission Amendment\n\n(b) the design specifications of the system, namely the general logic of the AI system and of the algorithms; the key design choices including the rationale and assumptions made, also with regard to persons or groups of persons on which the system is intended to be used; the main classification choices; what the system is a description of the architecture, design specifications, algorithms and the data structures including a decomposition of its components and interfaces, how they relate to one another and how they provide for the overall processing or logic of the AI system; the key design choices including the rationale and assumptions made, also with regard to persons or groups of persons on which the system is intended to be used; the main classification choices; what the system is designed to optimise for and the relevance of the different parameters; the decisions about any possible trade-off made regarding the technical solutions adopted to comply with the requirements set out in Title III, Chapter 2;",
    "## Annex IV – paragraph 1 – point 2 – point c\n\nText proposed by the Commission Amendment\n\nthe description of the system architecture explaining how software components build on or feed into each other and integrate into the overall processing; the computational resources used to develop, train, test and validate the AI system;\n(c) deleted",
    "## Annex IV – paragraph 1 – point 2 – point e\n\nText proposed by the Commission Amendment\n\n(e) assessment of the human oversight measures needed in accordance with Article 14, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the users, in accordance with Articles 13(3)(d); (e) assessment of the human oversight measures needed in accordance with Article 14, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, in accordance with Articles 13(3)(d);",
    "## Annex IV – paragraph 1 – point 2 – point g\n\nText proposed by the Commission Amendment\n\n(g) the validation and testing procedures used, including information about the validation and testing data used and their main characteristics; metrics used to measure accuracy, robustness, cybersecurity and compliance with other relevant requirements set out in Title III, Chapter 2 as well as potentially discriminatory impacts; test logs and all test reports dated and signed by the responsible persons, including with regard to pre-determined changes as referred to under point (f).\n\n(g) the validation and testing procedures used, including information about the validation and testing data used and their main characteristics; metrics used to measure accuracy, robustness and compliance with other relevant requirements set out in Title III, Chapter 2 as well as potentially discriminatory impacts; test logs and all test reports dated and signed by the responsible persons, including with regard to pre-determined changes as referred to under point (f).",
    "## Annex IV – paragraph 1 – point 3\n\nText proposed by the Commission Amendment\n\n3.\n\nDetailed information about the monitoring, functioning and control of the AI system, in particular with regard to: its capabilities and limitations in performance, including the degrees of accuracy for specific persons or groups of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose; the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rights and discrimination in view of the intended purpose of the AI system; the human oversight measures needed in accordance with Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the users; specifications on input data, as appropriate; 3.\n\nDetailed information about the monitoring, functioning and control of the AI system, in particular with regard to: its capabilities and limitations in performance, including the degrees of accuracy for specific persons or groups of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose; the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rights and discrimination in view of the intended purpose of the AI system; the human oversight measures needed in accordance with Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the deployers; specifications on input data, as appropriate;",
    "## Annex IV – paragraph 1 – point 6\n\nText proposed by the Commission Amendment\n\nA list of the harmonised standards applied in full or in part the references of which have been published in the Official Journal of the European Union; where no such harmonised standards have been applied, a detailed description of the solutions adopted to meet the requirements set out in Title III, Chapter 2, including a list of other relevant standards and technical specifications applied; A list of the harmonised standards applied in full or in part the references of which have been published in the Official Journal of the European Union; where no such harmonised standards have been applied, a detailed description of the solutions adopted to meet the requirements set out in Title III, Chapter 2, including a list of other relevant standards or common specifications applied;",
    "## Annex V – paragraph 1 – point 7\n\nText proposed by the Commission Amendment\n\nPlace and date of issue of the declaration, name and function of the person who signed it as well as an indication for, and on behalf of whom, that person signed, signature.\n\n7.\n\nPlace and date of issue of the declaration, signature, name and function of the person who signed it as well as an indication for, and on behalf of whom, that person signed, signature.",
    "## Annex VII – point 4 – point 4.5\n\nText proposed by the Commission Amendment\n\n4.5.\n\nWhere necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2 and upon a reasoned request, the notified body shall also be granted access to the source code of the AI system.\n\n4.5.\n\nWhere necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2, after all other reasonable ways to verify conformity have been exhausted and have proven to be insufficient, and upon a reasoned request, the notified body shall also be granted access to the training and trained models of the AI system, including its relevant parameters.\n\nSuch access shall be subject to existing Union law on the protection of intellectual property and trade secrets.\n\nThey shall take technical and organisational measures to ensure the protection of intellectual property and trade secrets.",
    "## Annex VIII – paragraph 1\n\nText proposed by the Commission Amendment\n\nThe following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to be registered in accordance with Article 51.\n\nSection A - The following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to be registered in accordance with Article 51 (1).",
    "## Annex VIII – point 5\n\nText proposed by the Commission Amendment\n\n5.\n\nDescription of the intended purpose of the AI system; A simple and comprehensible description of the intended purpose of the AI system; the components and functions supported through AI; a basic explanation of the logic of the AI system",
    "## Annex VIII – point 11\n\nText proposed by the Commission Amendment\n\nElectronic instructions for use; this information shall not be provided for high-risk AI systems in the areas of law enforcement and migration, asylum and border control management referred to in Annex III, points 1, 6 and 7. deleted",
    "## ANNEX VIII – SECTION B (new)\n\nText proposed by the Commission Amendment\nSECTION B - The following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to be registered in accordance with Article 51 (1a) (a) and (1b).\n\nthe name, address and contact details of the deployer ; the name, address and contact details of the person submitting information on behalf of the deployer ; the high risk AI system trade name and any additional unambiguous reference allowing identification and traceability of the AI system used; a) A simple and comprehensible description of the intended use of the AI system, including the specific outcomes sought through the use of the systemn, the geographic and temporal scope of application Where applicable, the categories and nature of data to be processed by the AI system; Arrangements for human oversi\n```",
    "# ght and governance\n\nWhere relevant, the bodies or natural persons responsible for decisions taken or supported by the AI system; a summary of the findings of the fundamental rights impact assessment conducted in accordance with Article 29a The URL of the entry of the AI system in the EU database by its provider A summary of the data protection impact assessment carried out in accordance with Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680 as specified in paragraph 6 of Article 29 of this Regulation, where applicable.",
    "## Proposal for a regulation Annex VIII – Section C (new)\n\nText proposed by the Commission Amendment Section C - The following information shall be provided and thereafter kept up to date with regard to foundation models to be registered in accordance with Article 28b (e).\n\n- Name, address and contact details of the provider;\n- Where submission of information is carried out by another person on behalf of the provider, the name, address and contact details of that person;\n- Name, address and contact details of the authorised representative, where applicable;\n- Trade name and any additional unambiguous reference allowing the identification of the foundation model\n- Description of the data sources used in the development of the foundational model\n- Description of the capabilities and limitations of the foundation model, including the reasonably foreseeable risks and the measures that have been taken to mitigate them as well as remaining non-mitigated risks with an explanation on the reason why they cannot be mitigated\n- Description of the training resources used by the foundation model including computing power required, training time, and other relevant information related to the size and power of the model\n- Description of the model’s performance, including on public benchmarks or state of the art industry benchmarks\n- Description of the results of relevant internal and external testing and optimisation of the model Member States in which the foundation model is or has been placed on the market, put into service or made available in the Union;\n- URL for additional information (optional).",
    "# PCAST Working Group on Generative AI Invites Public Input\n\nMAY 13, 2023\n\nThe President’s Council of Advisors on Science and Technology (PCAST) has launched a working group on generative artificial intelligence (AI) to help assess key opportunities and risks and provide input on how best to ensure that these technologies are developed and deployed as equitably, responsibly, and safely as possible.\n\nGenerative AI refers to a class of AI systems that, after being trained on large data sets, can be used to generate text, images, videos, or other outputs from a given prompt.\n\nThese technologies are advancing rapidly and have the potential to revolutionize many aspects of modern life.\n\nIn the sciences, these tools are being used to design new drugs, proteins, or materials, and promise to accelerate the pace of discovery.\n\nIn medicine, generative AI has the potential to provide advice to healthcare professionals.\n\nIn the workplace, these tools speed up the writing of computer code, help with composing presentations, and perform summarization.\n\nHowever, generative AI models can also be used for malicious purposes, such as creating disinformation, driving misinformation campaigns, and impersonating individuals.\n\nWhen used without safeguards, generative AI can stoke polarization, exacerbate biases, and inequities in society, and, more generally, threaten democracy by making it difficult for citizens to understand events in the world.\n\nFurther, generative AI systems can violate privacy and undermine intellectual property rights.\n\nAs with many advances in science and technology, a balance should be found between encouraging innovation and pursuing beneficial applications of the technology, and identifying and mitigating potential harms.\n\nU.S. government agencies are actively helping to achieve this balance.\n\nFor instance, the White House Blueprint for an AI Bill of Rights lays out core aspirational principles to guide the responsible design and deployment of AI technologies.\n\nThe National Institute of Standards and Technology (NIST) released the AI Risk Management Framework to help organizations and individuals characterize and manage the potential risks of AI technologies.\n\nCongress created the National Security Commission on AI, which studied opportunities and risks ahead and the importance of guiding the development of AI in accordance with American values around democracy and civil liberties.\n\nThe National Artificial Intelligence Initiative was launched to ensure U.S. leadership in the responsible development and deployment of trustworthy AI and support coordination of U.S. research, development, and demonstration of AI technologies across the Federal government.\n\nIn January 2023, the Congressionally mandated National AI Research Resource (NAIRR) Task Force released an implementation plan for providing computational, data, testbed, and software resources to AI researchers affiliated with U.S organizations.\n\nThe PCAST Working Group on Generative AI aims to build upon these existing efforts by identifying additional needs and opportunities and making recommendations to the President for how best to address them.\n\nOver the course of the year, PCAST will be consulting with experts from all sectors, beginning with panel discussions at our next public meeting on May 19, 2023.\n\nWe also welcome input from the public on the challenges and opportunities that should be considered, along with potential solutions, for the benefit of the Nation.",
    "## PCAST Public Session on Generative AI\n\nWe invite you to watch the public PCAST meeting on May 19.\n\nThis meeting will include two expert panel discussions on the topics of 1) AI Enabling Science and 2) AI Impacts on Society.\n\nThis meeting will be livestreamed and accessible via the PCAST website.",
    "## PCAST Invites Input from the Public on Generative AI\n\nWe also invite written submissions from the public on how to identify and promote the beneficial deployment of generative AI, and on how best to mitigate risks.\n\nSubmissions should be no more than 5 pages in length, provide actionable ideas, and not include proprietary information or any information inappropriate for public disclosure.\n\nPlease send your ideas by August 1, 2023 to  with “Generative AI” in the subject line.\n\nWe especially welcome comments addressing the following questions (please indicate in your submission which questions you are addressing):\n\n- In an era in which convincing images, audio, and text can be generated with ease on a massive scale, how can we ensure reliable access to verifiable, trustworthy information?\n\nHow can we be certain that a particular piece of media is genuinely from the claimed source?\n\n- How can we best deal with the use of AI by malicious actors to manipulate the beliefs and understanding of citizens?\n\n- What technologies, policies, and infrastructure can be developed to detect and counter AI-generated disinformation?\n\n- How can we ensure that the engagement of the public with elected representatives—a cornerstone of democracy—is not drowned out by AI-generated noise?\n\n- How can we help everyone, including our scientific, political, industrial, and educational leaders, develop the skills needed to identify AI-generated misinformation, impersonation, and manipulation?\n\nUnfortunately, we cannot commit to corresponding on all submissions, but we may invite contributors to present their ideas to the working group as part of our evolving process to develop recommendations.\n\nWe also encourage submissions to three formal federal agency requests for information/comments related to AI:\n\n- The Office of Science and Technology Policy (OSTP) Request for Information on National Priorities for Artificial Intelligence.\n\n- The Office of Science and Technology Policy (OSTP) Request for Information on how automated tools are being used to surveil, monitor, and manage workers.\n\n- The National Telecommunications and Information Administration (NTIA) request for comment on AI accountability policy.\n\nThank you in advance for your ideas.\n\nIn the months to come we may seek further public input on the topic of generative AI.",
    "## Disclaimer\n\nThis document is published by the World Economic Forum as a contribution to a project, insight area, or interaction.\n\nThe findings, interpretations, and conclusions expressed herein are a result of a collaborative process facilitated and endorsed by the World Economic Forum but whose results do not necessarily represent the views of the World Economic Forum, nor the entirety of its Members, Partners, or other stakeholders.\n\n© 2024 World Economic Forum.\n\nAll rights reserved.\n\nNo part of this publication may be reproduced or transmitted in any form or by any means, including photocopying and recording, or by any information storage and retrieval system.",
    "## Executive Summary\n\nShaping a prosperous and equitable global future with AI depends on international cooperation, jurisdictional interoperability, and inclusive governance.\n\nThe global landscape for artificial intelligence (AI) governance is complex and rapidly evolving, given the speed and breadth of technological advancements, as well as social, economic, and political influences.\n\nThis paper examines various national governance responses to AI around the world and identifies two areas of comparison:\n\n**Governance Approach**: AI governance may be focused on risk, rules, principles, or outcomes; and whether or not a national AI strategy has been outlined.\n\n**Regulatory Instruments**: AI governance may be based on existing regulations and authorities or on the development of new regulatory instruments.\n\nLending to the complexity of AI governance, the arrival of generative AI raises several governance debates, two of which are highlighted in this paper:\n\n- How to prioritize addressing current harms and potential risks of AI.\n\n- How governance should consider AI technologies on a spectrum of open-to-closed access.\n\nInternational cooperation is critical for preventing a fracturing of the global AI governance environment into non-interoperable spheres with prohibitive complexity and compliance costs.\n\nPromoting international cooperation and jurisdictional interoperability requires:\n\n- **International Coordination**: To ensure legitimacy for governance approaches, a multistakeholder approach is needed that embraces perspectives from government, civil society, academia, industry, and impacted communities, and is grounded in collaborative assessments of the socioeconomic impacts of AI.\n\n- **Compatible Standards**: To prevent substantial divergence in standards, relevant national bodies should increase compatibility efforts and collaborate with international standardization programmes.\n\nFor international standards to be widely adopted, they must reflect global participation and representation.\n\n- **Flexible Regulatory Mechanisms**: To keep pace with AI’s fast-evolving capabilities, investment in innovation and governance frameworks should be agile and adaptable.\n\nEquitable access and inclusion of the Global South in all stages of AI development, deployment, and governance is critical for innovation and for realizing the technology’s socioeconomic benefits and mitigating harms globally.\n\n- **Access to AI**: Access to AI innovations can empower jurisdictions to make progress on economic growth and development goals.\n\nGenuine access relies on overcoming structural inequalities that lead to power imbalances for the Global South, including in infrastructure, data, talent, and governance.\n\n- **Inclusion in AI**: To adequately address unique regional concerns and prevent a relegation of developing economies to mere endpoints in the AI value chain, there must be a reimagining of roles that ensure Global South actors can engage in AI innovation and governance.\n\nThe findings of this briefing paper are intended to inform actions by the different actors involved in AI governance and regulation.\n\nThese findings will also serve as a basis for future work of the World Economic Forum and its AI Governance Alliance that will raise critical considerations for resilient governance and regulation, including international cooperation, interoperability, access, and inclusion.",
    "## Introduction\n\nGenerative AI promises economic growth and social benefits but also poses challenges.\n\nThe rapid onset of generative artificial intelligence (AI) is promising socially and economically, including the potential to raise global gross domestic product (GDP) by 7% over a 10-year period.\n\nAt the same time, a range of complex challenges has emerged, such as the impact on employment, education, and the environment, as well as the potential amplification of online harms.\n\nAdditionally, there are increased demands for corporate transparency of AI systems and for clarity on data provenance and ownership.\n\nGovernance authorities worldwide face the daunting task of developing policies that harness the benefits of AI while establishing guardrails to mitigate its risks.\n\nAdditionally, they are attempting to reconcile AI governance approaches with existing legal structures such as privacy and data protection, human rights, including rights of the child, intellectual property, and online safety.",
    "## Global Developments in AI Governance\n\nThe nascent and fragmented global AI governance landscape is further complicated by challenges posed by generative AI.\n\nThe complex and fast-evolving AI governance landscape is marked by diverse national responses: risk-based, rules-based, principles-based, and outcomes-based, as delineated in Table 1.\n\nIt is important to note the difficulty of neatly attributing singular approaches to individual jurisdictions, as elements of multiple approaches can complement each other and are likely to be incorporated into hybrid responses.",
    "#### TABLE 1 Summary of AI Governance Approaches (Not Mutually Exclusive)\n\nThe recent provisional agreement reached on the EU AI Act represents the world’s first attempt at enacting comprehensive and binding AI regulation applicable to AI products and services within a risk-based and use case-driven structure.\n\nOther AI-specific regulatory efforts are also under development in various jurisdictions, such as in Canada, Brazil, Chile, and the Philippines.\n\nMeanwhile, the Indian government has weighed a non-regulatory approach, emphasizing the need to innovate, promote, and adapt to the rapid advancement of AI technologies.\n\nIn direct response to the rapid progress and widespread use of generative AI foundation models, China enacted regulations related to the use of generative AI.\n\nThe EU AI Act also incorporates specific obligations for foundation models underpinning general-purpose AI (GPAI) systems.\n\nAdditional countries such as Singapore, Malaysia, Saudi Arabia, Japan, and Rwanda are responding to the transformative potential of AI by developing national polices that outline governance intentions and explore a range of regulatory instruments, ranging from hard laws and mandatory compliance rules to soft guidance and voluntary best practices.\n\nLending to the intricacy of the governance landscape, regulatory responses are spread across a matrix of sector-specific considerations and cross-sectorial requirements.\n\nThe recently issued US Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence directs federal agencies to develop new standards and includes sector-specific guidance driven by risk management.\n\nIn addition to government regulatory efforts, there is a growing awareness of the importance of industry-responsible AI governance practices in safeguarding societal interests.\n\nFor example, in response to the US Executive Order, the National Institute of Standards and Technology (NIST) has established the AI Safety Consortium, which intends to collaborate closely with industry, among other stakeholders, to inform risk management best practices.",
    "## Evolving AI Governance Tensions\n\nThe existence of a spectrum of AI governance approaches considers debates arising from new and amplified challenges introduced by the scale, power, and design of generative AI technologies.\n\nTable 2 provides a snapshot of two prominent debates taking place with a sample of divergent positions regarding the nature of risks and access to AI models.\n\nOther emerging tensions include how generative AI will impact employment, its intersection with copyright protections, data transparency requirements, allocation of responsibility among actors within the generative AI life cycle, and addressing misinformation and disinformation concerns amplified by generative AI.\n\nMany of these emerging tensions have their roots in data governance issues, such as privacy concerns, data protection, embedded biases, identity, and security challenges from the use of data to train generative AI systems, and the resultant data created by generative AI systems.\n\nThere is a need to re-examine existing legal frameworks that provide legal assurance to the ownership of AI-generated digital identities.",
    "## International Cooperation and Jurisdictional Interoperability\n\nInternational cooperation to facilitate jurisdictional interoperability is vital to ensure global cohesion and trust in AI.\n\nInternational cooperation is critical to ensure societal trust in generative AI and to prevent a fracturing of the global AI governance environment into non-interoperable spheres with prohibitive complexity and compliance costs.\n\nFacilitating jurisdictional interoperability requires international coordination, compatible standards, and flexible regulatory mechanisms.\n\nFor example, the US has taken the initiative to enable cooperation with Europe through the US-EU Trade and Technology Council, while Chile, New Zealand, and Singapore have signed a Digital Economy Partnership Agreement.\n\nIndicative of a growing consensus on the need for AI regulation, delegate nations at the 2023 UK AI Safety Summit signed the Bletchley Declaration with a commitment to establish a shared understanding of AI opportunities and risks.",
    "### International Coordination and Collaboration\n\nTo ensure enduring legitimacy for governance proposals, global regulatory interoperability must adopt a multistakeholder approach that embraces a diversity of perspectives from government, civil society, academia, industry, and impacted communities.\n\nEffective grounding of efforts in a comprehensive assessment of the socioeconomic impacts of AI and the efficacy of regulatory responses demands collaboration in identifying and prioritizing critical issues.\n\nExamples of international coordination efforts in drafting AI policy guidance include UNICEF’s 2021 Policy Guidance on AI for Children and INTERPOL’s 2023 Toolkit for Responsible AI Innovation in Law Enforcement developed in collaboration with the United Nations Interregional Crime and Justice Research Institute (UNICRI).\n\nEfforts like the Organisation for Economic Co-operation and Development’s OECD.AI to map interoperability gaps between national governance frameworks are crucial to reducing conflicting regulatory requirements and establishing predictability and clarity for companies and people.\n\nAt the intergovernmental level, coordination efforts to address international AI governance matters are currently under way at the Council of Europe’s Committee on AI, OECD’s Working Party on Artificial Intelligence Governance, the African Union High-Level Panel on Emerging Technologies (APET), the Association of Southeast Asian Nations (ASEAN) workshops and the Guide on AI Governance and Ethics, the G7 and the G20, among others.\n\nIn May 2023, G7 leaders published a report on the Hiroshima Process on Generative AI to study the rapidly evolving technology and help guide discussions on common policy priorities related to generative AI.\n\nAdditionally, international efforts like the United Nations High-Level Advisory Body on AI and the World Economic Forum’s AI Governance Alliance are playing a critical role in coordinating multistakeholder dialogue and knowledge sharing to inform governance interoperability conversations.",
    "### Compatible AI Standards\n\nCreating the capacity and space for broader participation in the AI standards-making process is needed.\n\nGoverning bodies around the world are turning to standards as a method for governing AI.\n\nThe British Standards Institution launched an AI Standards Hub aimed at helping AI organizations in the UK understand, develop, and benefit from international AI standards.\n\nThe European Telecommunications Standards Institute (ETSI) and the European Committee for Electrotechnical Standardization (CENELEC) have published the European Standardization agenda that includes the adoption of external international standards already available or under development, in part stimulated by the proposed EU AI Regulation’s framework for standards.\n\nIn the US, NIST has developed an AI Risk Management Framework to support technical standards for trustworthy AI.\n\nDespite criticisms regarding the instrumentalization of standards to shift regulatory powers from governments to private actors, they are increasingly recognized as an important tool in international trade, investment, competitive advantage, and national values.\n\nThere is concern that substantial divergences in approaches to setting AI standards threaten a further fragmentation of the international AI governance landscape, lending to downstream social, economic, and political implications internationally.\n\nInternational standardization programmes are being developed by the Joint Technical Committee of the International Organization for Standardization and the International Electrotechnical Commission (ISO/IEC JTC1/SC42) as well as by the Institute of Electrical and Electronic Engineers Standards Association (IEEE SA).\n\nFor their part, the US, EU, and China have signalled commitments to undertake best efforts to align with internationally recognized standardization efforts.\n\nDespite these signals, there is no guarantee that every country will follow these standards, especially if there is concern that their development has not been inclusive of local interests.\n\nCreating the capacity and space for broader participation in the standards-making process is thus needed.",
    "### Flexible Regulatory Mechanisms\n\nThe fast-evolving capabilities of generative AI require investment in innovation and governance frameworks that are agile and adaptable.\n\nThis includes ongoing assessment of opportunity and risk emanating from applied practice and feedback from those directly impacted by the technology.\n\nFlexible regulatory mechanisms, beyond statutory instruments, are needed to account for societal implications and regulatory challenges that will emerge as generative AI technologies continue to advance and be adopted across various cultures and sectors.\n\nFor example, Singapore, the United Arab Emirates, Brazil, the UK, the EU, and Mauritius have pioneered “regulatory sandboxes” that allow organizations to test AI in a safe and controlled environment.\n\nSuch policy innovations must be coupled with additional efforts to clarify regulatory intent and the associated requirements for compliance.\n\nFor flexible mechanisms to scale, supervisory authorities will need to consider how they provide industry participants confidence to participate and help establish agile best practice approaches while addressing the fear of regulatory capture through participation.",
    "## Enabling Equitable Access and Inclusive Global AI Governance\n\nThe Global South’s role in AI development and governance is critical to shaping a responsible future.\n\nThe need for diversity and more equitably deployed generative AI systems is of significant global concern.\n\nInclusive governance that consults with diverse stakeholders, including from developing countries, can help surface challenges, priorities, and opportunities to make generative AI technologies work better for everyone and address widening inequalities associated with the pre-existing digital divide.\n\nBy ensuring the inclusion of underrepresented countries from Sub-Saharan Africa, the Caribbean and Latin America, the South Pacific, as well as some from Central and South Asia (collectively referred to as the Global South) in international discussions on AI governance, a more diverse and equitable deployment of generative AI systems and compatibility of governance regimes can be achieved.",
    "### Structural Limitations and Power Imbalances\n\nThe Global South’s priorities in areas such as healthcare, education, or food security often force trade-offs, hampering investments in long-term digital infrastructure.\n\nHowever, access to AI innovations can empower countries to make progress on economic growth and development goals where needs are greatest – transforming health services, improving education quality, increasing agricultural productivity, etc.\n\nto improve lives.\n\nSuccessfully deploying generative AI solutions at scale relies on overcoming several structural inequalities lending to power imbalances as detailed in Table 3.",
    "#### TABLE 3 Sources of Global Disparities and Exclusion in Generative AI (Non-Exhaustive)\n\nAccess to education and technical expertise\n\nStudents from the Global South often do not have access to the education and mentorship required to develop emerging technologies, such as generative AI.\n\nThis can contribute to a lack of global representation among generative AI researchers and engineers, with potential downstream effects of unintended algorithmic biases and discrimination in generative AI products.\n\nLocal access to high-quality education and generative AI expertise is key to creating a sustainable talent pipeline and widening the locations where generative AI research is done.\n\nFurther, more researchers and engineers from the Global South will lead to more diversity in generative AI ideas, enhanced innovation, and increased opportunities for local experts to build and wield generative AI with local issues in mind.\n\nGovernance Institutional Capacity and Policy Development\n\nEconomically disadvantaged countries often lack the financial, political, and technical resources needed to develop effective AI governance policies, and regulators within these jurisdictions remain severely underfunded.\n\nAccording to a 2023 study of 193 countries, 114 countries, almost exclusively from the Global South, lack any national AI strategy.\n\nDisparity in AI governance capabilities can reinforce existing power imbalances and hinder global participation in the benefits of generative AI.\n\nThe absence of governance policies for data and AI can lead to privacy violations, potential misuse of AI, and a missed opportunity to harness AI for positive socioeconomic development, among others.\n\nFurther, underfunded regulatory institutions may be ill-equipped to address the ethical, legal, and social implications of AI.",
    "### Inclusion of the Global South in AI Governance\n\nIn addition to equitable access, inclusion of the Global South in all stages of the development and governance of AI is essential to prevent a reinforced power imbalance whereby developing economies are relegated to mere endpoints in the global generative AI value chain, either as extractive digital workers or as consumers of the technology.\n\nThough AI policy and governance frameworks are predominantly being developed in China, the EU, and North America (46%), compared to 5.7% in Latin America and 2.4% in Africa, it is important to recognize the significant activities of different national bodies such as Colombia, Brazil, Mauritius, Rwanda, Sierra Leone, Viet Nam, and Indonesia, the recently introduced Digital Forum of Small States (FOSS) chaired by Singapore, as well as the emergence of AI research and industry ecosystems out of the Global South.\n\nThe absence of historical and geopolitical contexts of power and exploitation from dominant AI governance debates underscores the necessity for diverse voices and multistakeholder perspectives.\n\nThe significant differences between some concerns of the Global South and those elevated within more dominant discourses of AI risks warrant a restructuring of AI governance processes, moving beyond current frameworks of inclusion.\n\nTo adequately address regional concerns there must be a reimagining of roles that ensure Global South actors can engage in co-governance.",
    "## Conclusion\n\nThe global governance landscape for AI is complex, fragmented, and rapidly evolving, with new and amplified challenges presented by the advent of generative AI.\n\nTo effectively harness the global opportunities of generative AI and address its associated risks, there is a critical need for international cooperation and jurisdictional interoperability.\n\nCoordinated multistakeholder efforts, including government, civil society, academia, industry, and impacted communities, are essential.\n\nAs humans drive the development of this technology and policy, responses must be developed to increase equity and inclusion in the development of AI, including with the countries of the Global South.\n\nIt is up to stakeholders to take concrete action on access and inclusion.\n\nThe World Economic Forum and its AI Governance Alliance are committed to driving this change, using its unique platform as a catalyst to convene diverse voices from around the world and urge them to act on vital issues, promote shared learnings, and advance novel solutions.",
    "## Contributors\n\nThis paper is a combined effort based on numerous interviews, discussions, workshops, and research.\n\nThe opinions expressed herein do not necessarily reflect the views of the individuals or organizations involved in the project or listed below.\n\nSincere thanks are extended to those who contributed their insights via interviews and workshops, as well as those not captured below.",
    "### AI Governance Alliance Project Fellows\n\n- Arnab Chakraborty, Senior Managing Director, Global Responsible AI Lead, Accenture\n- Rafi Lazerson, GenAI Policy Manager, Accenture\n- Valerie Morignat, Global Responsible AI Lead for Life Sciences, Accenture\n- Manal Siddiqui, Responsible AI Manager, Accenture\n- Ali Shah, Global Principal Director for Responsible AI, Accenture\n- Kathryn White, Global Principal Director for Innovation Incubation, Accenture",
    "## Acknowledgements\n\nSincere appreciation is extended to the following working group members, who spent numerous hours providing critical input and feedback to the drafts.\n\nTheir diverse insights are fundamental to the success of this work.",
    "- Lovisa Afzelius, Chief Executive Officer, Apriori Bio\n- Hassan Al-Darbesti, Adviser to the Minister and Director, International Cooperation Department, Ministry of Information and Communication Technology (ICT) of Qatar\n- Uthman Ali, Senior Product Analyst, AI Ethics SME, BP\n- Erich David Andersen, General Counsel; Head, Corporate Affairs, TikTok\n- Jason Anderson, General Counsel, Vice-President and Corporate Secretary, DataStax\n- Norberto Andrade, Professor and Academic Director, IE University\n- Richard Benjamins, Chief AI and Data Strategist, Telefonica\n- Saqr Binghalib, Executive Director, Artificial Intelligence, Digital Economy and Remote Work Applications Office, United Arab Emirates\n- Anu Bradford, Professor of Law, Columbia Law School\n- Michal Brand-Gold, Vice-President General Counsel, Activefence\n- Adrian Brown, Executive Director, Center for Public Impact\n- Winter Casey, Senior Director, SAP\n- Simon Chesterman, Senior Director of AI Governance, AI Singapore, National University of Singapore\n- Melinda Claybaugh, Director, Privacy Policy, Meta Platforms\n- Amanda Craig, Senior Director, Responsible AI Public Policy, Microsoft\n- Renée Cummings, Data Science Professor and Data Activist in Residence, University of Virginia\n- Nicholas Dirks, President and Chief Executive Officer, The New York Academy of Sciences\n- Nita Farahany, Robinson O. Everett Professor of Law and Philosophy; Director, Duke Science and Society, Duke University\n- Max Fenkell, Vice-President, Government Relations, Scale AI\n- Kay Firth-Butterfield, Senior Research Fellow, University of Texas at Austin\n- Katharina Frey, Deputy Head, Digitalisation Division, Federal Department of Foreign Affairs, Federal Department of Foreign Affairs (FDFA) of Switzerland\n- Alice Friend, Head, Artificial Intelligence and Emerging Tech Policy, Google\n- Tony Gaffney, Chief Executive Officer, Vector Institute\n- Eugenio Garcia, Deputy Consul-General, San Francisco, Ministry of Foreign Affairs of Brazil\n- Urs Gasser, Dean, TUM School of Social Sciences and Technology, Technical University of Munich\n- Avi Gesser, Partner, Debevoise & Plimpton\n- Debjani Ghosh, President, National Association of Software and Services Companies (NASSCOM)\n- Danielle Gilliam-Moore, Director, Global Public Policy, Salesforce\n- Brian Green, Director, Technology Ethics, Santa Clara University\n- Samuel Gregory, Executive Director, WITNESS\n- Koiti Hasida, Director, Artificial Intelligence in Society Research Group, RIKEN Center for Advanced Intelligence Project, RIKEN\n- Dan Hendrycks, Executive Director, Center for AI Safety\n- Benjamin Hughes, Senior Vice-President, Artificial Intelligence (AI) & Real World Data (RWD), IQVIA\n- Dan Jermyn, Chief Decision Scientist, Commonwealth Bank of Australia\n- Jeff Jianfeng Cao, Senior Research, Tencent Research Institute\n- Sam Kaplan, Assistant General Counsel, Public Policy & Government Affairs, Palo Alto Networks\n- Kathryn King, General Manager, Technology & Strategy, Office of the eSafety Commissioner, Australia\n- Edward S. Knight, Executive Vice-Chairman, Nasdaq\n- Andrew JP Levy, Chief Corporate and Government Affairs Officer, Accenture\n- Caroline Louveaux, Chief Privacy and Data Responsibility Officer, Mastercard\n- Shawn Maher, Global Vice-Chair, Public Policy, EY\n- Gevorg Mantashyan, First Deputy Minister of High-Tech Industry, Ministry of High-Tech Industry of Armenia\n- Gary Marcus, Chief Executive Officer, Center for Advancement of Trustworthy AI\n- Gregg Melinson, Senior Vice-President, Corporate Affairs, Hewlett Packard Enterprise\n- Nicolas Miailhe, Founder and President, The Future Society (TFS)\n- Robert Middlehurst, Senior Vice-President, Regulatory Affairs, e& International\n- Casey Mock, Chief Policy and Public Affairs Officer, Center for Humane Technology\n- Chandler Morse, Vice-President, Corporate Affairs, Workday\n- Miho Naganuma, Senior Executive Professional, Digital Trust Business Strategy Department, NEC\n- Dan Nechita, Head, Cabinet, MEP Dragoș Tudorache, European Parliament\n- Michael Nunes, Head, Government Advisory, Visa\n- Bo Viktor Nylund, Director, UNICEF Innocenti Global Office of Research and Foresight, United Nations Children's Fund (UNICEF)\n- Madan Oberoi, Executive Director, Technology and Innovation, International Criminal Police Organization (INTERPOL)\n- Michael Ortiz, Senior Director, Policy, Sequoia Capital Operations\n- Florian Ostmann, Head, AI Governance and Regulatory Innovation, The Alan Turing Institute\n- Marc-Etienne Ouimette, Lead, Global AI Policy, Amazon Web Services\n- Timothy Persons, Principal, Digital Assurance and Transparency of US Trust Solutions, PwC\n- Tiffany Pham, Founder and Chief Executive Officer, Mogul\n- Valerie Pisano, President and Chief Executive Officer, MILA, Quebec Artificial Intelligence Institute\n- Oreste Pollicino, Professor, Constitutional Law, Bocconi University\n- Catherine Quinlan, Vice-President, AI Ethics, IBM\n- Martin Rauchbauer, Co-Director and Founder, Tech Diplomacy Network\n- Alexandra Reeve Givens, Chief Executive Officer, Center for Democracy and Technology\n- Philip Reiner, Chief Executive Officer, Institute for Security and Technology\n- Andrea Renda, Senior Research Fellow, Centre for European Policy Studies (CEPS)",
    "## Individuals and Organizations\n\n- **Global Policy Development**  \n  Ashwin Ad  \n  Zoom Video Communications\n\n- **Dell Technologies**  \n  John Roese  \n  Global Chief Technology Officer\n\n- **Ministry of Enterprises and Made in Italy**  \n  Arianna Rufini  \n  ICT Adviser to the Minister\n\n- **Centre for the Fourth Industrial Revolution, Rwanda**  \n  Crystal Rugege  \n  Managing Director\n\n- **INRIA Chile**  \n  Nayat Sanchez-Pi  \n  Chief Executive Officer\n\n- **Swiss Federal Office of Communications, DETEC**  \n  Thomas Schneider  \n  Ambassador, Director of International Affairs\n\n- **Apolitical**  \n  Robyn Scott  \n  Co-Founder and Chief Executive Officer\n\n- **Responsible Artificial Intelligence Institute**  \n  Var Shankar  \n  Director, Policy\n\n- **Credo AI**  \n  Navrina Singh  \n  Founder and Chief Executive Officer\n\n- **Federal Ministry for Digital and Transport, Germany**  \n  Irina Soeffky  \n  Director, National, European and International Digital Policy\n\n- **data.org**  \n  Uyi Stewart  \n  Chief Data and Technology Officer\n\n- **Ministry of Economy, Trade and Industry of Japan**  \n  Chizuru Suga  \n  Director, Digital Economy\n\n- **Stern School of Business, New York University**  \n  Arun Sundararajan  \n  Harold Price Professor, Entrepreneurship and Technology\n\n- **The Markup**  \n  Nabiha Syed  \n  Chief Executive Officer\n\n- **Private AI**  \n  Patricia Thaine  \n  Co-Founder and Chief Executive Officer\n\n- **Ministry of Economy, Malaysia**  \n  V Valluvan Veloo  \n  Director, Manufacturing Industry, Science and Technology Division\n\n- **Hewlett Packard Enterprise**  \n  Rishi Varma  \n  Senior Vice-President and General Counsel\n\n- **Ministry of Economic Affairs and Information Technology of Estonia**  \n  Ott Velsberg  \n  Government Chief Data Officer\n\n- **Equal AI**  \n  Miriam Vogel  \n  President and Chief Executive Officer\n\n- **Ministry of Economy of the Republic of Azerbaijan**  \n  Arif Zeynalov  \n  Transformation Chief Information Officer",
    "## World Economic Forum\n\n- **Metaverse Initiative**  \n  John Bradley, Lead  \n  Karyn Gorman, Communications Lead\n\n- **Artificial Intelligence, Quantum Technologies**  \n  Devendra Jain, Lead\n\n- **Artificial Intelligence and Machine Learning Specialists**  \n  Jenny Joung, Specialist  \n  Hannah Rosenfeld, Specialist\n\n- **AI Governance Alliance**  \n  Daegan Kingery, Early Careers Programme  \n\n- **Generative AI and Metaverse**  \n  Connie Kuang, Lead\n\n- **Data and Artificial Intelligence Ecosystems**  \n  Supheakmungkol Sarin, Head\n\n- **Data and AI Specialist**  \n  Stephanie Teeuwen, Specialist\n\n- **Digital Trust**  \n  Hesham Zafar, Lead",
    "## Sources and References\n\n- World Economic Forum, Unlocking value from Generative AI: Guidance for responsible transformation, 2024.\n\n- “Generative AI could raise global GDP by 7%”, Goldman Sachs, 05 April 2023.\n\n- World Economic Forum, Toolkit for Digital Safety Design Interventions and Innovations: Typology of Online Harms, 2023.\n\n- Schaake, Marietje, “There can be no AI regulation without corporate transparency”, Financial Times, 31 October 2023.\n\n- Appel, Gil, Juliana Neelbauer and David A. Schweidel, “Generative AI Has an Intellectual Property Problem”, Harvard Business Review, 7 April 2023.\n\n- Various complementary approaches to governing predictable and unpredictable risks.\n\n- Council of the EU and the European Council, Artificial intelligence act: Council and Parliament strike a deal, 9 December 2023.\n\n- Government of Canada, The Artificial Intelligence and Data Act (AIDA), 2023.\n\n- “Committee of jurists approves text with rules for artificial intelligence”, Senado Noticias, 1 December 2022.\n\n- “Legal Alert: Chile takes first steps towards regulation of Artificial Intelligence”, DLA PIPER, 15 June 2023.\n\n- Republic of the Phillippines, House Bill 7396, 1 March 2023.\n\n- Liu, Shoashan, “India’s AI Regulation Dilemma”, The Diplomat, 27 October 2023.\n\n- European Parliament, Artificial Intelligence Act: comprehensive rules for trustworthy AI, 9 December 2023.\n\n- Government of Singapore, AI for the Public Good, 2023.\n\n- Malaysia Ministry of Science, Technology & Innovation, National AI Roadmap 2021-2025.\n\n- National Strategy for Data & AI, Kingdom of Saudi Arabia, 2020.\n\n- Cabinet Office, Government of Japan, AI Strategy 2022.\n\n- Republic of Rwanda Ministry of ICT and Innovation, The National AI Policy, 2022.\n\n- OECD.AI Policy Observatory, National AI policies & strategies.\n\n- World Economic Forum, Data Equity: Foundational Concepts for Generative AI, 2023.\n\n- World Economic Forum, Jobs of Tomorrow: Large Language Models and Jobs, 2023.",
    "## Additional Publications and References\n\n- Stanford University Human-Centered Artificial Intelligence on foundation models and copyright questions.\n\n- D’Auria, Giuseppina and Arun Sundararajan, “Rethinking Intellectual Property Law in an Era of Generative AI”.\n\n- Workday, Position on Foundation Models and Generative AI, 2023.\n\n- Leibowicz, Claire, “Why watermarking AI-generated content won't guarantee trust online”, MIT Technology Review, 2023.\n\n- Talat, Zeerak, Aurélie Névéol, Stella Biderman, Miruna Clinciu, “Challenges of Bias Evaluation Under Multilingual Settings”, 2022.\n\n- Treat, David and Marie Wallace, “3 urgent questions as we navigate new digital identity”, World Economic Forum, 2023.\n\n- Hendrycks, Dan, Mantas Mazeika and Thomas Woodside, “An overview of catastrophic AI risks”, arXiv, 2023.",
    "## AI and Regulation\n\n- OECD, “AI language models: Technological, socio-economic and policy considerations”, 2023.\n\n- Policies and strategies across different countries including China, Singapore, the UAE, Brazil, and Mauritius.\n\n- African Union High-Level Panel on Emerging Technologies, AI for Africa: Socio-Economic Development, 2021.\n\n- Li, Pengfei, Jianyi Yang, Mohammad A. Islam and Shaolei Ren, “Addressing AI Models’ Water Footprint”, 2023.",
    "## Conclusion and Contact\n\nThe World Economic Forum, committed to improving the state of the world, is the International Organization for Public-Private Cooperation.\n\n- Address:  \n  World Economic Forum  \n  91–93 route de la Capite  \n  CH-1223 Cologny/Geneva, Switzerland\n\n- Contact:  \n  Tel.\n\n: +41 (0) 22 869 1212  \n  Fax: +41 (0) 22 786 2744  \n  Email:   \n  Website:",
    "## Summary\n\nThe Government of Jersey Children Young People Education and Skills (CYPES) Department recognises the potential of artificial intelligence (AI), including generative AI, in the education sector while acknowledging the need for responsible and informed implementation.\n\nThis policy aims to outline the department's position on the use of AI and provide evidence-based guidelines for its application.\n\nGenerative AI refers to a branch of artificial intelligence that focuses on creating or generating new content, such as images, videos, text, or audio, that is not directly sourced from existing data.\n\nIt involves training machine learning models to understand patterns and generate original content based on those patterns.",
    "## Key Messages\n\nThe advancements in generative AI technology have made it accessible to the general public, resulting in the production of AI-generated content.\n\nThis development brings both opportunities and challenges to the education sector.\n\nWhen utilised appropriately, technology, including generative AI, holds the potential to alleviate the workload within the education sector, enabling teachers to allocate more time to delivering excellent instruction.\n\nEducational institutions such as schools, colleges, universities, and awarding organisations must take necessary measures to prevent malpractice, including any misconduct related to the use of generative AI and other emerging technologies.\n\nSafeguarding data, resources, staff, and students remains of utmost importance in the education sector.\n\nPersonal and sensitive data should never be entered into generative AI tools to ensure its protection.\n\nAdditionally, educational institutions should regularly assess and enhance their cybersecurity protocols, and provide training to all staff, considering that generative AI may contribute to more sophisticated and credible cyber-attacks.\n\nMoreover, it is imperative for educational institutions to safeguard their students from harmful online content, including content generated by generative AI.",
    "## Background\n\nThe release of ChatGPT by OpenAI in November 2022 has generated significant interest among educationalists in generative artificial intelligence (AI) based on large language models (LLMs) like ChatGPT and Google Bard.\n\nThese AI tools have the ability to respond to prompts, answer questions, and complete written tasks in a manner that resembles human interaction.\n\nAdditionally, generative AI can produce various forms of content, including audio, code, images, video, and simulations.\n\nIt is worth noting that this technology is not entirely new and can already be found in everyday applications such as email spam filtering, media recommendation systems, navigation apps, and online chatbots.\n\nGenerative AI tools excel at quickly analysing, structuring, and generating text, as well as converting text prompts into audio, video, and images.\n\nHowever, it is crucial to recognize that the content they produce may not always be accurate or appropriate, as they often lack consideration for truthfulness and can generate biased information.\n\nWhile access to generative AI tools can facilitate certain written tasks, it is essential to understand that these tools cannot substitute the knowledge stored in long-term memory.\n\nProficiency in generative AI relies on the ability to write clearly, understand the domain being addressed, and have existing knowledge to draw upon.\n\nTo evaluate the results, one needs a schema or framework against which to compare them.\n\nTherefore, generative AI tools can streamline certain aspects of writing tasks, but they cannot replace the judgment and deep subject knowledge of human experts.\n\nIn light of these considerations, it is more crucial than ever for the education system to prioritise the acquisition of knowledge, expertise, and intellectual capability among students.\n\nWhile leveraging technology effectively, safely, and appropriately, the education sector should seize the opportunities it provides to deliver excellent education that prepares students to contribute meaningfully to society and the future workplace.",
    "## Authenticity\n\nAuthenticity holds significant importance in the context of student or staff work utilising generative AI, primarily due to its role in preserving integrity, upholding academic honesty, and maintaining ethical standards.\n\nAuthenticity serves as a pillar for maintaining academic integrity.\n\nIt is through authentic work that one can genuinely reflect their knowledge, skills, and efforts, thereby reinforcing the fundamental principles of academic integrity.\n\nBy promoting fair evaluation, authentic work ensures that individuals receive due recognition and rewards based on their own abilities and achievements.\n\nThe production of authentic work facilitates the process of learning, critical thinking, and problem-solving for students.\n\nAuthenticity engages individuals in actively participating in these cognitive processes, enabling them to develop their skills, broaden their knowledge, and enhance their understanding of the subject matter at hand.\n\nFailure to attribute or acknowledge the use of generative AI in work creation raises ethical concerns, particularly in relation to plagiarism.\n\nThe absence of proper attribution undermines the principles of originality, intellectual property rights, and fairness and honesty.\n\nEmphasis should be placed on authenticity when utilising generative AI in the creation of work.\n\nThis production cultivates a culture of honesty, intellectual integrity, and ethical conduct within educational and professional environments.\n\nThis prioritisation serves as a foundation for upholding academic standards, nurturing individual growth, and reinforcing ethical principles.\n\nThis policy outlines the position and guidelines for successful use of generative Ai.\n\nShould it be determined that any of the criteria can not be adequately met or considered in full the user will be subject to Acceptable Use policy defined for employees by the Government of Jersey or by the school/college for all other users.",
    "## Useful documents:\n\n- Generative Ai Department for Education UK - Generative artificial intelligence in education - GOV.UK ()\n- Ai Risk Management Framework – NIST - AI Risk Management Framework | NIST\n- UNESCO Ai ethics - Ethics of Artificial Intelligence | UNESCO\n- The Institute for Ethical Ai in Education - The-Institute-for-Ethical-AI-in-Education- The-Ethical-Framework-for-AI-in-Education.pdf (buckingham.ac.uk)",
    "##### Achieving Educational Goals\n\n- Define the Educational Objective: The school wants to use ChatGPT to provide real-time information on topics discussed in class and to assist students with research and assignment queries.\n\n- Selection of AI Resources: ChatGPT is selected for its ability to answer a wide range of topics and its user-friendly interface.\n\n- Desired Outcomes: Improved student engagement, faster responses to queries, and enriched classroom discussions.\n\n- Continuous Assessment: Track student performance and feedback to determine if the introduction of ChatGPT has a positive impact on learning outcomes.",
    "##### Authenticity\n\n- Students should be trained to always cite information gathered from ChatGPT, ensuring originality and academic honesty.\n\n**Implementation in Classroom:**  \n\n- Discussion Aid: During classroom discussions, if there's a topic or fact that needs clarity, students can use ChatGPT to get immediate information.\n\n- Assignment Support: For individual or group projects, students can use ChatGPT for research assistance.\n\n- Real-time Feedback: If a student doesn't understand a topic, they can get an immediate explanation from ChatGPT.\n\n**Outcome:**  \nBy integrating ChatGPT following the ethical framework, Jersey School manages to enhance the learning experience while ensuring that the tool is used responsibly, ethically, and effectively.\n\nThis results in a classroom environment that is information-rich, engaging, and in tune with modern technology while upholding strong educational and ethical standards.",
    "### SUBJECT: Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence\n\nArtificial intelligence (AI) is one of the most powerful technologies of our time, and the President has been clear that we must seize the opportunities AI presents while managing its risks.\n\nConsistent with the Artificial Intelligence in Government Act of 2020, the Advancing American AI Act, and President Biden’s Executive Order of October 30, 2023 (Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence), this memorandum directs agencies to advance AI governance and innovation while managing risks from the use of AI, particularly those affecting the safety and rights of the public.\n\nAs set forth in the accompanying Federal Register notice, the Office of Management and Budget is requesting public comment on this proposed memorandum.",
    "## OVERVIEW\n\nWhile AI is improving operations and efficiency across the Federal Government, agencies must effectively manage its use.\n\nAs such, this memorandum establishes new agency requirements and guidance for AI governance, innovation, and risk management, including through specific minimum risk management practices for uses of AI that impact the rights and safety of the public.",
    "### Strengthening AI Governance\n\nManaging AI risk and promoting AI innovation requires effective AI governance.\n\nAs required by President Biden’s October 30, 2023 Executive Order (the “AI Executive Order”), each agency must designate a Chief AI Officer (CAIO) within 60 days of the date of the issuance of this memorandum.\n\nThis memorandum describes the roles, responsibilities, seniority, position, and reporting structures for agency CAIOs.\n\nBecause AI is deeply interconnected with other technical and policy areas including data, information technology (IT), security, privacy, civil rights and civil liberties, customer experience, and workforce management, CAIOs must work in close coordination with existing responsible officials and organizations within their agencies.",
    "### Advancing Responsible AI Innovation\n\nWhen implemented responsibly, AI can improve operations across the Federal Government.\n\nAgencies must increase their capacity to successfully and responsibly adopt AI, including generative AI, into their operations.\n\nTo that end, this memorandum requires each agency identified in the Chief Financial Officer (CFO) Act to develop an enterprise AI strategy.\n\nThis memorandum also provides recommendations for how agencies should reduce barriers to the responsible use of AI, including barriers related to IT infrastructure, data, cybersecurity, workforce, and the particular challenges of generative AI.",
    "### Managing Risks from the Use of AI\n\nWhile agencies will realize significant benefits from AI, they must also manage a range of risks from the use of AI.\n\nAgencies are subject to existing risk management requirements relevant to AI, and this memorandum does not replace or supersede these requirements.\n\nInstead, it creates new requirements focused specifically on the risks from relying on AI to inform or carry out agency decisions and actions, particularly when such reliance impacts the rights and safety of the public.\n\nTo address these risks, this memorandum requires agencies to follow minimum practices when using rights-impacting and safety-impacting AI, and enumerates specific categories of AI that are presumed to impact rights and safety.\n\nFinally, this memorandum also establishes a series of recommendations for managing AI risks in the context of Federal procurement.",
    "## SCOPE\n\nAgency adoption of AI poses many challenges, some novel and specific to AI and some well-known.\n\nWhile agencies must give due attention to all aspects of AI, this memorandum is scoped to address risks specifically arising from the use of AI, as well as governance and innovation issues that are directly tied to agencies’ use of AI.\n\nThis memorandum does not address issues that are present regardless of the use of AI, for instance with respect to Federal information and systems in general.\n\nIn addition, this memorandum does not supersede other, more general Federal policies that apply to AI but are not focused specifically on AI, such as policies that relate to enterprise risk management, information resources management, privacy, Federal statistical activities, IT, or cybersecurity.\n\nAgencies must continue to comply with applicable OMB policies in other domains relevant to AI, and to coordinate compliance across the agency with all appropriate officials.\n\nAll agency responsible officials retain their existing authorities and responsibilities established in other laws and policies.",
    "### Covered Agencies\n\nExcept as specifically noted, this memorandum applies to all agencies defined in 44 U.S.C.\n\n§ 3502(1).\n\nAs noted in the relevant sections, some requirements in this memorandum only apply to CFO Act agencies, as identified in 31 U.S.C.\n\n§ 901(b), and other requirements do not apply to elements of the Intelligence Community, as defined in 50 U.S.C.\n\n§ 3003.",
    "### Covered AI\n\nThis memorandum provides requirements and recommendations that, as described in more detail below, apply to new and existing AI that is developed, used, or procured by or on behalf of covered agencies.\n\nThe principles of this memorandum do not, by contrast, govern agencies’ regulatory actions designed to prescribe law or policy regarding non-agency uses of AI.\n\nThe requirements of this memorandum apply to system functionality that implements or is reliant on AI, rather than to the entirety of an information system that incorporates AI.\n\nAs noted in the relevant sections, some requirements in this memorandum apply only in specific circumstances in which agencies use AI, such as when the AI may impact rights or safety.",
    "## STRENGTHENING ARTIFICIAL INTELLIGENCE GOVERNANCE\n\nThe head of each covered agency is responsible for pursuing AI innovation and ensuring that their agency complies with AI requirements in relevant law and policy, including that risks from the agency’s use of AI are adequately managed.\n\nThe head of each covered agency must also consider the necessary financial, human, information, and infrastructural resources to carry out these responsibilities effectively, including providing or requesting resources via the budget process to support the responsibilities identified in this memorandum.\n\nTo improve accountability for AI issues, agencies must designate a Chief AI Officer, consistent with Section 10.1(b) of the AI Executive Order.\n\nCAIOs bear primary responsibility on behalf of the head of their agency for implementing this memorandum and coordinating implementation with other agencies.\n\nThis section defines CAIOs’ roles, responsibilities, seniority, position, and reporting structure.",
    "### Actions\n\n- **Designating Chief AI Officers:** Within 60 days of the issuance of this memorandum, the head of each agency must designate a CAIO.\n\nTo ensure the CAIO can fulfill the responsibilities laid out in this memorandum, agencies that have already designated a CAIO must evaluate whether they need to provide that individual with additional authority or appoint a new CAIO.\n\nAgencies must identify these officers to OMB through OMB’s Integrated Data Collection process or an OMB-designated successor process, and they must update OMB within 30 days when the designated individual changes.\n\n- **Convening Agency AI Governance Bodies:** Within 60 days of the issuance of this memorandum, each CFO Act agency must convene its relevant senior officials to coordinate and govern AI issues, consistent with Section 10.1(b) of the AI Executive Order and the detailed guidance in Section 3(c) of this memorandum.\n\n- **Compliance Plans:** Consistent with Section 104(c)-(d) of the AI in Government Act, within 180 days of the issuance of this memorandum or any update to this memorandum and every two years thereafter until 2036, each agency must submit to OMB and post publicly on the agency’s website either a plan to achieve consistency with this memorandum, or a written determination that the agency does not use and does not anticipate using covered AI.\n\nAgencies must also include plans to update any existing internal AI principles and guidelines to ensure consistency with this memorandum.\n\nOMB will provide full templates for these compliance plans.\n\n- **AI Use Case Inventories:** Pursuant to Section 7225 of the Advancing American AI Act, and subject to the exclusions in that Act and Section 10.1(e) of the AI Executive Order, each agency (except for the Department of Defense and the Intelligence Community) must annually submit an inventory of its AI use cases to OMB and subsequently post a public version on the agency’s website.\n\nOMB will issue detailed instructions for the inventory through its Integrated Data Collection process or an OMB-designated successor process.\n\nBeginning with the use case inventory for 2024, agencies will be required, as applicable, to identify and report additional detail on how they are using safety-impacting and rights-impacting AI, the risks—including risks to equity—that such use poses, how they are managing those risks, and any related extensions and waivers granted under Section 5 of this memorandum.\n\n- **Reporting on AI Use Cases Not Subject to Inventory:** Some AI use cases are exempt from the Advancing American AI Act’s inventory requirement.\n\nOf those use cases, those within the Department of Defense are otherwise within the scope of this memorandum unless they concern AI used as a component of a national security system.\n\nThe Department of Defense must annually provide OMB with information on those in-scope AI use cases, including aggregate metrics about those in-scope AI uses cases, the number of such cases that impact rights and safety, their compliance with the practices of Section 5(c) of this memorandum, and any waivers granted under Section 5 of this memorandum.\n\nOMB will issue detailed instructions for this reporting through its Integrated Data Collection process or an OMB-designated successor process.",
    "### Roles, Responsibilities, Seniority, Position, and Reporting Structure of Chief Artificial Intelligence Officers\n\nConsistent with Section 10.1(b)(ii) of the AI Executive Order, this memorandum defines agency CAIOs’ roles, responsibilities, seniority, position, and reporting structures as follows:\n\n- **Roles:** CAIOs must have the necessary skills, knowledge, training, and expertise to perform the responsibilities described in this section.\n\nAt CFO Act agencies, the CAIO’s primary role must be coordination, innovation, and risk management for their agency’s use of AI.\n\nAgencies may choose to designate an existing official, such as a Chief Technology Officer, Chief Data Officer, or similar official with relevant or complementary authorities and responsibilities, provided they have significant expertise in AI and meet the other requirements in this section.\n\n- **Responsibilities:** The AI Executive Order tasks CAIOs with primary responsibility in their agencies, in coordination with other responsible officials, for coordinating their agency’s use of AI, promoting AI innovation, managing risks from the use of AI, and carrying out the agency responsibilities defined in Section 8(c) of Executive Order 13960 and Section 4(b) of Executive Order 14091.\n\nIn addition, CAIOs, in coordination with other responsible officials and appropriate stakeholders, are responsible for:\n\n  - Serving as the senior advisor for AI to the head of the agency and other senior agency leadership and within their agency’s senior decision-making forums;\n  - Maintaining awareness of agency AI activities, including through creating and maintaining the annual AI use case inventory;\n  - Developing a plan for compliance with this memorandum, as detailed in Section 3(a)(iii) of this memorandum, and an agency AI strategy, as detailed in Section 4(a) of this memorandum;\n  - Advising the agency CFO and Chief Human Capital Officer (CHCO) on the resourcing requirements and workforce skillsets necessary for applying AI to the agency’s mission and adequately managing its risks;\n  - Supporting agency involvement with appropriate interagency coordination bodies related to their agency’s AI activities, including representing the agency to the council described in Section 10.1(a) of the AI Executive Order;\n  - Supporting and coordinating their agency’s involvement in AI standards-setting bodies, as appropriate, and encouraging agency adoption of voluntary consensus standards for AI, as appropriate and consistent with OMB Circular No.\n\nA-119;\n  - Promoting AI Innovation:\n  - Working with their agency to identify and prioritize appropriate uses of AI that will improve their agency’s mission and advance equity;\n  - Identifying and removing barriers to the responsible use of AI in the agency, including through the advancement of AI-enabling enterprise infrastructure, workforce development measures, policy, and other resources for AI innovation;\n  - Advocating within their agency and to the public on the opportunities and benefits of AI to the agency’s mission;\n  - Managing Risks from the Use of AI:\n  - Managing an agency program that supports the enterprise in identifying and managing risks from the use of AI, especially for safety-impacting and rights-impacting AI;\n  - Working with relevant senior agency officials to establish or update processes to measure, monitor, and evaluate the ongoing performance of AI applications and whether they are achieving their intended objectives;\n  - Overseeing agency compliance with requirements to manage risks from the use of AI, including those established in this memorandum and in relevant law and policy;\n  - Conducting risk assessments, as necessary, of agency AI applications to ensure compliance with this memorandum;\n  - Overseeing development of agency-specific lists, as necessary, of purposes for which AI is presumed to be safety-impacting or rights-impacting;\n  - Waiving individual applications of AI from elements of Section 5 of this memorandum through the processes detailed in that section; and\n  - In partnership with relevant agency officials (e.g., authorizing, procurement, legal, human capital, and oversight officials), ensuring that their agency does not use AI that is not in compliance with this memorandum, including by assisting these relevant agency officials in evaluating Authorizations to Operate based on risks from the use of AI.\n\n- **Seniority:** For CFO Act agencies, the CAIO must be a position at the Senior Executive Service, Scientific and Professional, or Senior Leader level, or equivalent.\n\nIn other agencies, the CAIO must be at least a GS-15 or equivalent.\n\n- **Position and Reporting Structure:** CAIOs must have the necessary authority to perform the responsibilities in this section and must be positioned highly enough to engage regularly with other agency leadership, to include the Deputy Secretary or equivalent.",
    "Further, CAIOs must coordinate with other responsible officials at their agency to ensure that the agency’s use of AI complies with and is appropriate in light of applicable law and government-wide guidance.",
    "### Internal Agency AI Coordination\n\nAgencies must ensure that AI issues receive adequate attention from the agency’s senior leadership.\n\nConsistent with Section 10.1(b) of the AI Executive Order, agencies must take appropriate steps, such as through the convening of an AI governance body, to coordinate internally among officials responsible for aspects of AI adoption and risk management.\n\nLikewise, the CAIO must be involved, at appropriate times, in broader agency-wide risk management bodies and processes, including in the development of the agency risk management strategy.\n\nThe agency’s AI coordination mechanisms should be aligned to the needs of the agency based on, for example, the degree to which the agency currently uses AI, the degree to which AI could improve the agency’s mission, and the risks posed by the agency’s current and potential uses of AI.\n\nCFO Act agencies are required specifically to establish AI Governance Boards to convene relevant senior officials no less than quarterly to govern the agency’s use of AI, including to remove barriers to the use of AI and to manage its associated risks.\n\nThose agencies are permitted to rely on existing governance bodies to fulfill this requirement as long as they currently satisfy or are made to satisfy both of the following:\n\n- Agency AI Governance Boards must be chaired by the Deputy Secretary of the agency or equivalent and vice-chaired by the agency CAIO, and these responsibilities should not be assigned to other officials.\n\nWorking through this Board, CAIOs will support their respective Deputy Secretaries in coordinating AI activities across the agency and implementing relevant sections of the AI Executive Order.\n\n- Agency AI Governance Boards must include appropriate representation from senior agency officials responsible for key enablers of AI adoption and risk management, including at least IT, cybersecurity, data, human capital, procurement, budget, agency management, customer experience, performance evaluation, statistics, risk management, equity, privacy, civil rights and civil liberties, and officials responsible for implementing AI within an agency’s program office(s).\n\nAgencies should also consider including representation from their respective Office of the Inspector General.",
    "## ADVANCING RESPONSIBLE ARTIFICIAL INTELLIGENCE INNOVATION\n\nIf implemented responsibly, AI can improve operations and deliver efficiencies across the Federal Government.\n\nAgencies must improve their ability to use AI in ways that benefit the public and increase mission effectiveness, while recognizing the limitations of AI and when it is not suited for a given task.\n\nTo achieve this, agencies should build internal enterprise capacity to support responsible AI innovation and take actions to improve their procurement of AI.",
    "### AI Strategies\n\nWithin 365 days of the issuance of this memorandum, each CFO Act agency must develop and release publicly on the agency’s website a strategy for identifying and removing barriers to the responsible use of AI and achieving enterprise-wide advances in AI maturity, including:\n\n- The agency’s current and planned top use cases of AI;\n- A current assessment of the agency’s AI maturity and the agency’s AI maturity goals based on the method established under Section 10.1(c) of the AI Executive Order;\n- The agency’s plans to effectively govern its use of AI, including through its Chief AI Officer, AI Governance Boards, and improvements to their AI use case inventory;\n- A plan for developing sufficient enterprise capacity for AI innovation, including mature AI-enabling infrastructure for the data, computing, development, testing, cybersecurity compliance, deployment, and continuous-monitoring infrastructure necessary to build, test, and maintain AI;\n- A plan for building sufficient enterprise capacity to manage risks from the use of AI;\n- A current assessment of the agency’s AI workforce capacity and projected AI workforce needs, as well as a plan to recruit, hire, train, retain and empower AI practitioners and achieve AI literacy for non-practitioners involved in AI to meet those needs; and\n- Specific, prioritized areas and planning for future AI investment.",
    "### Removing Barriers to the Responsible Use of Artificial Intelligence\n\nEmbracing innovation requires removing unnecessary and unhelpful barriers to the use of AI while retaining and strengthening the guardrails that ensure its responsible use.\n\nAgencies should create internal environments where those developing and deploying AI have flexibility and do not face hindrances that divert limited resources and expertise away from the AI innovation and risk management.\n\nAgencies should take steps to remove such barriers, paying special attention to the following recommendations:\n\n- **IT Infrastructure:** Agencies should ensure that their AI projects have access to adequate IT infrastructure, including high-performance computing infrastructure specialized for AI training and inference, where necessary.\n\nAgencies should also ensure adequate access for AI developers to the software tools, open-source libraries, and deployment and monitoring capabilities necessary to rapidly develop, test, and maintain AI applications.\n\n- **Data:** Agencies should develop adequate infrastructure and capacity to sufficiently curate agency datasets for use in training, testing, and operating AI.\n\nThis includes an agency’s capacity to maximize appropriate access to internal data and share such data within the agency.\n\nAgencies should also explore the utility of public access datasets and encourage their use, where appropriate and consistent with the data practices outlined in this memorandum, to help develop, test, and maintain AI applications.\n\nThese activities should be supported by resources to enable sound data governance and management practices, particularly as it relates to data curation, labeling, and stewardship.\n\n- **Cybersecurity:** Agencies should update, as necessary, cybersecurity authorization processes to better address the needs of AI applications, including to advance the use of continuous authorizations for AI.\n\nConsistent with Section 10.1(f) of the AI Executive Order, agency authorizing officials should also prioritize generative AI and other critical emerging technologies in Authorizations to Operate and any other applicable release or oversight processes.\n\n- **Workforce:** Consistent with Sections 5.1 and 10.2 of the AI Executive Order, agencies should take full advantage of available special hiring and retention authorities to fill gaps in AI talent, encouraging applications from individuals with diverse perspectives and experiences, and ensure the use of recruitment best practices for AI positions, such as descriptive job titles and skills-based assessments.\n\nWhen identifying and filling workforce needs for AI, agencies should include both technical roles, such as data scientists and engineers, and non-technical roles, such as designers, behavioral scientists, contracting officials, managers, and attorneys, whose contribution and competence with AI are important for successful and responsible AI outcomes.\n\nAgencies should provide resources and training to develop such AI talent internally and should also increase AI training offerings for Federal employees, including opportunities that provide Federal employees pathways to AI occupations and that assist employees affected by the application of AI to their work.\n\n- **Generative AI:** In addition to heeding the guidance provided in Section 10.1(f) of the AI Executive Order, agencies should assess potential beneficial use cases of generative AI in their missions and establish adequate safeguards and oversight mechanisms that allow generative AI to be used in the agency without posing undue risk.",
    "## MANAGING RISKS FROM THE USE OF ARTIFICIAL INTELLIGENCE\n\nAgencies have a range of policies, procedures, and officials in place to manage risks related to agency information and systems.\n\nTo better address risks from the use of AI, and particularly risks to the rights and safety of the public, all agencies that are not elements of the Intelligence Community are required to implement minimum practices, detailed below, to manage risks",
    "## Implementation of Risk Management Practices and Termination of Non-Compliant AI\n\nBy August 1, 2024, agencies must implement the minimum practices in Section 5(c) of this memorandum for safety-impacting or rights-impacting AI, or else stop using any AI that is not compliant with the minimum practices, consistent with the details and caveats in that section.",
    "## Recommendation on AI Documentation\n\nWithin 180 days of the issuance of this memorandum, the council described in Section 10.1(a) of the AI Executive Order will provide the Director of OMB with a list of recommended documentation that should be required from a selected vendor in the fulfillment of a Federal AI contract.\n\nAs part of their recommendation, the council must consider the minimum risk management practices in Section 5(c) and the associated materials that may be required of vendors to demonstrate that they have completed such tasks.",
    "## Determining Which Artificial Intelligence Is Presumed to Be Safety-Impacting or Rights-Impacting\n\nAll AI within the scope of this section that matches the definitions of “safety-impacting AI” or “rights-impacting AI” as defined in Section 6 must follow the minimum practices in Section 5(c) by the appropriate deadline.\n\nAgencies must review each use of AI that they are developing or using to determine whether it matches the definition of safety-impacting or rights- impacting.\n\nThe categories in this subsection only identify a subset of specific purposes for which AI is automatically presumed to be safety-impacting or rights-impacting, and they do not represent an exhaustive list of purposes for which AI is safety-impacting or rights-impacting.\n\nAgencies are also encouraged to define specific purposes that, within their agency, are presumed to be safety-impacting or rights-impacting and so must follow the practices in Section 5(c).\n\nAgencies are required to report any such agency-specific lists to OMB on an annual basis.\n\nWhere an agency currently uses or plans to use AI for a purpose described below, the CAIO, in coordination with other relevant officials as specified by the agency, may make a determination (or reverse a prior determination) that the AI application or component does not match the definitions of “safety-impacting AI” or “rights-impacting AI” and is therefore not subject to the minimum practices.\n\nThe agency CAIO may make or reverse this determination only with a documented context-specific and system-specific risk assessment.\n\nAny such determination or reversal must be reported to OMB within 30 days.",
    "## Purposes That Are Presumed to Be Safety-Impacting\n\nUnless the CAIO determines otherwise, covered AI within the scope of this memorandum is presumed to be safety-impacting and must follow the minimum practices for safety-impacting AI if it is used to control or meaningfully influence the outcomes of the following activities:\n\n- The functioning of dams, emergency services, electrical grids or the generation or movement of energy, fire safety systems, food safety mechanisms, integrity of elections and voting infrastructure, traffic control systems and other systems controlling physical transit, water and wastewater systems, and nuclear reactors, materials, and waste;\n- Physical movements, including in human-robot teaming, such as the movements of a robotic appendage or body, within a workplace, school, housing, transportation, medical, or law enforcement setting;\n- The application of kinetic force, delivery of biological or chemical agents, or delivery of potentially damaging electromagnetic impulses;\n- The movements of vehicles, whether on land, underground, at sea, in the air, or in space;\n- The transport, safety, design, or development of hazardous chemicals or biological entities or pathways;\n- Industrial emissions and environmental impact control processes;\n- The transportation or management of industrial waste or other controlled pollutants;\n- The design, construction, or testing of industrial equipment, systems, or structures that, if they failed, would pose a meaningful risk to safety;\n- Responses to insider threats;\n- Access to or security of government facilities; or\n- Enforcement actions pursuant to sanctions, trade restrictions, or other controls on exports, investments, or shipping.",
    "## Purposes That Are Presumed to Be Rights-Impacting\n\nUnless the CAIO determines otherwise, covered AI is presumed to be rights-impacting (and potentially also safety-impacting) and agencies must follow the minimum practices for rights-impacting AI and safety-impacting AI if it is used to control or meaningfully influence the outcomes of any of the following activities or decisions:\n\n- Decisions to block, remove, hide, or limit the reach of protected speech;\n- Law enforcement or surveillance-related risk assessments about individuals, criminal recidivism prediction, offender prediction, predicting perpetrators' identities, victim prediction, crime forecasting, license plate readers, iris matching, facial matching, facial sketching, genetic facial reconstruction, social media monitoring, prison monitoring, forensic analysis, forensic genetics, the conduct of cyber intrusions, physical location-monitoring devices, or decisions related to sentencing, parole, supervised release, probation, bail, pretrial release, or pretrial detention;\n- Deciding immigration, asylum, or detention status; providing risk assessments about individuals who intend to travel to, or have already entered, the U.S. or its territories; determining border access or access to Federal immigration related services through biometrics (e.g., facial matching) or other means (e.g., monitoring of social media or protected online speech); translating official communication to an individual in an immigration, asylum, detention, or border context; or immigration, asylum, or detention-related physical location-monitoring devices.\n\n- Detecting or measuring emotions, thought, or deception in humans;\n- In education, detecting student cheating or plagiarism, influencing admissions processes, monitoring students online or in virtual-reality, projecting student progress or outcomes, recommending disciplinary interventions, determining access to educational resources or programs, determining eligibility for student aid, or facilitating surveillance (whether online or in-person);\n- Tenant screening or controls, home valuation, mortgage underwriting, or determining access to or terms of home insurance;\n- Determining the terms and conditions of employment, including pre-employment screening, pay or promotion, performance management, hiring or termination, time-on-task tracking, virtual or augmented reality workplace training programs, or electronic workplace surveillance and management systems;\n- Decisions regarding medical devices, medical diagnostic tools, clinical diagnosis and determination of treatment, medical or insurance health-risk assessments, drug-addiction risk assessments and associated access systems, suicide or other violence risk assessment, mental-health status detection or prevention, systems that flag patients for interventions, public insurance care-allocation systems, or health-insurance cost and underwriting processes;\n- Loan-allocation processes, financial-system access determinations, credit scoring, determining who is subject to a financial audit, insurance processes including risk assessments, interest rate determinations, or financial systems that apply penalties (e.g., that can garnish wages or withhold tax returns);\n- Decisions regarding access to, eligibility for, or revocation of government benefits or services; allowing or denying access—through biometrics or other means (e.g., signature matching)—to IT systems for accessing services for benefits; detecting fraud; assigning penalties in the context of government benefits; or\n- Recommendations or decisions about child welfare, child custody, or whether a parent or guardian is suitable to gain or retain custody of a child.",
    "## Minimum Practices for Safety-Impacting and Rights-Impacting Artificial Intelligence\n\nExcept as prevented by applicable law and governmentwide guidance, agencies must apply the minimum practices in this section to safety-impacting and rights-impacting AI by August 1, 2024, or else stop using the AI until it becomes compliant.\n\nPrior to August 1, 2024, agency CAIOs should work with their agencies’ relevant officials to bring potentially non-compliant AI into conformity, which may include voluntary requests to third-party vendors to take appropriate action (e.g., via updated documentation or testing measures).\n\nTo ensure compliance with this requirement, relevant agency officials must use existing mechanisms wherever possible, for example, the Authorization to Operate process.\n\nAn agency may also request an extension or grant a waiver to this requirement through its CAIO using the processes detailed below.\n\nAgencies must document their implementation of these practices and be prepared to report them to OMB, either as a component of the annual AI use case inventory, periodic accountability reviews such as a TechStat process, or on request as determined by OMB.\n\nThe practices in this section represent a minimum baseline for managing risk from the use of AI.\n\nAgencies must identify additional context-specific risks that are associated with their determined use cases and address them as appropriate.\n\nSuch risk considerations may include impacts to safety, security, civil rights, civil liberties, privacy, democratic values, human rights, equal opportunities, potential harms to worker wellbeing, access to critical resources and services, and effects on market competition.\n\nTo fill potential risk management gaps, agencies are encouraged to promote and to incorporate, as appropriate, additional best practices for AI risk management, such as from the National Institute of Standards and Technology (NIST) AI Risk Management Framework, the Blueprint for an AI Bill of Rights, applicable international standards, and the workforce principles established pursuant to Section 6 of the AI Executive Order.\n\nAgencies are also encouraged to continue developing their own agency-specific practices, as appropriate and consistent with this memorandum and the principles in Executive Order 13960, Executive Order 14091, and the October 30, 2023 AI Executive Order.\n\nThe practices in this section also do not supersede, modify, or direct an interpretation of existing requirements mandated by law or governmentwide policy, and agency responsible officials must coordinate to ensure that the performance of these practices does not conflict with other applicable law or governmentwide guidance.",
    "## Exclusions from Minimum Practices\n\nAgencies are not required to follow the minimum practices outlined in this section when using AI solely for one or more of the following purposes:\n\n- Evaluation of a potential vendor, commercial capability, or freely available AI capability that is not otherwise used in agency operations, solely for the purpose of making a procurement or acquisition decision;\n- Evaluation of a particular AI application because the AI provider is the target or potential target of a regulatory enforcement, law enforcement, or national security action; and\n- Research and development.",
    "## Extensions for Minimum Practices\n\nUntil August 1, 2024, agencies may request from OMB an extension of limited and defined duration for a particular use of AI that cannot feasibly meet the minimum requirements in this section by that date.\n\nThe request must be accompanied by a detailed justification for why the agency cannot achieve compliance for the use case in question and what practices the agency has in place to mitigate the risks from noncompliance, as well as a plan for how the agency will come to implement the full set of required minimum practices from this section.",
    "## Waivers from Minimum Practices\n\nIn coordination with other relevant officials, an agency CAIO may waive one or more of the requirements in this section for a specific covered AI application or component after making a written determination, based upon a system-specific risk assessment, that fulfilling the requirement would increase risks to safety or rights overall or would create an unacceptable impediment to critical agency operations.\n\nSuch waivers are applicable for the duration of the AI’s use, but must be reassessed by the CAIO if there are significant changes to the conditions or context in which the AI is used.\n\nAn agency CAIO may also revoke a previously issued waiver at any time.\n\nAgencies must report to OMB within 30 days of granting such a waiver, detailing the scope, justifications, and supporting evidence.",
    "## Minimum Practices for Either Safety-Impacting or Rights-Impacting AI\n\nStarting on August 1, 2024, agencies must follow these practices before using new or existing covered safety-impacting or rights-impacting AI:\n\n- **Complete an AI impact assessment**: Impact assessments must document the following:\n  - The intended purpose for the AI and its expected benefit, supported by specific metrics or qualitative analysis.\n\nMetrics should be quantifiable measures of positive outcomes for an agency’s mission, for example to reduce costs, wait time for customers, or risk to human life, that can be measured after the AI is deployed to confirm or disprove the value of using AI.\n\nWhere quantification is not feasible, qualitative analysis should demonstrate an expected positive outcome, such as for improvements to customer experience or human interactions—and demonstrate that AI is a good fit to accomplish the relevant task.\n\n- The potential risks of using AI, as well as what, if any, additional mitigation measures, beyond these minimum practices, the agency will take to help reduce these risks.\n\nAgencies should document the stakeholders that will be most impacted by the use of the system and assess the possible failure modes of the AI and of the broader system, both in isolation and as a result of human users and other likely variables outside the scope of the system itself.\n\nAgencies should be especially attentive to the potential risks to underserved communities.\n\nThe expected benefits of the AI functionality should be considered against its potential risks, and if the benefits do not meaningfully outweigh the risks, agencies should not use the AI.\n\n- The quality and appropriateness of the relevant data.\n\nAgencies must assess the quality of the data used in the AI’s design, development, training, testing, and operation and its fitness to the AI’s intended purpose.\n\nIf the agency cannot access such data after a reasonable effort to do so, it must obtain sufficient descriptive information from the AI or data provider to satisfy the reporting requirements in this paragraph.\n\nAt a minimum, agencies must document:\n    - the provenance and quality of the data for its intended purpose;\n    - how the data is relevant to the task being automated and has a reasonable expectation of being useful for the AI’s development, testing, and operation;\n    - whether the data contains sufficient breadth to address the range of real-world inputs the AI might encounter;\n    - whether the data comes from an adequately reliable source; and\n    - how errors from data entry, machine processing, or other sources are adequately measured and limited, to include errors from relying on AI-generated data as training data or model inputs.\n\n- **Test the AI for performance in a real-world context**: Agencies must conduct adequate testing to ensure the AI, as well as components that rely on it, will work in its intended real-world context.\n\nSuch testing should follow domain-specific best practices, when available, and should take into account both the specific technology used and feedback from human operators, reviewers, employees, and customers that use the service who impact the system’s outcomes.\n\nTesting conditions should mirror as closely as possible the conditions in which the AI will be deployed.\n\nThrough test results, agencies should demonstrate, to the extent practicable, that the AI will achieve its expected benefits while sufficiently mitigating risks associated with the AI, or else the agency should not use the AI.\n\nAgencies are also encouraged to leverage pilots and limited releases, with strong monitoring, evaluation, and safeguards in place, to carry out the final stages of testing before a wider release.\n\n- **Independently evaluate the AI**: Agencies, through the CAIO, an agency AI oversight board, or other appropriate agency office with existing test and evaluation responsibilities, must review relevant AI documentation to ensure that the system works appropriately and as intended, and that its expected benefits outweigh its potential risks.\n\nAt a minimum, this documentation must include the completed impact assessment and results from testing AI performance in a real-world context, both referenced in Section 5(c)(iv).\n\nAgencies must incorporate this independent evaluation into an applicable release or oversight process, or the Authorization to Operate process.\n\nThe independent reviewing authority must not have been directly involved in the system’s development.\n\nStarting on August 1, 2024 and on an ongoing basis while using new or existing covered safety-impacting or rights-impacting AI, agencies must ensure these practices are followed for the AI:\n\n- **Conduct ongoing monitoring and establish thresholds for periodic human review**: In addition to pre-deployment testing, agencies must institute ongoing procedures to monitor degradation to the AI’s functionality and to detect changes in the AI’s impact on rights or safety.",
    "Part of this monitoring process must include periodic human reviews to determine whether the existing implementation of the minimum practices in this section adequately mitigates any new risk.\n\nSuch human review, including renewed testing for performance of the AI in a real-world context, must be conducted at least annually, and after significant modifications to the AI or to the conditions or context in which the AI is used.\n\nReviews must include oversight and consideration by an appropriate internal agency authority not directly involved in the system’s development or operation.\n\nAgencies should also scale up the use of new or updated AI features incrementally where possible, to provide adequate time to monitor for adverse performance or outcomes.\n\nAgencies should also monitor and defend the AI from AI-specific exploits, particularly those that would adversely impact rights or safety.\n\n- **Mitigate emerging risks to rights and safety**: Upon identifying new or significantly altered risks to rights or safety through continuous monitoring, periodic review, or other mechanisms, agencies must take steps to mitigate those risks, including, as appropriate, through updating the AI to reduce its risks or implementing non-technical mitigations, such as greater human oversight.\n\nAs significant modifications make the existing implementation of the other minimum practices in this section less effective, such as by making training or documentation inaccurate, agencies must update or repeat those practices, as appropriate.\n\nWhere the AI’s risks to rights or safety exceed an acceptable level and where mitigation is not practicable, agencies must stop using the affected AI as soon as is practicable.\n\n- **Ensure adequate human training and assessment**: Agencies must ensure there is sufficient training, assessment, and oversight for operators of the AI to interpret and act on the AI’s output, combat any human-machine teaming issues (such as automation bias), and ensure the human-based components of the system effectively manage risks from the use of AI.\n\nTraining should be conducted on a periodic basis, determined by the agency, and should be specific to the AI use case, product, or service being operated.\n\n- **Provide appropriate human consideration as part of decisions that pose a high risk to rights or safety**: Agencies should identify AI functionality that plays a role in decisions that pose a high risk to rights or safety and ensure that the AI functionality is not permitted to intervene directly in such situations without appropriate human consideration and accountability.\n\n- **Provide public notice and plain-language documentation through the AI use case inventory**: Agencies must ensure, to the extent consistent with applicable law and governmentwide guidance, including those concerning protection of privacy and of sensitive law enforcement, national security, and other protected information, that the AI’s entry in the use case inventory serves as adequately detailed and generally accessible documentation of the system’s functionality that provides public notice of the AI to its users and the general public.\n\nWhere practicable, agencies should include this documentation or link to it in contexts where people will interact with or be impacted by the AI.\n\nWhere agencies’ use cases are excluded from the public inventory requirements described in this guidance, they may still be required to report relevant information to OMB and must ensure adequate transparency in their use of AI, as appropriate and consistent with applicable law.",
    "## Additional Minimum Practices for Rights-Impacting AI\n\nStarting on August 1, 2024, agencies must follow the above minimum practices for AI that is either safety-impacting or rights-impacting.\n\nIn addition, agencies must also follow these minimum practices before initiating use of new or existing rights-impacting AI:\n\n- **Take steps to ensure that the AI will advance equity, dignity, and fairness**:\n  - Proactively identifying and removing factors contributing to algorithmic discrimination or bias.\n\nAgencies must assess whether their rights-impacting AI materially relies on information about a class protected by Federal nondiscrimination laws in a way that could result in algorithmic discrimination or bias against that protected class.\n\nAgencies should also assess whether proxies produce undue influence on their rights-impacting AI.\n\nIn either case, if the AI’s reliance on such information results in unlawful discrimination or harmful bias against protected classes, the agency must cease the use of the information before using the AI for decision-making.\n\n- Assessing and mitigating disparate impacts.\n\nAgencies must test their AI to determine whether there are significant disparities in the AI’s performance across demographic groups, including in the AI’s real-world deployment, and, consistent with applicable law, appropriately address disparities that have the potential to lead to discrimination, cause meaningful harm, or decrease equity, dignity, or fairness.\n\nIf adequate mitigation of the disparity is not possible, then agencies should not use or integrate the AI tool.\n\n- Using representative data.\n\nAgencies should ensure that data used to develop, operate, and assess their AI is adequately representative of the communities who will be affected by the AI, and has been reviewed for improper bias based on the historical and societal context of the data.\n\n- **Consult and incorporate feedback from affected groups**: To the extent practicable and consistent with applicable law and governmentwide guidance, agencies must consult affected groups, including underserved communities, in the design, development, and use of the AI, and use such feedback to inform agency decision-making regarding the AI.\n\nIn the event of negative feedback, agencies must consider not deploying the AI or removing the AI from use.\n\nAgencies are strongly encouraged to solicit feedback on an ongoing basis from affected groups, such as customers, Federal employee groups, and employees’ union representatives, particularly after significant modifications to the AI or the conditions or context in which it is used.\n\nTo carry out such consultations, agencies should take adequate steps to solicit input from the groups affected by the AI, which could include:\n  - Direct user testing, such as observing users interacting with the system;\n  - General solicitations of comments from the public, such as a request for information in the Federal Register or a “Tell Us About Your Experience” sheet with open-ended space for responses;\n  - Post-transaction customer feedback collections;\n  - Public hearings or meetings, such as a listening session; or\n  - Any other transparent process that seeks public input, comments, or feedback from the affected groups in a meaningful, equitable, accessible, and effective manner.",
    "## Customer Engagement\n\n- Customers can include individuals, businesses, or organizations that interact with an agency.\n\n- Agencies are not required to conduct consultations requiring OMB clearance under the Paperwork Reduction Act (44 U.S.C.\n\n§ 3507), provided steps are adequate to solicit input from groups affected by the AI.\n\n- Information on post-transaction customer feedback surveys can be found in OMB Circular A-11, Section 280 – Managing Customer Experience and Improving Service Delivery.",
    "## Notification Requirements\n\n- Agencies must notify individuals when AI influences decision outcomes concerning them, such as benefit denial, in a manner consistent with the Plain Writing Act of 2010.\n\n- The notice should include a means for contacting the agency and requesting remediation.\n\n- Agencies are encouraged to provide explanations for AI-driven decisions and actions.",
    "## Key Terms\n\n- **Agency**: Defined in 44 U.S.C.\n\n§ 3502(1).\n\n- **Algorithmic discrimination**: As defined in Section 10(f) of Executive Order 14091.\n\n- **Artificial Intelligence**: Definitions from the John S. McCain National Defense Authorization Act for Fiscal Year 2019.\n\n- **Automation Bias**: Favoring automated suggestions over contradictory non-automated information.\n\n- **Rights-Impacting AI**: AI impacting significant legal or material outcomes for individuals or communities.\n\n- **Safety-Impacting AI**: AI affecting safety of human life, environment, infrastructure, or strategic assets.",
    "### Summary\n\nSince the public release of OpenAI’s ChatGPT on November 30, 2022, questions and concerns have rapidly circulated concerning the role of generative artificial intelligence (AI) in higher education, particularly in instructional or curricular contexts.\n\nWhile ChatGPT produces text-based output, other generative AI can output text, data, image, sound, video and mixed media formats.\n\nThe International Association of Privacy Professionals defines generative AI as “[a] field of AI that uses machine learning models trained on large data sets to create new content, such as written text, code, images, music, simulations and videos.\n\nThese models are capable of generating novel outputs based on input data or user prompts.”1\n\nSidney Dobrin, author of *AI and Writing*, explains that “[w]e can think of a GenAI as participating in a rudimentary conversation with a user” who “ask[s] the AI to create a specific deliverable — an essay, a song, an image, the solution to a math problem or so on.\n\nThe AI then scrubs through all of the data available, looking for patterns and recurring information about the requested task.\n\nIt then reorganizes that data into a pattern that it deems to answer the prompt.”2\n\nIn June 2023, the University of Kentucky empaneled UK ADVANCE, a broad-based committee of experts to examine and make recommendations to help the campus and community regarding the implications of generative AI and tools such as ChatGPT for higher education, research and beyond.\n\nUK ADVANCE is taking an evidence-based approach with experts from multiple disciplines and ongoing monitoring of experiences among our campus, community, and nationally.\n\nFor these guidelines, UK ADVANCE has sought input from multiple stakeholders as well.\n\nAI already is heralding tremendous changes in academia and the economy, from innovations in farming and the development of therapeutics to customer service and workplace innovations.\n\nAt the same time, there are significant concerns over disruption and displacement of the workforce, embedded bias, data security and privacy, and the spread of misinformation.\n\nWithin academia, there is the potential to create even greater access to personalized and customized learning, expanded student engagement, intelligent tutoring systems, and innovative approaches to curriculum design.\n\nAt the same time, there are concerns over academic integrity, infringements on privacy, and the ability to develop data and information literacies for AI tools.3\n\nAfter reviewing emerging evidence and experiences related to instruction and learning environments, UK ADVANCE offers the following guidelines and recommendations for the Fall 2023 semester regarding (1) the development of course policies concerning generative AI, (2) the response to potential misuse of generative AI in instructional contexts, and (3) approaches to assignment and learning design that mitigate the risk of misuse and leverage the positive potential of generative AI.\n\nIt’s important to note that the following guidelines and recommendations primarily focus on text-based generative AI tools such as Bard (Google), Bing Chat (Microsoft and OpenAI), ChatGPT and ChatGPT Plus (OpenAI), Claude (Anthropic) and Llama (Meta).\n\nAt the same time, the recommendations and insights may be transferrable to situations involving other modalities of generative AI (e.g., image, audio).\n\nGenerative AI is a rapidly evolving technology.\n\nThese guidelines reflect our best understanding at the current time and may be updated to reflect the nature of the field as it continues to change.",
    "#### Recommendation\n\nWe recommend clear course policies for the use of generative AI in four key areas as follows:4\n- People-centered: policies are student- and instructor-centered\n- Adaptability: policies are adapted to the needs and circumstances of the course\n- Effectiveness: policies demonstrate characteristics of an effective course policy\n- Awareness: policies promote awareness and understanding of generative AI\n\n**People-centered**: AI is a tool that instructors and students can use to enhance education, but it should be used with human oversight and an awareness of its strengths and limitations.\n\nStudents and instructors should exercise judgment and control over the use of generative AI so that it is used to augment — rather than replace — instructor decision-making and student learning.\n\nThe U.S. Department of Education’s Office of Educational Technology describes this as keeping “humans in the loop,” whereby “the human is fully aware and fully in control, but their burden is less, and their effort is multiplied by a complementary technological enhancement.”5\n\n**Adaptability**: Course policies regarding the use of generative AI are best adapted to the local context of the course, including the instructor’s expertise and perspectives; the course learning goals; the nature of the coursework, discipline and/or profession; and the learning needs of the students.\n\nWhile generative AI may not be as useful for one course, it may present an opportunity in another course.\n\nMoreover, different modalities and tools for generative AI may be more or less desirable for a course.\n\nWhile instructors may adopt a range of course policies regarding the use of generative AI, those policies likely will fall within one of four areas on a spectrum:\n- No use\n- Use only when/as directed\n- Use freely in certain cases\n- Use freely in all cases\n\nWhile course policies may be more restrictive than what is found in the senate rules, they must reflect the approved senate syllabi template and language.\n\nFor more restrictive course policies an instructor may need to consider measures to ensure that the policy is followed appropriately.\n\nThis may involve adapting assignments and holding certain activities during class meetings.\n\nAdditionally, for more restrictive course policies it is important to understand the limitations of AI detectors as described in the section of these guidelines on responding to possible misuse.\n\nFor more permissive course policies an instructor may need to consider measures to ensure transparency and appropriate documentation of the use of generative AI.6\n\nAny questions or requests from students related to the use of generative AI as learning accommodations should be referred to the UK Disability Resource Center.\n\n**Effectiveness**: While instructors will adopt a range of policies and approaches for students’ use of generative AI in courses, the written account of those policies should be included clearly on syllabi and other locations where students regularly interact with information about the course.\n\nThis is important for several reasons, including:\n- Students will likely be navigating different policies, requirements, and approaches to the appropriate use of generative AI for their coursework.\n\n- Students may be unsure about generative AI, especially whether it can or should be used for their coursework.7\n- Students may be reluctant to ask about course expectations for the use of generative AI in the face of ambiguity or uncertainty.8\n\nFactors to consider when developing course policies on generative AI include:9\n- A definition of generative AI.\n\nFor example, “Generative AI refers to a range of emerging technologies that draw from training on large datasets to generate new content in written, visual and other forms based on user instructions.” This definition may be expanded or revised to include more specific information that is relevant to the course and discipline.\n\n- A statement on whether the use of generative AI will be permitted for coursework, and if so, how and to what degree it will be permitted.\n\n- A specific description of what constitutes inappropriate use of generative AI in the course as well as the consequences for inappropriate use.\n\n- A process for students to document or cite the use of generative AI for assignments and other course activities (if it is permitted).10\n- A rationale for the policy grounded in the context of the discipline/profession, the learning goals of the course, the skills that will be assessed and/or ethics and academic integrity.\n\n- Links to resources for understanding and using generative AI ethically and effectively.\n\nA learner-centered and student-friendly tone that builds understanding and motivation for students in the course.\n\nAn invitation for students to discuss any questions or concerns with the instructor.",
    "A learner-centered and student-friendly tone that builds understanding and motivation for students in the course.\n\nAn invitation for students to discuss any questions or concerns with the instructor.\n\n**Awareness**: In addition to providing clear course policies regarding generative AI, it is important for instructors and students to be aware of the larger evolving issues concerning the technology:\n- While generative AI continues to become more sophisticated, it has a well-documented history of producing fabricated, incorrect, and misleading information.11 Should instructors or students use generative AI tools, verification of the output will be a critical component of informed use.\n\n- Privacy is a major concern and caution.\n\nMany generative AI tools do not guarantee the protections for private, confidential or sensitive data that may be required (or desired) for certain information.12 For example, student education records (as defined by FERPA)13 and protected research data14 should not be provided to generative AI tools unless/until they have been vetted for data privacy and other governance issues and approved by the University for the proposed use.\n\n- There is an ongoing conversation about the ethics of how generative AI models have been trained on openly available data, both in terms of any biases that the models might inherit from the datasets15 as well as issues of intellectual property in the training data.16\n- Generative AI technologies continue to evolve at a rapid pace, as do the ways in which we can (and can’t) access and use them.\n\nWe cannot assume that the performance nor the use-conditions of generative AI at a particular moment will remain the same or stable in the long term.17\n\nFor assistance in talking with students about generative AI as it relates to teaching and learning, instructors can work with UK’s Center for the Enhancement of Learning and Teaching (CELT) at CELT UKY.",
    "#### Provenance\n\nUniversity Senate maintains requirements, policies and procedures for academic offenses.\n\nSee Senate Rules 6.3 and 6.4 at UKY University Senate.\n\nThe Academic Ombud lists the procedure for processing academic offenses at Ombud UKY.\n\n**Detectors**: Several detectors for text-based generative AI output have been developed.\n\nFor example, TurnItIn (to which all UK instructors have access through the assignments feature in Canvas) activated its own proprietary AI detector on April 4, 2023.\n\nOn July 20, 2023, OpenAI closed off access to its AI classifier, citing a “low rate of accuracy.”18 These technologies are still emerging, but they have demonstrated several problems thus far:\n- They can be prone to false positives, i.e., indicating that all or part of a student’s work is likely AI-generated when it is, in fact, not.19 Relying on these percentages can introduce bias in the assessment process and demoralize students if further action is taken.\n\n- They can be evaded with a combination of prompt manipulation (e.g., iterating a prompt to receive ideal output) and hand-editing the output.20\n- They cannot be verified with other evidence (e.g., as opposed to similarity detectors that link to matching sources).\n\n- They do not guarantee the protection, privacy and confidentiality of any information, data or intellectual property that a student or instructor inputs.21\n- They risk creating a surveillance-based learning environment that can negatively affect student motivation, learning and belonging.22\n\nData on detectors will continue to emerge and require ongoing reassessment, which may result in updated guidelines.\n\n**Data Privacy**: Student education records should not be input into third-party generative AI detection tools or systems unless and until the tools/systems have been vetted for data privacy and other AI governance issues and approved by the university for use.\n\nThis includes generative AI systems that include detection tools.\n\nThe Family Education Rights and Privacy Act (FERPA) applies to all student education records, which are “records that are directly related to a student and that are maintained by an educational agency or institution or a party acting for or on behalf of the agency or institution.”23\n\n**Range of Misuse**: Misuse of generative AI can include “copy/paste” plagiarism scenarios — i.e., submitting text generated by an AI program without attribution, as if it were one’s own writing — but it also can include more ambiguous scenarios such as:\n- Using ideas or approaches that generative AI programs have suggested\n- Reusing information, calculations, analyses or solutions provided by generative AI\n- Revising text generated by an AI program without indicating the initial source\n\nAdditionally, even if a student attributes their use of generative AI, it may still be “misuse” if course policy prohibits it in that scenario.\n\nClear course policies will help students — and instructors — navigate the range of possible misuse cases and understand what to expect if an instructor determines that misuse has occurred.\n\nIt’s important to note that because generative AI’s language models draw from a variety of textual sources (including openly available text on the world wide web), it is possible that writing, ideas and other output produced by generative AI may themselves lack proper attribution of source material and may imitate or reproduce copyrighted material on which they were trained.24\n\n**Responding**: Should an instructor suspect that a student has used generative AI inappropriately in the completion or all or part of an activity or assignment, we would suggest consultation with the department chair or school director and the Academic Ombud.\n\n**Recommendation**: Because of these concerns, we recommend against the use of generative AI detectors for determining academic offenses.\n\nUK ADVANCE will, however, continue to monitor the landscape and update the guidelines as appropriate.",
    "### Assessing Learning with Generative AI in Mind\n\nWhile generative AI can feel like a disruption of instructors’ ability to teach and assess student learning, it is also an invitation to refine pedagogy and assessment.25\n\n**Principles of Assessment Design**: The design of assessments is driven by many context-specific considerations, most importantly:\n- Course and program learning outcomes\n- Competencies and skills in the discipline or profession\n- Scaffolding along the course curriculum\n- Authentic and engaging learning experiences\n- Equity, clarity and structure\n- Opportunities to practice and improve\n- Iterative feedback on performance\n- Reflection on progress and learning\n\nEffective, learning-centered assessments follow from these principles whether they seek to avoid the use of generative AI or engage students in it.\n\n**Design Strategies to Enhance Learning and Mitigate Misuse of Generative AI**: Strategies that mitigate the conditions that may lead to misuse of generative AI also draw from principles of strong learning design.\n\nInstructors may consider how their assessments may draw from any of the following strategies, as is appropriate for the course, students and other factors.\n\n- Segment larger assessments with checkpoints and/or multiple deliverables that value process along with content.\n\nThis strategy helps students to organize and plan their efforts, make steady progress towards a larger goal, increase their self-efficacy and reflect on their decision-making by documenting their process.\n\n- Integrate work towards assessments into class meetings and other planned interactions.\n\nThis strategy ensures that opportunities to study, practice, plan, research, draft, revise, etc., are embedded in the culture of the course, and students are held accountable for doing the work in ways that are constructive for learning.\n\n- Ground assessments in the specific context of the course.\n\nThis strategy asks instructors to integrate unique aspects of the course, whether particular readings, concepts, methods, discussions, cases, etc.\n\n(or a combination thereof) rather than assigning generic tasks that are easily replicated elsewhere.\n\n- Incorporate opportunities to receive iterative feedback on learning and performance via drafts and revisions, development phases, interactive practice, observations, etc.\n\nThis strategy leverages frequent, formative assessment as a way to lower the stakes, engage and motivate students, and focus on opportunities for growth.\n\n- Ask students to contribute their own insights, original analyses and other perspectives.\n\nThis strategy ensures that students are incentivized to add their own informed contributions to the issues at hand, which frames learning as reckoning with rather than merely reproducing extant knowledge.\n\n- Include moments when students need to be conversant about the nature and progress of their own work.\n\nThis strategy engages students in multiple modes and contexts for communicating about their learning and emphasizes the importance of reflecting on their efforts in addition to proving that they have learned something.\n\n- Ask students to express their learning in different genres, formats and modalities.\n\nThis strategy engages students in multimodal and multigenre communication in preparation for careers that will engage them with different stakeholders.\n\n**Uses of Generative AI in Assessments and Other Learning Activities**: While some situations may not call for the use of generative AI, others may be enhanced when it is used strategically.\n\nImportantly, if an instructor integrates generative AI into an assignment, it is a consideration of equity to ensure that all students are supported in the use of digital technologies rather than relying on extant — and uneven — literacies and comfort levels with generative AI tools.26 This might mean walking students through the process of using generative AI for a particular task, providing tips for effective use of generative AI for that task, addressing the tool during office hours, etc.\n\nGenerative AI has shown potential to be effective for learning when used to:27\n- Brainstorm or discover ideas\n- Organize projects or plan efforts\n- Summarize or synthesize information\n- Analyze and visualize data\n- Provide output that students critique or verify\n- Offer different possibilities for approaches to a task\n- Give feedback on ideas or drafts\n- Engage students in retrieval practice, concept review, or other studying\n- Generate materials for demonstrations or case studies\n\nAs part of ensuring that students learn to become strategic and ethical users of generative AI, it will be important to emphasize that students should verify the claims and information in the output, and to understand the framing of that output as only one possibility among many others.",
    "It will also be important for students to document their use of generative AI in a consistent way.28\n\n**Support for Instructors**: For support in any way related to designing activities and assessments in relation to generative AI, instructors can work with CELT at CELT UKY.\n\n**Support for the UK Community**: Contact  with questions, ideas, and recommendations as well as feedback regarding AI-related efforts underway on campus.\n\nThe UK ADVANCE webpage can be viewed at advance.uky.edu.",
    "### References\n\n- Acevedo, Matthew M. “The Panoptic Gaze and the DIscourse of Academic Integrity” In *Critical Digital Pedagogy in Higher Education*, Athabasca University Press, 2023.\n\nDOI: 10.15215/aupress/9781778290015.01.\n\n- Alkaissi, Hussam and Samy I McFarlane.\n\n“Artificial Hallucinations in ChatGPT: Implications in Scientific Writing.” *Cureus*, vol 15, no 2, 19 Feb 2023.\n\nDOI: 10.7759/cureus.35179.\n\n- Amigud, Alexander and David J Pell.\n\n“When academic integrity rules should not apply: a survey of academic staff.” *Assessment & Evaluation in Higher Education*, vol 46, no 6, 2021.\n\nDOI: 10.1080/02602938.2020.1826900.\n\n- Anderson, Nash, et al.\n\n“AI did not write this manuscript, or did it?\n\nCan we trick the AI text detector into generated texts?\n\nThe potential future of ChatGPT and AI in Sports & Exercise Medicine manuscript generation.” *BMJ Open Sport and Exercise Medicine*, vol 9, 2023.\n\nDOI: 10.1136/bmjsem-2023-001568.\n\n- Appel, Gil, Juliana Neelbauer, and David A. Schweidel.\n\n“Generative AI Has an Intellectual Property Problem.” *Harvard Business Review*, 7 April 2023. hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem.\n\n- Athaluri, Sai Anirudh, et al.\n\n“Exploring the Boundaries of Reality: Investigating the Phenomenon of Artificial Intelligence Hallucination in Scientific Writing Through ChatGPT References.” *Cureus*, vol 15, no 4, 11 April 2023.\n\nDOI: 10.7759/cureus.37432.\n\n- Barnett, Sofia.\n\n“ChatGPT Is Making Universities Rethink Plagiarism.” *Wired*, 30 Jan 2023. wired.com/story/chatgpt-college-university-plagiarism.\n\n- Bender, Emily, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.\n\n“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency*, March 2021, pp 610-623.\n\nDOI: 10.1145/3442188.3445922.\n\n- Bens, Susan L. “Helping Students Resolve the Ambiguous Expectations of Academic Integrity.” In *Academic Integrity in Canada: An Enduring and Essential Challenge*, Springer, 2022, pp 377-392.\n\nDOI: 10.1007/978-3-030-83255-1_19.\n\n- Bogost, Ian.\n\n“The First Year of AI College Ends in Ruin.” *The Atlantic*, 16 May 2023. theatlantic.com/technology/archive/2023/05/chatbot-cheating-college-campuses/674073.\n\n- Brown, Tom B, et al.\n\n“Language Models are Few-Shot Learners.” *Proceedings of the 34th International Conference on Neural Information Processing Systems*, 6 Dec 2020. dl.acm.org/doi/abs/10.5555/3495724.3495883.\n\n- Busch, Kristen E. Generative Artificial Intelligence and Data Privacy: A Primer.\n\nCongressional Research Service, 23 May 2023, R47569.\n\ncrsreports.congress.gov/product/pdf/R/R47569.\n\n28.\n\nFor best practices in citing or documenting the use of generative AI, disciplinary or professional style guides may be a useful place to start, e.g., McAdoo 2023.\n\nOther approaches to generative AI documentation may resemble a brief description or summary of research methods.\n\n- Byrd, Antonio, et al.\n\n“MLA-CCCC Joint Task Force on Writing and AI Working Paper: Overview of the Issues, Statement of Principles, and Recommendations.” Modern Language Association of America and Conference on College Composition and Communication, July 2023. aiandwriting.hcommons.org/working-paper-1.\n\n- Caliskan, Aylin, Joanna J Bryson, and Arvind Narayanan.\n\n“Semantics derived automatically from language corpora contain human-like biases.” *Science*, vol 365, no 6334, pp 183-186.\n\nDOI: 10.1126/science.aal4230.\n\n- CCCC-IP Caucus.\n\n“Recommendations Regarding Academic Integrity and the Use of Plagiarism Detection Services.” *Conference on College Composition and Communication.\n\n* culturecat.net/files/CCCC-IPpositionstatementDraft.pdf.\n\n- Chechitelli, Annie.\n\n“AI writing detection update from Turnitin's Chief Product Officer.” *Turnitin*, 23 May 2023. turnitin.com/blog/ai-writing-detection-update-from-turnitins-chief-product-officer.\n\n- Chan, Cecelia Ka Yuk.\n\n“A comprehensive AI policy education framework for university teaching and learning.” *International Journal of Educational Technology in Higher Education*, vol 20, 2023.\n\nDOI: 10.1186/s41239-023-00408-3.\n\n- Chen, Lingjiao, Matei Zaharia, and James Zou.\n\n“How is ChatGPT's behavior changing over time?” arXiv, 18 July 2023.\n\nDOI: 10.48550/arXiv.2307.09009.\n\n- Christian, Jon.\n\n“CNET's Article-Writing AI Is Already Publishing Very Dumb Errors.” *Futurism*, 29 Jan 2023. futurism.com/cnet-ai-errors.\n\n- Cingillioglu, Ilker.\n\n“Detecting AI-generated essays: The ChatGPT challenge.” *International Journal of Information and Learning Technology*, vol 40, no 3, 2023, pp 259-268.\n\nDOI: 10.1108/IJILT-03-2023-0043.\n\n- Council of Writing Program Administrators.\n\n“Defining and Avoiding Plagiarism: The WPA Statement on Best Practices.” 30 Dec 2019. wpacouncil.org/aws/CWPA/pt/sd/news_article/272555/_PARENT/layout_details/false.",
    "“Defining and Avoiding Plagiarism: The WPA Statement on Best Practices.” 30 Dec 2019. wpacouncil.org/aws/CWPA/pt/sd/news_article/272555/_PARENT/layout_details/false.\n\n- Cullen, Courtney S. “Pivoting From Punitive Programs to Educational Experiences: Knowledge and Advice From Research.” *Journal of College and Character*, vol 23, no 1, 2022, pp 48-59.\n\nDOI: 10.1080/2194587X.2021.2017973.\n\n- D’Agostino, Susan.\n\n“Turnitin’s AI Detector: Higher-Than-Expected False Positives.” *Inside Higher Ed*, 1 June 2023. insidehighered.com/news/quick-takes/2023/06/01/turnitins",
    "# AI-Detector-Higher-Expected-False-Positives\n\nDalalah, Doriad and Osama MA Dalalah.\n\n“The false positives and false negatives of generative AI detection tools in education and academic research: The case of ChatGPT.” The International Journal of Management Education, vol 21, no 2, 2023.\n\nDOI: 10.1016/j.ijme.2023.100822.\n\nDobrin, Sidney I.\n\nTalking About Generative AI: A Guide for Educators.\n\nBroadview Press, 15 May 2023. sites.broadviewpress.com/ai/talking/.\n\nEdwards, Benj.\n\n“Why AI detectors think the US Constitution was written by AI.” Ars Technica, 14 July 2023. arstechnica.com/information-technology/2023/07/why-ai-detectors-think-the-us-constitution-was-written-by-ai/.\n\nElectronic Privacy Information Center.\n\nGenerating Harms: Generative AI’s Impact and Paths Forward.\n\nMay 2023. epic.org/documents/generating-harms-generative-ais-impact-paths-forward/.\n\nEllis, Amanda R and Emily Slade.\n\n“A New Era of Learning: Considerations for ChatGPT as a Tool to Enhance Statistics and Data Science Education.” Journal of Statistics and Data Science Education, vol 31, no 2, pp 128-133.\n\nDOI: 10.1080/26939169.2023.2223609.\n\nFrancl, Michelle.\n\n“ChatGPT saves the day.” Nature Chemistry, vol 15, no 7, 19 June 2023, pp 890-891.\n\nDOI: 10.1038/s41557-023-01253-7.\n\nFoltynek, Tomas, et al.\n\n“ENAI Recommendations on the ethical use of Artificial Intelligence in Education.” International Journal for Educational Integrity, vol 19, 2023.\n\nDOI: 10.1007/s40979-023-00133-4.\n\nFowler, Geoffrey A.\n\n“Detecting AI may be impossible.\n\nThat’s a big problem for teachers.” The Washington Post, 2 June 2023.\n\n/.\n\nGannon, Kevin.\n\n“Should You Add an AI Policy to Your Syllabus?” The Chronicle of Higher Education, 31 July 2023. .\n\nHenrique, Da Silva Gameiro, Andrei Kucharavy, and Rachid Guerraoui.\n\n“Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs.” arXiv, 18 April 2023.\n\nDOI: 10.48550/arXiv.2304.08968.\n\nHicks, Maggie.\n\n“Scared of AI?\n\nDon’t Be, Computer-Science Instructors Say.” The Chronicle of Higher Education, 2 Aug 2023. .\n\nHosseini, Mohammed, Lisa M Rasmussen, and David B Resnik.\n\n“Using AI to write scholarly publications.” Accountability in Research, 25 Jan 2023.\n\nDOI: 10.1080/08989621.2023.2168535.\n\nHosseini, Mohammed and Serge PJM Horbach.\n\n“Fighting reviewer fatigue or amplifying bias?\n\nConsiderations and recommendations for use of ChatGPT and other large language models in scholarly peer review.” Research Integrity and peer Review, vol 8, no 4, 18 May 2023.\n\nDOI: 10.1186/s41073-023-00133-5.\n\nHovy, Dirk and Shrimai Prabhumoye.\n\n“Five sources of bias in natural language processing.” Language and Linguistics Compass, vol 15, no 8, 20 Aug 2021.\n\nDOI: 10.1111/lnc3.12432.\n\nHuang, Lan.\n\n“Ethics of Artificial Intelligence in Education: Student Privacy and Data Protection.” Science Insights Education Frontiers, vol 16, no 2, 30 June 2023, pp 2577-2587.\n\nDOI: 10.15354/sief.23.re202.\n\nIAPP.\n\n“Key Terms for AI Governance.” International Association of Privacy Professionals, June 2023. iapp.org/resources/article/key-terms-for-ai-governance/.\n\nJi, Ziwei, et al.\n\n“Survey of Hallucination in Natural Language Generation.” ACM Computing Surveys, vol 55, no 12, 3 March 2023, pp 1-38.\n\nDOI: 10.1145/3571730.\n\nJiang, Zhengyuan, Jinghuai Zhang, and Neil Zhenqiang Gong.\n\n“Evading Watermark based Detection of AI-Generated Content.” arXiv, 5 May 2023.\n\nDOI: 10.48550/arXiv.2305.03807.\n\nKidd, Celeste and Abeba Birhane.\n\n“How AI can distort human beliefs: Models can convey biases and false information to users.” Science, vol 380, no 6651, 22 June 2023, pp 1222-1223.\n\nDOI: 10.1126/science.adi0248.\n\nKrishna, Kalpesh, et al.\n\n“Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense.” arXiv, 23 March 2023.\n\nDOI: 10.48550/arXiv.2303.13408.\n\nLang, James M. Cheating Lessons: Learning from Academic Dishonesty.\n\nHarvard University Press, 2013.\n\nLauer, Mike, Stephanie Constant and Amy Wernimont.\n\n“Using AI in Peer Review Is a Breach of Confidentiality.” NIH Extramural Nexus, 23 June 2023. nexus.od.nih.gov/all/2023/06/23/using-ai-in-peer-review-is-a-breach-of- confidentiality/.\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro.\n\n“Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” New England Journal of Medicine, no 388, 30 March 2023, pp 1233-1239.\n\nDOI: 10.1056/NEJMsr2214184.\n\nLiang, Weixin, et al.\n\n“GPT detectors are biased against non-native English writers.” Patterns, vol 4, no 7, July 2023.\n\nDOI: 10.1016/j.patter.2023.100779.\n\nLiang, Paul Pu, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov.\n\n“Towards Understanding and Mitigating Social Biases in Language Models.” arXiv, 24 June 2021.\n\nDOI: 10.48550/arXiv.2106.13219.\n\nLightman, Hugh, et al.\n\n“Let's Verify Step by Step.” arXiv, 31 May 2023.\n\nDOI: 10.48550/arXiv.2305.20050.\n\nLiu, Nelson F, Tianyi Zhang, and Percy Liang.\n\n“Evaluating Verifiability in Generative Search Engines.” arXiv, 19 April 2023.",
    "DOI: 10.48550/arXiv.2305.20050.\n\nLiu, Nelson F, Tianyi Zhang, and Percy Liang.\n\n“Evaluating Verifiability in Generative Search Engines.” arXiv, 19 April 2023.\n\nDOI: 10.48550/arXiv.2304.09848.\n\nLucchi, Nicola.\n\n“ChatGPT: A Case Study on Copyright Challenges for Generative AI Systems.” SSRN, 25 June 2023.\n\nDOI: 10.2139/ssrn.4483390.\n\nMcAdoo, Timothy.\n\n“How to cite ChatGPT.” APA Style, 23 April 2023. apastyle.apa.org/blog/how-to-cite-chatgpt.\n\nMegahed, Fadel M, et al.\n\n“How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research?\n\nAn Exploratory Study.” arXiv, 17 Feb. 2023.\n\nDOI: 10.48550/arXiv.2302.10916.\n\nMeskó, Bertalan and Eric J Topol.\n\n“The imperative for regulatory oversight of large language models (or generative AI) in healthcare.” npj Digital Medicine, vol 6, July 2023.\n\nDOI: 10.1038/s41746-023-00873-0.\n\nMetze, Konradin, Rosana C Morandin-Reis, Irene Lorand-Metze, and João B Florindo.\n\n“The Amount of Errors in ChatGPT’s Responses is Indirectly Correlated with the Number of Publications Related to the Topic Under Investigation.” Annals of Biomedical Engineering, vol 51, 2023, pp 1360–1361.\n\nDOI: 10.1007/s10439-023-03205-1.\n\nMilla, Anna.\n\n“AI Text Generators and Teaching Writing: Starting Points For Inquiry.” WAC Clearinghouse, 14 Feb 2023. wac.colostate.edu/repository/collections/ai-text-generators-and-teaching-writing-starting-points-for- inquiry/.\n\nMollick, Ethan R and Lilach Mollick.\n\n“Assigning AI: Seven Approaches for Students, with Prompts.” SSRN, 21 June 2023.\n\nDOI: 10.2139/ssrn.4475995.\n\nNajibi, Alex.\n\n“Racial Discrimination in Face Recognition Technology.” Science in the News, 24 Oct 2020, sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/.\n\nNational Institutes of Health.\n\n“The Use of Generative Artificial Intelligence Technologies is Prohibited for the NIH Peer Review Process.” 23 June 2023. grants.nih.gov/grants/guide/notice-files/NOT-OD-23-149.html.\n\nNazer, Lama H, et al.\n\n“Bias in artificial intelligence algorithms and recommendations for mitigation.” PLOS Digital Health, vol 2, no 6, 22 June 2023.\n\nDOI: 10.1371/journal.pdig.0000278.\n\nNicholas, Gabriel and Aliya Bhatia.\n\nLost in Translation: Large Language Models in Non-English Content Analysis.\n\nCenter for Democracy and Technology, May 2023. cdt.org/insights/lost-in-translation-large-language-models-in-non-english-content-analysis/.\n\nOpenAI.\n\n“New AI classifier for indicating AI-written text.” 31 Jan 2023 [20 July 2023].\n\nopenai.com/blog/new-ai- classifier- for-indicating-ai-written-text.\n\nOpenAI.\n\n“GPT-4 Technical Report.” arXiv, 27 March 2023.\n\nDOI: 10.48550/arXiv.2303.08774.\n\nPerkins, Mike.\n\n“Academic Integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond.” Journal of University Teaching and Learning Practice, vol 20, no 2, 2023.\n\nDOI: 10.53761/1.20.02.07.\n\nPoritz, Isaiah.\n\n“OpenAI Hit With First Defamation Suit Over ChatGPT Hallucination.” Bloomberg Law, 7 June 2023. news.bloomberglaw.com/tech-and-telecom-law/openai-hit-with-first-defamation-suit-over-chatgpt-hallucination.\n\nRussell Group.\n\n“Russell Group principles on the use of generative AI tools in education.” 4 July 2023. russellgroup.ac.uk/news/new-principles-on-use-of-ai-in-education/.\n\nRyan, Allison M, Margaret H Gheen, and Carol Midgley.\n\n“Why Do Some Students Avoid Asking for Help?\n\n: An Examination of the Interplay Among Students' Academic Efficacy, Teachers' Social–Emotional Role, and the Classroom Goal Structure.” Journal of Educational Psychology, vol 90, no 3, 1998, pp 528-535.\n\nRyan, Allison M, Paul R. Pintrich and Carol Midgley.\n\n“Avoiding Seeking Help in the Classroom: Who and Why?” Educational Psychology Review, vol 13, 2001, pp 93-114.DOI: 10.1023/a:1009013420053.\n\nSadasivan, Vinu Sankar, et al.\n\n“Can AI-Generated Text be Reliably Detected?” arXiv, 28 June 2023.\n\nDOI: 10.48550/arXiv.2303.11156.\n\nSaveri, Joseph and Matthew Butterick.\n\nLLM Litigation, 2023. llmlitigation.com/.\n\nSchwartz, David L and Maz Rogers.\n\n“ ‘Inventorless’ Inventions?\n\nThe Constitutional Conundrum of AI-Produced Inventions.” Harvard Journal of Law and Technology, vol 35, no 2, Spring 2022, pp 531-579. jolt.law.harvard.edu/volumes/volume-35.\n\nSchwartz, Reva, et al.\n\nTowards a Standard for Identifying and Managing Bias in Artificial Intelligence.\n\nNational Institute of Standards and Technology, March 2022.\n\nDOI: 10.6028/NIST.SP.1270.\n\nSheu, Hung-Bin, Shiqin Stephanie Chong, and Mary E Dawes.\n\n“The Chicken or the Egg?\n\nTesting Temporal Relations Between Academic Support, Self-Efficacy, Outcome Expectations, and Goal Progress Among College Students.” Journal of Counseling Psychology, vol 69, no 5, 2022, pp 589-601.\n\nDOI: 10.1037/COU0000628.\n\nSmall, Zachary.\n\n“Black Artists Say A.I.\n\nShows Bias, With Algorithms Erasing Their History.” The New York Times, 4 July 2023. .\n\nSmall, Zachary.",
    "DOI: 10.1037/COU0000628.\n\nSmall, Zachary.\n\n“Black Artists Say A.I.\n\nShows Bias, With Algorithms Erasing Their History.” The New York Times, 4 July 2023. .\n\nSmall, Zachary.\n\n“Sarah Silverman Sues OpenAI and Meta Over Copyright Infringement.” The New York Times, 10 July 2023.\n\n/.\n\nSmits, Jan and Tijn Borghuis.\n\n“Generative AI and Intellectual Property Rights.” In Law and Artificial Intelligence, T.M.C.\n\nAsser Press, 2022, pp 323–344.\n\nDOI: 10.1007/978-94-6265-523-2_17.\n\nSobel, Benjamin LW.\n\n“Artificial Intelligence’s Fair Use Crisis.” The Columbia Journal of Law and the Arts, vol 41, no 1, 2018, pp 45-97.\n\nDOI: 10.7916/jla.v41i1.2036.\n\nStrowel, Alain.\n\n“ChatGPT and Generative AI Tools: Theft of Intellectual Labor?” IIC - International Review of Intellectual Property and Competition Law, vol 54, 2023, pp 491-494.\n\nDOI: 10.1007/s40319-023-01321-y.\n\nThompson, Stuart A and Tiffany Hsu.\n\n“How Easy Is It to Fool A.I.-Detection Tools?” The New York Times, 4 July 2023. .\n\nThorbecke, Catherine.\n\n“Google hit with lawsuit alleging it stole data from millions of users to train its AI tools.” CNN, 12 July 2023.\n\n/.\n\nThorpe, Vanessa.\n\n“ ‘ChatGPT said I did not exist’: how artists and writers are fighting back against AI.” The Guardian, 18 March 2023. .\n\nTrust, Torrey.\n\n“Essential Considerations for Addressing the Possibility of AI-Driven Cheating, Part 2.” Faculty Focus, 4 Aug 2023.\n\n/.\n\nUS Department of Education Office of Educational Technology.\n\nArtificial Intelligence and the Future of Teaching and Learning: Insights and Recommendations.\n\nWashington DC, 2023 tech.ed.gov/ai-future-of-teaching-and-learning/.\n\nUS Department of Education Student Privacy Policy Office.\n\n“What is an education record?” studentprivacy.ed.gov/faq/what-education-record.\n\nWeber-Wulff, Debora, et al.\n\n“Testing of Detection Tools for AI-Generated Text.” arXiv, 10 July 2023.\n\nDOI: 10.48550/arXiv.2306.15666.\n\nWeise, Karen and Cade Metz.\n\n“When AI Chatbots ‘Hallucinate.’ ” The New York Times, 8 May 2023. .\n\nWeiser, Benjamin.\n\n“Here’s What Happens When Your Lawyer Uses ChatGPT.” The New York Times, 27 May 2023. .\n\nWhite House Office of Science and Technology Policy.\n\nBlueprint for an AI Bill of Rights: Making Automated Systems Work for the American People.\n\nWashington D.C. October 2022. .\n\nWhittaker, Meredith, et al.\n\nDisability, Bias, and AI.\n\nAI Now Institute, ainowinstitute.org/publication/disabilitybiasai-2019.\n\nZamfrescu-Pereira, JD, Richmond Wong, Bjoern Hartmann, and Qian Yang.\n\n“Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts.” Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, April 2023.\n\nDOI: 10.1145/3544548.3581388.\n\nZhang, Muru, et al.\n\n“How Language Model Hallucinations Can Snowball.” arXiv, 22 May 2023.\n\nDOI: 10.48550/arXiv.2305.13534.\n\nZhao, Ruochen, et al.\n\n“Can ChatGPT-like Generative Models Guarantee Factual Accuracy?\n\nOn the Mistakes of New Generation Search Engines.” arXiv, 3 March 2023.\n\nDOI: 10.48550/arXiv.2304.11076.\n\nZhavoronkov, Alex.\n\n“Caution with AI-generated content in biomedicine.” Nature Medicine, vol 29, no 3, 7 Feb. 2023, p 532.\n\nDOI: 10.1038/d41591-023-00014-w.\n\nZhuo, Terry Yue, Yujin Huang, Chunyang Chen, and Zhenchang Xing.\n\n“Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity.” arXiv, 29 May 2023.\n\nDOI: 10.48550/arXiv.2301.12867.\n\nZirpoli, Christopher T. “Generative Artificial Intelligence and Copyright Law.” Congressional Research Service, 11 May 2023, LSB10922, crsreports.congress.gov/product/pdf/LSB/LSB10922.",
    "## About this Guidance\n\nArtificial intelligence is not a new concept for DPOs and data protection professionals.\n\nGenerative AI, however, is.\n\nWhen OpenAI’s ChatGPT launched in November 2022, the majority of data protection professionals had never heard of generative AI and were certainly not concerned with such technologies in their day-to-day work.\n\nNow, with ChatGPT in the hands of over 100m users globally, and many other providers such as Google Bard and Anthropic’s Claude entering the market, it has become an operational reality, and necessity, for data protection professionals to deal with the consequences of generative AI tools being rapidly utilised within organisations.\n\nWhether these tools are adopted simpliciter or are fine-tuned by organisations using their own data sets, novel and as-yet unexamined data protection implications exist, all of which data protection professionals must rapidly come to terms with.\n\nThe aim of this paper is to guide data protection professionals through the maze of issues that are unfolding as these technologies gain rapid adoption in organisations.\n\nAmongst other key issues, this paper looks at data-sharing risks, accuracy of personal data, conducting DPIAs on generative AI tools, implementing data protection by design, selecting a lawful basis for training generative AI systems, optimising organisational structures, applying privacy-enhancing techniques, and handling data subject rights in the context of these technologies.\n\nThere will be no future without generative AI, and with data playing such a pivotal role in the training and operating of these systems, DPOs will play a central role in ensuring that both data protection and data governance standards are at the heart of these technologies.",
    "## Accuracy of Personal Data\n\nThe accuracy of personal data processed by generative artificial intelligence (AI) tools is a fundamental data protection issue with such technologies.\n\nArticle 5 (1) (d) of the GDPR states that ‘Personal data shall be accurate, and where necessary, kept up to date; every reasonable step must be taken to ensure that personal data that are inaccurate (…) are erased or rectified without delay.’\n\nIt is obvious, and a matter of common sense that the processing of inaccurate personal data can have very real-world implications for the data subject behind the data, yet generative AI tools, such as OpenAI’s widely used text-based chatbot, ChatGPT, inherently have numerous inaccuracies in the data they process.\n\nBy their nature these tools ingest vast amounts of training data, sourced from massive data scraping exercises across the internet.\n\nNecessarily, this data comes with all of its imperfections, and becomes a part of the data bank which users of ChatGPT make queries against.\n\nWhen a user receives an answer that is either wholly or partly inaccurate, this generates what AI providers call ‘hallucinations’ or, in the vernacular, ‘falsehoods’.\n\nEven OpenAI itself, on its website, warns users about the perils involved and that the accuracy of data retrieved cannot be automatically trusted.\n\nUnder a section entitled ‘Limitations’ it notes ‘ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers’.\n\nCompounding the issue, OpenAI further notes that the tool will often add to inaccuracies by essentially guessing what an uncertain user means.\n\nIt states: ‘Ideally, the model would ask clarifying questions when the user provided an ambiguous query.\n\nInstead, our current models usually guess what the user intended.’\n\nWhen coupled with the fact that ChatGPT’s data processing terms make it clear that the user is the data controller, while OpenAI is merely the data processor, it should be clear, for users, that this is very much a ‘buyer beware’ market.\n\nWhy?\n\nBecause if any party further processes inaccurate personal data, it will become liable for any non-compliance with Article 5 (1) (d) above.\n\nIn the context of ChatGPT, then, relying upon inaccurate personal data provided by the tool will make the user liable to non-compliance with the GDPR, especially where such re-use impacts the fundamental rights and freedoms of data subjects.\n\nOrganisations should understand that this is not merely a theoretical point and that regulators have already called generative AI companies to account for the accuracy of their data.\n\nIn March 2023, the Italian Data Protection Authority blocked the deployment of ChatGPT in Italy, noting, amongst other matters, that the data was frequently not accurate.\n\nIt noted, based on ‘tests carried out so far, the information made available by ChatGPT does not always match factual circumstances, so that inaccurate personal data are processed.’\n\nData protection officers (DPOs) must remain aware, then, of the risks of processing inaccurate data.\n\nUsers within a DPO’s organisation should be given clear guidelines to help them to understand that the outputs of any generative AI tools, such as ChatGPT, come with a health warning, namely, that the human user is still ultimately responsible for verifying the accuracy of any personal data obtained.\n\nThis is a critical point.\n\nA further related risk comes from the second clause of Article 5 (1) (d), that personal data shall be ‘kept up to date’.\n\nChatGPT, and similar tools such as Google’s Bard and Anthropic’s Claude, rely on data scraping activities up to a certain point in time, meaning that their data bank becomes out- of-date and so, eventually, are necessarily not responding to up–to-date events.\n\nThis creates the clear risk that users will obtain personal data that is no longer relevant, or perhaps lacks context, or is simply outrightly inaccurate, given how events have changed or how information has moved forward in the intervening period.\n\nDPOs should also remain aware of the ways in which unmitigated bias and discrimination in the training sets could indirectly lead to inaccurate data outputs, again opening up the user to the risks of further processing inaccurate data.\n\nA final, global risk with generative AI chatbots is the tone that they adopt: an oracular level of certainty and authority that might almost be called a dark pattern, so misleading is it in its effect on the evaluation of search results.\n\nWhen generative AI chatbots are palpably wrong or inaccurate, they are often wrong in a very confident and confusingly definitive manner, an attitude that masks the fact that, as OpenAI, for instance admits, the answer may simply be ‘nonsense’.\n\nIn any search results, the tone of the response should be ignored, and again, users should realise that the output of these tools requires human evaluation, certainly when it concerns questions over the accuracy of any personal data involved.",
    "## Sharing Personal Data with Generative AI Tools\n\nArtificial Intelligence (AI) has rapidly evolved from a concept of science fiction to a relatively common feature of our life.\n\nA rapidly emerging branch of AI is generative AI, which can create new, previously non-existing data that closely mimics the input data.\n\nGenerative AI models can, under the right conditions, generate high-quality text, images, music, and more.\n\nHowever, the convenience and innovative potential of generative AI comes with a cost.\n\nDespite its promising capabilities, the sharing of personal data with these systems presents substantial risks for privacy, confidentiality, and the integrity and security of data.\n\nUnderstanding these risks is essential in order to protect individual data protection rights, and to maintain a secure digital environment.\n\nLike most AI systems, generative AI is data-driven.\n\nTraditional AI training involves feeding large datasets into AI models which can then learn patterns and features from this data.\n\nOnce the training is complete, the AI system is equipped to generate outputs based on the patterns and features learned.\n\nThis means that once personal data is part of the AI’s training set, it contributes to the formation of the AI’s internal model, and will invariably influence its behaviour and outputs.\n\nEffectively, the data becomes “part” of the AI, in the sense that it informs the system’s understanding and knowledge.\n\nThis presents significant data protection concerns where personal data features as training data.\n\nGenerative AI models trained on personal data can potentially extract sensitive information like names, addresses, health information, or even financial data, and then republish that data in search results for different users.\n\nAdditionally, generative AI models can amplify exposure by generating more data similar to the original input.\n\nThird parties may then exploit this data for unlawful activities including invasive advertising, phishing scams, or in more serious cases, fraud or identity theft.\n\nThis highlights the complexities of controlling how personal data is used by generative AI models.\n\nOnce personal data has been shared with generative AI models, managing and tracking its usage becomes an intricate (if not impossible) task, due to the nature of how AI systems process information as well as store and replicate data across different systems.\n\nTherefore, retracting personal data shared with generative AI models may be incredibly difficult or unrealistic.\n\nThe lesson for DPOs is that users must understand precisely what kinds of information can and cannot be shared with generative AI tools, because once personal data is shared, the Rubicon has been crossed, and it will be very difficult to undo what has been done.\n\nOne of the more alarming risks associated with sharing personal data with generative AI is the creation and proliferation of ‘deepfakes’.\n\nDeepfakes refer to the application of AI to create, alter or manipulate content, such as images, audio, and video, in such a way that it fabricates hyper-realistic but entirely false content.\n\nBy training on personal data, generative AI can generate synthetic media that convincingly impersonate natural or legal persons.\n\nThese deepfakes can then be used maliciously, such as in disinformation campaigns, fraud, or harassment.\n\nRelated to this is the fact that the accuracy of generative AI decisions heavily depends on the quality and diversity of the input training data.\n\nIf this personal data is biased, the AI's outputs can also become biased, leading to unfair consequences.\n\nGenerative AI holds significant promise for numerous applications, but its use of personal data must be carefully managed to mitigate potential risks.\n\nBy employing strong data protection controls, ethical AI practices, and robust legal protections, it may be possible to harness the potential of generative AI while safeguarding individual data protection rights and fostering a safe and secure digital environment.",
    "## What is an Appropriate Lawful Basis?\n\nThe lawful basis that properly applies to the training of AI systems with personal data is a key consideration.\n\nPrima facie, there is no obvious candidate that would both clearly legitimise this processing activity and also uphold the data protection rights of affected individuals.\n\nThis is a critical consideration because the volume of training data that is used for generative AI applications is enormous, and only growing in size.\n\nIf such training activities are to continue, and if AI is to deliver on its promise, then it cannot be founded on an uncertain lawful basis as regards personal data.\n\nMoreover, the Artificial Intelligence Act (AI Act) is not particularly instructive on this point given that Article 10, (which deals with data governance and the governance of training data for AI systems), does not create a lawful basis specific to the use of personal data for the training of AI systems.\n\nIt is, then, to the GDPR that we must turn for a suitable lawful basis for this activity.\n\nFirstly, we will briefly look at how data is used to train generative AI systems.\n\nThis takes place in four broad ways:\n- Based on personal data that has been scraped from the internet;\n- Where the personal data has been provided by the users of the AI system, such as when they submit prompts to Generative AI tools;\n- Where the personal data has been collected from third parties, such as data brokers, or companies that have databases which are relevant to the AI training phase (for instance, a database of court decisions for a predictive AI tool in the legal domain); and\n- When AI developers/operators use the personal data held in their own databases to train the AI system.\n\nIn these cases, under Article 6 of the GDPR, three lawful bases are most relevant: contract, legitimate interest, and consent.",
    "### Contract\n\nArticle 6(1)(b) of the GDPR notes that contract may form a legal basis for processing personal data where that ‘processing is necessary for the performance of a contract to which the data subject is party or in order to take steps at the request of the data subject prior to entering into a contract.’\n\nThe application of the first branch of the contract legal basis (i.e., the performance of the contract itself) would require demonstrating that the training of the AI system (and not the use of the AI once trained) is strictly necessary to the performance of a contract with the data subject.\n\nThis necessity requirement is interpreted very narrowly by the data protection authorities.\n\nAccording to the European Data Protection Board (EDPB), it should not be possible to perform the main subject-matter of the specific contract with the data subject if the processing of the personal data in question does not occur.\n\nIn other words, processing the personal data in this way should be a necessary condition for performing the contract.\n\nConsidering this narrow interpretation, there is very little room for the contract basis when training an AI system.\n\nThis basis could theoretically be applied when the use of the AI system is the subject matter of the contract entered into between the AI operator and the user, and when there is no other way to perform this contract than to train the AI with the data of the users.\n\nAs for the second branch of this legal basis, i.e., the pre-contractual steps, its application would require demonstrating that a data subject made a request in the context of potentially entering a contract and that there is no other way to meet his/her demands than to train (and not only use once trained) the AI.\n\nThis is an even more restrictive and limited option than the first part of this legal basis.\n\nOn the whole, the circumstances in which the lawful basis of contract might be used to justify training AI systems with personal data are very limited and, in practical terms, this basis will not be a viable option for grounding such processing activities.\n\nIn the case of generative AI, contract as a lawful basis is, in any case, particularly unsuitable given that typically no contract exists between the data subjects whose data is used, and the organisations responsible for training such systems with that data.",
    "### Legitimate Interests\n\nThe legitimate interests basis could only apply provided that a legitimate interest assessment is completed by the data controller to ensure that these interests are not overridden by the interests or fundamental rights and freedoms of the data subject.\n\nThis may however be challenging especially since, most of the time, the organisation behind training generative AI tools, such as OpenAI, is not in direct contact with the data subjects, nor does it have any form of relationship with those data subjects.\n\nIn this regard, the recent actions of the Italian Data Protection Supervisory Authority (Garante per la protezione dei dati) against ChatGPT should be noted.\n\nIn March 2023, the authority blocked ChatGPT in the Italian territory until OpenAI was able to satisfactorily answer certain questions, one of which was that OpenAI needed to specify the lawful basis for training ChatGPT with personal data.\n\nIn its response to this point, OpenAI identified legitimate interests as the lawful basis.\n\nThis is a highly significant commitment and statement by OpenAI as it effectively ties the huge task of training generative AI systems to a lawful basis that is inherently uncertain, given data subjects’ explicit right under Article 21 of the GDPR to object to such processing.\n\nTo effectively be able to rely on the legitimate interests basis would require in particular:\n- a study on a case-by-case basis of the context of training and of use of the AI, as well of the collection of the personal data used to verify that the data processing will meet the reasonable expectations of the data subjects;\n- a demonstration of the strict necessity of this processing and of the fact that the AI cannot work efficiently without being trained with the personal data in question;\n- an enhanced transparency of the data processing towards the data subjects.\n\nThe provision of all the required information under GDPR would need to be provided to the data subjects in an appropriate way;\n- an effective opt-out system brought to the knowledge of the data subjects within a reasonable period before their data are provided to the AI system;\n- more generally an efficient system for ensuring the respect of the data subjects’ rights which would be difficult to implement given the particularities of generative AI functioning.",
    "### Consent\n\nThe consent basis could also apply, but only in very clearly circumscribed circumstances.\n\nWhile in the extreme cases, it may be the only legal basis possible (for instance when processing special categories of data or data concerning minors) as a general rule it has very little place in the training of generative AI systems, as currently conceived.\n\nThe entire apparatus used for the training of AI systems makes it almost impossible to obtain consent.\n\nThis is because, in the first instance, the majority of the data used to train such systems is purchased from data brokers that have obtained this data by scraping the internet, an activity which necessarily does not involve the obtaining of consent from underlying data subjects.\n\nIndeed, the very lawfulness of data scraping as a commercial activity is far from certain and the recent joint communication by twelve global data protection authorities, including the UK’s Information Commissioner’s Office, underlines this point.\n\nTo use consent as a lawful basis, would require meeting all the requirements for valid consent under the GDPR, meaning that it would need to result from a clear affirmative action, be freely given, specific, informed and unambiguous.\n\nThis is indeed a very high bar to reach in the world of training AI systems.\n\nIf the AI provider is not in contact with the data subjects, as is generally the case, this consent would have to be collected by the user of the AI system, the organisation with whom the data subject does have a relationship.\n\nHowever, this will usually be after the fact, when the AI system has already been trained, so to object to the processing would most of the time be irrelevant since the data processing would have already occurred.\n\nMoreover, it would also be very difficult to reverse when vast amounts of personal data regarding numerous data subjects will already have been ingested by the AI system.\n\nIn conclusion, legitimate interest is most likely the most suitable basis for training AI systems with personal data, however, as stated above, it does not provide a certain foundation given the need for a legitimate interests assessment to be carried out, as well as the fact that data subjects can object to such processing at any point.",
    "## Risks of Jailbreaking and Data Protection Safeguards\n\nSoon after ChatGPT was released, hackers began attempting to \"jailbreak\" the AI chatbot, trying to bypass its safeguards and make it say inappropriate or irrational things.\n\nThese intricately phrased prompts that aim to bypass the restrictions imposed on AI programmes have come to be known as ‘Jailbreaks’.\n\nThis term was originally used in the context of digital technology to refer to the act of gaining access to the operating system of a smartphone or tablet, especially one manufactured by Apple, in order to run modified or unauthorised software.\n\nIn the context of Generative AI models, the term now refers to the design of prompts that make the chatbots bypass rules around producing hateful content or writing about illegal acts.\n\nThese attacks involve manipulating the generative AI systems to produce content that goes against their intended rules, such as generating hateful or illegal material.\n\nAnother use of these attacks could be slander and a personal attack upon an individual once personal data has been leaked.\n\nA security firm that specialises in AI, was able to break GPT-4, OpenAI's latest text-generating chatbot, in just a few hours after the initial release of the system.\n\nUsing carefully crafted prompts, the CEO of the security firm bypassed OpenAI's safety systems and quickly had GPT-4 generating homophobic statements, creating phishing emails, and endorsing violence.\n\nThis deviant behaviour poses a serious risk as it has the potential to expose personal data that has been inadvertently, or perhaps even intentionally, input into the system and, thus, has the potential to be manipulated by bad actors.\n\nA closely-related attack is the prompt injection attack that can quietly insert malicious data or instructions into AI models.\n\nA prompt injection attack aims to elicit an unintended response from LLM-based tools.\n\nAnd then achieve unauthorised access, manipulate responses, or bypass security measures.\n\nThe specific techniques and consequences of prompt injection attacks vary depending on the system.\n\nJailbreaks and prompt injection attacks are a form of unconventional hacking, using well-crafted sentences instead of code to exploit weaknesses in AI systems.\n\nWhile these attacks are currently focused on bypassing content filters, security researchers warn of the potential for data theft and widespread cybercriminal activities as generative AI systems become more prevalent.\n\nNumerous popular online services and products heavily rely on large datasets to train and improve their AI algorithms.\n\nData streams from networks, social media platforms, mobile devices, and various other sources contribute to the vast amount of information that businesses utilise to train their machine learning systems.\n\nIt is, hence, important to note that some of the data contained within these datasets could probably be considered personal data, even by users who are less concerned about data protection.\n\nUnfortunately, due to the misuse and mishandling of personal data by certain companies, data protection has consequently become a pressing global policy issue.\n\nIn a similar vein, much of our sensitive data is also gathered to enhance AI-enabled processes.\n\nThis data plays a crucial role in driving the adoption of machine learning, as sophisticated algorithms rely on such data for real-time decision-making.\n\nSearch algorithms, voice assistants, recommendation engines, and other AI solutions leverage extensive datasets of real-world user data to provide personalised and relevant outputs.\n\nEarly in 2023, a website called Jailbreak Chat was launched where prompts for AI chatbots like ChatGPT from online forums are collected and shared.\n\nVisitors to the site can contribute their own jailbreaks, try out prompts submitted by others, and vote on their effectiveness.\n\nMalicious users could leverage these jailbreaks to gather personal data contained within the systems to carry out crimes like identity theft and to create deepfakes to impersonate living individuals.\n\nThe implications of jailbreaks and prompt injection attacks become more significant when these systems gain access to personal and sensitive data.\n\nFor example, if a successful prompt injection attack instructs a personal assistant AI to ignore previous instructions and send an email to all contacts, it could lead not only to embarrassment on the part of the individual but to widespread issues for the affected individuals and the rapid spread of harmful content across the individual’s personal and working networks.\n\nEnsuring the safety of foundation models like ChatGPT is paramount as their use becomes more widespread.\n\nThe hackers, however, will not give up easily.\n\nAs AI systems have evolved, the jailbreaks have become more complex.\n\nSome involve multiple characters, intricate backstories, translation, and even elements of coding to generate specific outputs.",
    "As AI systems have evolved, the jailbreaks have become more complex.\n\nSome involve multiple characters, intricate backstories, translation, and even elements of coding to generate specific outputs.\n\nSome authorised \"red teams\" prompt attacks on AI models to uncover vulnerabilities.\n\nA red team in cybersecurity represents the offensive security team, which is responsible for discovering security vulnerabilities through penetration testing.\n\nWith GAI, these teams look for exploits that include actual vulnerabilities, influencing the system’s behaviour, or deceiving users to get around the system's security.\n\nOther attempts come from hobbyists who like to showcase humorous or disturbing outputs on social media.\n\nThis approach to security is suboptimal as it is fragmented and relies on viral exposure and influential individuals to prompt fixes.\n\nWhile companies like OpenAI, Google, and Microsoft have taken steps to address jailbreaking and prompt injection attacks, the researchers behind these attacks continue to find new ways to exploit vulnerabilities.\n\nThe development of generative AI systems requires approaches beyond traditional red-teaming methods, such as using a second AI model to analyse prompts or clearly separating system prompts from user prompts.\n\nAutomation and advanced techniques are necessary to identify and mitigate jailbreaks and injection attacks at scale.\n\nBy automating the process of identifying vulnerabilities and unintended behaviours, researchers aim to discover and address a greater number of these security risks.\n\nThese types of automated techniques can be seen as the starting point for a deeper commitment from AI developers to assess and evaluate the safety of their systems.\n\nBy involving a diverse range of participants and prioritising transparency and accountability, the goal is to enhance the safety, reliability, and ethical use of generative AI technology.\n\nThird-party assessments, automated mitigation of jailbreaks, and using red-teaming will play a pivotal role in achieving this goal and improving the practices surrounding AI development in order to meet the requirements of both the GDPR and the forthcoming AI Act.",
    "## How are Data Subject Rights Implemented with Generative AI Tools?\n\nGenerative AI, or GenAI, are AI systems capable of generating text, images, or other media in response to prompts.\n\nGenerative models learn the patterns and structure of input data, subsequently generating new content similar to the training data, but with a degree of novelty, as opposed to merely classifying or predicting data.\n\nThese AI systems are often based on Generative Pretrained Transformers (GPT), artificial neural networks built on the transformer architecture, pretrained on large sets of unlabelled text data, and capable of generating human-like text.",
    "# Understanding Generative AI and Data Protection\n\nThey employ large language models (LLMs) to produce data based on the training dataset that was used to create them.\n\nUnderstanding the technology behind generative AI is vital to realising that these tools encompass various phases, and personal data can be processed at each phase.\n\nHowever, the processing of personal data at one phase does not necessarily imply data processing at another.\n\nThe stages under data protection law where data subject rights pertaining to personal data might apply in the generative AI context include:\n\n- The training data phase, when personal data is incorporated.\n\n- The deployment phase, where personal data is used to generate content and the content result itself.\n\n- The model itself, which might contain personal data.\n\nIt is also essential to point out that generative AI software can indirectly process data, particularly related to the user of the solution, such as account data or metadata related to the use of the solution.\n\nIn common machine learning models, identifying the individuals that the training data is about is a potential challenge to ensuring their rights.\n\nUsually, this data includes only the information pertinent to predictions, without unique data subject identifiers.\n\nIt undergoes various pre-processing measures to make it suitable for machine learning algorithms, often transforming personal data into a form that's harder (but not impossible) to link back to specific individuals.\n\nData protection laws might, therefore, still apply to this transformed data, as it could still be used to identify individuals.\n\nThis process necessitates consideration when responding to individuals' rights requests.\n\nThis process is different for generative AI models than it is for common machine learning models as explained in the previous paragraph.\n\nGenerative AI models are often trained with data accessible on the web, and their value also often lies in generating results related to physical persons, implying a significant amount of personal data in the training data for these models.\n\nAs a result, these datasets could be the target of data subject requests.\n\nIn generative AI models, 'continuous learning' also poses unique challenges for GDPR compliance.\n\nThese models are regularly updated based on user interactions, meaning personal data is continuously processed.\n\nThis data mostly originates from the interactions and prompts of the tool's users, and it should be noted that the data subjects and the data providers are not necessarily the same entity in the context of continuously learning AI models.\n\nGiven these considerations, navigating data rights under the General Data Protection Regulation (GDPR) in the context of generative AI models presents unique challenges, particularly for the rights of Erasure, Rectification, Access, and Objection.\n\nThe first shared issue is the non-retrievability of data in generative AI models.\n\nAs previously mentioned, these models source data from a wide array of origins, like web scraping and user interactions.\n\nThis multifaceted approach to data collection makes it more difficult to trace individual contributions.\n\nFurthermore, in contrast to traditional data storage systems, in GenAI systems, personal data are also deeply embedded within complex algorithms, complicating the isolation of specific data.\n\nThis makes it challenging to fulfil GDPR rights since identifying whether and where personal data are processed within the system.\n\nAdding another layer of complexity is the issue of \"inferred personal data.\"\n\nThese are conclusions that the model may draw based on its training.\n\nFor example, a generative AI model could deduce a user's political affiliations based on past data interactions.\n\nThe prevailing opinion leans towards including these inferences when responding to rights requests, as they could indirectly reveal personal information.\n\nThe concept of \"inferred group data\" also deserves attention.\n\nThis type of data is generated based on broader patterns recognised during training.\n\nWhether this group data is considered personal depends on its subsequent processing and utilisation.\n\nBesides common challenges, there are also specific ones related to individual rights that require data modification or erasure.\n\nNotably, altering or removing data from the training set after a Data Subject Request (DSR) could impact the model's validation and correctness.\n\nThe original data often serves as a foundation for such validation processes.\n\nMoreover, the erasure or modification of data that is already embedded in the model would often imply removing or modifying this data to retrain the model, a task that is both costly and time-consuming.\n\nIn summary, the intersection of GDPR rights and generative AI models presents a labyrinth of challenges, each with its own intricacies and complications.",
    "In summary, the intersection of GDPR rights and generative AI models presents a labyrinth of challenges, each with its own intricacies and complications.\n\nThe very nature of these models, from the way they embed and process data to the difficulties in tracking individual contributions, adds layers of complexity to GDPR compliance.\n\nWhile no silver bullet exists to seamlessly navigate these challenges, the evolving landscape does offer some emerging solutions that could serve as starting points for compliance.",
    "# Data Protection by Design\n\nData Protection by Design: How to Build Generative AI Tools in Compliance with the GDPR\n\nData protection by design plays a pivotal role in ensuring compliance with the General Data Protection Regulation (GDPR).\n\nIt entails safeguarding personal data from the very early stages of design throughout the entire lifecycle of the system.\n\nThe idea of data protection by design came from a more general set of privacy principles entitled Privacy by Design first developed in Canada in the early 2000s.\n\nPrivacy by Design is an approach to systems engineering that was initially developed by Ann Cavoukian and formalised in a report on privacy-enhancing technologies by a joint team of the Information and Privacy Commissioner of Ontario (Canada), the Dutch Data Protection Authority, and the Netherlands Organisation for Applied Scientific Research in 1995.\n\nThe Privacy by Design framework was published in 2009 and adopted by the International Assembly of Privacy Commissioners and Data Protection Authorities in 2010.\n\nIn the same year, the International Conference of Data Protection Authorities and Privacy Commissioners unanimously passed a resolution recognising Privacy by Design as an essential component of fundamental privacy protection.\n\nThis was followed by the U.S. Federal Trade Commission’s inclusion of Privacy by Design as one of three recommended practices for protecting online privacy.\n\nShortly after 2010, Europe began working on revising its data protection laws.\n\nInspired by Privacy by Design and its principles, Europe put together data protection by design principles which were introduced into law via Article 25 of the General Data Protection Regulation (GDPR) in 2018.\n\nIn recent years, the swift development of generative AI has given rise to an increased awareness of potential risks and ethical considerations when designing systems that process personal data.\n\nThese concerns encompass not only complex data protection risks like the leakage of sensitive information and chat histories but also a range of threats to the data subject rights of EU citizens, including the \"right to be forgotten.\"\n\nThis right allows individuals to request the deletion of their personal data by a company.\n\nWhile deleting data from databases is relatively straightforward, removing data from machine learning models is a more complex task.\n\nAnonymisation techniques and data minimisation practices can help strike a balance between upholding individuals' rights and preserving the overall usefulness of the generative AI model.\n\nSomething to consider from a human perspective is that due to the complexity of modern AI systems, the people involved in building and deploying AI systems are often likely to have a wider range of skills and backgrounds than the usual systems developers, including traditional software engineering, systems administration, data scientists, statisticians, as well as domain experts.\n\nBecause of this wide range of expertise, there may be less understanding of broader security compliance requirements, as well as those of data protection law more specifically.\n\nFor these individuals, security of personal data may not always have been a key priority, especially if someone was previously building AI applications with non-personal data or in a research capacity where personal data was protected in sandboxes.\n\nBiased algorithms are another significant data protection concern.\n\nGenerative AI systems learn from vast amounts of data, and if that data is biased, the algorithms can perpetuate and amplify these biases in their outputs.\n\nThis raises ethical questions about fairness, discrimination, and the potential harm caused by biased AI-generated content when used to make important, life-changing decisions about data subjects.\n\nAI hallucinations refer to instances where generative AI systems produce outputs that are not based on real or accurate information.\n\nThese hallucinations can mislead users and have potential implications for the safety of data subjects.\n\nGenerative AI systems must provide reliable and trustworthy outputs, especially about European citizens whose personal data and its accuracy is protected under the GDPR.\n\nThe rise of deepfakes, which are realistic but manipulated audio or video content, has also been associated with generative AI technology.\n\nDeepfakes have the potential to manipulate public opinion, spread misinformation, and pose risks to public safety.\n\nThe ethical implications of deepfakes highlight the need for robust measures to prevent their creation and to detect and combat their dissemination.\n\nA fundamental aspect of data protection by design is transparency.\n\nIt plays a crucial role in data protection by design and ensures accountability within AI systems.\n\nOrganisations must be transparent about their data practices, providing clear explanations of how AI systems work and the decisions they make.",
    "Organisations must be transparent about their data practices, providing clear explanations of how AI systems work and the decisions they make.\n\nHowever, achieving transparency in AI systems can be challenging due to their complexity.\n\nIt is essential to develop methods and tools that enable the explanation of algorithmic predictions to end-users in a meaningful and understandable manner.\n\nFurther complications arise because common practices about how to process personal data securely in data science and AI engineering are still under development.\n\nAs part of an organisation’s compliance with the security principle of GDPR, they should ensure that they actively monitor and take into account the state-of-the-art security practices when developing AI systems and when using personal data in an AI context.\n\nIt is not possible to list all known security risks that might be exacerbated by the use of AI to process personal data.\n\nWhatever the risk, however, companies should ensure that staff have appropriate skills and knowledge to address not only security risks but also data protection risks.\n\nThis is where the importance of GDPR training comes in.\n\nThe effectiveness of AI models heavily relies on the quality of the data they receive, making data protection an integral aspect of their design.\n\nThe utilisation of sensitive data during the training of generative AI algorithms can result in the emergence of personal information in chatbot outputs or compromise data security during cyberattacks.\n\nThus, when designing AI products, it is paramount to decouple personal data from individual users through the use of synthetic datasets with full anonymisation and non-reversible identifiers for algorithmic training, auditing, and quality assurance, among other practices.\n\nImplementing strict controls on data access within the company and conducting regular audits can help prevent data breaches.\n\nIt is also important to acknowledge that more data does not necessarily equate to better solutions.\n\nTesting algorithms using data minimisation can help determine the least amount of data required for a viable use case.\n\nAdditionally, providing a streamlined process for users to request the removal of their personal data is critical.\n\nAdopting adversarial learning techniques, which involve combining conflicting datasets during the machine learning process, can help identify flaws and biases in AI algorithm outputs.\n\nAdditionally, exploring the use of synthetic datasets that do not contain actual personal data is a potential approach, although further research is required to assess their effectiveness.",
    "# Privacy-Enhancing Techniques and Synthetic Data\n\nGenerative AI tools are complex tools, and like all such technologies, they present many significant legal challenges.\n\nGenerative AI is hungry for data, but such data, (especially quality data), may be hard to come by or may be legally protected, either from an intellectual property or a data protection legislation's standpoint.\n\nFrom the data protection perspective, privacy-enhancing technologies (PETs) may represent a valid solution to tackle data protection concerns, in terms of data minimisation, integrity, confidentiality, and data protection by design.\n\nThe European Union Agency for Cybersecurity (ENISA) defines PETs as “software and hardware solutions (e.g., systems encompassing technical processes, methods or knowledge) to achieve specific privacy or data protection functionality or to protect against risks of privacy of an individual or a group of natural persons.”\n\nAmong the various PETs that could be deployed in the context of generative AI, data synthesis algorithms which generate “artificial” data, better known as synthetic data, can play a pivotal role.\n\nAccording to the European Data Protection Supervisor (EDPS) “Synthetic data is artificial data that is generated from original data and a model that is trained to reproduce the characteristics and structure of the original data (...).\n\nThe generation process, also called synthesis, can be performed using different techniques, such as decision trees or deep learning algorithms.\n\nSynthetic data can be classified with respect to the type of the original data: the first type employs real datasets, the second employs knowledge gathered by the analysts instead, and the third type is a combination of these two.”\n\nIn essence, synthetic data is computer-generated data which is derived from existing real data, or from algorithms and models which replicate, fully or partially, features, patterns, and properties of real-world data.\n\nThe use of synthetic data may therefore bring many advantages when it comes to the training of generative AI tools, particularly as it:\n\n- reduces the need for harvesting large amounts of real personal data.\n\nIn the AI model-training phase, this is especially important as it allows engineers to generate much larger datasets from relatively small amounts of personal data;\n- allows near-perfect labelling (e.g., exactly defined for the developing of a specific AI model) and higher quality data, thereby supplementing or substituting real-world datasets.\n\nA study from Gartner has predicted that “by 2024, 60% of the data used for the development of AI and Analytics projects will be synthetically generated”;\n- if properly detected and corrected, potentially reduces the bias or statistical imbalance of the original datasets, thereby increasing the fairness of decision-making that relies on the data;\n- strengthens privacy and reduces the cybersecurity attack surface by limiting the risk of loss of confidentiality, integrity, or availability of real personal information;\n- reduces the costs involved at all stages of the data value chain by limiting the need for excessive data collection, cleaning, preparation, and data storage.\n\nHowever, this does not mean that synthetic data is the complete solution for all data protection issues.\n\nThere are still some legal concerns that must be taken into consideration by DPOs.\n\nFirstly, synthetic data does not necessarily correspond to anonymous data, which means that re-identification risk, to one degree or another, will remain.\n\nIn practice, synthetic data aims at replicating real-world data, and the more it is an accurate proxy, keeping all the features and patterns of the original data, the more efficient it will be for the generative AI model trained on such data; but, on the other hand, the downside is that such efficiency will, in direct proportion, increase the risk of re-identification.\n\nThis means that the risk of inferring data related to a specific individual from the synthetic dataset, or from the AI model itself, will not be extinguished.\n\nAs noted by the UK’s Information Commissioner’s Office (ICO), “You should focus on the extent to which people are identified or identifiable in the synthetic data, and what information about them would be revealed if identification is successful.\n\nSome synthetic data generation methods have been shown to be vulnerable to model inversion attacks, membership inference attacks and attribute disclosure risk.\n\nThese can increase the risk of inferring a person’s identity….\"\n\nThe use of other PET’s (such as differential privacy) or the suppression of outliers (data points with some uniquely identifying features), can serve to reduce the risk of re-identification of personal data, but not entirely eliminate it.",
    "Furthermore, synthetic data’s generation phase may involve the processing of personal data, especially upon collection and analysis of real datasets, which entails the need to abide by the GDPR and related obligations.\n\nSpecific mention should also be made of the duty to provide full information under Art.\n\n13 of GDPR to data subjects whose data is being collected and then used for AI training purposes, as well as to identify a lawful basis of processing under Art.\n\n6 of GDPR.\n\nFinally, the obligation to strictly respect the principles under Art.\n\n5 of GDPR always stands where personal data is concerned.\n\nIn particular, some of the following principles from Art.\n\n5 are worth mentioning in the case of generative AI:\n\n- transparency: this is not limited to the information to be provided to data subjects under Art.\n\n13 GDPR as mentioned above, but also towards users, with reference to synthetic outputs generated by AI models, in order to avoid the risk of deep fakes and/or social manipulation.\n\n- purpose limitation: as synthetic data may be derived from real data, which may contain personal information, there is the need to outline that such data has been collected for specified, explicit and legitimate purposes and that the further processing (e.g., for data synthetisation and subsequent AI model training) is not incompatible with the initial purposes.\n\nA similar principle has been established in relation to the anonymisation process by WP Art.\n\n29 (opinion 5/2014) according to which: “the anonymisation process, meaning the processing of (…) personal data to achieve their anonymisation, is an instance of “further processing”.\n\nAs such, this processing must comply with the test of compatibility in accordance with the guidelines provided by the Working Party in its Opinion 03/2013 on purpose limitation”.\n\nEspecially with regard to the training phase of AI models, the reference to the “statistical purposes” as not in principle incompatible with the initial purposes under lett.\n\nb) of art.\n\n5, ss.1, might serve this purpose.\n\n- accuracy and fairness: attention must be given here to avoiding the risk of “hallucination”, or of duplicating bias, errors or inaccuracies contained in the original dataset.\n\nThis is particularly important if the AI model trained by the synthetic data will then be used to adopt decisions which might affect people’s rights or interests.\n\nOf paramount importance for this specific purpose will be the development of techniques that enable the explainability of the outputs generated by AI systems trained by making use of synthetic data.",
    "# Issues Specific to Image- and Audio-Based Generative AI\n\nIn the case of non-text-based generative AI applications, such as image, audio and video generating tools, clear data protection implications exist.\n\nPopular applications, such as Midjourney and Stable Diffusion, which allow users to rapidly generate images and videos by inputting text prompts, are built on large volumes of image and video content.\n\nThis underlying data includes numerous categories of personal data sufficient to identify data subjects, the central one being the very image and likeness of a data subject that will often be represented in the outputs.\n\nSpecifically, DPOs can expect the following personal data categories to be involved in such tools:\n\n- photo images of data subjects;\n- artistic representations of data subjects;\n- video footage of data subjects; and\n- audio, voice-based data.\n\nOrganisations will have to understand that the further processing of such data brings the GDPR into scope.\n\nFor instance, if a marketing department wants to create promotional material, and uses images of data subjects garnered from generative AI, it will have to process those images in line with data protection laws, and respect fundamental principles such as transparency, lawfulness, and fairness.\n\nFurthermore, the issue of combining the data from generative AI sources with data from other sources should be considered.\n\nWhile the data received from the generative AI tool may not identify the data subject, the act of combining it with alternative data may do so, and once again, bring GDPR requirements into view.\n\nThis could be particularly relevant where, for example, the pasting together of images from different sources leads to the identification of individuals.\n\nIn the more creative use-cases, where organisations may wish to modify, alter or significantly change the presentation of images, videos, or audio content, this should be carried out with respect for data subjects’ fundamental rights and freedoms.\n\nRisks, for example, of defaming or damaging data subjects, should always be taken into account, and where it is considered that the processing may be high risk, a DPIA should be conducted.\n\nFinally, where organisations wish to create legitimate ‘deepfake’ content, such as, perhaps, official corporate videos, issues of data subject consent and transparency of processing should be key considerations.",
    "# Managing Data Protection Risk\n\nCarrying out a data protection impact assessment (DPIA) when implementing or using a generative AI system becomes even more crucial when, as is often the case, these tools have not yet been properly understood, both from the perspective of business strategy and risk management.\n\nThe understanding of the risks to personal data from generative AI processing is still evolving and all DPOs must try to be alive to as-yet unanticipated threats and challenges.\n\nTo manage these emerging risks, the following factors should be taken into account.",
    "## Risks to Data Subjects\n\nThe relationship between the user and AI, as well as the impacts that the processing will have on individuals should be at the heart of the analysis.\n\nPotential risks to data subjects include:\n\n- Impacts from a partially or fully automated decision produced by generative AI.\n\nThe consequences of such decisions may consist of financial opportunity losses or even restrictions on fundamental rights.\n\n- Risks of reinforcing discrimination and bias against certain users.\n\n- Risks arising from the processing of special category data as outlined in Art.\n\n9 GDPR.\n\nFor instance, a generative AI tool could infer from certain personal data of the person concerned, (from their expression modalities or the use of certain words), their ethnic origin, political or philosophical positions, or even the sexual orientation of the person concerned, and apply differential treatment on this basis.\n\nIn order to identify such risks, the company deploying the generative AI tool should conduct a regular review of the quality of the results generated.\n\nIn terms of IT security, information available to the attacker in the AI system can be a threat vector.\n\nA so-called \"white box\" scenario, where the attacker can deduce/find a lot of technical information to prepare his attack creates more exposure compared to a \"black box\" system where the attacker can only access the information produced by the system as a",
    "# Identifying Mitigation Measures\n\nThe DPIA, as always, should be conducted before project initiation and should then, via data protection by design, inform and guide the design stage for any generative AI tool.\n\nIn the case of generative AI, the following mitigants should be taken into account to manage the identified risks:\n\n- Supervised fine-tuning with exemplary conversations where an LLM is trained to reproduce a corpus of conversations that illustrate what is deemed to be a desired behavior.\n\n- Fine-tuning with a human value model where human operators will reward the most satisfactory results.\n\n- In addition, organizational measures should aim to ensure a constant evaluation of the results provided by the generative AI tool, both at the level of the human operator who uses it and an organizational entity that analyzes the results on a large scale in order to ensure a high level of result quality over time.\n\n- Similarly, we should strive as much as possible for a situation of explainability of the decisions taken by the generative AI model to allow genuine human control.\n\nIn this regard, human control ultimately remains the best method of mitigating risks raised by generative AI systems.\n\nBy this means, excessive confidence in the results produced by generative AI tools can be avoided.\n\nSuch overconfidence would lead, in the absence of effective human controls, to the production of entirely automated decisions.\n\nAn additional consideration for DPOs is the emerging AI governance requirement to conduct Fundamental Rights Impact Assessments (FRIAs).\n\nIn the draft text of the AI Act, which, at the date of publication of this paper, is still in the trialogue stage of discussions within the EU Legislature, a requirement to carry out FRIAs is included.\n\nThe intention is that such an assessment would need to be completed by either a provider or user of an AI system, where there are risks to the fundamental rights and freedoms of individuals who are affected by the output.\n\nGiven that FRIAs are, in effect, akin to DPIAs for the world of AI, with particular overlaps in understanding how processing activities impact fundamental rights, DPOs should expect that this work will be assigned to them once the AI Act comes into effect.\n\nAlthough, in some respects, DPOs are uniquely placed, and qualified, to do this work, they are not necessarily naturally conversant in the novel technological risks that are rapidly being created by AI technologies.\n\nFor this reason, DPOs should already be researching and understanding AI-specific risks to personal data.\n\nFrom the practical perspective, it may be possible to conduct FRIAs and DPIAs as one exercise, but whatever method is ultimately chosen, DPOs must start developing knowledge of AI risk now, in anticipation of the AI Act.",
    "# Transparency and Generative AI\n\nWhen gathering and feeding data, including personal data to an AI for training, and when this data processing is governed by GDPR, the entity operating this training (the AI operator) must ensure the transparency of said data processing pursuant to Article 5 § 1 a) and 12 et seqq.\n\nof said regulation.\n\nThree different sources of data can be identified:\n\n- The scraping of data from websites with the help of robots or AI systems (Use Case 1);\n- The provision of data by users of the system or data suppliers concerning other individuals (Use Case 2);\n- The provision of data concerning themselves by users of the AI (Use Case 3).",
    "## Use Case 1\n\nTransparency is a delicate and perhaps challenging issue when considering online data scraping, mainly due to the fact that any personal data gathered in this manner is not gathered directly from the data subject.\n\nAs a result, Article 14 of the GDPR should apply to such data, i.e., personal data that has not been gathered from the data subject directly, entitles the data subject to the right to obtain from the controller confirmation as to whether their personal data is being processed, and, if so, the access to their personal data should be provided along with other vital information such as the purpose for processing and the categories of data that is being processed and so on.\n\nAdditionally, Article 15 of the GDPR regarding the right of access by the data subject to their personal information should apply.\n\nIn such a scenario, however, several difficulties present themselves to the AI operator.\n\nEspecially the following:\n\n- Identifying personal data among the data automatically retrieved by the AI, which usually consists of vast amounts of data;\n- Directly identifying each individual data subject;\n- Obtaining sufficient contact information to inform each data subject of the processing of their data.\n\nIn light of these difficulties, Article 14.5 (b) of the GDPR could be applied.\n\nThis section of the article stipulates that a data controller would not have to provide the specified information to each data subject when “the provision of such information proves impossible or would involve a disproportionate effort.” Case law from various data protection authorities shows that this exception should be interpreted very strictly.\n\nThis being said, given the difficulties identified above regarding generative AI models, it could be applied here.\n\nIf so, the AI operator would, however, still be bound under the transparency requirements to the data subject.\n\nPursuant to said Article 14.5 (b), the data controller should take appropriate measures to protect the data subject's rights and freedoms and legitimate interests.\n\nSuch measures include the publication of the controller’s privacy policy on its website, but also, possibly more stringent measures like the example given by the Italian Data Protection Authority when regulating ChatGPT earlier in 2023.\n\nUltimately, OpenAI agreed to carry out an information campaign, of a non-promotional nature, across all the main Italian mass media (radio, television, newspapers, and the Internet) to inform people of the probable collection of their personal data for the purpose of training ChatGPT.\n\nThey also agreed to make a tool available on the data controller’s website, through which all interested parties could exercise their right to access their personal data.\n\nOn the other hand, regarding such a right to access, Article 11 of the GDPR may also apply, which stipulates that:\n\n- “1.\n\nIf the purposes for which a controller processes personal data do not or do no longer require the identification of a data subject by the controller, the controller shall not be obliged to maintain, acquire or process additional information in order to identify the data subject for the sole purpose of complying with this Regulation.\n\n- Where, in cases referred to in paragraph 1 of this Article, the controller is able to demonstrate that it is not in a position to identify the data subject, the controller shall inform the data subject accordingly, if possible.\n\nIn such cases, Articles 15 to 20 shall not apply except where the data subject, for the purpose of exercising his or her rights under those articles, provides additional information enabling his or her identification”.\n\nAdditionally, we are reminded in Recital 4 of the GDPR that, “the right to the protection of personal data is not an absolute right; it must be considered in relation to its function in society and be balanced against other fundamental rights, in accordance with the principle of proportionality.” As a result, it could be argued that disproportionate efforts cannot be imposed on the AI operator to identify the applicant and detect their personal data in the training data of the AI.\n\nIn light of the above, the AI operator facing an access request should:\n\n- Verify if the personal data concerning the applicant can be identified;\n- Provide the applicant will all personal data identified;\n- Inform the data subject that there may be personal data concerning them that the AI operator is not in a position to detect/provide given the characteristics of the data processing being carried out.\n\nAlso, to comply with Article 25 GDPR and the data protection by design principle, the AI operator may also be obliged to demonstrate that they can anticipate such access requests and that they have reviewed all the technical possibilities that they could reasonably deploy to detect the personal data concerning each applicant (and that it reassesses regularly these possibilities).",
    "## Use Case 2\n\nSince data is usually supplied to the AI operators along the supply chain by other third parties further up the supply chain (a user or a data supplier).\n\nThese third parties could assist the AI operator in ensuring transparency in the processing of data by providing tools and guidance on how best to extract personal data from the data set, given that it is these third parties that supply the data sets in the first place.\n\nThese third parties could also help the AI operator when dealing with access requests from data subjects for the same reasons.",
    "## Use Case 3\n\nWhen personal data is collected directly from the users, Article 13 of the GDPR applies.\n\nThe data controller must provide specific information to the data subject at the time of collection, for instance, the identity and the contact details of the data controller; the contact details of their data protection officer; the purposes of the processing for which the personal data is intended as well as the legal basis for the processing; along with other specific information.",
    "# Optimising Organisational Structures\n\nWithin any organization, from a management structure-perspective, the topic of Generative AI will have to be addressed in a multidimensional way, as a reflection of the complexity of the technology and its impacts.\n\nIt will not be viable for companies to have each function working alone and not interacting with each other.\n\nThe impact of AI is an enterprise issue; therefore, it requires a joined-up enterprise-wide approach.\n\nSuch an integrated approach is essential in order to avoid duplication of efforts, but more importantly, to ensure that key decisions receive multi-disciplinary input.\n\nTo achieve this, organizations should put in place an AI taskforce, focusing on responsible AI and its governance.\n\nThe creation of such a task force could be an initiative driven by the DPO, as one of the functions that will have the biggest exposure to this topic due to the fact that he has to manage some AI considerations in a context where personal data is involved.\n\nAlternatively, it could be initiated and led by an IT function, such as a Chief Data Officer, or Chief Technology Officer.\n\nThis task force will significantly involve the legal department, compliance functions, and specifically, data protection.\n\nFor the technical aspects, the IT Security department should be represented.\n\nThe task force may involve communications and PR staff, as it will be necessary to communicate internally, and potentially externally, on the decisions taken by the task force.\n\nThe leader of the task force may establish focus groups in which selected members of the taskforce focus on specific questions and report back their results to the taskforce.\n\nThe above diagram gives an indicative idea of the composition of these focus groups and how they would relate to the Responsible AI Governance Taskforce.\n\nThe mission of the task force is to respond to the immediate need for Responsible AI governance within the organization and to examine and manage the risks in the use of generative AI, specifically, with regard to personal data, bias, ethical concerns, emerging AI regulation, and numerous legal issues such as intellectual property rights and liability exposure.\n\nThe main goal of this task force will be to define an action plan.\n\nA critical aspect of this action plan will be to conduct an inventory of the AI systems used in the company, which includes generative AI.\n\nAnother critical aspect is to define roles and responsibilities for all the functions in the group.\n\nThe role of this AI taskforce is also to raise awareness of AI issues at all levels of the company.\n\nThis point is important, as the risk will naturally come from the employees that are the day-to-day users of the technology, but it has to be linked with the highest level of decision-making, because deciding on the way to use (or not use) generative AI is an enterprise strategy.\n\nAs an initial task, the task force should prepare preliminary guidance for the organization regarding the responsible use of generative AI, which would, for example, include the recommendation not to enter personal data in prompts of relevant tools like ChatGPT, nor to upload images with identifiable persons.\n\nRegardless of the complexity of the technology, and its implementation, the DPO’s role in this taskforce is ultimately to ensure that any personal data processed via AI technologies is compliant with the GDPR.",
    "## Generative Artificial Intelligence and Copyright Law\n\nUpdated September 29, 2023\n\nInnovations in artificial intelligence (AI) are raising new questions about how copyright law principles such as authorship, infringement, and fair use will apply to content created or used by AI.\n\nSo-called \"generative AI\" computer programs—such as Open AI’s DALL-E and ChatGPT programs, Stability AI’s Stable Diffusion program, and Midjourney’s self-titled program—are able to generate new images, texts, and other content (or “outputs”) in response to a user’s textual prompts (or “inputs”).\n\nThese generative AI programs are trained to generate such outputs partly by exposing them to large quantities of existing works such as writings, photos, paintings, and other artworks.\n\nThis Legal Sidebar explores questions that courts and the U.S.\n\nCopyright Office have begun to confront regarding whether generative AI outputs may be copyrighted and how generative AI might infringe copyrights in other works.",
    "### Do AI Outputs Enjoy Copyright Protection?\n\nThe question of whether or not copyright protection may be afforded to AI outputs—such as images created by DALL-E or texts created by ChatGPT—likely hinges at least partly on the concept of \"authorship.\"\n\nThe U.S. Constitution authorizes Congress to “secur[e] for limited Times to Authors .\n\n.\n\n.\n\nthe exclusive Right to their .\n\n.\n\n.\n\nWritings.” Based on this authority, the Copyright Act affords copyright protection to “original works of authorship.” Although the Constitution and Copyright Act do not explicitly define who (or what) may be an “author,” the U.S.\n\nCopyright Office recognizes copyright only in works “created by a human being.” Courts have likewise declined to extend copyright protection to nonhuman authors, holding that a monkey who took a series of photos lacked standing to sue under the Copyright Act; that some human creativity was required to copyright a book purportedly inspired by celestial beings; and that a living garden could not be copyrighted as it lacked a human author.\n\nA recent lawsuit challenged the human-authorship requirement in the context of works purportedly “authored” by AI.\n\nIn June 2022, Stephen Thaler sued the Copyright Office for denying his application to register a visual artwork that he claims was authored “autonomously” by an AI program called the Creativity Machine.\n\nDr. Thaler argued that human authorship is not required by the Copyright Act.\n\nOn August 18, 2023, a federal district court granted summary judgment in favor of the Copyright Office.\n\nThe court held that “human authorship is an essential part of a valid copyright claim,” reasoning that only human authors need copyright as an incentive to create works.\n\nDr. Thaler has stated that he plans to appeal the decision.\n\nAssuming that a copyrightable work requires a human author, works created by humans using generative AI could still be entitled to copyright protection, depending on the nature of human involvement in the creative process.\n\nHowever, a recent copyright proceeding and subsequent Copyright Registration Guidance indicate that the Copyright Office is unlikely to find the requisite human authorship where an AI program generates works in response to text prompts.\n\nIn September 2022, Kris Kashtanova registered a copyright for a graphic novel illustrated with images that Midjourney generated in response to text inputs.\n\nIn October 2022, the Copyright Office initiated cancellation proceedings, noting that Kashtanova had not disclosed the use of AI.\n\nKashtanova responded by arguing that the images were made via “a creative, iterative process.” On February 21, 2023, the Copyright Office determined that the images were not copyrightable, deciding that Midjourney, rather than Kashtanova, authored the “visual material.” In March 2023, the Copyright Office released guidance stating that, when AI “determines the expressive elements of its output, the generated material is not the product of human authorship.”\n\nSome commentators assert that some AI-generated works should receive copyright protection, arguing that AI programs are like other tools that human beings have used to create copyrighted works.\n\nFor example, the Supreme Court has held since the 1884 case Burrow-Giles Lithographic Co. v. Sarony that photographs can be entitled to copyright protection where the photographer makes decisions regarding creative elements such as composition, arrangement, and lighting.\n\nGenerative AI programs might be seen as a new tool analogous to the camera, as Kashtanova argued.\n\nOther commentators and the Copyright Office dispute the photography analogy and question whether AI users exercise sufficient creative control for AI to be considered merely a tool.\n\nIn Kashtanova’s case, the Copyright Office reasoned that Midjourney was not “a tool that [] Kashtanova controlled and guided to reach [their] desired image” because it “generates images in an unpredictable way.” The Copyright Office instead compared the AI user to “a client who hires an artist” and gives that artist only “general directions.” The office’s March 2023 guidance similarly claims that “users do not exercise ultimate creative control over how [generative AI] systems interpret prompts and generate materials.” One of Kashtanova’s lawyers, on the other hand, argues that the Copyright Act does not require such exacting creative control, noting that certain photographs and modern art incorporate a degree of happenstance.\n\nSome commentators argue that the Copyright Act’s distinction between copyrightable “works” and noncopyrightable “ideas” supplies another reason that copyright should not protect AI-generated works.\n\nOne law professor has suggested that the human user who enters a text prompt into an AI program—for instance, asking DALL-E “to produce a painting of hedgehogs having a tea party on the beach”—has “contributed nothing more than an idea” to the finished work.",
    "According to this argument, the output image lacks a human author and cannot be copyrighted.\n\nWhile the Copyright Office’s actions indicate that it may be challenging to obtain copyright protection for AI-generated works, the issue remains unsettled.\n\nApplicants may file suit in U.S. district court to challenge the Copyright Office’s final decisions to refuse to register a copyright (as Dr. Thaler did), and it remains to be seen whether federal courts will agree with all of the office’s decisions.\n\nWhile the Copyright Office notes that courts sometimes give weight to the office’s experience and expertise in this field, courts will not necessarily adopt the office’s interpretations of the Copyright Act.\n\nIn addition, the Copyright Office’s guidance accepts that works “containing” AI-generated material may be copyrighted under some circumstances, such as “sufficiently creative” human arrangements or modifications of AI-generated material or works that combine AI-generated and human-authored material.\n\nThe office states that the author may only claim copyright protection “for their own contributions” to such works, and they must identify and disclaim AI-generated parts of the work if they apply to register their copyright.\n\nIn September 2023, for instance, the Copyright Office Review Board affirmed the office’s refusal to register a copyright for an artwork that was generated by Midjourney and then modified in various ways by the applicant, since the applicant did not disclaim the AI-generated material.",
    "### Who Owns the Copyright to Generative AI Outputs?\n\nAssuming some AI-created works may be eligible for copyright protection, who owns that copyright?\n\nIn general, the Copyright Act vests ownership “initially in the author or authors of the work.” Given the lack of judicial or Copyright Office decisions recognizing copyright in AI-created works to date, however, no clear rule has emerged identifying who the “author or authors” of these works could be.\n\nReturning to the photography analogy, the AI’s creator might be compared to the camera maker, while the AI user who prompts the creation of a specific work might be compared to the photographer who uses that camera to capture a specific image.\n\nOn this view, the AI user would be considered the author and, therefore, the initial copyright owner.\n\nThe creative choices involved in coding and training the AI, on the other hand, might give an AI’s creator a stronger claim to some form of authorship than the manufacturer of a camera.\n\nCompanies that provide AI software may attempt to allocate the respective ownership rights of the company and its users via contract, such as the company’s terms of service.\n\nOpenAI’s Terms of Use, for example, appear to assign any copyright to the user: “OpenAI hereby assigns to you all its right, title and interest in and to Output.” A previous version, by contrast, purported to give OpenAI such rights.\n\nAs one scholar commented, OpenAI appears to “bypass most copyright questions through contract.”",
    "## Copyright Infringement by Generative AI\n\nGenerative AI also raises questions about copyright infringement.\n\nCommentators and courts have begun to address whether generative AI programs may infringe copyright in existing works, either by making copies of existing works to train the AI or by generating outputs that resemble those existing works.",
    "### Does the AI Training Process Infringe Copyright in Other Works?\n\nAI systems are “trained” to create literary, visual, and other artistic works by exposing the program to large amounts of data, which may include text, images, and other works downloaded from the internet.\n\nThis training process involves making digital copies of existing works.\n\nAs the U.S. Patent and Trademark Office has described, this process “will almost by definition involve the reproduction of entire works or substantial portions thereof.” OpenAI, for example, acknowledges that its programs are trained on “large, publicly available datasets that include copyrighted works” and that this process “involves first making copies of the data to be analyzed” (although it now offers an option to remove images from training future image generation models).\n\nCreating such copies without permission may infringe the copyright holders’ exclusive right to make reproductions of their work.\n\nAI companies may argue that their training processes constitute fair use and are therefore noninfringing.\n\nWhether or not copying constitutes fair use depends on four statutory factors under 17 U.S.C.\n\n§ 107:\n1. the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes;\n2. the nature of the copyrighted work;\n3. the amount and substantiality of the portion used in relation to the copyrighted work as a whole; and\n4. the effect of the use upon the potential market for or value of the copyrighted work.\n\nSome stakeholders argue that the use of copyrighted works to train AI programs should be considered a fair use under these factors.\n\nRegarding the first factor, OpenAI argues its purpose is “transformative” as opposed to “expressive” because the training process creates “a useful generative AI system.” OpenAI also contends that the third factor supports fair use because the copies are not made available to the public but are used only to train the program.\n\nFor support, OpenAI cites The Authors Guild, Inc. v. Google, Inc., in which the U.S. Court of Appeals for the Second Circuit held that Google’s copying of entire books to create a searchable database that displayed excerpts of those books constituted fair use.\n\nRegarding the fourth fair use factor, some generative AI applications have raised concern that training AI programs on copyrighted works allows them to generate similar works that compete with the originals.\n\nFor example, an AI-generated song called “Heart on My Sleeve,” made to sound like the artists Drake and The Weeknd, was heard millions of times on streaming services.\n\nUniversal Music Group, which has deals with both artists, argues that AI companies violate copyright by using these artists’ songs in training data.\n\nOpenAI states that its visual art program DALL-E 3 “is designed to decline requests that ask for an image in the style of a living artist.”\n\nPlaintiffs have filed multiple lawsuits claiming the training process for AI programs infringed their copyrights in written and visual works.\n\nThese include lawsuits by the Authors Guild and authors Paul Tremblay, Michael Chabon, Sarah Silverman, and others against OpenAI; separate lawsuits by Michael Chabon, Sarah Silverman, and others against Meta Platforms; proposed class action lawsuits against Alphabet Inc. and Stability AI and Midjourney; and a lawsuit by Getty Images against Stability AI.\n\nThe Getty Images lawsuit, for instance, alleges that “Stability AI has copied at least 12 million copyrighted images from Getty Images’ websites .\n\n.\n\n.\n\nin order to train its Stable Diffusion model.” This lawsuit appears to dispute any characterization of fair use, arguing that Stable Diffusion is a commercial product, weighing against fair use under the first statutory factor, and that the program undermines the market for the original works, weighing against fair use under the fourth factor.\n\nIn September 2023, a U.S. district court ruled that a jury trial would be needed to determine whether it was fair use for an AI company to copy case summaries from Westlaw, a legal research platform, to train an AI program to quote pertinent passages from legal opinions in response to questions from a user.\n\nThe court found that, while the defendant’s use was “undoubtedly commercial,” a jury would need to resolve factual disputes concerning whether the use was “transformative” (factor 1), to what extent the nature of the plaintiff’s work favored fair use (factor 2), whether the defendant copied more than needed to train the AI program (factor 3), and whether the AI program would constitute a “market substitute” for Westlaw (factor 4).\n\nWhile the AI program at issue might not be considered “generative” AI, the same kinds of facts might be relevant to a court’s fair-use analysis of making copies to train generative AI models.",
    "### Do AI Outputs Infringe Copyrights in Other Works?\n\nAI programs might also infringe copyright by generating outputs that resemble existing works.\n\nUnder U.S. case law, copyright owners may be able to show that such outputs infringe their copyrights if the AI program both (1) had access to their works and (2) created “substantially similar” outputs.\n\nFirst, to establish copyright infringement, a plaintiff must prove the infringer “actually copied” the underlying work.\n\nThis is sometimes proven circumstantially by evidence that the infringer “had access to the work.” For AI outputs, access might be shown by evidence that the AI program was trained using the underlying work.\n\nFor instance, the underlying work might be part of a publicly accessible internet site that was downloaded or “scraped” to train the AI program.\n\nSecond, a plaintiff must prove the new work is “substantially similar” to the underlying work to establish infringement.\n\nThe substantial similarity test is difficult to define and varies across U.S. courts.\n\nCourts have variously described the test as requiring, for example, that the works have “a substantially similar total concept and feel” or “overall look and feel” or that “the ordinary reasonable person would fail to differentiate between the two works.” Leading cases have also stated that this determination considers both “the qualitative and quantitative significance of the copied portion in relation to the plaintiff’s work as a whole.” For AI-generated outputs, no less than traditional works, the “substantial similarity” analysis may require courts to make these kinds of comparisons between the AI output and the underlying work.\n\nThere is significant disagreement as to how likely it is that generative AI programs will copy existing works in their outputs.\n\nOpenAI argues that “[w]ell-constructed AI systems generally do not regenerate, in any nontrivial portion, unaltered data from any particular work in their training corpus.” Thus, OpenAI states, infringement “is an unlikely accidental outcome.” By contrast, the Getty Images lawsuit alleges that “Stable Diffusion at times produces images that are highly similar to and derivative of the Getty Images.” One study has found “a significant amount of copying” in less than 2% of the images created by Stable Diffusion, but the authors claimed that their methodology “likely underestimates the true rate” of copying.\n\nTwo kinds of AI outputs may raise special concerns.\n\nFirst, some AI programs may be used to create works involving existing fictional characters.\n\nThese works may run a heightened risk of copyright infringement insofar as characters sometimes enjoy copyright protection in and of themselves.\n\nSecond, some AI programs may be prompted to create artistic or literary works “in the style of” a particular artist or author, although—as noted above—some AI programs may now be designed to “decline” such prompts.\n\nThese outputs are not necessarily infringing, as copyright law generally prohibits the copying of specific works rather than an artist’s overall style.\n\nRegarding the AI-generated song “Heart on My Sleeve,” for instance, one commentator notes that the imitation of Drake’s voice appears not to violate copyright law, although it may raise concerns under state right-of-publicity laws.\n\nNevertheless, some artists are concerned that AI programs are uniquely capable of mass-producing works that copy their style, potentially undercutting the value of their work.\n\nPlaintiffs in one lawsuit against Stable Diffusion, for example, claim that few human artists can successfully mimic another artist’s style, whereas “AI Image Products do so with ease.”\n\nA final question is who is (or should be) liable if generative AI outputs do infringe copyrights in existing works.\n\nUnder current doctrines, both the AI user and the AI company could potentially be liable.\n\nFor instance, even if a user were directly liable for infringement, the AI company could potentially face liability under the doctrine of “vicarious infringement,” which applies to defendants who have “the right and ability to supervise the infringing activity” and “a direct financial interest in such activities.” The lawsuit against Stable Diffusion, for instance, claims that the defendant AI companies are vicariously liable for copyright infringement.\n\nOne complication of AI programs is that the user might not be aware of—or have access to—a work that was copied in response to the user’s prompt.\n\nUnder current law, this may make it challenging to analyze whether the user is liable for copyright infringement.",
    "## Considerations for Congress\n\nCongress may consider whether any of the copyright law questions raised by generative AI programs require amendments to the Copyright Act or other legislation.\n\nCongress may, for example, consider legislation clarifying whether AI-generated works are copyrightable, who should be considered the author of such works, or when the process of training generative AI programs constitutes fair use.\n\nGiven how little opportunity the courts and Copyright Office have had to address these issues, Congress may adopt a wait-and-see approach.\n\nAs the courts gain experience handling cases involving generative AI, they may be able to provide greater guidance and predictability in this area through judicial opinions.\n\nBased on the outcomes of these cases, Congress may reassess whether legislative action is needed.",
    "## Disclaimer\n\nThis document was prepared by the Congressional Research Service (CRS).\n\nCRS serves as nonpartisan shared staff to congressional committees and Members of Congress.\n\nIt operates solely at the behest of and under the direction of Congress.\n\nInformation in a CRS Report should not be relied upon for purposes other than public understanding of information that has been provided by CRS to Members of Congress in connection with CRS’s institutional role.\n\nCRS Reports, as a work of the United States Government, are not subject to copyright protection in the United States.\n\nAny CRS Report may be reproduced and distributed in its entirety without permission from CRS.\n\nHowever, as a CRS Report may include copyrighted images or material from a third party, you may need to obtain the permission of the copyright holder if you wish to copy or otherwise use copyrighted material.",
    "## About this Document\n\nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was published by the White House Office of Science and Technology Policy in October 2022.\n\nThis framework was released one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered world.” Its release follows a year of public engagement to inform this initiative.\n\nThe framework is available online at:",
    "## About the Office of Science and Technology Policy\n\nThe Office of Science and Technology Policy (OSTP) was established by the National Science and Technology Policy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office of the President with advice on the scientific, engineering, and technological aspects of the economy, national security, health, foreign relations, the environment, and the technological recovery and use of resources, among other topics.\n\nOSTP leads interagency science and technology policy coordination efforts, assists the Office of Management and Budget (OMB) with an annual review and analysis of Federal research and development in budgets, and serves as a source of scientific and technological analysis and judgment for the President with respect to major policies, plans, and programs of the Federal Government.",
    "## Legal Disclaimer\n\nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper published by the White House Office of Science and Technology Policy.\n\nIt is intended to support the development of policies and practices that protect civil rights and promote democratic values in the building, deployment, and governance of automated systems.\n\nThe Blueprint for an AI Bill of Rights is non-binding and does not constitute U.S. government policy.\n\nIt does not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, or international instrument.\n\nIt does not constitute binding guidance for the public or Federal agencies and therefore does not require compliance with the principles described herein.\n\nIt also is not determinative of what the U.S. government’s position will be in any international negotiation.\n\nAdoption of these principles may not meet the requirements of existing statutes, regulations, policies, or international instruments, or the requirements of the Federal agencies that enforce them.\n\nThese principles are not intended to, and do not, prohibit or limit any lawful activity of a government agency, including law enforcement, national security, or intelligence activities.\n\nThe appropriate application of the principles set forth in this white paper depends significantly on the context in which automated systems are being utilized.\n\nIn some circumstances, application of these principles in whole or in part may not be appropriate given the intended use of automated systems to achieve government agency missions.\n\nFuture sector-specific guidance will likely be necessary and important for guiding the use of automated systems in certain settings such as AI systems used as part of school building security or automated health diagnostic systems.\n\nThe Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of equities, for example, between the protection of sensitive law enforcement information and the principle of notice; as such, notice may not be appropriate or may need to be adjusted to protect sources, methods, and other law enforcement equities.\n\nEven in contexts where these principles may not apply in whole or in part, federal departments and agencies remain subject to judicial, privacy, and civil liberties oversight as well as existing policies and safeguards that govern automated systems, including, for example, Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020).\n\nThis white paper recognizes that national security (which includes certain law enforcement and homeland security activities) and defense activities are of increased sensitivity and interest to our nation’s adversaries and are often subject to special requirements, such as those governing classified information and other protected data.\n\nSuch activities require alternative, compatible safeguards through existing policies that govern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and Responsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and Framework.\n\nThe implementation of these policies to national security and defense activities can be informed by the Blueprint for an AI Bill of Rights where feasible.\n\nThe Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, or defense, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a waiver of sovereign immunity.",
    "## Introduction\n\nAmong the great challenges posed to democracy today is the use of technology, data, and automated systems in ways that threaten the rights of the American public.\n\nToo often, these tools are used to limit our opportunities and prevent our access to critical resources or services.\n\nThese problems are well documented.\n\nIn America and around the world, systems supposed to help with patient care have proven unsafe, ineffective, or biased.\n\nAlgorithms used in hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embed new harmful bias and discrimination.\n\nUnchecked social media data collection has been used to threaten people’s opportunities, undermine their privacy, or pervasively track their activity—often without their knowledge or consent.\n\nThese outcomes are deeply harmful—but they are not inevitable.\n\nAutomated systems have brought about extraordinary benefits, from technology that helps farmers grow food more efficiently and computers that predict storm paths, to algorithms that can identify diseases in patients.\n\nThese tools now drive important decisions across sectors, while data is helping to revolutionize global industries.\n\nFueled by the power of American innovation, these tools hold the potential to redefine every part of our society and make life better for everyone.\n\nThis important progress must not come at the price of civil rights or democratic values, foundational American principles that President Biden has affirmed as a cornerstone of his Administration.\n\nOn his first day in office, the President ordered the full Federal government to work to root out inequity, embed fairness in decision-making processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.\n\nThe President has spoken forcefully about the urgent challenges posed to democracy today and has regularly called on people of conscience to act to preserve civil rights—including the right to privacy, which he has called “the basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this country.”\n\nTo advance President Biden’s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence.\n\nThe Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats—and uses technologies in ways that reinforce our highest values.\n\nResponding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by a technical companion—a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process.\n\nThese principles help provide guidance whenever automated systems can meaningfully impact the public’s rights, opportunities, or access to critical needs.",
    "## Blueprint for an AI Bill of Rights\n\nSafe and Effective Systems  \nYou should be protected from unsafe or ineffective systems.\n\nAutomated systems should be developed with consultation from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts of the system.\n\nSystems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based on their intended use, mitigation of unsafe outcomes including those beyond the intended use, and adherence to domain-specific standards.\n\nOutcomes of these protective measures should include the possibility of not deploying the system or removing a system from use.\n\nAutomated systems should not be designed with an intent or reasonably foreseeable possibility of endangering your safety or the safety of your community.\n\nThey should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems.\n\nYou should be protected from inappropriate or irrelevant data use in the design, development, and deployment of automated systems, and from the compounded harm of its reuse.\n\nIndependent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible.\n\nAlgorithmic Discrimination Protections  \nYou should not face discrimination by algorithms and systems should be used and designed in an equitable way.\n\nAlgorithmic discrimination occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law.\n\nDepending on the specific circumstances, such algorithmic discrimination may violate legal protections.\n\nDesigners, developers, and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination and to use and design systems in an equitable way.\n\nThis protection should include proactive equity assessments as part of the system design, use of representative data and protection against proxies for demographic features, ensuring accessibility for people with disabilities in design and development, pre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight.\n\nIndependent evaluation and plain language reporting in the form of an algorithmic impact assessment, including disparity testing results and mitigation information, should be performed and made public whenever possible to confirm these protections.",
    "## Data Privacy\n\nYou should be protected from abusive data practices via built-in protections and you should have agency over how data about you is used.\n\nYou should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected.\n\nDesigners, developers, and deployers of automated systems should seek your permission and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used.\n\nSystems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive.\n\nConsent should only be used to justify collection of data in cases where it can be appropriately and meaningfully given.\n\nAny consent requests should be brief, be understandable in plain language, and give you agency over data collection and the specific context of use; current hard-to-understand notice-and-choice practices for broad uses of data should be changed.\n\nEnhanced protections and restrictions for data and inferences related to sensitive domains, including health, work, education, criminal justice, and finance, and for data pertaining to youth should put you first.\n\nIn sensitive domains, your data and related inferences should only be used for necessary functions, and you should be protected by ethical review and use prohibitions.\n\nYou and your communities should be free from unchecked surveillance; surveillance technologies should be subject to heightened oversight that includes at least pre-deployment assessment of their potential harms and scope limits to protect privacy and civil liberties.\n\nContinuous surveillance and monitoring should not be used in education, work, housing, or in other contexts where the use of such surveillance technologies is likely to limit rights, opportunities, or access.\n\nWhenever possible, you should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.",
    "## Notice and Explanation\n\nYou should know that an automated system is being used and understand how and why it contributes to outcomes that impact you.\n\nDesigners, developers, and deployers of automated systems should provide generally accessible plain language documentation including clear descriptions of the overall system functioning and the role automation plays, notice that such systems are in use, the individual or organization responsible for the system, and explanations of outcomes that are clear, timely, and accessible.\n\nSuch notice should be kept up-to-date and people impacted by the system should be notified of significant use case or key functionality changes.\n\nYou should know how and why an outcome impacting you was determined by an automated system, including when the automated system is not the sole input determining the outcome.\n\nAutomated systems should provide explanations that are technically valid, meaningful and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context.\n\nReporting that includes summary information about these automated systems in plain language and assessments of the clarity and quality of the notice and explanations should be made public whenever possible.",
    "## Human Alternatives, Consideration, and Fallback\n\nYou should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter.\n\nYou should be able to opt out from automated systems in favor of a human alternative, where appropriate.\n\nAppropriateness should be determined based on reasonable expectations in a given context and with a focus on ensuring broad accessibility and protecting the public from especially harmful impacts.\n\nIn some cases, a human or other alternative may be required by law.\n\nYou should have access to timely human consideration and remedy by a fallback and escalation process if an automated system fails, it produces an error, or you would like to appeal or contest its impacts on you.\n\nHuman consideration and fallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and should not impose an unreasonable burden on the public.\n\nAutomated systems with an intended use within sensitive domains, including, but not limited to, criminal justice, employment, education, and health, should additionally be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting with the system, and incorporate human consideration for adverse or high-risk decisions.\n\nReporting that includes a description of these human governance processes and assessment of their timeliness, accessibility, outcomes, and effectiveness should be made public whenever possible.",
    "## Framework Applicability\n\nWhile many of the concerns addressed in this framework derive from the use of AI, the technical capabilities and specific definitions of such systems change with the speed of innovation, and the potential harms of their use occur even with less technologically sophisticated tools.\n\nThus, this framework uses a two-part test to determine what systems are in scope.\n\nThis framework applies to (1) automated systems that (2) have the potential to meaningfully impact the American public’s rights, opportunities, or access to critical resources or services.\n\nThese rights, opportunities, and access to critical resources of services should be enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in our lives.\n\nThis framework describes protections that should be applied with respect to all automated systems that have the potential to meaningfully impact individuals' or communities' exercise of: Rights, Opportunities, or Access\n\nA list of examples of automated systems for which these principles should be considered is provided in the Appendix.\n\nThe Technical Companion, which follows, offers supportive guidance for any person or entity that creates, deploys, or oversees automated systems.",
    "## Relationship to Existing Law and Policy\n\nThe Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public is protected from the potential harms and can fully enjoy the benefits of automated systems.\n\nIt describes principles that can help ensure these protections.\n\nSome of these protections are already required by the U.S. Constitution or implemented under existing U.S. laws.\n\nFor example, government surveillance, and data search and seizure are subject to legal requirements and judicial oversight.\n\nThere are Constitutional requirements for human review of criminal investigative matters and statutory requirements for judicial review.\n\nCivil rights laws protect the American people against discrimination.\n\nThere are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-specific privacy and security protections.\n\nEnsuring some of the additional protections proposed in this framework would require new laws to be enacted or new policies and practices to be adopted.\n\nIn some cases, exceptions to the principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, conform to the practicalities of a specific use case, or balance competing public interests.\n\nIn particular, law enforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, and privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in this framework.\n\nThe Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in moving principles into practice.\n\nThe expectations given in the Technical Companion are meant to serve as a blueprint for the development of additional technical standards and practices that should be tailored for particular sectors and contexts.\n\nWhile existing laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail those laws beyond providing them as examples, where appropriate, of existing protective measures.\n\nThis framework instead shares a broad, forward-leaning vision of recommended principles for automated system development and use to inform private and public involvement with these systems where they have the potential to meaningfully impact rights, opportunities, or access.\n\nAdditionally, this framework does not analyze or take a position on legislative and regulatory proposals in municipal, state, and federal government, or those in other countries.\n\nWe have seen modest progress in recent years, with some state and local governments responding to these problems with legislation, and some courts extending longstanding statutory protections to new and emerging technologies.\n\nThere are companies working to incorporate additional protections in their design and use of automated systems, and researchers developing innovative guardrails.\n\nAdvocates, researchers, and government organizations have proposed principles for the ethical use of AI and other automated systems.\n\nThese include the Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial Intelligence, which includes principles for responsible stewardship of trustworthy AI and which the United States adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government, which sets out principles that govern the federal government’s use of AI.\n\nThe Blueprint for an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985 on Advancing Racial Equity and Support for Underserved Communities Through the Federal Government.\n\nThese principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 report of an advisory committee to the U.S. Department of Health, Education, and Welfare, Records, Computers, and the Rights of Citizens.\n\nWhile there is no single, universal articulation of the FIPPs, these core principles for managing information about individuals have been incorporated into data privacy laws and policies across the globe.\n\nThe Blueprint for an AI Bill of Rights embraces elements of the FIPPs that are particularly relevant to automated systems, without articulating a specific set of FIPPs or scoping applicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties, ethics, or risk management.\n\nThe Technical Companion builds on this prior work to provide practical next steps to move these principles into practice and promote common approaches that allow technological innovation to flourish while protecting people from harm.",
    "## Definitions\n\nALGORITHMIC DISCRIMINATION: \"Algorithmic discrimination\" occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law.\n\nDepending on the specific circumstances, such algorithmic discrimination may violate legal protections.\n\nThroughout this framework the term \"algorithmic discrimination\" takes this meaning (and not a technical understanding of discrimination as distinguishing between items).\n\nAUTOMATED SYSTEM: An \"automated system\" is any system, software, or process that uses computation as whole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collect data or observations, or otherwise interact with individuals and/or communities.\n\nAutomated systems include, but are not limited to, systems derived from machine learning, statistics, or other data processing or artificial intelligence techniques, and exclude passive computing infrastructure.\n\n\"Passive computing infrastructure\" is any intermediary technology that does not influence or determine the outcome of decision, make or aid in decisions, inform policy implementation, or collect data or observations, including web hosting, domain registration, networking, caching, data storage, or cybersecurity.\n\nThroughout this framework, automated systems that are considered in scope are only those that have the potential to meaningfully impact individuals' or communities' rights, opportunities, or access.\n\nCOMMUNITIES: \"Communities\" include: neighborhoods; social network connections (both online and offline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organizational ties.\n\nThis includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities.\n\nAI and other data-driven automated systems most directly collect data on, make inferences about, and may cause harm to individuals.\n\nBut the overall magnitude of their impacts may be most readily visible at the level of communities.\n\nAccordingly, the concept of community is integral to the scope of the Blueprint for an AI Bill of Rights.\n\n```",
    "# Policy Frameworks for AI\n\nPolicy frameworks have long employed approaches for protecting the rights of individuals, but existing frameworks have sometimes struggled to provide protections when effects manifest most clearly at a community level.\n\nFor these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of automated systems should be evaluated, protected against, and redressed at both the individual and community levels.",
    "## Equity\n\n“Equity” means the consistent and systematic fair, just, and impartial treatment of all individuals.\n\nSystemic, fair, and just treatment must take into account the status of individuals who belong to underserved communities that have been denied such treatment, such as Black, Latino, and Indigenous and Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely affected by persistent poverty or inequality.",
    "## Rights, Opportunities, or Access\n\n“Rights, opportunities, or access” is used to indicate the scoping of this framework.\n\nIt describes the set of: civil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both public and private sector contexts; equal opportunities, including equitable access to education, housing, credit, employment, and other programs; or, access to critical resources or services, such as healthcare, financial services, safety, social services, non-deceptive information about goods and services, and government benefits.",
    "## Sensitive Data\n\nData and metadata are sensitive if they pertain to an individual in a sensitive domain (defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship history and legal status such as custody and divorce information, and home, work, or school environmental data); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm due to identity theft.\n\nData and metadata generated by or about those who are not yet legal adults are also sensitive, even if not related to a sensitive domain.\n\nSuch data includes, but is not limited to, numerical, text, image, audio, or video data.",
    "## Sensitive Domains\n\n“Sensitive domains” are those in which activities being conducted can cause material harms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights.\n\nDomains that have historically been singled out as deserving of enhanced data protections or where such enhanced protections are reasonably expected by the public include, but are not limited to, health, family planning and care, employment, education, criminal justice, and personal finance.\n\nIn the context of this framework, such domains are considered sensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains and data that are considered sensitive are understood to change over time based on societal norms and context.",
    "## Surveillance Technology\n\n“Surveillance technology” refers to products or services marketed for or that can be lawfully used to detect, monitor, intercept, collect, exploit, preserve, protect, transmit, and/or retain data, identifying information, or communications concerning individuals or groups.\n\nThis framework limits its focus to both government and commercial use of surveillance technologies when juxtaposed with real-time or subsequent automated analysis and when such systems have a potential for meaningful impact on individuals’ or communities’ rights, opportunities, or access.",
    "# From Principles to Practice\n\nA Technical Companion to The Blueprint for an AI Bill of Rights\n\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificial intelligence.\n\nThis technical companion considers each principle in the Blueprint for an AI Bill of Rights and provides examples and concrete steps for communities, industry, governments, and others to take in order to build these protections into policy, practice, or the technological design process.\n\nTaken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can help guard the American public against many of the potential and actual harms identified by researchers, technologists, advocates, journalists, policymakers, and communities in the United States and around the world.\n\nThis technical companion is intended to be used as a reference by people across many circumstances – anyone impacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy to govern the use of an automated system.\n\nEach principle is accompanied by three supplemental sections:\n\n- The examples provided are not critiques or endorsements, but rather are offered as illustrative cases to help provide a concrete vision for actualizing the Blueprint for an AI Bill of Rights.\n\nEffectively implementing these processes requires the cooperation of and collaboration among industry, civil society, researchers, policymakers, technologists, and the public.",
    "## Safe and Effective Systems\n\nYou should be protected from unsafe or ineffective systems.\n\nAutomated systems should be developed with consultation from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts of the system.\n\nSystems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based on their intended use, mitigation of unsafe outcomes including those beyond the intended use, and adherence to domain-specific standards.\n\nOutcomes of these protective measures should include the possibility of not deploying the system or removing a system from use.\n\nAutomated systems should not be designed with an intent or reasonably foreseeable possibility of endangering your safety or the safety of your community.\n\nThey should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems.\n\nYou should be protected from inappropriate or irrelevant data use in the design, development, and deployment of automated systems, and from the compounded harm of its reuse.\n\nIndependent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible.\n\nWhile technologies are being deployed to solve problems across a wide array of issues, our reliance on technology can also lead to its use in situations where it has not yet been proven to work—either at all or within an acceptable range of error.\n\nIn other cases, technologies do not work as intended or as promised, causing substantial and unjustified harm.\n\nAutomated systems sometimes rely on data from other systems, including historical data, allowing irrelevant information from past decisions to infect decision-making in unrelated situations.\n\nIn some cases, technologies are purposefully designed to violate the safety of others, such as technologies designed to facilitate stalking; in other cases, intended or unintended uses lead to unintended harms.\n\nMany of the harms resulting from these technologies are preventable, and actions are already being taken to protect the public.\n\nSome companies have put in place safeguards that have prevented harm from occurring by ensuring that key development decisions are vetted by an ethics review; others have identified and mitigated harms found through pre-deployment testing and ongoing monitoring processes.\n\nGovernments at all levels have existing public consultation processes that may be applied when considering the use of new automated systems, and existing product development and testing practices already protect the American public from many potential harms.\n\nStill, these kinds of practices are deployed too rarely and unevenly.\n\nExpanded, proactive protections could build on these existing practices, increase confidence in the use of automated systems, and protect the American public.\n\nInnovators deserve clear rules of the road that allow new ideas to flourish, and the American public deserves protections from unsafe outcomes.\n\nAll can benefit from assurances that automated systems will be designed, tested, and consistently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harmful outcomes.\n\nA proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was implemented at hundreds of hospitals around the country.\n\nAn independent study showed that the model predictions underperformed relative to the designer’s claims while also causing ‘alert fatigue’ by falsely alerting likelihood of sepsis.\n\nOn social media, Black people who quote and criticize racist messages have had their own speech silenced when a platform’s automated moderation system failed to distinguish this “counter speech” (or other critique and journalism) from the original hateful messages to which such speech responded.\n\nA device originally developed to help people track and find lost items has been used as a tool by stalkers to track victims’ locations in violation of their privacy and safety.\n\nThe device manufacturer took steps after release to protect people from unwanted tracking by alerting people on their phones when a device is found to be moving with them over time and also by having the device make an occasional noise, but not all phones are able to receive the notification and the devices remain a safety concern due to their misuse.\n\nAn algorithm used to deploy police was found to repeatedly send police to neighborhoods they regularly visit, even if those neighborhoods were not the ones with the highest crime rates.\n\nThese incorrect crime predictions were the result of a feedback loop generated from the reuse of data from previous arrests and algorithm predictions.",
    "These incorrect crime predictions were the result of a feedback loop generated from the reuse of data from previous arrests and algorithm predictions.\n\nAI-enabled “nudification” technology that creates images where people appear to be nude—including apps that enable non-technical users to create or alter images of individuals without their consent—has proliferated at an alarming rate.\n\nSuch technology is becoming a common form of image-based abuse that disproportionately impacts women.\n\nAs these tools become more sophisticated, they are producing altered images that are increasingly realistic and are difficult for both humans and AI to detect as inauthentic.\n\nRegardless of authenticity, the experience of harm to victims of non-consensual intimate images can be devastatingly real—affecting their personal and professional lives and impacting their mental and physical health.\n\nA company installed AI-powered cameras in its delivery vans in order to evaluate the road safety habits of its drivers, but the system incorrectly penalized drivers when other cars cut them off or when other events beyond their control took place on the road.\n\nAs a result, drivers were incorrectly ineligible to receive a bonus.\n\nIn order to ensure that an automated system is safe and effective, it should include safeguards to protect the public from harm in a proactive and ongoing manner; avoid use of data inappropriate for or irrelevant to the task at hand, including reuse that could cause compounded harm; and demonstrate the safety and effectiveness of the system.\n\nThese expectations are explained below.",
    "### Protect the Public from Harm in a Proactive and Ongoing Manner\n\n**Consultation.\n\n** The public should be consulted in the design, implementation, deployment, acquisition, and maintenance phases of automated system development, with emphasis on early-stage consultation before a system is introduced or a large change implemented.\n\nThis consultation should directly engage diverse impacted communities to consider concerns and risks that may be unique to those communities, or disproportionately prevalent or severe for them.\n\nThe extent of this engagement and the form of outreach to relevant stakeholders may differ depending on the specific automated system and development phase, but should include subject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as civil rights, civil liberties, and privacy experts.\n\nFor private sector applications, consultations before product launch may need to be confidential.\n\nGovernment applications, particularly law enforcement applications or applications that raise national security considerations, may require confidential or limited engagement based on system sensitivities and preexisting oversight laws and structures.\n\nConcerns raised in this consultation should be documented, and the automated system developers were proposing to create, use, or deploy should be reconsidered based on this feedback.\n\n**Testing.\n\n** Systems should undergo extensive testing before deployment.\n\nThis testing should follow domain-specific best practices, when available, for ensuring the technology will work in its real-world context.\n\nSuch testing should take into account both the specific technology used and the roles of any human operators or reviewers who impact system outcomes or effectiveness; testing should include both automated systems testing and human-led (manual) testing.\n\nTesting conditions should mirror as closely as possible the conditions in which the system will be deployed, and new testing may be required for each deployment to account for material differences in conditions from one deployment to another.\n\nFollowing testing, system performance should be compared with the in-place, potentially human-driven, status quo procedures, with existing human performance considered as a performance baseline for the algorithm to meet pre-deployment, and as a lifecycle minimum performance standard.\n\nDecision possibilities resulting from performance testing should include the possibility of not deploying the system.\n\n**Risk Identification and Mitigation.\n\n** Before deployment, and in a proactive and ongoing manner, potential risks of the automated system should be identified and mitigated.\n\nIdentified risks should focus on the potential for meaningful impact on people’s rights, opportunities, or access and include those to impacted communities that may not be direct users of the automated system, risks resulting from purposeful misuse of the system, and other concerns identified via the consultation process.\n\nAssessment and, where possible, measurement of the impact of risks should be included and balanced such that high impact risks receive attention and mitigation proportionate with those impacts.\n\nAutomated systems with the intended purpose of violating the safety of others should not be developed or used; systems with such safety violations as identified unintended consequences should not be used until the risk can be mitigated.\n\nOngoing risk mitigation may necessitate rollback or significant modification to a launched automated system.\n\n**Ongoing Monitoring.\n\n** Automated systems should have ongoing monitoring procedures, including recalibration procedures, in place to ensure that their performance does not fall below an acceptable level over time, based on changing real-world conditions or deployment contexts, post-deployment modification, or unexpected conditions.\n\nThis ongoing monitoring should include continuous evaluation of performance metrics and harm assessments, updates of any systems, and retraining of any machine learning models as necessary, as well as ensuring that fallback mechanisms are in place to allow reversion to a previously working system.\n\nMonitoring should take into account the performance of both technical system components (the algorithm as well as any hardware components, data inputs, etc.)\n\nand human operators.\n\nIt should include mechanisms for testing the actual accuracy of any predictions or recommendations generated by a system, not just a human operator’s determination of their accuracy.\n\nOngoing monitoring procedures should include manual, human-led monitoring as a check in the event there are shortcomings in automated monitoring systems.\n\nThese monitoring procedures should be in place for the lifespan of the deployed automated system.\n\n**Clear Organizational Oversight.\n\n** Entities responsible for the development or use of automated systems should lay out clear governance structures and procedures.",
    "**Clear Organizational Oversight.\n\n** Entities responsible for the development or use of automated systems should lay out clear governance structures and procedures.\n\nThis includes clearly-stated governance procedures before deploying the system, as well as responsibility of specific individuals or entities to oversee ongoing assessment and mitigation.\n\nOrganizational stakeholders including those with oversight of the business process or operation being automated, as well as other organizational divisions that may be affected due to the use of the system, should be involved in establishing governance procedures.\n\nResponsibility should rest high enough in the organization that decisions about resources, mitigation, incident response, and potential rollback can be made promptly, with sufficient weight given to risk mitigation objectives against competing concerns.\n\nThose holding this responsibility should be made aware of any use cases with the potential for meaningful impact on people’s rights, opportunities, or access as determined based on risk identification procedures.\n\nIn some cases, it may be appropriate for an independent ethics review to be conducted before deployment.",
    "### Avoid Inappropriate, Low-Quality, or Irrelevant Data Use and the Compounded Harm of Its Reuse\n\n**Relevant and High-Quality Data.\n\n** Data used as part of any automated system’s creation, evaluation, or deployment should be relevant, of high quality, and tailored to the task at hand.\n\nRelevancy should be established based on research-backed demonstration of the causal influence of the data to the specific use case or justified more generally based on a reasonable expectation of usefulness in the domain and/or for the system design or ongoing development.\n\nRelevance of data should not be established solely by appealing to its historical connection to the outcome.\n\nHigh quality and tailored data should be representative of the task at hand and errors from data entry or other sources should be measured and limited.\n\nAny data used as the target of a prediction process should receive particular attention to the quality and validity of the predicted outcome or label to ensure the goal of the automated system is appropriately identified and measured.\n\nAdditionally, justification should be documented for each data attribute and source to explain why it is appropriate to use that data to inform the results of the automated system and why such use will not violate any applicable laws.\n\nIn cases of high-dimensional and/or derived attributes, such justifications can be provided as overall descriptions of the attribute generation process and appropriateness.\n\n**Derived Data Sources Tracked and Reviewed Carefully.\n\n** Data that is derived from other data through the use of algorithms, such as data derived or inferred from prior model outputs, should be identified and tracked, e.g., via a specialized type in a data schema.\n\nDerived data should be viewed as potentially high-risk inputs that may lead to feedback loops, compounded harm, or inaccurate results.\n\nSuch sources should be carefully validated against the risk of collateral consequences.\n\n**Data Reuse Limits in Sensitive Domains.\n\n** Data reuse, and especially data reuse in a new context, can result in the spreading and scaling of harms.\n\nData from some domains, including criminal justice data and data indicating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in some cases its reuse is limited by law.\n\nAccordingly, such data should be subject to extra oversight to ensure safety and efficacy.\n\nData reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal matters or private sector use) should only occur where use of such data is legally authorized and, after examination, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reasonable measures have been implemented to mitigate the identified risks.\n\nSuch data should be clearly labeled to identify contexts for limited reuse based on sensitivity.\n\nWhere possible, aggregated datasets may be useful for replacing individual-level sensitive data.",
    "### Demonstrate the Safety and Effectiveness of the System\n\n**Independent Evaluation.\n\n** Automated systems should be designed to allow for independent evaluation (e.g., via application programming interfaces).\n\nIndependent evaluators, such as researchers, journalists, ethics review boards, inspectors general, and third-party auditors, should be given access to the system and samples of associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual property law), in order to perform such evaluations.\n\nMechanisms should be included to ensure that system access for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to provide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot be revoked without reasonable and verified justification.\n\n**Reporting.\n\n** Entities responsible for the development or use of automated systems should provide regularly-updated reports that include: an overview of the system, including how it is embedded in the organization’s business processes or other activities, system goals, any human-run procedures that form a part of the system, and specific performance expectations; a description of any data used to train machine learning models or for other purposes, including how data sources were processed and interpreted, a summary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the results of public consultation such as concerns raised and any decisions made due to these concerns; risk identification and management assessments and any steps taken to mitigate potential harms; the results of performance testing including, but not limited to, accuracy, differential demographic impact, resulting error rates (overall and per demographic group), and comparisons to previously deployed systems; ongoing monitoring procedures and regular performance testing reports, including monitoring frequency, results, and actions taken; and the procedures for and results from independent evaluations.\n\nReporting should be provided in a plain language and machine-readable manner.\n\nExecutive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government requires that certain federal agencies adhere to nine principles when designing, developing, acquiring, or using AI for purposes other than national security or defense.\n\nThese principles—while taking into account the sensitive law enforcement and other contexts in which the federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and respectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) safe, secure, and resilient; (e) understandable; (f) responsible and traceable; (g) regularly monitored; (h) transparent; and, (i) accountable.\n\nThe Blueprint for an AI Bill of Rights is consistent with the Executive Order.\n\nAffected agencies across the federal government have released AI use case inventories and are implementing plans to bring those AI systems into compliance with the Executive Order or retire them.\n\nThe law and policy landscape for motor vehicles shows that strong safety regulations—and measures to address harms when they occur—can enhance innovation in the context of complex technologies.\n\nCars, like automated digital systems, comprise a complex collection of components.\n\nThe National Highway Traffic Safety Administration, through its rigorous standards and independent evaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to innovate.\n\nAt the same time, rules of the road are implemented locally to impose contextually appropriate requirements on drivers, such as slowing down near schools or playgrounds.\n\nFrom large companies to start-ups, industry is providing innovative solutions that allow organizations to mitigate risks to the safety and efficacy of AI systems, both before deployment and through monitoring over time.\n\nThese innovative solutions include risk assessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing monitoring, documentation procedures specific to model assessments, and many other strategies that aim to mitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety and effectiveness concerns.\n\nThe Office of Management and Budget (OMB) has called for an expansion of opportunities for meaningful stakeholder engagement in the design of programs and services.\n\nOMB also points to numerous examples of effective and proactive stakeholder engagement, including the Community-Based Participatory Research Program developed by the National Institutes of Health and the participatory technology assessments developed by the National Oceanic and Atmospheric Administration.",
    "The National Institute of Standards and Technology (NIST) is developing a risk management framework to better manage risks posed to individuals, organizations, and society by AI.\n\nThe NIST AI Risk Management Framework, as mandated by Congress, is intended for voluntary use to help incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems.\n\nThe NIST framework is being developed through a consensus-driven, open, transparent, and collaborative process that includes workshops and other opportunities to provide input.\n\nThe NIST framework aims to foster the development of innovative approaches to address characteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, robustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of harmful uses.\n\nThe NIST framework will consider and encompass principles such as transparency, accountability, and fairness during pre-design, design and development, deployment, use, and testing and evaluation of AI technologies and systems.\n\nIt is expected to be released in the winter of 2022-23.\n\nSome U.S government agencies have developed specific frameworks for ethical use of AI systems.\n\nThe Department of Energy (DOE) has activated the AI Advancement Council that oversees coordination and advises on the implementation of the DOE AI Strategy and addresses issues and/or escalations.",
    "# Ethical AI Principles\n\nThe Department of Defense has adopted Artificial Intelligence Ethical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national security and defense activities.\n\nSimilarly, the U.S. Intelligence Community (IC) has developed the Principles of Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to develop and use AI in furtherance of the IC's mission, as well as an AI Ethics Framework to help implement these principles.\n\nThe National Science Foundation (NSF) funds extensive research to help foster the development of automated systems that adhere to and advance their safety, security, and effectiveness.\n\nMultiple NSF programs support research that directly addresses many of these principles: the National AI Research Institutes support research on all aspects of safe, trustworthy, fair, and explainable AI algorithms and systems; the Cyber Physical Systems program supports research on developing safe autonomous and cyber-physical systems with AI components; the Secure and Trustworthy Cyberspace program supports research on cybersecurity and privacy-enhancing technologies in automated systems; the Formal Methods in the Field program supports research on rigorous formal verification and analysis of automated systems and machine learning, and the Designing Accountable Software Systems program supports research on rigorous and reproducible methodologies for developing software systems with legal and regulatory compliance in mind.\n\nSome state legislatures have placed strong transparency and validity requirements on the use of pretrial risk assessments.\n\nThe use of algorithmic pretrial risk assessments has been a cause of concern for civil rights groups.\n\nIdaho Code Section 19-1910, enacted in 2019, requires that any pretrial risk assessment, before use in the state, first be \"shown to be free of bias against any class of individuals protected from discrimination by state or federal law\", that any locality using a pretrial risk assessment must first formally validate the claim of its being free of bias, that \"all documents, records, and information used to build or validate the risk assessment shall be open to public inspection,\" and that assertions of trade secrets cannot be used \"to quash discovery in a criminal matter by a party to a criminal case.\"",
    "# Algorithmic Discrimination Protections\n\nYou should not face discrimination by algorithms, and systems should be used and designed in an equitable way.\n\nAlgorithmic discrimination occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law.\n\nDepending on the specific circumstances, such algorithmic discrimination may violate legal protections.\n\nDesigners, developers, and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination and to use and design systems in an equitable way.\n\nThis protection should include proactive equity assessments as part of the system design, use of representative data and protection against proxies for demographic features, ensuring accessibility for people with disabilities in design and development, pre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight.\n\nIndependent evaluation and plain language reporting in the form of an algorithmic impact assessment, including disparity testing results and mitigation information, should be performed and made public whenever possible to confirm these protections.\n\nThere is extensive evidence showing that automated systems can produce inequitable outcomes and amplify existing inequity.\n\nData that fails to account for existing systemic biases in American society can result in a range of consequences.\n\nFor example, facial recognition technology that can contribute to wrongful and discriminatory arrests, hiring algorithms that inform discriminatory decisions, and healthcare algorithms that discount the severity of certain diseases in Black Americans.\n\nInstances of discriminatory practices built into and resulting from AI and other automated systems exist across many industries, areas, and contexts.\n\nWhile automated systems have the capacity to drive extraordinary advances and innovations, algorithmic discrimination protections should be built into their design, deployment, and ongoing use.\n\nMany companies, non-profits, and federal government agencies are already taking steps to ensure the public is protected from algorithmic discrimination.\n\nSome companies have instituted bias testing as part of their product quality assessment and launch procedures, and in some cases, this testing has led products to be changed or not launched, preventing harm to the public.\n\nFederal government agencies have been developing standards and guidance for the use of automated systems in order to help prevent bias.\n\nNon-profits and companies have developed best practices for audits and impact assessments to help identify potential algorithmic discrimination and provide transparency to the public in the mitigation of such biases.\n\nBut there is much more work to do to protect the public from algorithmic discrimination and use and design automated systems in an equitable way.\n\nThe guardrails protecting the public from discrimination in their daily lives should include their digital lives and impacts—basic safeguards against abuse, bias, and discrimination to ensure that all people are treated fairly when automated systems are used.\n\nThis includes all dimensions of their lives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminal justice system.\n\nEnsuring equity should also go beyond existing guardrails to consider the holistic impact that automated systems make on underserved communities and to institute proactive protections that support these communities.",
    "# Examples of Algorithmic Discrimination\n\nAn automated system using nontraditional factors such as educational attainment and employment history as part of its loan underwriting and pricing model was found to be much more likely to charge an applicant who attended a Historically Black College or University (HBCU) higher loan prices for refinancing a student loan than an applicant who did not attend an HBCU.\n\nThis was found to be true even when controlling for other credit-related factors.\n\nA hiring tool that learned the features of a company's employees (predominantly men) rejected women applicants for spurious and discriminatory reasons; resumes with the word “women’s,” such as “women’s chess club captain,” were penalized in the candidate ranking.\n\nA predictive model marketed as being able to predict whether students are likely to drop out of school was used by more than 500 universities across the country.\n\nThe model was found to use race directly as a predictor, and also shown to have large disparities by race; Black students were as many as four times as likely as their otherwise similar white peers to be deemed at high risk of dropping out.\n\nThese risk scores are used by advisors to guide students towards or away from majors, and some worry that they are being used to guide Black students away from math and science subjects.\n\nA risk assessment tool designed to predict the risk of recidivism for individuals in federal custody showed evidence of disparity in prediction.\n\nThe tool overpredicts the risk of recidivism for some groups of color on the general recidivism tools, and underpredicts the risk of recidivism for some groups of color on some of the violent recidivism tools.\n\nThe Department of Justice is working to reduce these disparities and has publicly released a report detailing its review of the tool.\n\nAn automated sentiment analyzer, a tool often used by technology platforms to determine whether a statement posted online expresses a positive or negative sentiment, was found to be biased against Jews and gay people.\n\nFor example, the analyzer marked the statement “I’m a Jew” as representing a negative sentiment, while “I’m a Christian” was identified as expressing a positive sentiment.\n\nThis could lead to the preemptive blocking of social media comments such as: “I’m gay.” A related company with this bias concern has made their data public to encourage researchers to help address the issue and has released reports identifying and measuring this problem as well as detailing attempts to address it.",
    "# Search and Advertisement\n\nSearches for “Black girls,” “Asian girls,” or “Latina girls” return predominantly sexualized content, rather than role models, toys, or activities.\n\nSome search engines have been working to reduce the prevalence of these results, but the problem remains.\n\nAdvertisement delivery systems that predict who is most likely to click on a job advertisement end up delivering ads in ways that reinforce racial and gender stereotypes, such as overwhelmingly directing supermarket cashier ads to women and jobs with taxi companies to primarily Black people.",
    "# TSA Scanners\n\nBody scanners, used by TSA at airport checkpoints, require the operator to select a “male” or “female” scanning setting based on the passenger’s sex, but the setting is chosen based on the operator’s perception of the passenger’s gender identity.\n\nThese scanners are more likely to flag transgender travelers as requiring extra screening done by a person.\n\nTransgender travelers have described degrading experiences associated with these extra screenings.\n\nTSA has recently announced plans to implement a gender-neutral algorithm while simultaneously enhancing the security effectiveness capabilities of the existing technology.\n\nThe National Disabled Law Students Association expressed concerns that individuals with disabilities were more likely to be flagged as potentially suspicious by remote proctoring AI systems because of their disability-specific access needs such as needing longer breaks or using screen readers or dictation software.\n\nAn algorithm designed to identify patients with high needs for healthcare systematically assigned lower scores (indicating that they were not as high need) to Black patients than to those of white patients, even when those patients had similar numbers of chronic conditions and other markers of health.\n\nIn addition, healthcare clinical algorithms that are used by physicians to guide clinical decisions may include sociodemographic variables that adjust or “correct” the algorithm’s output on the basis of a patient’s race or ethnicity, which can lead to race-based health inequities.",
    "# Ensuring Equity\n\nAny automated system should be tested to help ensure it is free from algorithmic discrimination before it can be sold or used.\n\nProtection against algorithmic discrimination should include designing to ensure equity, broadly construed.\n\nSome algorithmic discrimination is already prohibited under existing anti-discrimination law.\n\nThe expectations set out below describe proactive technical and policy steps that can be taken to not only reinforce those legal protections but extend beyond them to ensure equity for underserved communities even in circumstances where a specific legal protection may not be clearly established.\n\nThese protections should be instituted throughout the design, development, and deployment process and are described below roughly in the order in which they would be instituted.",
    "## Proactive Assessment of Equity in Design\n\nThose responsible for the development, use, or oversight of automated systems should conduct proactive equity assessments in the design phase of the technology research and development or during its acquisition to review potential input data, associated historical context, accessibility for people with disabilities, and societal goals to identify potential discrimination and effects on equity resulting from the introduction of the technology.\n\nThe assessed groups should be as inclusive as possible of the underserved communities mentioned in the equity definition: Black, Latino, and Indigenous and Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely affected by persistent poverty or inequality.\n\nAssessment could include both qualitative and quantitative evaluations of the system.\n\nThis equity assessment should also be considered a core part of the goals of the consultation conducted as part of the safety and efficacy review.",
    "## Representative and Robust Data\n\nAny data used as part of system development or assessment should be representative of local communities based on the planned deployment setting and should be reviewed for bias based on the historical and societal context of the data.\n\nSuch data should be sufficiently robust to identify and help to mitigate biases and potential harms.",
    "## Guarding Against Proxies\n\nDirectly using demographic information in the design, development, or deployment of an automated system (for purposes other than evaluating a system for discrimination or using a system to counter discrimination) runs a high risk of leading to algorithmic discrimination and should be avoided.\n\nIn many cases, attributes that are highly correlated with demographic features, known as proxies, can contribute to algorithmic discrimination.\n\nIn cases where use of the demographic features themselves would lead to illegal algorithmic discrimination, reliance on such proxies in decision-making (such as that facilitated by an algorithm) may also be prohibited by law.\n\nProactive testing should be performed to identify proxies by testing for correlation between demographic information and attributes in any data used as part of system design, development, or use.\n\nIf a proxy is identified, designers, developers, and deployers should remove the proxy; if needed, it may be possible to identify alternative attributes that can be used instead.\n\nAt a minimum, organizations should ensure a proxy feature is not given undue weight and should monitor the system closely for any resulting algorithmic discrimination.",
    "## Ensuring Accessibility During Design, Development, and Deployment\n\nSystems should be designed, developed, and deployed by organizations in ways that ensure accessibility to people with disabilities.\n\nThis should include consideration of a wide variety of disabilities, adherence to relevant accessibility standards, and user experience research both before and after deployment to identify and address any accessibility barriers to the use or effectiveness of the automated system.",
    "## Disparity Assessment\n\nAutomated systems should be tested using a broad set of measures to assess whether the system components, both in pre-deployment testing and in-context deployment, produce disparities.\n\nThe demographics of the assessed groups should be as inclusive as possible of race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law.\n\nThe broad set of measures assessed should include demographic performance measures, overall and subgroup parity assessment, and calibration.\n\nDemographic data collected for disparity assessment should be separated from data used for the automated system and privacy protections should be instituted; in some cases, it may make sense to perform such assessment using a data sample.\n\nFor every instance where the deployed automated system leads to different treatment or impacts disfavoring the identified groups, the entity governing, implementing, or using the system should document the disparity and a justification for any continued use of the system.",
    "## Disparity Mitigation\n\nWhen a disparity assessment identifies a disparity against an assessed group, it may be appropriate to take steps to mitigate or eliminate the disparity.\n\nIn some cases, mitigation or elimination of the disparity may be required by law.\n\nDisparities that have the potential to lead to algorithmic discrimination, cause meaningful harm, or violate equity goals should be mitigated.\n\nWhen designing and evaluating an automated system, steps should be taken to evaluate multiple models and select the one that has the least adverse impact, modify data input choices, or otherwise identify a system with fewer disparities.\n\nIf adequate mitigation of the disparity is not possible, then the use of the automated system should be reconsidered.\n\nOne of the considerations in whether to use the system should be the validity of any target measure; unobservable targets may result in the inappropriate use of proxies.\n\nMeeting these standards may require instituting mitigation procedures and other protective measures to address algorithmic discrimination, avoid meaningful harm, and achieve equity goals.",
    "## Ongoing Monitoring and Mitigation\n\nAutomated systems should be regularly monitored to assess algorithmic discrimination that might arise from unforeseen interactions of the system with inequities not accounted for during the pre-deployment testing, changes to the system after deployment, or changes to the context of use or associated data.\n\nMonitoring and disparity assessment should be performed by the entity deploying or using the automated system to examine whether the system has led to algorithmic discrimination when deployed.\n\nThis assessment should be performed regularly and whenever a pattern of unusual results is occurring.\n\nIt can be performed using a variety of approaches, taking into account whether and how demographic information of impacted people is available, for example via testing with a sample of users or via qualitative user experience research.\n\nRiskier and higher-impact systems should be monitored and assessed more frequently.\n\nOutcomes of this assessment should include additional disparity mitigation, if needed, or fallback to earlier procedures in the case that equity standards are no longer met and can't be mitigated, and prior mechanisms provide better adherence to equity standards.",
    "### Independent Evaluation\n\nAs described in the section on Safe and Effective Systems, entities should allow independent evaluation of potential algorithmic discrimination caused by automated systems they use or oversee.\n\nIn the case of public sector uses, these independent evaluations should be made public unless law enforcement or national security restrictions prevent doing so.\n\nCare should be taken to balance individual privacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and controls allow access to such data without compromising privacy.",
    "### Reporting\n\nEntities responsible for the development or use of automated systems should provide reporting of an appropriately designed algorithmic impact assessment, with clear specification of who performs the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in response to the assessment.\n\nThis algorithmic impact assessment should include at least: the results of any consultation, design stage equity assessments (potentially including qualitative analysis), accessibility designs and testing, disparity testing, document any remaining disparities, and detail any mitigation implementation and assessments.\n\nThis algorithmic impact assessment should be made public whenever possible.\n\nReporting should be provided in a clear and machine-readable manner using plain language to allow for more straightforward public accountability.",
    "## Algorithmic Discrimination Protections\n\nReal-life examples of how these principles can become reality include:\n\n- **Mortgage Lending Discrimination**: The federal government is working to combat discrimination in mortgage lending.\n\nThe Department of Justice has launched a nationwide initiative to combat redlining, which includes reviewing how lenders who may be avoiding serving communities of color are conducting targeted marketing and advertising.\n\nThis initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial Protection Bureau and prudential regulators.\n\nThe Action Plan to Advance Property Appraisal and Valuation Equity includes a commitment from the agencies that oversee mortgage lending to include a nondiscrimination standard in the proposed rules for Automated Valuation Models.\n\n- **Use in Employment**: The Equal Employment Opportunity Commission and the Department of Justice have clearly laid out how employers’ use of AI and other automated systems can result in discrimination against job applicants and employees with disabilities.\n\nThe documents explain how employers’ use of software that relies on algorithmic decision-making may violate existing requirements under Title I of the Americans with Disabilities Act (“ADA”).\n\nThis technical assistance also provides practical tips to employers on how to comply with the ADA, and to job applicants and employees who think that their rights may have been violated.\n\n- **Healthcare Disparities**: Disparity assessments identified harms to Black patients' healthcare access.\n\nA widely used healthcare algorithm relied on the cost of each patient’s past medical care to predict future medical needs, recommending early interventions for the patients deemed most at risk.\n\nThis process discriminated against Black patients, who generally have less access to medical care and therefore have generated less cost than white patients with similar illness and need.\n\nA landmark study documented this pattern and proposed practical ways that were shown to reduce this bias, such as focusing specifically on active chronic health conditions or avoidable future costs related to emergency visits and hospitalization.\n\n- **Hiring Practices**: Large employers have developed best practices to scrutinize the data and models used for hiring.\n\nAn industry initiative has developed Algorithmic Bias Safeguards for the Workforce, a structured questionnaire that businesses can use proactively when procuring software to evaluate workers.\n\nIt covers specific technical questions such as the training data used, model training process, biases identified, and mitigation steps employed.\n\n- **Accessibility in Technology**: Standards organizations have developed guidelines to incorporate accessibility criteria into technology design processes.\n\nThe most prevalent in the United States is the Access Board’s Section 508 regulations, which are the technical standards for federal information communication technology (software, hardware, and web).\n\nOther standards include those issued by the International Organization for Standardization, and the World Wide Web Consortium Web Content Accessibility Guidelines, a globally recognized voluntary consensus standard for web content and other information and communications technology.\n\n- **Bias in AI**: NIST has released Special Publication 1270, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.\n\nThe special publication describes the stakes and challenges of bias in artificial intelligence and provides examples of how and why it can chip away at public trust; identifies three categories of bias in AI – systemic, statistical, and human – and describes how and where they contribute to harms; and describes three broad challenges for mitigating bias – datasets, testing and evaluation, and human factors – and introduces preliminary guidance for addressing them.\n\nThroughout, the special publication takes a socio-technical perspective to identifying and managing AI bias.",
    "# Data Privacy\n\nYou should be protected from abusive data practices via built-in protections, and you should have agency over how data about you is used.\n\nYou should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected.\n\nDesigners, developers, and deployers of automated systems should seek your permission and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used.\n\nSystems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive.\n\nConsent should only be used to justify collection of data in cases where it can be appropriately and meaningfully given.\n\nAny consent requests should be brief, be understandable in plain language, and give you agency over data collection and the specific context of use; current hard-to-understand notice-and-choice practices for broad uses of data should be changed.\n\nEnhanced protections and restrictions for data and inferences related to sensitive domains, including health, work, education, criminal justice, and finance, and for data pertaining to youth should put you first.\n\nIn sensitive domains, your data and related inferences should only be used for necessary functions, and you should be protected by ethical review and use prohibitions.\n\nYou and your communities should be free from unchecked surveillance; surveillance technologies should be subject to heightened oversight that includes at least pre-deployment assessment of their potential harms and scope limits to protect privacy and civil liberties.\n\nContinuous surveillance and monitoring should not be used in education, work, housing, or in other contexts where the use of such surveillance technologies is likely to limit rights, opportunities, or access.\n\nWhenever possible, you should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.\n\nData privacy is a foundational and cross-cutting principle required for achieving all others in this framework.\n\nSurveillance and data collection, sharing, use, and reuse now sit at the foundation of business models across many industries, with more and more companies tracking the behavior of the American public, building individual profiles based on this data, and using this granular-level information as input into automated systems that further track, profile, and impact the American public.\n\nGovernment agencies, particularly law enforcement agencies, also use and help develop a variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as input into other automated systems that directly impact people’s lives.\n\nFederal law has not grown to address the expanding scale of private data collection, or of the ability of governments at all levels to access that data and leverage the means of private collection.\n\nMeanwhile, members of the American public are often unable to access their personal data or make critical decisions about its collection and use.\n\nData brokers frequently collect consumer data from numerous sources without consumers’ permission or knowledge, making it difficult for individuals to manage personal information, comprehend how it has been used, or determine the accuracy and impact of such usage.",
    "# Data Privacy and Security Concerns\n\nKnowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used to make decisions about their lives, such as whether they will qualify for a loan or get a job.\n\nUse of surveillance technologies has increased in schools and workplaces, and, when coupled with consequential management and evaluation decisions, it is leading to mental health harms such as lowered self-confidence, anxiety, depression, and a reduced ability to use analytical reasoning.61 Documented patterns show that personal data is being aggregated by data brokers to profile communities in harmful ways.62 The impact of all this data harvesting is corrosive, breeding distrust, anxiety, and other mental health problems; chilling speech, protest, and worker organizing; and threatening our democratic process.63 The American public should be protected from these growing risks.\n\nIncreasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumer privacy into their products by design and by default, including by minimizing the data they collect, communicating collection and use clearly, and improving security practices.\n\nFederal government surveillance and other collection and use of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention in some cases.\n\nMany states have also enacted consumer data privacy protection regimes to address some of these harms.\n\nHowever, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory framework governing the rights of the public when it comes to personal data.\n\nWhile a patchwork of laws exists to guide the collection and use of personal data in specific contexts, including health, employment, education, and credit, it can be unclear how these laws apply in other contexts and in an increasingly automated society.\n\nAdditional protections would assure the American public that the automated systems they use are not monitoring their activities, collecting information on their lives, or otherwise surveilling them without context-specific consent or legal authority.\n\nAn insurer might collect data from a person's social media presence as part of deciding what life insurance rates they should be offered.64 A data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds of thousands of people to potential identity theft.65 A local public housing authority installed a facial recognition system at the entrance to housing complexes to assist law enforcement with identifying individuals viewed via camera when police reports are filed, leading the community, both those living in the housing complex and not, to have videos of them sent to the local police department and made available for scanning by its facial recognition software.66 Companies use surveillance software to track employee discussions about union activity and use the resulting data to surveil individual employees and surreptitiously intervene in discussions.67\n\nTraditional terms of service—the block of text that the public is accustomed to clicking through when using a website or digital app—are not an adequate mechanism for protecting privacy.\n\nThe American public should be protected via built-in privacy protections, data minimization, use and collection limitations, and transparency, in addition to being entitled to clear mechanisms to control access to and use of their data—including their metadata—in a proactive, informed, and ongoing way.\n\nAny automated system collecting, using, sharing, or storing personal data should meet these expectations.",
    "# Protect Privacy by Design and by Default\n\nPrivacy by design and by default.\n\nAutomated systems should be designed and built with privacy protected by default.\n\nPrivacy risks should be assessed throughout the development life cycle, including privacy risks from reidentification, and appropriate technical and policy mitigation measures should be implemented.\n\nThis includes potential harms to those who are not users of the automated system, but who may be harmed by inferred data, purposeful privacy violations, or community surveillance or other community harms.\n\nData collection should be minimized and clearly communicated to the people whose data is collected.\n\nData should only be collected or used for the purposes of training or testing machine learning models if such collection and use is legal and consistent with the expectations of the people whose data is collected.\n\nUser experience research should be conducted to confirm that people understand what data is being collected about them and how it will be used, and that this collection matches their expectations and desires.\n\nData collection and use-case scope limits.\n\nData collection should be limited in scope, with specific, narrow identified goals, to avoid \"mission creep.\"\n\nAnticipated data collection should be determined to be strictly necessary to the identified goals and should be minimized as much as possible.\n\nData collected based on these identified goals and for a specific context should not be used in a different context without assessing for new privacy risks and implementing appropriate mitigation measures, which may include express consent.\n\nClear timelines for data retention should be established, with data deleted as soon as possible in accordance with legal or policy-based limitations.\n\nDetermined data retention timelines should be documented and justified.\n\nRisk identification and mitigation.\n\nEntities that collect, use, share, or store sensitive data should attempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropriately to identified risks.\n\nAppropriate responses include determining not to process data when the privacy risks outweigh the benefits or implementing measures to mitigate acceptable risks.\n\nAppropriate responses do not include sharing or transferring the privacy risks to users via notice or consent requests where users could not reasonably be expected to understand the risks without further support.\n\nPrivacy-preserving security.\n\nEntities creating, using, or governing automated systems should follow privacy and security best practices designed to ensure data and metadata do not leak beyond the specific consented use case.\n\nBest practices could include using privacy-enhancing cryptography or other types of privacy-enhancing technologies or fine-grained permissions and access control mechanisms, along with conventional system security protocols.",
    "# Protect the Public from Unchecked Surveillance\n\nHeightened oversight of surveillance.\n\nSurveillance or monitoring systems should be subject to heightened oversight that includes at a minimum assessment of potential harms during design (before deployment) and in an ongoing manner, to ensure that the American public’s rights, opportunities, and access are protected.\n\nThis assessment should be done before deployment and should give special attention to ensure there is not algorithmic discrimination, especially based on community membership, when deployed in a specific real-world context.\n\nSuch assessment should then be reaffirmed in an ongoing manner as long as the system is in use.\n\nLimited and proportionate surveillance.\n\nSurveillance should be avoided unless it is strictly necessary to achieve a legitimate purpose and it is proportionate to the need.\n\nDesigners, developers, and deployers of surveillance systems should use the least invasive means of monitoring available and restrict monitoring to the minimum number of subjects possible.\n\nTo the greatest extent possible consistent with law enforcement and national security needs, individuals subject to monitoring should be provided with clear and specific notice before it occurs and be informed about how the data gathered through surveillance will be used.\n\nScope limits on surveillance to protect rights and democratic values.\n\nCivil liberties and civil rights must not be limited by the threat of surveillance or harassment facilitated or aided by an automated system.\n\nSurveillance systems should not be used to monitor the exercise of democratic rights, such as voting, privacy, peaceful assembly, speech, or association, in a way that limits the exercise of civil rights or civil liberties.\n\nInformation about or algorithmically-determined assumptions related to identity should be carefully limited if used to target or guide surveillance systems in order to avoid algorithmic discrimination; such identity-related information includes group characteristics or affiliations, geographic designations, location-based and association-based inferences, social networks, and biometrics.\n\nContinuous surveillance and monitoring systems should not be used in physical or digital workplaces (regardless of employment status), public educational institutions, and public accommodations.\n\nContinuous surveillance and monitoring systems should not be used in a way that has the effect of limiting access to critical resources or services or suppressing the exercise of rights, even where the organization is not under a particular duty to protect those rights.",
    "# Provide the Public with Mechanisms for Appropriate and Meaningful Consent, Access, and Control Over Their Data\n\nUse-specific consent.\n\nConsent practices should not allow for abusive surveillance practices.\n\nWhere data collectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specific time durations, and for use by specific entities.\n\nConsent should not extend if any of these conditions change; consent should be re-acquired before using data if the use case changes, a time limit elapses, or data is transferred to another entity (including being shared or sold).\n\nConsent requested should be limited in scope and should not request consent beyond what is required.\n\nRefusal to provide consent should be allowed, without adverse effects, to the greatest extent possible based on the needs of the use case.\n\nBrief and direct consent requests.\n\nWhen seeking consent from users short, plain language consent requests should be used so that users understand for what use contexts, time span, and entities they are providing data and metadata consent.\n\nUser experience research should be performed to ensure these consent requests meet performance standards for readability and comprehension.\n\nThis includes ensuring that consent requests are accessible to users with disabilities and are available in the language(s) and reading level appropriate for the audience.\n\nUser experience design choices that intentionally obfuscate or manipulate user choice (i.e., “dark patterns”) should not be used.\n\nData access and correction.\n\nPeople whose data is collected, used, shared, or stored by automated systems should be able to access data and metadata about themselves, know who has access to this data, and be able to correct it if necessary.\n\nEntities should receive consent before sharing data with other entities and should keep records of what data is shared and with whom.\n\nConsent withdrawal and data deletion.\n\nEntities should allow (to the extent legally permissible) withdrawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of their data from any systems (e.g., machine learning models) derived from that data.68 Automated system support.\n\nEntities designing, developing, and deploying automated systems should establish and maintain the capabilities that will allow individuals to use their own automated systems to help them make consent, access, and control decisions in a complex data ecosystem.\n\nCapabilities include machine-readable data, standardized data formats, metadata or tags for expressing data processing permissions and preferences and data provenance and lineage, context of use and access-specific tags, and training models for assessing privacy risk.",
    "# Demonstrate that Data Privacy and User Control are Protected\n\nIndependent evaluation.\n\nAs described in the section on Safe and Effective Systems, entities should allow independent evaluation of the claims made regarding data policies.\n\nThese independent evaluations should be made public whenever possible.\n\nCare will need to be taken to balance individual privacy with evaluation data access needs.\n\nReporting.\n\nWhen members of the public wish to know what data about them is being used in a system, the entity responsible for the development of the system should respond quickly with a report on the data it has collected or stored about them.\n\nSuch a report should be machine-readable, understandable by most users, and include, to the greatest extent allowable under law, any data and metadata about them or collected from them, when and how their data and metadata were collected, the specific ways that data or metadata are being used, who has access to their data and metadata, and what time limitations apply to these data.\n\nIn cases where a user login is not available, identity verification may need to be performed before providing such a report to ensure user privacy.\n\nAdditionally, summary reporting should be proactively made public with general information about how peoples’ data and metadata are used, accessed, and stored.\n\nSummary reporting should include the results of any surveillance pre-deployment assessment, including disparity assessment in the real-world deployment context, the specific identified goals of any data collection, and the assessment done to ensure only the minimum required data is collected.\n\nIt should also include documentation about the scope limit assessments, including data retention timelines and associated justification, and an assessment of the impact of surveillance or data collection on rights, opportunities, and access.\n\nWhere possible, this assessment of the impact of surveillance should be done by an independent party.\n\nReporting should be provided in a clear and machine-readable manner.",
    "# Extra Protections for Data Related to Sensitive Domains\n\nSome domains, including health, employment, education, criminal justice, and personal finance, have long been singled out as sensitive domains deserving of enhanced data protections.\n\nThis is due to the intimate nature of these domains as well as the inability of individuals to opt out of these domains in any meaningful way, and the historical discrimination that has often accompanied data knowledge.69 Domains understood by the public to be sensitive also change over time, including because of technological developments.\n\nTracking and monitoring technologies, personal tracking devices, and our extensive data footprints are used and misused more than ever before; as such, the protections afforded by current legal guidelines may be inadequate.\n\nThe American public deserves assurances that data related to such sensitive domains is protected and used appropriately and only in narrowly defined contexts with clear benefits to the individual and/or society.\n\nTo this end, automated systems that collect, use, share, or store data related to these sensitive domains should meet additional expectations.\n\nData and metadata are sensitive if they pertain to an individual in a sensitive domain (defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship history and legal status such as custody and divorce information, and home, work, or school environmental data); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm due to identity theft.\n\nData and metadata generated by or about those who are not yet legal adults is also sensitive, even if not related to a sensitive domain.\n\nSuch data includes, but is not limited to, numerical, text, image, audio, or video data.\n\n“Sensitive domains” are those in which activities being conducted can cause material harms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights.\n\nDomains that have historically been singled out as deserving of enhanced data protections or where such enhanced protections are reasonably expected by the public include, but are not limited to, health, family planning and care, employment, education, criminal justice, and personal finance.\n\nIn the context of this framework, such domains are considered sensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains and data that are considered sensitive are understood to change over time based on societal norms and context.",
    "## Example Cases\n\nContinuous positive airway pressure machines gather data for medical purposes, such as diagnosing sleep apnea, and send usage data to a patient’s insurance company, which may subsequently deny coverage for the device based on usage data.\n\nPatients were not aware that the data would be used in this way or monitored by anyone other than their doctor.70\n\nA department store company used predictive analytics applied to collected consumer data to determine that a teenage girl was pregnant, and sent maternity clothing ads and other baby-related advertisements to her house, revealing to her father that she was pregnant.71\n\nSchool audio surveillance systems monitor student conversations to detect potential \"stress indicators\" as a warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on an exam using biometric markers.73 These systems have the potential to limit student freedom to express a range of emotions at school and may inappropriately flag students with disabilities who need accommodations or use screen readers or dictation software as cheating.74\n\nLocation data, acquired from a data broker, can be used to identify people who visit abortion clinics.75\n\nCompanies collect student data such as demographic information, free or reduced lunch status, whether they've used drugs, or whether they've expressed interest in LGBTQI+ groups, and then use that data to forecast student success.76 Parents and education experts have expressed concern about collection of such sensitive data without express parental consent, the lack of transparency in how such data is being used, and the potential for resulting discriminatory impacts.\n\nMany employers transfer employee data to third party job verification services.\n\nThis information is then used by potential future employers, banks, or landlords.\n\nIn one case, a former employee alleged that a company supplied false data about her job title which resulted in a job offer being revoked.77",
    "## Additional Expectations for Sensitive Domains\n\nIn addition to the privacy expectations above for general non-sensitive data, any system collecting, using, sharing, or storing sensitive data should meet the expectations below.\n\nDepending on the technological use case and based on an ethical assessment, consent for sensitive data may need to be acquired from a guardian and/or child.",
    "### Provide Enhanced Protections for Data Related to Sensitive Domains\n\nNecessary functions only.\n\nSensitive data should only be used for functions strictly necessary for that domain or for functions that are required for administrative reasons (e.g., school attendance records), unless consent is acquired, if appropriate, and the additional expectations in this section are met.\n\nConsent for non-necessary functions should be optional, i.e., should not be required, incentivized, or coerced in order to receive opportunities or access to services.\n\nIn cases where data is provided to an entity (e.g., health insurance company) in order to facilitate payment for such a need, that data should only be used for that purpose.\n\nEthical review and use prohibitions.\n\nAny use of sensitive data or decision process based in part on sensitive data that might limit rights, opportunities, or access, whether the decision is automated or not, should go through a thorough ethical review and monitoring, both in advance and by periodic review (e.g., via an independent ethics committee or similarly robust process).\n\nIn some cases, this ethical review may determine that data should not be used or shared for specific uses even with consent.\n\nSome novel uses of automated systems in this context, where the algorithm is dynamically developing and where the science behind the use case is not well established, may also count as human subject experimentation, and require special review under organizational compliance bodies applying medical, scientific, and academic human subject experimentation ethics rules and governance procedures.\n\nData quality.\n\nIn sensitive domains, entities should be especially careful to maintain the quality of data to avoid adverse consequences arising from decision-making based on flawed or inaccurate data.\n\nSuch care is necessary in a fragmented, complex data ecosystem and for datasets that have limited access such as for fraud prevention and law enforcement.\n\nIt should be not left solely to individuals to carry the burden of reviewing and correcting data.\n\nEntities should conduct regular, independent audits and take prompt corrective measures to maintain accurate, timely, and complete data.\n\nLimit access to sensitive data and derived data.\n\nSensitive data and derived data should not be sold, shared, or made public as part of data brokerage or other agreements.\n\nSensitive data includes data that can be used to infer sensitive information; even systems that are not directly marketed as sensitive domain technologies are expected to keep sensitive data private.\n\nAccess to such data should be limited based on necessity and based on a principle of local control, such that those individuals closest to the data subject have more access while those who are less proximate do not (e.g., a teacher has access to their students’ daily progress data while a superintendent does not).",
    "### Reporting\n\nIn addition to the reporting on data privacy (as listed above for non-sensitive data), entities developing technologies related to a sensitive domain and those collecting, using, storing, or sharing sensitive data should, whenever appropriate, regularly provide public reports describing: any data security lapses or breaches that resulted in sensitive data leaks; the number, type, and outcomes of ethical pre-reviews undertaken; a description of any data sold, shared, or made public, and how that data was assessed to determine it did not present a sensitive data risk; and ongoing risk identification and management procedures, and any mitigation added based on these procedures.\n\nReporting should be provided in a clear and machine-readable manner.",
    "# Privacy Act and Other Laws\n\nThe Privacy Act of 1974 requires privacy protections for personal information in federal records systems, including limits on data retention, and also provides individuals a general right to access and correct their data.\n\nAmong other things, the Privacy Act limits the storage of individual information in federal systems of records, illustrating the principle of limiting the scope of data retention.\n\nUnder the Privacy Act, federal agencies may only retain data about an individual that is “relevant and necessary” to accomplish an agency’s statutory purpose or to comply with an Executive Order of the President.\n\nThe law allows for individuals to be able to access any of their individual information stored in a federal system of records, if not included under one of the systems of records exempted pursuant to the Privacy Act.\n\nIn these cases, federal agencies must provide a method for an individual to determine if their personal information is stored in a particular system of records, and must provide procedures for an individual to contest the contents of a record about them.\n\nFurther, the Privacy Act allows for a cause of action for an individual to seek legal relief if a federal agency does not comply with the Privacy Act’s requirements.\n\nAmong other things, a court may order a federal agency to amend or correct an individual’s information in its records or award monetary damages if an inaccurate, irrelevant, untimely, or incomplete record results in an adverse determination about an individual’s “qualifications, character, rights, … opportunities…, or benefits.”\n\nNIST’s Privacy Framework provides a comprehensive, detailed and actionable approach for organizations to manage privacy risks.\n\nThe NIST Framework gives organizations ways to identify and communicate their privacy risks and goals to support ethical decision-making in system, product, and service design or deployment, as well as the measures they are taking to demonstrate compliance with applicable laws or regulations.\n\nIt has been voluntarily adopted by organizations across many different sectors around the world.78\n\nA  school  board’s  attempt  to  surveil  public  school  students—undertaken without adequate community input—sparked a state-wide biometrics moratorium.79 Reacting to a plan in the city of Lockport, New York, the state’s legislature banned the use of facial recognition systems and other “biometric identifying technology” in schools until July 1, 2022.80 The law additionally requires that a report on the privacy, civil rights, and civil liberties implications of the use of such technologies be issued before biometric identification technologies can be used in New York schools.\n\nFederal law requires employers, and any consultants they may retain, to report the costs of surveilling employees in the context of a labor dispute, providing a transparency mechanism to help protect worker organizing.\n\nEmployers engaging in workplace surveillance \"where an object thereof, directly or indirectly, is […] to obtain information concerning the activities of employees or a labor organization in connection with a labor dispute\" must report expenditures relating to this surveillance to the Department of Labor Office of Labor-Management Standards, and consultants who employers retain for these purposes must also file reports regarding their activities.81\n\nPrivacy choices on smartphones show that when technologies are well designed, privacy and data agency can be meaningful and not overwhelming.\n\nThese choices—such as contextual, timely alerts about location tracking—are brief, direct, and use-specific.\n\nMany of the expectations listed here for privacy by design and use-specific consent mirror those distributed to developers as best practices when developing for smartphone devices,82 such as being transparent about how user data will be used, asking for app permissions during their use so that the use-context will be clear to users, and ensuring that the app will still work if users deny (or later revoke) some permissions.",
    "# Notice and Explanation\n\nYou should know that an automated system is being used, and understand how and why it contributes to outcomes that impact you.\n\nDesigners, developers, and deployers of automated systems should provide generally accessible plain language documentation including clear descriptions of the overall system functioning and the role automation plays, notice that such systems are in use, the individual or organization responsible for the system, and explanations of outcomes that are clear, timely, and accessible.\n\nSuch notice should be kept up-to-date and people impacted by the system should be notified of significant use case or key functionality changes.\n\nYou should know how and why an outcome impacting you was determined by an automated system, including when the automated system is not the sole input determining the outcome.\n\nAutomated systems should provide explanations that are technical.",
    "# Notice and Explanation of Automated Systems\n\nAutomated systems now determine opportunities, from employment to credit, and directly shape the American public’s experiences, from the courtroom to online classrooms, in ways that profoundly impact people’s lives.\n\nBut this expansive impact is not always visible.\n\nAn applicant might not know whether a person rejected their resume or a hiring algorithm moved them to the bottom of the list.\n\nA defendant in the courtroom might not know if a judge denying their bail is informed by an automated system that labeled them “high risk.” From correcting errors to contesting decisions, people are often denied the knowledge they need to address the impact of automated systems on their lives.\n\nNotice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonableness of a recommendation before enacting it.\n\nIn order to guard against potential harms, the American public needs to know if an automated system is being used.\n\nClear, brief, and understandable notice is a prerequisite for achieving the other protections in this framework.\n\nLikewise, the public is often unable to ascertain how or why an automated system has made a decision or contributed to a particular outcome.\n\nThe decision-making processes of automated systems tend to be opaque, complex, and, therefore, unaccountable, whether by design or by omission.\n\nThese factors can make explanations both more challenging and more important, and should not be used as a pretext to avoid explaining important decisions to the people impacted by those choices.\n\nIn the context of automated systems, clear and valid explanations should be recognized as a baseline requirement.\n\nProviding notice has long been a standard practice, and in many cases is a legal requirement, when, for example, making a video recording of someone (outside of a law enforcement or national security context).\n\nIn some cases, such as credit, lenders are required to provide notice and explanation to consumers.\n\nTechniques used to automate the process of explaining such systems are under active research and improvement and such explanations can take many forms.\n\nInnovative companies and researchers are rising to the challenge and creating and deploying explanatory systems that can help the public better understand decisions that impact them.\n\nWhile notice and explanation requirements are already in place in some sectors or situations, the American public deserves to know consistently and across sectors if an automated system is being used in a way that impacts their rights, opportunities, or access.\n\nThis knowledge should provide confidence in how the public is being treated, and trust in the validity and reasonable use of automated systems.\n\nA lawyer representing an older client with disabilities who had been cut off from Medicaid-funded home health-care assistance couldn't determine why, especially since the decision went against historical access practices.\n\nIn a court hearing, the lawyer learned from a witness that the state in which the older client lived had recently adopted a new algorithm to determine eligibility.\n\nThe lack of a timely explanation made it harder to understand and contest the decision.\n\nA formal child welfare investigation is opened against a parent based on an algorithm and without the parent ever being notified that data was being collected and used as part of an algorithmic child maltreatment risk assessment.\n\nThe lack of notice or an explanation makes it harder for those performing child maltreatment assessments to validate the risk assessment and denies parents knowledge that could help them contest a decision.\n\nA predictive policing system claimed to identify individuals at greatest risk to commit or become the victim of gun violence (based on automated analysis of social ties to gang members, criminal histories, previous experiences of gun violence, and other factors) and led to individuals being placed on a watch list with no explanation or public transparency regarding how the system came to its conclusions.\n\nBoth police and the public deserve to understand why and how such a system is making these determinations.\n\nA system awarding benefits changed its criteria invisibly.\n\nIndividuals were denied benefits due to data entry errors and other system flaws.\n\nThese flaws were only revealed when an explanation of the system was demanded and produced.\n\nThe lack of an explanation made it harder for errors to be corrected in a timely manner.",
    "### Generally Accessible Plain Language Documentation\n\nThe entity responsible for using the automated system should ensure that documentation describing the overall system (including any human components) is public and easy to find.\n\nThe documentation should describe, in plain language, how the system works and how any automated component is used to determine an action or decision.\n\nIt should also include expectations about reporting described throughout this framework, such as the algorithmic impact assessments described as part of Algorithmic Discrimination Protections.",
    "### Timely and Up-to-date\n\nUsers should receive notice of the use of automated systems in advance of using or while being impacted by the technology.\n\nAn explanation should be available with the decision itself, or soon thereafter.\n\nNotice should be kept up-to-date and people impacted by the system should be notified of use case or key functionality changes.",
    "### Brief and Clear\n\nNotices and explanations should be assessed, such as by research on users’ experiences, including user testing, to ensure that the people using or impacted by the automated system are able to easily find notices and explanations, read them quickly, and understand and act on them.\n\nThis includes ensuring that notices and explanations are accessible to users with disabilities and are available in the language(s) and reading level appropriate for the audience.\n\nNotices and explanations may need to be available in multiple forms, (e.g., on paper, on a physical sign, or online) to meet these expectations and to be accessible to the American public.",
    "#### Tailored to the Purpose\n\nExplanations should be tailored to the specific purpose for which the user is expected to use the explanation and should clearly state that purpose.\n\nAn informational explanation might differ from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the context of a dispute or contestation process.\n\nFor the purposes of this framework, 'explanation' should be construed broadly.\n\nAn explanation need not be a plain-language statement about causality but could consist of any mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the stated purpose.\n\nTailoring should be assessed (e.g., via user experience research).",
    "#### Tailored to the Target of the Explanation\n\nExplanations should be targeted to specific audiences and clearly state that audience.\n\nAn explanation provided to the subject of a decision might differ from one provided to an advocate, or to a domain expert or decision maker.\n\nTailoring should be assessed (e.g., via user experience research).",
    "#### Tailored to the Level of Risk\n\nAn assessment should be done to determine the level of risk of the automated system.\n\nIn settings where the consequences are high as determined by a risk assessment, or extensive oversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should be built into the system design so that the system’s full behavior can be explained in advance (i.e., only fully transparent models should be used), rather than as an after-the-decision interpretation.\n\nIn other settings, the extent of explanation provided should be tailored to the risk level.",
    "#### Valid\n\nThe explanation provided by a system should accurately reflect the factors and influences that led to a particular decision, and should be meaningful for the particular customization based on purpose, target, and level of risk.\n\nWhile approximation and simplification may be necessary for the system to succeed based on the explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns related to revealing decision-making information, such simplifications should be done in a scientifically supportable way.\n\nWhere appropriate based on the explanatory system, error ranges for the explanation should be calculated and included in the explanation, with the choice of presentation of such information balanced with usability and overall interface complexity concerns.",
    "### Reporting\n\nSummary reporting should document the determinations made based on the above considerations, including: the responsible entities for accountability purposes; the goal and use cases for the system, identified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of the explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment of how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of risk.\n\nIndividualized profile information should be made readily available to the greatest extent possible that includes explanations for any system impacts or inferences.\n\nReporting should be provided in a clear plain language and machine-readable manner.\n\n---",
    "## Human Alternatives, Consideration, and Fallback\n\nYou should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter.\n\nYou should be able to opt out from automated systems in favor of a human alternative, where appropriate.\n\nAppropriateness should be determined based on reasonable expectations in a given context and with a focus on ensuring broad accessibility and protecting the public from especially harmful impacts.\n\nIn some cases, a human or other alternative may be required by law.\n\nYou should have access to timely human consideration and remedy by a fallback and escalation process if an automated system fails, it produces an error, or you would like to appeal or contest its impacts on you.\n\nHuman consideration and fallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and should not impose an unreasonable burden on the public.\n\nAutomated systems with an intended use within sensitive domains, including, but not limited to, criminal justice, employment, education, and health, should additionally be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting with the system, and incorporate human consideration for adverse or high-risk decisions.",
    "### Brief, Clear, Accessible Notice and Instructions\n\nThose impacted by an automated system should be given a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out.\n\nInstructions should be provided in an accessible form and should be easily findable by those impacted by the automated system.\n\nThe brevity, clarity, and accessibility of the notice and instructions should be assessed (e.g., via user experience research).",
    "### Human Alternatives Provided When Appropriate\n\nIn many scenarios, there is a reasonable expectation of human involvement in attaining rights, opportunities, or access.\n\nWhen automated systems make up part of the attainment process, alternative timely human-driven processes should be provided.\n\nThe use of a human alternative should be triggered by an opt-out process.",
    "### Proportionate\n\nThe availability of human consideration and fallback, along with associated training and safeguards against human bias, should be proportionate to the potential of the automated system to meaningfully impact rights, opportunities, or access.\n\nAutomated systems that have greater control over outcomes, provide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential to meaningfully impact rights, opportunities, or access should have greater availability (e.g., staffing) and oversight of human consideration and fallback mechanisms.",
    "### Accessible\n\nMechanisms for human consideration and fallback, whether in-person, on paper, by phone, or otherwise provided, should be easy to find and use.\n\nThese mechanisms should be tested to ensure that users who have trouble with the automated system are able to use human consideration and fallback, with the understanding that it may be these users who are most likely to need the human assistance.\n\nSimilarly, it should be tested to ensure that users with disabilities are able to find and use human consideration and fallback and also request reasonable accommodations or modifications.",
    "### Timely\n\nHuman consideration and fallback are only useful if they are conducted and concluded in a timely manner.\n\nThe determination of what is timely should be made relative to the specific automated system, and the review system should be staffed and regularly assessed to ensure it is providing timely consideration and fallback.\n\nIn time-critical systems, this mechanism should be immediately available or, where possible, available before the harm occurs.\n\nTime-critical systems include, but are not limited to, voting-related systems, automated building access and other access systems, systems that form a critical component of healthcare, and systems that have the ability to withhold wages or otherwise cause immediate financial penalties.",
    "### Effective\n\nThe organizational structure surrounding processes for consideration and fallback should be designed so that if the human decision-maker charged with reassessing a decision determines that it should be overruled, the new decision will be effectively enacted.\n\nThis includes ensuring that the new decision is entered into the automated system throughout its components, any previous repercussions from the old decision are also overturned, and safeguards are put in place to help ensure that future decisions do not result in the same errors.",
    "### Training and Assessment\n\nAnyone administering, interacting with, or interpreting the outputs of an automated system should receive training in that system, including how to properly interpret outputs of a system in light of its intended purpose and in how to mitigate the effects of automation bias.\n\nThe training should reoccur regularly to ensure it is up to date with the system and to ensure the system is used appropriately.\n\nAssessment should be ongoing to ensure that the use of the system with human involvement provides for appropriate results, i.e., that the involvement of people does not invalidate the system's assessment as safe and effective or lead to algorithmic discrimination.",
    "### Oversight\n\nHuman-based systems have the potential for bias, including automation bias, as well as other concerns that may limit their effectiveness.\n\nThe results of assessments of the efficacy and potential bias of such human-based systems should be overseen by governance structures that have the potential to update the operation of the human-based system to mitigate these effects.\n\n---",
    "## Implement Additional Human Oversight and Safeguards\n\nAutomated systems used within sensitive domains, including criminal justice, employment, education, and health, should meet the expectations laid out throughout this framework, especially avoiding capricious, inappropriate, and discriminatory impacts of these technologies.\n\nAdditionally, automated systems used within sensitive domains should meet these expectations:\n- **Narrowly Scoped Data and Inferences**: Human oversight should ensure that automated systems in sensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attribute as relevant to the specific use case.\n\nData included should be carefully limited to avoid algorithmic discrimination resulting from, e.g., use of community characteristics, social network analysis, or group-based inferences.\n\n- **Tailored to the Situation**: Human oversight should ensure that automated systems...",
    "# Sensitive Domains and Human Consideration\n\nIn sensitive domains, the conditions are tailored to the specific use case and real-world deployment scenario.\n\nEvaluation testing should confirm the system's safety and effectiveness for that specific situation.\n\nValidation testing at one location or for one use case shouldn't be assumed transferable to another.\n\nAutomated systems used in sensitive domains may provide information or positive outcomes to impacted individuals.\n\nHowever, such systems shouldn't directly intervene in high-risk situations like sentencing decisions or medical care without human consideration.",
    "# Meaningful Access to Examine the System\n\nDesigners and developers of automated systems should consider limited waivers of confidentiality, such as trade secrets, for meaningful oversight in sensitive domains.\n\nThis involves protecting intellectual property while allowing limited access to source code, documentation, and related data during legal discovery.\n\nAccess should align with the principle on Notice and Explanation using a transparent model to ensure understanding for those examining it.",
    "# Reporting: Human Consideration and Alternatives\n\nReports should assess the timeliness and extent of burdens for human alternatives, including statistics on their choice and results.\n\nReporting must include how human consideration and fallback options are accessed and utilized.\n\nThis involves aggregated data on requests, response timeliness, procedural handling, and effectiveness.\n\nFurthermore, systems in sensitive domains should disclose training and governance information and document goals and results in a machine-readable manner.",
    "# Healthcare Navigators\n\nHealthcare navigators help individuals deal with enrollment forms to obtain healthcare.\n\nNavigators are trained to assist consumers, small businesses, and employees with health coverage through government marketplaces, including eligibility and enrollment.\n\nThe Biden-Harris Administration increased funding for the 2022 plan year to train and certify over 1,500 navigators to help uninsured consumers.",
    "# Ballot Curing Laws\n\nIn at least 24 states, ballot curing laws allow voters to correct issues if a signature matching algorithm misflags their ballot.\n\nFederal courts have deemed such procedures constitutionally required.\n\nCuring processes vary by state and involve direct contact methods by election officials to resolve issues like signature mismatches.",
    "# Alliance\n\n- Carnegie Mellon University\n- Center for Democracy & Technology\n- Center for New Democratic Processes\n- Center for Research and Education on Accessible Technology and Experiences at University of Washington, Devva Kasnitz, L Jean Camp, Jonathan Lazar, Harry Hochheiser\n- Center on Privacy & Technology at Georgetown Law\n- Cisco Systems\n- City of Portland Smart City PDX Program\n- CLEAR\n- Clearview AI\n- Cognoa\n- Color of Change\n- Common Sense Media\n- Computing Community Consortium at Computing Research Association\n- Connected Health Initiative\n- Consumer Technology Association\n- Courtney Radsch\n- Coworker\n- Cyber Farm Labs\n- Data & Society Research Institute\n- Data for Black Lives\n- Data to Actionable Knowledge Lab at Harvard University\n- Deloitte\n- Dev Technology Group\n- Digital Therapeutics Alliance\n- Digital Welfare State & Human Rights Project and Center for Human Rights and Global Justice at New York University School of Law, and Temple University Institute for Law, Innovation & Technology\n- Dignari\n- Douglas Goddard\n- Edgar Dworsky\n- Electronic Frontier Foundation\n- Electronic Privacy Information Center, Center for Digital Democracy, and Consumer Federation of America\n- FaceTec\n- Fight for the Future\n- Ganesh Mani\n- Georgia Tech Research Institute\n- Google\n- Health Information Technology Research and Development Interagency Working Group\n- HireVue\n- HR Policy Association\n- ID.me\n- Identity and Data Sciences Laboratory at Science Applications International Corporation\n- Information Technology and Innovation Foundation\n- Information Technology Industry Council\n- Innocence Project\n- Institute for Human-Centered Artificial Intelligence at Stanford University\n- Integrated Justice Information Systems Institute\n- International Association of Chiefs of Police\n- International Biometrics + Identity Association\n- International Business Machines Corporation\n- International Committee of the Red Cross\n- Inventionphysics\n- iProov\n- Jacob Boudreau\n- Jennifer K. Wagner, Dan Berger, Margaret Hu, and Sara Katsanis\n- Jonathan Barry-Blocker\n- Joseph Turow\n- Joy Buolamwini\n- Joy Mack\n- Karen Bureau\n- Lamont Gholston\n- Lawyers’ Committee for Civil Rights Under Law",
    "# Additional Stakeholders\n\n- Lisa Feldman Barrett\n- Madeline Owens\n- Marsha Tudor\n- Microsoft Corporation\n- MITRE Corporation\n- National Association for the Advancement of Colored People Legal Defense and Educational Fund\n- National Association of Criminal Defense Lawyers\n- National Center for Missing & Exploited Children\n- National Fair Housing Alliance\n- National Immigration Law Center\n- NEC Corporation of America\n- New America’s Open Technology Institute\n- New York Civil Liberties Union\n- No Name Provided\n- Notre Dame Technology Ethics Center\n- Office of the Ohio Public Defender\n- Onfido\n- Oosto\n- Orissa Rose\n- Palantir\n- Pangiam\n- Parity Technologies\n- Patrick A. Stewart, Jeffrey K. Mullins, and Thomas J. Greitens\n- Pel Abbott\n- Philadelphia Unemployment Project\n- Project On Government Oversight\n- Recording Industry Association of America\n- Robert Wilkens\n- Ron Hedges\n- Science, Technology, and Public Policy Program at University of Michigan Ann Arbor\n- Security Industry Association\n- Sheila Dean\n- Software & Information Industry Association\n- Stephanie Dinkins and the Future Histories Studio at Stony Brook University\n- TechNet\n- The Alliance for Media Arts and Culture, MIT Open Documentary Lab and Co-Creation Studio, and Immerse\n- The International Brotherhood of Teamsters\n- The Leadership Conference on Civil and Human Rights\n- Thorn\n- U.S. Chamber of Commerce’s Technology Engagement Center\n- Uber Technologies\n- University of Pittsburgh Undergraduate Student Collaborative\n- Upturn\n- US Technology Policy Committee of the Association of Computing Machinery\n- Virginia Puccio\n- Visar Berisha and Julie Liss\n- XR Association\n- XR Safety Initiative",
    "# Additional Efforts\n\nAs an additional effort to reach out to stakeholders regarding the RFI, OSTP conducted two listening sessions for members of the public.\n\nThe listening sessions together drew upwards of 300 participants.\n\nThe Science and Technology Policy Institute produced a synopsis of both the RFI submissions and the feedback at the listening sessions.\n\nOSTP conducted meetings with a variety of stakeholders in the private sector and civil society.\n\nSome of these meetings were specifically focused on providing ideas related to the development of the Blueprint for an AI Bill of Rights while others provided useful general context on the positive use cases, potential harms, and/or oversight possibilities for these technologies.\n\nParticipants in these conversations from the private sector and civil society included:\n\n- Adobe\n- American Civil Liberties Union (ACLU)\n- The Aspen Commission on Information Disorder\n- The Awood Center\n- The Australian Human Rights Commission\n- Biometrics Institute\n- The Brookings Institute\n- BSA | The Software Alliance\n- Cantellus Group\n- Center for American Progress\n- Center for Democracy and Technology\n- Center on Privacy and Technology at Georgetown Law\n- Christiana Care\n- Color of Change\n- Coworker\n- Data Robot\n- Data Trust Alliance\n- Data and Society Research Institute\n- Deepmind\n- EdSAFE AI Alliance\n- Electronic Privacy Information Center (EPIC)\n- Encode Justice\n- Equal AI\n- Google\n- Hitachi's AI Policy Committee\n- The Innocence Project\n- Institute of Electrical and Electronics Engineers (IEEE)\n- Intuit\n- Lawyers Committee for Civil Rights Under Law\n- Legal Aid Society\n- The Leadership Conference on Civil and Human Rights\n- Meta\n- Microsoft\n- The MIT AI Policy Forum\n- Movement Alliance Project\n- The National Association of Criminal Defense Lawyers\n- O’Neil Risk Consulting & Algorithmic Auditing\n- The Partnership on AI\n- Pinterest\n- The Plaintext Group\n- pymetrics\n- SAP\n- The Security Industry Association\n- Software and Information Industry Association (SIIA)\n- Special Competitive Studies Project\n- Thorn\n- United for Respect\n- University of California at Berkeley Citris Policy Lab\n- University of California at Berkeley Labor Center\n- Unfinished/Project Liberty\n- Upturn\n- US Chamber of Commerce\n- US Chamber of Commerce Technology Engagement Center\n- A.I.\n\nWorking Group\n- Vibrent Health\n- Warehouse Worker Resource Center\n- Waymap",
    "# Executive Orders and Reports\n\n- The Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the Federal Government.\n\nLink\n\n- The White House.\n\nRemarks by President Biden on the Supreme Court Decision to Overturn Roe v. Wade.\n\nJun.\n\n24, 2022.\n\nLink\n\n- The White House.\n\nJoin the Effort to Create A Bill of Rights for an Automated Society.\n\nNov. 10, 2021.\n\nLink\n\n- U.S. Dept.\n\nof Health, Educ.\n\n& Welfare, Report of the Sec’y’s Advisory Comm.\n\non Automated Pers.\n\nData Sys., Records, Computers, and the Rights of Citizens (July 1973).\n\nLink.\n\n- See, e.g., Office of Mgmt.\n\n& Budget, Exec.\n\nOffice of the President, Circular A-130, Managing Information as a Strategic Resource, app.\n\nII § 3 (July 28, 2016); Org.\n\nof Econ.\n\nCo-Operation & Dev., Revision of the Recommendation of the Council Concerning Guidelines Governing the Protection of Privacy and Transborder Flows of Personal Data, Annex Part Two (June 20, 2013).\n\nLink79/en/pdf).",
    "# Articles and Publications\n\n- Andrew Wong et al.\n\nExternal validation of a widely implemented proprietary sepsis prediction model in hospitalized patients.\n\nJAMA Intern Med.\n\n2021; 181(8):1065-1070. doi:10.1001/jamainternmed.2021.2626\n\n- Jessica Guynn.\n\nFacebook while black: Users call it getting 'Zucked,' say talking about racism is censored as hate speech.\n\nUSA Today.\n\nApr.\n\n24, 2019.\n\nLink\n\n- See, e.g., Michael Levitt.\n\nAirTags are being used to track people and cars.\n\nHere's what is being done about it.\n\nNPR.\n\nFeb. 18, 2022.\n\nLink; Samantha Cole.\n\nPolice Records Show Women Are Being Stalked With Apple AirTags Across the Country.\n\nMotherboard.\n\nApr.\n\n6, 2022.\n\nLink\n\n- Kristian Lum and William Isaac.\n\nTo Predict and Serve?\n\nSignificance.\n\nVol.\n\n13, No.\n\n5, p. 14-19.\n\nOct. 7, 2016.\n\nLink; Aaron Sankin, Dhruv Mehrotra, Surya Mattu, and Annie Gilbertson.\n\nCrime Prediction Software Promised to Be Free of Biases.\n\nNew Data Shows It Perpetuates Them.\n\nThe Markup and Gizmodo.\n\nDec. 2, 2021.\n\nLink\n\n- Samantha Cole.\n\nThis Horrifying App Undresses a Photo of Any Woman With a Single Click.\n\nMotherboard.\n\nJune 26, 2019.\n\nLink\n\n- Lauren Kaori Gurley.\n\nAmazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make.\n\nMotherboard.\n\nSep. 20, 2021.\n\nLink\n\n- Expectations about reporting are intended for the entity developing or using the automated system.\n\nThe resulting reports can be provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should be made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property or law enforcement considerations may prevent public release.\n\nThese reporting expectations are important for transparency, so the American people can have confidence that their rights, opportunities, and access as well as their expectations around technologies are respected.",
    "# National Artificial Intelligence Initiative Office\n\n- Agency Inventories of AI Use Cases.\n\nAccessed Sept. 8, 2022.\n\nLink\n\n- National Highway Traffic Safety Administration.\n\nLink\n\n- See, e.g., Charles Pruitt.\n\nPeople Doing What They Do Best: The Professional Engineers and NHTSA.\n\nPublic Administration Review.\n\nVol.\n\n39, No.\n\n4.\n\nJul.-Aug., 1979.\n\nLink\n\n- The US Department of Transportation has publicly described the health and other benefits of these “traffic calming” measures.\n\nSee, e.g.\n\n: U.S. Department of Transportation.\n\nTraffic Calming to Slow Vehicle Speeds.\n\nAccessed Apr.\n\n17, 2022.\n\nLink\n\n- Karen Hao.\n\nWorried about your firm’s AI ethics?\n\nThese startups are here to help.\n\nA growing ecosystem of “responsible AI” ventures promise to help organizations monitor and fix their AI models.\n\nMIT Technology Review.\n\nJan 15., 2021.\n\nLink; Disha Sinha.\n\nTop Progressive Companies Building Ethical AI to Look Out for in 2021.\n\nAnalytics Insight.\n\nJune 30, 2021.\n\nLink\n\n- Office of Management and Budget.\n\nStudy to Identify Methods to Assess Equity: Report to the President.\n\nAug. 2021.\n\nLink\n\n- National Institute of Standards and Technology.\n\nAI Risk Management Framework.\n\nAccessed May 23, 2022.\n\nLink",
    "# National Science Foundation\n\n- National Artificial Intelligence Research Institutes.\n\nAccessed Sept. 12, 2022.\n\nLink\n\n- Cyber-Physical Systems.\n\nAccessed Sept. 12, 2022.\n\nLink\n\n- Secure and Trustworthy Cyberspace.\n\nAccessed Sept. 12, 2022.\n\nLink\n\n- Formal Methods in the Field.\n\nAccessed Sept. 12, 2022.\n\nLink\n\n- Designing Accountable Software Systems.\n\nAccessed Sept. 12, 2022.\n\nLink",
    "# Civil Rights Education Fund\n\n- The Use Of Pretrial “Risk Assessment” Instruments: A Shared Statement Of Civil Rights Concerns.\n\nJul.\n\n30, 2018.\n\nLink\n\n- Idaho Legislature.\n\nHouse Bill 118.\n\nJul.\n\n1, 2019.\n\nLink\n\n- See, e.g., Executive Office of the President.\n\nBig Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights.\n\nMay, 2016.\n\nLink\n\n- Cathy O’Neil.\n\nWeapons of Math Destruction.\n\nPenguin Books.\n\n2017.\n\nLink\n\n- Ruha Benjamin.\n\nRace After Technology: Abolitionist Tools for the New Jim Code.\n\nPolity.\n\n2019.\n\nLink\n\n- See, e.g., Kashmir Hill.\n\nAnother Arrest, and Jail Time, Due to a Bad Facial Recognition Match: A New Jersey man was accused of shoplifting and trying to hit an officer with a car.\n\nHe is the third known Black man to be wrongfully arrested based on face recognition.\n\nNew York Times.\n\nDec. 29, 2020, updated Jan. 6, 2021.\n\nLink\n\n- Khari Johnson.\n\nHow Wrongful Arrests Based on AI Derailed 3 Men's Lives.\n\nWired.\n\nMar.\n\n7, 2022.\n\nLink\n\n- Student Borrower Protection Center.\n\nEducational Redlining.\n\nStudent Borrower Protection Center Report.\n\nFeb. 2020.\n\nLink\n\n- Jeffrey Dastin.\n\nAmazon scraps secret AI recruiting tool that showed bias against women.\n\nReuters.\n\nOct. 10, 2018.\n\nLink\n\n- Todd Feathers.\n\nMajor Universities Are Using Race as a “High Impact Predictor” of Student Success: Students, professors, and education experts worry that that’s pushing Black students in particular out of math and science.\n\nThe Markup.\n\nMar.\n\n2, 2021.\n\nLink\n\n- Carrie Johnson.\n\nFlaws plague a tool meant to help low-risk federal prisoners win early release.\n\nNPR.\n\nJan. 26, 2022.\n\nLink\n\n- Carrie Johnson.\n\nJustice Department works to curb racial bias in deciding who's released from prison.\n\nNPR.\n\nApr.\n\n19, 2022.\n\nLink\n\n- National Institute of Justice.\n\n2021 Review and Revalidation of the First Step Act Risk Assessment Tool.\n\nNational Institute of Justice NCJ 303859.\n\nDec., 2021.\n\nLink\n\n- Andrew Thompson.\n\nGoogle’s Sentiment Analyzer Thinks Being Gay Is Bad.\n\nVice.\n\nOct. 25, 2017.\n\nLink\n\n- Kaggle.\n\nJigsaw Unintended Bias in Toxicity Classification: Detect toxicity across a diverse range of conversations.\n\n2019.\n\nLink\n\n- Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.\n\nMeasuring and Mitigating Unintended Bias in Text Classification.\n\nProceedings of AAAI/ACM Conference on AI, Ethics, and Society.\n\nFeb. 2-3, 2018.\n\nLink\n\n- Paresh Dave.\n\nGoogle cuts racy results by 30% for searches like 'Latina teenager'.\n\nReuters.\n\nMar.\n\n30, 2022.\n\nLink\n\n- Safiya Umoja Noble.\n\nAlgorithms of Oppression: How Search Engines Reinforce Racism.\n\nNYU Press.\n\nFeb. 2018.\n\nLink\n\n- Miranda Bogen.\n\nAll the Ways Hiring Algorithms Can Introduce Bias.\n\nHarvard Business Review.\n\nMay 6, 2019.\n\nLink\n\n- Arli Christian.\n\nFour Ways the TSA Is Making Flying Easier for Transgender People.\n\nAmerican Civil Liberties Union.\n\nApr.\n\n5, 2022.\n\nLink\n\n- U.S. Transportation Security Administration.\n\nTransgender/ Non Binary / Gender Nonconforming Passengers.\n\nTSA.\n\nAccessed Apr.\n\n21, 2022.\n\nLink\n\n- See, e.g., National Disabled Law Students Association.\n\nReport on Concerns Regarding Online Administration of Bar Exams.\n\nJul.\n\n29, 2020.\n\nLink\n\n- Lydia X.\n\nZ.\n\nBrown.\n\nHow Automated Test Proctoring Software Discriminates Against Disabled Students.\n\nCenter for Democracy and Technology.\n\nNov. 16, 2020.\n\nLink\n\n- Ziad Obermeyer, et al., Dissecting racial bias in an algorithm used to manage the health of populations, 366 Science (2019), Link.\n\n- Darshali A. Vyas et al., Hidden in Plain Sight – Reconsidering the Use of Race Correction in Clinical Algorithms, 383 N. Engl.\n\nJ. Med.874, 876-78 (Aug. 27, 2020), Link.\n\n- The definitions of 'equity' and 'underserved communities' can be found in the Definitions section of this framework as well as in Section 2 of The Executive Order On Advancing Racial Equity and Support for Underserved Communities Through the Federal Government.\n\nLink",
    "# Proposals and Reports\n\n- Various organizations have offered proposals for how such assessments might be designed.\n\nSee, e.g., Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf.\n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest.\n\nData & Society Research Institute Report.\n\nJune 29, 2021.\n\nLink\n\n- Nicol Turner Lee, Paul Resnick, and Genie Barton.\n\nAlgorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms.\n\nBrookings Report.\n\nMay 22, 2019.\n\nLink\n\n- Andrew D. Selbst.\n\nAn Institutional View Of Algorithmic Impact Assessments.\n\nHarvard Journal of Law & Technology.\n\nJune 15, 2021.\n\nLink\n\n- Dillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker.\n\nAlgorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.\n\nAI Now Institute Report.\n\nApril 2018.\n\nLink",
    "# Department of Justice and Others\n\n- Justice Department Announces New Initiative to Combat Redlining.\n\nOct. 22, 2021.\n\nLink\n\n- PAVE Interagency Task Force on Property Appraisal and Valuation Equity.\n\nAction Plan to Advance Property Appraisal and Valuation Equity: Closing the Racial Wealth Gap by Addressing Mis-valuations for Families and Communities of Color.\n\nMarch 2022.\n\nLink\n\n- U.S.\n\nEqual Employment Opportunity Commission.\n\nThe Americans with Disabilities Act and the Use of Software, Algorithms, and Artificial Intelligence to Assess Job Applicants and Employees.\n\nEEOC-NVTA-2022-2.\n\nMay 12, 2022.\n\nLink\n\n- U.S. Department of Justice.\n\nAlgorithms, Artificial Intelligence, and Disability Discrimination in Hiring.\n\nMay 12, 2022.\n\nLink\n\n- Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan.\n\nDissecting racial bias in an algorithm used to manage the health of populations.\n\nScience.\n\nVol.\n\n366, No.\n\n6464.\n\nOct. 25, 2019.\n\nLink\n\n- Data & Trust Alliance.\n\nAlgorithmic Bias Safeguards for Workforce: Overview.\n\nJan. 2022.\n\nLink",
    "# Accessibility and Standards\n\n- Section 508.gov.\n\nIT Accessibility Laws and Policies.\n\nAccess Board.\n\nLink\n\n- ISO Technical Management Board.\n\nISO/IEC Guide 71:2014.\n\nGuide for addressing accessibility in standards.\n\nInternational Standards Organization.\n\n2021.\n\nLink\n\n- World Wide Web Consortium.\n\nWeb Content Accessibility Guidelines (WCAG) 2.0.\n\nDec. 11, 2008.\n\nLink\n\n- Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, and Andrew Bert.\n\nNIST Special Publication 1270: Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.\n\nThe National Institute of Standards and Technology.\n\nMarch, 2022.\n\nLink\n\n- See, e.g., the 2014 Federal Trade Commission report “Data Brokers A Call for Transparency and Accountability”.\n\nLink\n\n- See, e.g., Nir Kshetri.\n\nSchool surveillance of students via laptops may do more harm than good.\n\nThe Conversation.\n\nJan. 21, 2022.\n\nLink",
    "# Reports and Case Studies\n\n- Matt Scherer.\n\nWarning: Bossware May be Hazardous to Your Health.\n\nCenter for Democracy & Technology Report.\n\nLink\n\n- Human Impact Partners and WWRC.\n\nThe Public Health Crisis Hidden in Amazon Warehouses.\n\nHIP and WWRC report.\n\nJan. 2021.\n\nLink\n\n- Drew Harwell.\n\nContract lawyers face a growing invasion of surveillance programs that monitor their work.\n\nThe Washington Post.\n\nNov. 11, 2021.\n\nLink\n\n- Virginia Doellgast and Sean O'Brady.\n\nMaking Call Center Jobs Better: The Relationship between Management Practices and Worker Stress.\n\nA Report for the CWA.\n\nJune 2020.\n\nLink\n\n- See, e.g., Federal Trade Commission.\n\nData Brokers: A Call for Transparency and Accountability.\n\nMay 2014.\n\nLink\n\n- Cathy O’Neil.\n\nWeapons of Math Destruction.\n\nPenguin Books.\n\n2017.\n\nLink",
    "# Surveillance and Privacy\n\n- Rachel Levinson-Waldman, Harsha Pandurnga, and Faiza Patel.\n\nSocial Media Surveillance by the U.S. Government.\n\nBrennan Center for Justice.\n\nJan. 7, 2022.\n\nLink\n\n- Shoshana Zuboff.\n\nThe Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power.\n\nPublic Affairs.\n\n2019.\n\n- Angela Chen.\n\nWhy the Future of Life Insurance May Depend on Your Online Presence.\n\nThe Verge.\n\nFeb. 7, 2019.\n\nLink\n\n- See, e.g., Scott Ikeda.\n\nMajor Data Broker Exposes 235 Million Social Media Profiles in Data Leak: Info Appears to Have Been Scraped Without Permission.\n\nCPO Magazine.\n\nAug. 28, 2020.\n\nLink",
    "# Facial Recognition and Surveillance\n\n- Lily Hay Newman.\n\n1.2 Billion Records Found Exposed Online in a Single Server.\n\nWIRED, Nov. 22, 2019.\n\nLink\n\n- Lola Fadulu.\n\nFacial Recognition Technology in Public Housing Prompts Backlash.\n\nNew York Times.\n\nSept. 24, 2019.\n\nLink\n\n- Jo Constantz.\n\n‘They Were Spying On Us’: Amazon, Walmart, Use Surveillance Technology to Bust Unions.\n\nNewsweek.\n\nDec. 13, 2021.\n\nLink\n\n- See, e.g., enforcement actions by the FTC against the photo storage app Everalbum (Link)\n\n- Enforcement against Weight Watchers and their subsidiary Kurbo (Link)\n\n- See, e.g., HIPAA, Pub.\n\nL 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub.\n\nL. 95-109 (1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C.\n\n§ 1232g), Children's Online Privacy Protection Act of 1998, 15 U.S.C.\n\n6501–6505, and Confidential Information Protection and Statistical Efficiency Act (CIPSEA) (116 Stat.\n\n2899)",
    "# Articles and Publications\n\n- Marshall Allen.\n\nYou Snooze, You Lose: Insurers Make The Old Adage Literally True.\n\nProPublica.\n\nNov. 21, 2018.\n\nLink\n\n- Charles Duhigg.\n\nHow Companies Learn Your Secrets.\n\nThe New York Times.\n\nFeb. 16, 2012.\n\nLink\n\n- Jack Gillum and Jeff Kao.\n\nAggression Detectors: The Unproven, Invasive Surveillance Technology Schools are Using to Monitor Students.\n\nProPublica.\n\nJun.\n\n25, 2019.\n\nLink\n\n- Drew Harwell.\n\nCheating-detection companies made millions during the pandemic.\n\nNow students are fighting back.\n\nWashington Post.\n\nNov. 12, 2020.\n\nLink\n\n- See, e.g., Heather Morrison.\n\nVirtual Testing Puts Disabled Students at a Disadvantage.",
    "# Government Technology\n\nMay 24, 2022.\n\nLydia X.\n\nZ.\n\nBrown, Ridhi Shetty, Matt Scherer, and Andrew Crawford.\n\nAbleism And Disability Discrimination In New Surveillance Technologies: How new surveillance technologies in education, policing, health care, and the workplace disproportionately harm disabled people.\n\nCenter for Democracy and Technology Report.\n\nMay 24, 2022.\n\nSee, e.g., Sam Sabin.\n\nDigital surveillance in a post-Roe world.\n\nPolitico.\n\nMay 5, 2022.\n\nFederal Trade Commission.\n\nFTC Sues Kochava for Selling Data that Tracks People at Reproductive Health Clinics, Places of Worship, and Other Sensitive Locations.\n\nAug. 29, 2022.\n\nTodd Feathers.\n\nThis Private Equity Firm Is Amassing Companies That Collect Data on America’s Children.\n\nThe Markup.\n\nJan. 11, 2022.\n\nReed Albergotti.\n\nEvery employee who leaves Apple becomes an ‘associate’: In job databases used by employers to verify resume information, every former Apple employee’s title gets erased and replaced with a generic title.\n\nThe Washington Post.\n\nFeb. 10, 2022.\n\nNational Institute of Standards and Technology.\n\nPrivacy Framework Perspectives and Success Stories.\n\nAccessed May 2, 2022.\n\nACLU of New York.\n\nWhat You Need to Know About New York’s Temporary Ban on Facial Recognition in Schools.\n\nAccessed May 2, 2022.\n\nNew York State Assembly.\n\nAmendment to Education Law.\n\nEnacted Dec. 22, 2020.\n\nU.S Department of Labor.\n\nLabor-Management Reporting and Disclosure Act of 1959, As Amended.\n\n(Section 203).\n\nSee also: U.S Department of Labor.\n\nForm LM-10.\n\nOLMS Fact Sheet, Accessed May 2, 2022.\n\nSee, e.g., Apple.\n\nProtecting the User’s Privacy.\n\nAccessed May 2, 2022.\n\nGoogle Developers.\n\nDesign for Safety: Android is secure by default and private by design.\n\nAccessed May 3, 2022.\n\nKaren Hao.\n\nThe coming war on the hidden algorithms that trap people in poverty.\n\nMIT Tech Review.\n\nDec. 4, 2020.\n\nAnjana Samant, Aaron Horowitz, Kath Xu, and Sophie Beiers.\n\nFamily Surveillance by Algorithm.\n\nACLU.\n\nAccessed May 2, 2022.\n\nMick Dumke and Frank Main.\n\nA look inside the watch list Chicago police fought to keep secret.\n\nThe Chicago Sun Times.\n\nMay 18, 2017.\n\nJay Stanley.\n\nPitfalls of Artificial Intelligence Decisionmaking Highlighted In Idaho ACLU Case.\n\nACLU.\n\nJun.\n\n2, 2017.\n\nIllinois General Assembly.\n\nBiometric Information Privacy Act.\n\nEffective Oct. 3, 2008.\n\nPartnership on AI.\n\nABOUT ML Reference Document.\n\nAccessed May 2, 2022.\n\nSee, e.g., the model cards framework: Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.\n\nModel Cards for Model Reporting.\n\nIn Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19).\n\nAssociation for Computing Machinery, New York, NY, USA, 220–229.\n\nSarah Ammermann.\n\nAdverse Action Notice Requirements Under the ECOA and the FCRA.\n\nConsumer Compliance Outlook.\n\nSecond Quarter 2013.\n\nFederal Trade Commission.\n\nUsing Consumer Reports for Credit Decisions: What to Know About Adverse Action and Risk-Based Pricing Notices.\n\nAccessed May 2, 2022.\n\nConsumer Financial Protection Bureau.\n\nCFPB Acts to Protect the Public from Black-Box Credit Models Using Complex Algorithms.\n\nMay 26, 2022.\n\nAnthony Zaller.\n\nCalifornia Passes Law Regulating Quotas In Warehouses – What Employers Need to Know About AB 701.\n\nZaller Law Group California Employment Law Report.\n\nSept. 24, 2021.\n\nNational Institute of Standards and Technology.\n\nAI Fundamental Research – Explainability.\n\nAccessed Jun.\n\n4, 2022.\n\nDARPA.\n\nExplainable Artificial Intelligence (XAI).\n\nAccessed July 20, 2022.\n\nNational Science Foundation.\n\nNSF Program on Fairness in Artificial Intelligence in Collaboration with Amazon (FAI).\n\nAccessed July 20, 2022.\n\nKyle Wiggers.\n\nAutomatic signature verification software threatens to disenfranchise U.S. voters.\n\nVentureBeat.\n\nOct. 25, 2020.\n\nBallotpedia.\n\nCure period for absentee and mail-in ballots.\n\nArticle retrieved Apr 18, 2022.\n\nLarry Buchanan and Alicia Parlapiano.\n\nTwo of these Mail Ballot Signatures are by the Same Person.\n\nWhich Ones?\n\nNew York Times.\n\nOct. 7, 2020.\n\nRachel Orey and Owen Bacskai.\n\nThe Low Down on Ballot Curing.\n\nNov. 04, 2020.\n\nAndrew Kenney.\n\n'I'm shocked that they need to have a smartphone': System for unemployment benefits exposes digital divide.\n\nUSA Today.\n\nMay 2, 2021.\n\nAllie Gross.\n\nUIA lawsuit shows how the state criminalizes the unemployed.\n\nDetroit Metro-Times.\n\nSep. 18, 2015.\n\nMaia Szalavitz.\n\nThe Pain Was Unbearable.\n\nSo Why Did Doctors Turn Her Away?\n\nWired.\n\nAug. 11, 2021.\n\nSpencer Soper.\n\nFired by Bot at Amazon: \"It's You Against the Machine\".\n\nBloomberg, Jun.\n\n28, 2021.\n\nDefinitions of ‘equity’ and ‘underserved communities’ can be found in the Definitions section of this document as well as in Executive Order on Advancing Racial Equity and Support for Underserved Communities Through the Federal Government:  \n\n\nHealthCare.gov.\n\nNavigator - HealthCare.gov Glossary.\n\nAccessed May 2, 2022.",
    "Navigator - HealthCare.gov Glossary.\n\nAccessed May 2, 2022.\n\nCenters for Medicare & Medicaid Services.\n\nBiden-Harris Administration Quadruples the Number of Health Care Navigators Ahead of HealthCare.gov Open Enrollment Period.\n\nAug. 27, 2021.\n\nSee, e.g., McKinsey & Company.\n\nThe State of Customer Care in 2022.\n\nJuly 8, 2022.\n\nSara Angeles.\n\nCustomer Service Solutions for Small Businesses.\n\nBusiness News Daily.\n\nJun.\n\n29, 2022.\n\nMike Hughes.\n\nAre We Getting The Best Out Of Our Bots?\n\nCo-Intelligence Between Robots & Humans.\n\nForbes.\n\nJul.\n\n14, 2022.\n\nRachel Orey and Owen Bacskai.\n\nThe Low Down on Ballot Curing.\n\nNov. 04, 2020.\n\nZahavah Levine and Thea Raymond-Seidel.\n\nMail Voting Litigation in 2020, Part IV: Verifying Mail Ballots.\n\nOct. 29, 2020.\n\nNational Conference of State Legislatures.\n\nTable 15: States With Signature Cure Processes.\n\nJan. 18, 2022.\n\nWhite House Office of Science and Technology Policy.\n\nJoin the Effort to Create A Bill of Rights for an Automated Society.\n\nNov. 10, 2021.\n\nWhite House Office of Science and Technology Policy.\n\nNotice of Request for Information (RFI) on Public and Private Sector Uses of Biometric Technologies.\n\nIssued Oct. 8, 2021.\n\nNational Artificial Intelligence Initiative Office.\n\nPublic Input on Public and Private Sector Uses of Biometric Technologies.\n\nAccessed Apr.\n\n19, 2022.\n\nThomas D. Olszewski, Lisa M. Van Pay, Javier F. Ortiz, Sarah E. Swiersz, and Laurie A. Dacus.\n\nSynopsis of Responses to OSTP’s Request for Information on the Use and Governance of Biometric Technologies in the Public and Private Sectors.\n\nScience and Technology Policy Institute.\n\nMar.\n\n2022.",
    "## Authors and contributors\n\n- Matteo Adamoli, University of Bologna (Italy)\n- Janine Berg, International Labour Organization (Switzerland)\n- Nicolas Blanc, General Confederation of Executives, CFE-CGC (France)\n- Mark Graham, Oxford Internet Institute (United Kingdom)\n- Marek Havrda, Office of the Government of Czech Republic (Czech Republic)\n- Pamela Krzypkowska, Ministry of Digital Affairs (Poland)\n- Anita Macauda, University of Bologna (Italy)\n- Aditya Mohan, National Standards Authority of Ireland (Ireland)\n- Chiara Panciroli, University of Bologna (Italy)\n- Matthias Peissner, Fraunhofer IAO (Germany)\n- Saiph Savage, Northeastern University; Civic A.I.\n\nLab.\n\nand UNAM (Mexico)\n- B. Shadrach, CEMCA; Commonwealth of Learning (India)\n- Fernando Schapachnik, Fundación Sadosky and Universidad de Buenos Aires (Argentina)\n- Alexandre Shee, SAMA (Canada)\n- Lucía Velasco, School of Transnational Governance, EUI.\n\n(Spain)\n- Kyoko Yoshinaga, Graduate School of Media, and Governance of Keio University (Japan)",
    "## Executive Summary\n\nGenerative AI and the Future of Work remains notably absent from the global AI governance dialogue.\n\nGiven the transformative potential of this technology in the workplace, this oversight suggests a significant gap, especially considering the substantial implications this technology has for workers, economies and society at large.\n\nAs interest grows in the effects of Generative AI on occupations, debates centre around roles being replaced or enhanced by technology.\n\nYet there is an incognita, the \"Big Unknown\", an important number of workers whose future depends on decisions yet to be made.\n\nIn this brief, recent articles about the topic are surveyed with special attention to the \"Big Unknown\".\n\nIt is not a marginal number: nearly 9% of the workforce, or 281 million workers worldwide, are in this category.\n\nUnlike previous AI developments which focused on automating narrow tasks, Generative AI models possess the scope, versatility, and economic viability to impact jobs across multiple industries and at varying skill levels.\n\nTheir ability to produce human-like outputs in areas like language, content creation and customer interaction, combined with rapid advancement and low deployment costs, suggest potential near-term impacts that are much broader and more abrupt than prior waves of AI.\n\nGovernments, companies, and social partners should aim to minimize any potential negative effects from Generative AI technology in the world of work, as well as harness potential opportunities to support productivity growth and decent work.\n\nThis brief presents concrete policy recommendations at the global and local level.\n\nThese insights, are aimed to guide the discourse towards a balanced and fair integration of Generative AI in our professional landscape.\n\nTo navigate this uncertain landscape and ensure that the benefits of Generative AI are equitably distributed, we recommend 10 policy actions that could serve as a starting point for discussion and implementation.",
    "## The Missing Conversation on Generative AI and the Future of Work in Policy Agendas\n\nThe transformative potential of Generative AI for the future of work is a subject of immense relevance.\n\nUnlike previous AI developments which focused on automating narrow tasks, generative AI models possess the scope, versatility and economic viability to impact jobs across multiple industries and at varying skill levels.\n\nTheir ability to produce human-like outputs in areas like language, content creation and customer interaction, combined with rapid advancement and low deployment costs, suggest potential near-term impacts that are much broader and more abrupt than prior waves of AI.\n\nClear political direction and support will be essential for both organizations and workers to adapt to the rapid development and widespread use of this new generation of AI systems.\n\nYet, Generative AI and the Future of Work policy remains absent from mainstream AI policy discussions.\n\nIt has been a notable omission in recent high-level policy statements and summits, including the G20, G7 and the 2023 State of the European Union address.\n\nEarlier in 2023, the G7 Digital and Technology Ministers committed to future discussions on various aspects of Generative AI, including governance, intellectual property rights, transparency, and responsible utilization.\n\nThis initiative was further endorsed in the G7 Summit in May, leading to the launch of the \"Hiroshima AI Process\" to continue these dialogues.\n\nTo advance the G7 Hiroshima AI Process initiated by the G7 leaders, Japan distributed a survey to G7 countries in June 2023.\n\nThe purpose of this survey was to assess the current and forthcoming policy measures of G7 members in relation to the key benefits and challenges posed by Generative AI technologies.\n\nDespite the comprehensive focus on responsible use, governance, disinformation, cybersecurity, and even biosecurity in the context of Generative AI, there is a notable absence of discussions on the technology's impact on work1.\n\nThis critical aspect seems to be side-lined in global coordination efforts, leaving a gap in the understanding and preparation for how these advanced AI models could reshape employment, job roles, and workplace dynamics.\n\nThe lack of attention to the future of work could limit the effectiveness and scope of policy measures, potentially leading to unforeseen economic and social consequences.\n\nIn the \"Harnessing Artificial Intelligence (AI) Responsibly for Good and for All\" section of the G20 New Delhi Leaders' Declaration2, leaders recognize the potential of AI to foster prosperity in the global digital economy and address the importance of ethical, transparent, and responsible development and deployment.\n\nWhile the declaration addresses the promise of AI and emphasizes protecting rights, ensuring safety, and promoting international cooperation, it omits the challenges and opportunities that AI poses to the global labor market.\n\nBy not addressing the impact of Generative AI there is a gap in an essential component for planning for a sustainable and inclusive future.\n\nLastly, in her State of the European Union 2023 speech3 on Digital and AI, President Ursula von der Leyen discussed the complex landscape of Artificial Intelligence (AI), mentioning its potential to revolutionize healthcare, boost productivity, and tackle climate change while also posing existential risks.\n\nShe outlined a new global framework for AI governance based on three pillars: setting human-centric guardrails, establishing a unified governance system, and guiding responsible innovation.\n\nThe AI Act, she mentioned, is the world's first comprehensive pro-innovation AI law and serves as a blueprint for global AI regulation.\n\nShe also suggested\n\n1  en.pdf?expires=1695049912&id=id&accname=guest&checksum=6CDEBE1E58D1851219A1ED8CBD942D82\n2 \n3 \n\nthe creation of a global body similar to the IPCC for climate, but for AI, to guide policy making.\n\nThe speech concluded with an announcement of a new initiative to provide AI start-ups access to Europe's high-performance computers.\n\nNo mention was made of the labor market.\n\nAddressing the \"future of work\", —specifically, how Generative AI impacts jobs—is vital.\n\nIt equips all labor market stakeholders to navigate these shifts by implementing proactive policies for a smooth transition.\n\nMoreover, the remit of current discussions on ethical concerns and governance structures should be enlarged to include the future of work, given pressing concerns over job quality and income inequality.\n\nGiven the transformative potential of AI in the workplace, the absence of this topic from such a high-profile policy agenda suggests a potential gap in comprehensive planning for the digital future as we can see from the overview of national and regional initiatives to address risks related to Generative AI.",
    "## The Third Path: Beyond Automation and Augmentation\n\nEven in its infancy, Generative AI offers myriad business applications, from customer service, coding and design to marketing and legal analysis.\n\nThis technological leap promises to reshape tasks demanding expertise and creativity, potentially hastening the adoption timeline.\n\nAs we enter a new phase of automation and artificial intelligence, most of the attention has been on how these technologies will replace manual and repetitive jobs.\n\nHowever, Generative AI —capable of creating content, solving complex problems, and even mimicking human-like text— brings a different set of challenges and opportunities that extend far beyond traditional notions of automation.\n\nThese technologies have the power to redefine not just low-skilled jobs, but also high-skilled professions that have historically been considered \"automation-resistant.\"\n\nThe implications are profound, affecting economic structures, workforce development, and social equality.\n\nThis policy brief aims to shed light on this underexplored facet of AI, offering actionable insights for stakeholders to proactively address the transformative impact of Generative AI on the future of work.\n\nGenerally, jobs tend to fall into one of these two categories: automation, where technology takes over tasks, or augmentation, where technology assists humans in performing tasks more efficiently.\n\nBuilding upon the categorization of jobs as either exposed to automation or augmentation, there is an emerging third category that does not neatly fit into either of these definitions.\n\nThese are a significant number of jobs where it's still unclear whether they will be automated, augmented, or undergo some other form of transformation.\n\nThis third category represents uncharted territory, which is both a challenge and an opportunity for policymakers.\n\nIn addition to addressing the challenges emanating from automation and augmentation, the uncertainty surrounding some occupations represents a critical area that needs more exploration and understanding.\n\nInstead of reacting to changes after they happen, policy can proactively shape how automation and AI integrate into the workforce.\n\nBy focusing on this unknown area of jobs that could be either automated or augmented, policymakers have the chance to guide the development and implementation of technology in a way that maximizes benefits for both workers, the economy and the whole society.\n\nThis proactive approach could set a new standard for how we manage technological change, ensuring that it serves the broader good.",
    "## Generative AI and Jobs: Some findings\n\nA decade ago, Oxford AI researchers estimated that 47% of jobs were vulnerable to automation.\n\nIn an updated study4, they argue that while Generative AI extends automation's reach, it also simplifies tasks for less-skilled workers.\n\nKey findings include remote jobs face higher automation risks, while in-person communication becomes more valuable; firms will often retain human oversight due to AI errors; high-stakes scenarios will see limited Generative AI use; and although creative roles are less automatable, enhanced content creation by AI may heighten competition and depress wages for creative professionals.\n\nAccording to further research published in 2023, Generative AI is expected to have a significant impact on the future of work, but it is not likely to cause widespread job displacement.\n\nMcKinsey5 suggests that by 2030, activities accounting for up to 30% of hours currently worked across the US economy could be automated, with Generative AI accelerating this trend.\n\nHowever, the report also states that Generative AI is more likely to enhance the way STEM, creative, and business and legal professionals work rather than eliminating a significant number of jobs outright.\n\nAnother BCG survey6 reveals that the percentage of respondents who say their company uses AI has jumped from 22% in 2018 to 50% in 2023.\n\nNearly half of the respondents (46%) say they have experimented with Generative AI, and 27% say they use it regularly.\n\nHowever, only 14% of frontline employees have received training to address how AI will change their jobs, while 86% of employees say they'll need it.\n\nIn a study7 with Boston Consulting Group involving 758 consultants, researchers assessed the impact of Large Language Models (LLMs) like GPT-4 on complex, knowledge-intensive tasks.\n\nThe study identified a \"jagged technological frontier,\" where AI excels in some tasks but struggles in others that seem similarly complex.\n\nWhen using AI, consultants were significantly more productive, completing tasks 25.1% faster and achieving over 40% higher quality compared to a control group.\n\nBoth low- and high-performing consultants benefited from AI augmentation, improving their scores by 43% and 17%, respectively.\n\nHowever, for tasks beyond AI's current capabilities, consultants using AI were 19% less likely to produce correct solutions.\n\nTwo distinct human-AI integration patterns emerged: \"Centaurs,\" who divided tasks between themselves and the AI, and \"Cyborgs,\" who fully integrated their workflow with AI technology.\n\nThe pressing question that looms large for individuals across the economy is, \"How will Generative AI impact my job?\"\n\nThis issue is not restricted to specific sectors or professions; rather, Generative AI has the capacity to either automate or enhance a wide range of jobs, from manual labor to specialized roles.\n\nPwC's recent Global Workforce Hopes and Fears Survey indicates that a significant portion of the global workforce is enthusiastic about acquiring new skills and engaging with AI.\n\nHowever, many organizations are not fostering environments where dissenting views and minor failures are accepted.\n\nGathering feedback from around 54,000 participants across 46 countries, the study highlights a crucial concern for top-level executives: one third of employees are worried.\n\n4  8006685edbae/download_file?file_format=application%2Fpdf&safe_filename=Frey_and_Osborne_2023_generative_AI_and.pdf&type_of_ work=Journal+article\n5 \n6 \n7 Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality by Fabrizio Dell'Acqua, Edward McFowland, Ethan R. Mollick, Hila Lifshitz-Assaf, Katherine Kellogg, Saran Rajendran, Lisa Krayer, François Candelon, Karim R. Lakhani :: SSRN\n\nAutomation generally refers to the use of technology, such as robotics, artificial intelligence (AI), and computer-controlled systems, to perform tasks that were previously done by humans, with the potential to increase efficiency and productivity.\n\nAugmentation refers to the process where AI complements human work by automating some tasks within an occupation, rather than fully automating the entire occupation.\n\nIn this context, AI helps improve the quality, intensity, and autonomy of jobs by working hand-in-hand with humans, creating a digitally capable and technologically progressive future for everyone.\n\nA recent analysis by the ILO of the potential effects of Generative AI on the world of work finds that most occupations will be “augmented” by the technology, rather than automated.\n\nAs shown in Figure 1, the risk of automation, mainly concerns the broad occupational category of clerical support workers, as 24% of their tasks have a high level of exposure to automation and 58% have a medium-level of exposure.",
    "Other occupational groups are much less exposed, with only 1 to 4% of tasks considered highly exposed, and medium-exposed tasks not exceeding 25%.\n\nThis means that certain functions they perform may be automated, but that the majority of tasks they perform are not automatable.\n\nAutomating some of their tasks can enable efficiency gains, allowing more time to be spent on other areas of work, thus “augmenting” their work.8 Proper policies should be put in place to ensure that the productivity gains are shared with workers.\n\n8 Occupations comprise a ‘bundle’ of tasks and only those occupations whose tasks, on average, demonstrate a high level of exposure (defined as task average above 0.75 in a 0-1 scale) are considered to be highly exposed to automation.\n\nOccupations with a high augmentation potential have low-occupational level mean scores and a high standard deviation of task scores.\n\nFor more details, see Gmyrek et al., 2023.\n\nFigure 1.\n\nTasks with medium and high GPT-exposure, by occupational category (ISCO 1-digit)\n\nNote: Medium level of exposure, defined as 0.5-0.75 (in 0-1 scale of potential exposure to AI); high level of exposure defined as greater than 0.75.\n\nSource: Gmyrek et al., 2023.\n\nTranslating these findings into global employment numbers reveals that it is high-income countries that are most exposed to the potential risk of automation, with 5.1% of employment potentially exposed.\n\nAnd because there is a high share of women in clerical occupations, it is women that will be most affected by potential technological redundancy (see Figure 2).\n\nThe analysis also reveals the enormous potential for augmentation across countries, but notes that the possibilities for take-up in lower income countries may be constrained by poor infrastructure (internet connection, electricity).\n\nAs such there is a risk that the existing digital divide will further widen the productivity divide.\n\nThis is echoed in a recent survey by the World Economic Forum9 which calculates a 2% net job loss (14 million jobs).\n\nMoreover, the list of jobs predicted to increase (highly professionalized and requiring higher education) and those predicted to decline (those usually available to the less educated).\n\nThis can be taken as a geographical call of attention, as the latter are usually more concentrated in lower income countries.\n\n9 \n\nFigure 2.\n\nGlobal estimates: jobs with augmentation and automation potential as share of total employment\n\nSource: Gmyrek et al., 2023.\n\nWhile the data on automation (second panel of figure 2) may seem alarming, especially when expressed in millions, it is important to note that potential exposure is not equivalent to job loss as some organizations will not adopt the technology.\n\nMoreover, new jobs are likely to be created as a result of the technology.\n\nIndeed, AI development currently relies on millions of human laborers who train AI systems through tagging and repetitive feedback.\n\nMuch of this work, however, is conducted through crowdsourcing platforms with low paid workers hired as independent contractors without the rights and benefits associated with an employment relationship.\n\nAlso, these jobs have been reported as being alienating because of repetitiveness, exposure to violence, verticalism and other work conditions.10 Ensuring that the new AI-related jobs are of good quality will benefit labour markets, also by potentially offering a source of positive employment opportunities for workers who may be displaced.\n\n10 \n\nAs the data show, the potential number of jobs likely to be transformed by AI is six times greater than those that may potentially be automated.\n\nWhether the transformation of tasks in these occupations is positive or negative depends on its design and integration at the workplace.\n\nWhile the data represent an upper bound for the potential of current Generative AI, when other forms of technological progress are considered, they more likely represent a lower bound of potential exposure.\n\nAlso, these numbers should be considered as a lower bound estimation, because they are estimations based on Generative AI alone.\n\nAs specialized tools start to emerge, new tasks might be exposed.\n\nAs an example, current day AI tools do not automatically handle spreadsheets or ERP software, but those integrations are very likely to appear in the near future.\n\nWhile the technology can allow some tasks to be automated, potentially leaving time for more engaging work, it can also be implemented in a way that worsens job quality.\n\nWhen algorithms are used to manage work, it can potentially restrain worker autonomy and increase work intensity, and perhaps more importantly, limit workers ability to provide feedback or discussion with management about the organization of their work.\n\nIn the worst cases, such algorithms can make decisions about dismissal.",
    "## The big unknown\n\nBeyond the jobs clearly earmarked for either automation or augmentation, there's a significant group that defies easy categorization.\n\nComprising nearly 9% of the workforce, or 281 million people, this \"big unknown\" mainly includes professionals and technicians.\n\nThese roles score high on the automation scale, yet they also involve a diverse range of tasks that could go either way—being enhanced by AI or replaced by it.\n\nGiven the potential for either radical transformation or significant job loss, this group urgently demands policy focus.",
    "### Figure 3.\n\nThe “Big Unknown”: occupations between augmentation and automation potential\n\nThe first essential step for policymakers is to identify the demographics of workers in the \"big unknown\" category, breaking down factors such as industry, income level, educational background, and geographic location.\n\nThis information will guide targeted policy interventions to support these specific worker groups as the adoption of AI technologies, including Generative AI, expands.\n\nGiven the uncertain nature of their roles, this worker group is at heightened risk of negative outcomes from a rapid transition to AI.\n\nProactive policy can ensure a more equitable distribution of the benefits of AI, thereby empowering these workers rather than displacing them.\n\nThe effectiveness of future productivity and societal gains, particularly those enabled by Generative AI technologies like GPT, will depend on the speed and precision with which workforce adaptation is managed.",
    "## Managing the transition\n\nNo, the effects of Generative AI on work will not lead to the “end of work”, but suggest, nonetheless, many important transformations.\n\nSuch transformations can be either positive or negative – with outcomes highly dependent on how the technology is integrated and used at work.\n\nAs such, policies to manage the transformation, developed through social dialogue, are needed.\n\nGovernments and social partners should aim to minimize any potential negative effects from Generative AI technology in the world of work, as well as harness potential opportunities to support productivity growth and decent work.\n\nWe propose the following recommendations at the global and local level:\n\n- Include the impact of Generative AI on the future of work as a key item on the AI agenda.\n\n- Support, through debt restructuring and alleviation, efforts by lower-income countries to invest in needed infrastructure and education that can lessen the digital divide.\n\n- Work with government, trade unions and employer representatives at the 2025-26 International Labour Conference to develop an international labour standard on «Decent work in the platform economy» to ensure that AI-related jobs are of good quality, as well as its eventual adoption into national legislation.\n\n- Invest in sectors that are under-funded and which have the potential to be a source of good quality jobs, such as in the care or green economy.\n\n- Design and institute social protection and skills development programs for those displaced by new AI technologies.\n\n- Prioritize redeployment and training over job loss.\n\nEngage with workers’ representatives and competent authorities to devise measures to avert or minimize terminations.\n\n- Involve workers in the design, implementation, and use of technology at the workplace by building and strengthening mechanisms of workplace consultation.\n\n- As a precondition for broad and effective co-determination and co-design of workers and their representatives, start and/or strengthen information, education and training about AI technologies, their potential and limits, possible AI applications at the workplace, critical/ethical design issues and challenges.\n\n- Make sure that all workers and citizens can consider the big AI-driven transformation as an opportunity for personal improvements instead of a threat to their professional and social status.\n\nExamples can include education/training offerings and incentives for continuous learning as well as fostering a general positive culture of change on all levels of society and economy.\n\n- Engage in workday reduction without diminishing salaries and other public policies to ensure that productivity gains from AI translate into wellbeing for the most.\n\n- Provide compensation mechanisms to account for the migration of human jobs done in low-income countries to automation of the same jobs by services provided on high income ones.\n\n- Encourage the private sector to voluntary work towards the minimum standards enshrined in the GPAI Fairwork in AI principles.\n\nIncentives for the adherence to these minimum standards exist through the Fairwork Pledge: a scheme through which organisations commit to only contracting with companies with high Fairwork scores.",
    "## Policy implications\n\nThe path of automation is far from predetermined, especially as Generative AI continues to evolve.\n\nJobs with high automation potential also show a wide range of variability in their component tasks, leaving them at a crossroads: they could be either dramatically enhanced through AI-assisted augmentation or face substantial job loss due to automation.\n\nThis uncertainty presents a golden opportunity for proactive policy intervention.\n\nBy acting now, the benefits of AI can be more evenly distributed across the labour market, uplifting workers in these uncertain roles rather than making them obsolete.\n\nThe future gains in productivity and well-being, powered by technologies like GPT, hinge on how swiftly and thoughtfully we can help the workforce adapt to these changes.\n\nTo navigate this uncertain landscape and ensure that the benefits of Generative AI are equitably distributed, here are 10 policy actions that could serve as a starting point for discussion and implementation:",
    "## Conclusion\n\nA joint effort that includes policymakers, the workforce, and employers can unlock the full potential for augmenting jobs through Generative AI.\n\nBy working together, they can ensure a seamless adaptation of the labour market to this emerging AI landscape.\n\nSuch a unified approach aims not just to reduce the risks tied to automation, but also to elevate human capabilities, setting a new standard for equitable growth and well-being in the era of Generative AI.\n\nImportantly, this collaborative path offers a way to build trust in the technology, transforming it from a source of apprehension into a tool for empowerment and progress.",
    "## Table of Contents\n\n1.\n\nCover Photo: Julienne Schaer, NYC & Co.\n2.\n\nLetter from the Mayor\n3.\n\nLetter from the CTO\n4.\n\nIntroduction\n5.\n\nFoundational Efforts\n6.\n\nMoving Forward with Action\n7.\n\nA Note on Scope\n8.\n\nEngagement for this Plan\n9.\n\nWhat is Artificial Intelligence (AI)?\n\n10.\n\nThe NYC AI Action Plan\n11.\n\nSummary of Initiatives and Actions\n12.\n\nDesign and Implement a Robust Governance Framework\n13.\n\nBuild External Relationships\n14.\n\nFoster Public Engagement",
    "## Letter from the Mayor\n\nMy Fellow New Yorkers:  \nFrom my earliest days as mayor, I vowed that this administration would approach technology in a smart way and use it to deliver better services and opportunities for New York City.\n\nWe’ve kept this promise by launching the nation’s largest free municipal broadband program, expanding wireless infrastructure across historically underserved communities, and creating a digital one-stop shop to provide seamless access to government.\n\nThis is what “Getting Stuff Done” for New Yorkers looks like.\n\nArtificial intelligence (AI) is one of the most impactful technological advances of our time.\n\nWhile AI has the potential to improve services and processes across our government, we must also be mindful of its associated risks.\n\nWith the release of our AI Action Plan, the first-of-its-kind for a major U.S. city, we are cementing our commitment to this emerging technology’s responsible use, and ensuring we are deploying the right tools in the right ways.\n\nThe New York City AI Action Plan — produced by the Office of Technology and Innovation — represents the knowledge, expertise, and contributions of 50 city employees from 18 agencies, as well as the insights of industry, academia, and civil society.\n\nIt illuminates the path forward, outlining seven new initiatives that emphasize approaches to policy, guidance, support, and resources that will help us responsibly harness the power of AI for good.\n\nAlmost two years into our administration, we remain energized by technology’s potential to drive safety, opportunity, and efficiency across this great city.\n\nWe know that New York’s best days are ahead — and it is with that sense of optimism about our city’s future that we present this report.\n\nEric Adams  \nMayor",
    "## Letter from the CTO\n\nMy Fellow New Yorkers:  \nAcross New York City government, artificial intelligence (AI) tools are inspiring paradigm shifts in how we serve New Yorkers.\n\nThese transformational technologies are critical to efforts such as preventing the next public health outbreak, empowering business owners and entrepreneurs, and optimizing the use of city resources.\n\nAnd we’ve only scratched the surface on AI’s massive potential impact on our city government, the nation’s largest municipal workforce, and our eight million residents.\n\nWith more city agencies expected to embrace these tools in the near future, the New York City AI Action Plan provides a vital roadmap for productive and responsible use that also affirms New York City’s status as a national and global leader in governing this emerging technology space.\n\nThis landmark plan astutely recognizes that it’s not enough to encourage agencies to deploy more AI-based solutions; we must also provide them with the framework and support to mitigate risks of misuse, inaccuracy, or bias and discrimination.\n\nI look forward to this plan providing a strong foundation for successful and responsible city applications of these tools that improve the lives of New Yorkers in the years ahead.\n\nMatthew C. Fraser  \nChief Technology Officer",
    "## Introduction\n\nArtificial intelligence (AI) is often described as a revolutionary technology that is rapidly changing the way we work, travel, conduct research, deliver healthcare, provide public services, and more.\n\nIn particular, the emergence of ground-breaking generative AI tools over the last year has simultaneously sparked tremendous excitement, profound concern, and intense speculation about their potential far-reaching impacts on humanity.\n\nBroadly defined, AI describes a wide variety of technologies that use data to make predictions, inferences, recommendations, rankings, or other decisions.\n\nWhile AI technologies have recently captured the public imagination by producing images and text on command, the reality is that they have existed for decades in diverse forms and uses.\n\nIndeed, these technologies have for many years impacted our lives and society, through now familiar and mundane tools that filter spam from our email, support our medical care, and optimize the use of energy in our homes and workplaces.\n\nAnd while the current moment has brought the promise and perils of AI into sharper focus, these technologies have long presented both wide-ranging opportunities and risks.\n\nToday, governments across the world are grappling with these realities, working to harness the power of AI tools for public benefit, while governing them carefully to protect their people and values.\n\nAs a global innovation leader, New York City must be at the forefront of these efforts.\n\nAI technologies offer a wide range of opportunities to make government run better for New Yorkers.\n\nPrudent use of AI can improve operational efficiency, social equity, environmental sustainability, and more.\n\nAt the same time, use of AI tools can pose a range of risks for individuals and communities – whether due to lack of appropriate governance, misuse, flawed design, or other factors.\n\nAbsent appropriate oversight and governance, some uses of AI could lead not only to benign inaccuracies or unintended results, but also data privacy or cybersecurity vulnerabilities, negative environmental impacts, or even serious bias, disparate impacts, and active harms.\n\nFurther, the complexity of many AI applications and the fact that their mechanics are not always visible or understandable pose unique transparency and accountability challenges, which are particularly pronounced for governments working in service to the public.\n\nIn recent years, New York City has grappled with the opportunities and challenges AI presents through pioneering efforts in the public reporting of algorithmic tools, foundational work to clarify opportunities and risks and assess the state of the local AI ecosystem, and diverse agency-level work to develop innovative AI solutions.\n\nWhile this work represents significant progress, there is now a need to press forward with a holistic set of actions that will help the city harness the power of AI to benefit New Yorkers while protecting them from its",
    "## Foundational Efforts\n\nIn recent years, the City of New York has been at the forefront of efforts to seize opportunities and mitigate risks presented by AI.\n\nIn 2020-21, the city worked with stakeholders across sectors to outline a novel framework for identifying algorithmic tools; developed policies, organizational structures, and processes to support public reporting of these tools; and published the city’s first-ever directory of tools.\n\nThis work has continued since, and in 2023, the city completed its third year of public reporting.\n\nIn 2021, the city embarked on a broad-based effort to define the meaning and wide-ranging implications of AI, understand the state of the local AI ecosystem, and surface key opportunities and challenges that AI presents for the city and its residents.\n\nAfter engaging over fifty stakeholders – from government, industry, academia, and civil society – the city released two publications, an “AI Primer” intended to provide local decision-makers with an accurate and shared understanding of the technology and the issues it presents, and a broader AI Strategy that identified key areas for future city work.\n\nIn addition to these more centrally-managed efforts, a number of city agencies have already begun to work with AI tools.\n\nAcross diverse domains and use cases, from prioritizing properties for safety inspections to predicting the next public health threat, agency teams have deployed AI and learned important lessons about how to organize, resource, and manage this work.",
    "## Moving Forward with Action\n\nIn January 2022, Mayor Adams consolidated the city’s technology and data teams into the Office of Technology & Innovation (OTI), led by the citywide Chief Technology Officer.\n\nThis consolidation gives OTI new insight into existing technology practices and needs across city government, as well as new authority to set citywide policies and procedures.\n\nIn October 2022, OTI released a Strategic Plan that outlined the strategic priorities for the agency’s work in the years ahead, as well as key initiatives toward realizing them.\n\nAmong these is the goal to foster innovation via a set of efforts to “ensure that city government is poised to take full advantage of new technologies.”\n\nThis new structure and framework provide a strong grounding for the city to advance its work on AI.\n\nAnd OTI is poised to build on prior efforts to take bold action.\n\nThis Action Plan presents a set of concrete commitments, focused specifically on city government’s AI use, that advance the city’s capacity to leverage these powerful tools to benefit the public, and ensure we are doing so responsibly, with appropriate measures in place to mitigate varied risks.\n\nAI technologies are dynamic and continually evolving.\n\nAt the same time, AI regulation and broader best practices for governance are still in their infancy, and stakeholders are grappling with a complex set of policy questions while often struggling to keep up with the rate of technological change.\n\nNevertheless, efforts to establish frameworks through which governments and the private sector can introduce responsible practices for using AI are emerging across many levels of government.\n\nAt the federal level, a number of agencies and offices have undertaken efforts to produce suggested policy or guidance for organizations to manage AI, most notably the Government Accountability Office (GAO) AI Accountability Framework, the National Institute for Standards and Technology (NIST) AI Risk Management Framework and the White House Office of Science and Technology Policy (OSTP) Blueprint for an AI Bill of Rights.\n\nOutside the U.S., the European Union is expected to pass the AI Act this year, which will establish Union-wide requirements for both private and public organizations related to the use and development of AI tools, based on tiered layers of risk.\n\nAs the technology develops further, and as governance measures and regulatory practices begin to emerge across the field, the city’s efforts will need to be iterative and ongoing.\n\nAccordingly, this Plan incorporates steps to maintain and update the actions outlined as the landscape evolves.",
    "## A Note on Scope\n\nAI has broad implications for the city and New Yorkers, including the varied impacts of private sector use of AI, as well as workforce and economic development concerns.\n\nWhile the efforts described in this Plan are focused on city government’s use and governance of AI, the city recognizes the importance and impact of these other issues, and work is underway at a range of agencies and offices to address them.\n\nLikewise, broader efforts to enhance the quality and usability of city government data – a key foundation for AI use – are not in scope for this document but are being addressed through parallel efforts.",
    "## Engagement for this Plan\n\nIn preparing this AI Action Plan, the OTI team not only built upon insights from the more than fifty stakeholder interviews conducted toward the development of the NYC AI Strategy, but also conducted a range of new interviews and workshops with stakeholders across sectors.\n\nThis included more than fifty city staff members representing a wide range of roles from eighteen agencies and offices, as well as external experts from ten organizations.\n\nA full list of organizations engaged specifically for the development of the Action Plan is below.\n\nOngoing input from stakeholders across sectors, and directly from New Yorkers is a key component of future efforts outlined in this Plan.",
    "### City Stakeholders\n\n- Administration of Children’s Services\n- Commission on Human Rights\n- Department of Buildings\n- Department of Citywide Administrative Services\n- Department of Consumer and Worker Protection\n- Department of Education\n- Department of Finance\n- Department of Health and Human Services\n- Department of Social Services\n- Department of Transportation\n- Fire Department of New York\n- Mayor’s Office of Contract Services\n- Mayor’s Office of Efficiency\n- Mayor’s Office of Equity\n- Mayor’s Office for People with Disabilities\n- Mayor’s Office of Risk and Compliance\n- New York City Police Department\n- Office of Technology and Innovation",
    "## What is Artificial Intelligence (AI)?\n\nBecause a wide variety of technologies and approaches may be considered AI, the city currently defines AI broadly as “an umbrella term without precise boundaries, that encompasses a range of technologies and techniques of varying sophistication that are used to, among other tasks, make predictions, inferences, recommendations, rankings, or other decisions with data, and that includes topics such as machine learning, deep learning, supervised learning, unsupervised learning, reinforcement learning, statistical inference, statistical regression, statistical classification, ranking, clustering, and expert systems.”\n\nThe term “AI” can include, for example, such diverse tools as:\n\n- **Machine Learning Algorithms**: Such as those that recommend viewing options on streaming platforms, that predict consumer demand for goods and services, or that model the risk of a disease outbreak in a community.\n\n- **Computer Vision Technologies**: Such as those that match identities based on fingerprint or iris scans, that detect objects in images to enable better search and accessibility, that enforce cash-free tolling, or that count pedestrians in a public space.\n\n- **Natural Language Processing Applications**: Such as those that auto-populate search results, provide predictive text in messaging apps, provide dynamic customer support with chatbots, or translate text into another language.\n\nThese examples show just how wide the array of AI technologies is.\n\nAs this Plan outlines below, additional work is needed to create a shared understanding about what AI is and the role it plays in city government.\n\nUsing a broad definition allows the city to maintain an appropriately wide lens to account for the range of both opportunities and risks that AI technologies present.\n\nIt also highlights the complexity of the topic, the quick-changing nature of these technological developments, and the efforts required for this work to be meaningful across applications.",
    "## The NYC AI Action Plan\n\nThe NYC AI Action Plan introduces seven new initiatives, each comprised of a set of phased actions the city will undertake toward harnessing the power of AI to deliver positive outcomes for New Yorkers, while carefully mitigating the risks these technologies present.\n\nActions in each initiative are organized into immediate- and medium-term time frames and include specific timelines for implementation.\n\nAcross these initiatives, there is an emphasis on providing more centralized policy, guidance, support, and resources for city agencies, to help ensure that appropriate governance measures are in place across city government, that agencies are better equipped to advance their AI efforts, and that the city is working efficiently, collaboratively, and with accountability to the public.\n\nCollaborative engagement is broadly a key theme across all seven initiatives.\n\nThis includes bringing in expertise from the rich local community of experts present in New York City – from academia, industry, civil society, community organizations, and organized labor, engaging with our counterparts at all levels of government, and speaking directly with New Yorkers to hear their ideas, understand their concerns, and account for their experience with AI across communities.",
    "## Summary of Initiatives and Actions\n\n- **Design and Implement a Robust Governance Framework**\n  - Establish a City AI Steering Committee\n  - Establish Guiding Principles and Definitions\n  - Provide Preliminary Use Guidance on Emerging Tools\n  - Create a Typology of AI Projects\n  - Expand Public AI Reporting\n  - Develop an AI Risk Assessment and Project Review Process\n  - Publish an Initial Set of AI Policies and Guidance Documents\n  - Pursue Ongoing Monitoring to Review AI Tools in Operation\n\n- **Build External Relationships**\n  - Create an Agenda for External Engagement\n  - Establish an External Advisory Network\n  - Use AI Advisory Network to Solicit Targeted Input on the City’s Efforts\n  - Explore Opportunities for Structured Partnerships with External Groups\n  - Support Information Sharing Across Governments\n\n- **Foster Public Engagement**\n  - Hold Introductory Public Listening Sessions\n  - Establish Plans to Integrate Public Input on Citywide Actions\n  - Explore Public Education Resources and Partnerships\n  - Create Guidance for Agencies on AI Public Engagement\n  - Design and Build a Public-Facing Website\n\n- **Build AI Knowledge and Skills in City Government**\n  - Explore and Pursue Opportunities to Foster Information Sharing Across Agencies and Teams\n  - Identify High-Priority Agency Skills Needs\n  - Assess the Landscape of Internal and External Resources to Support AI Knowledge-Building Efforts\n  - Launch Initial Knowledge-Building Efforts\n  - Explore Opportunities to Bring AI Talent into City Government for Limited-Term Projects\n  - Centrally Track and Share Emerging Tools, Use Cases, and Considerations\n  - Encourage Alignment on AI Skills and Duties\n\n- **Support AI Implementation**\n  - Identify Opportunities for In-House Tool Development\n  - Develop Example Project Lifecycles and Identify Bottlenecks\n  - Define OTI and Agency Roles in Support of Projects\n  - Scale, Reuse, and Repurpose Identified In-House Projects\n  - Provide Implementation Support, Tracking, and Risk Analysis\n  - Enable Streamlined and Responsible AI Acquisition\n  - Conduct an Agency Needs Assessment\n  - Establish a Directory of Procured AI Tools and Guidance on Appropriate Use\n  - Support AI Piloting\n  - Develop AI-Specific Procurement Standards, Terms, or Guidance\n  - Explore Creating New Contracts to Support Agency Needs\n  - Ensure Action Plan Measures are Maintained and Updated, and Report Annually on the City’s Progress\n  - Institute and Implement Processes for Refreshing Key Aspects of the City’s Plan on an Ongoing Basis\n  - Publish an Annual AI Progress Report",
    "### Objective\n\nEstablish a holistic, adaptable framework for AI governance that acknowledges the risks of AI, including bias and disparate impact, and which will help ensure the responsible use of AI tools consistent with values of reliability, transparency, accountability, fairness and non-discrimination, privacy, cybersecurity, and sustainability, among others defined through the implementation of this Action Plan.",
    "### Target Outcomes\n\n- Clear principles and definitions guide city government’s AI use and governance.\n\n- Agencies are engaged to help define procedures that both address their needs and ensure responsible use of AI.\n\n- Agencies are supported by centralized AI policy, guidance, and processes that address risks.\n\n- The public is well-informed about city AI use.",
    "#### Immediate Actions – projects start within 1 year\n\n- **Establish a City AI Steering Committee**: Establish an AI Steering Committee, composed of representatives of OTI divisions and other city agencies, to bring stakeholders from across city government together to provide input toward and oversight of AI activities.\n\nOTI will develop a charter for the Steering Committee to codify the scope, guiding principles and membership, and how the committee will operate and interact with city AI projects.\n\nComplete within 3 months.\n\n- **Establish Guiding Principles and Definitions**: Set goals and guiding principles for the responsible use of AI across agencies and define key terms for the city’s governance work.\n\nThese efforts may draw lessons from frameworks that have been developed at the national and international level.\n\nComplete within 3 months.\n\n- **Provide Preliminary Use Guidance on Emerging Tools**: In alignment with the guiding principles and goals described above, provide agencies with immediate-term guidance on the uses and risks of emerging forms of AI, focusing first on generative AI tools – in particular, large language models and other related technologies.\n\nComplete within 3 months.\n\n- **Create a Typology of AI Projects**: Using the city’s existing public reporting of algorithmic tools, as well as additional research, create a typology of AI projects to reflect the variety of technologies and uses that may fall under the umbrella term of “AI” for New York City.\n\nThe resulting typology can be used to inform governance efforts, clarify agency support needs, and enhance public engagement and understanding.\n\nComplete within 6 months.\n\n- **Expand Public AI Reporting**: Provide agencies with enhanced AI reporting guidelines, building on reporting currently conducted under Local Law 35 of 2022, to increase public awareness of AI initiatives citywide.\n\nThis may include broadening the scope of reporting, adding reporting requirements related to models and performance, and establishing guidelines for explainability.\n\nAdditionally, make resulting reports readily accessible to the public through the Open Data platform.\n\nInitiate within 6 months, then ongoing.\n\n- **Develop an AI Risk Assessment and Project Review Process**: Begin to develop an AI Risk Assessment and Project Review Process to enable the analysis of existing and proposed AI projects that address major considerations of AI risk, including reliability, fairness, bias, accountability, transparency, data privacy, cybersecurity, and sustainability.\n\nAfter creating a preliminary assessment model and process, OTI will update both on an ongoing basis as policies, guidance, and use cases develop.\n\nThese steps will be designed to pair with existing citywide privacy and cybersecurity policies and procedures.\n\nInitiate within 12 months, then ongoing.",
    "#### Medium-Term Actions – projects start within 1–2 years\n\n- **Publish an Initial Set of AI Policies and Guidance Documents**: In consultation with the Steering Committee, create an initial set of policies and guidance documents to support responsible AI use.\n\nPolicies and guidance may draw from existing standards frameworks, such as the National Institute for Standards and Technology (NIST) AI Risk Management Framework or the Government Accountability Office (GAO) AI Accountability Framework.\n\nOTI will prioritize and incrementally publish individual policies and guidance documents where practical.\n\nParticular steps related to procurement of AI tools by the city are outlined below in item 2.3.\n\nInitiate within 18 months, then ongoing.\n\n- **Pursue Ongoing Monitoring to Review AI Tools in Operation**: Review AI tools in accordance with the outlined Project Review Process, including at different stages of the project lifecycle, as relevant.\n\nThis may involve utilizing metrics to evaluate the impact of AI solutions and help to ensure that they are operating consistent with the city’s guiding principles for AI.\n\nReviews conducted after a solution is implemented will assess the tool’s effectiveness at fulfilling its stated goals, protecting against drift or shifting objectives, and facilitating project improvement.\n\nInitiate within 24 months, then ongoing.",
    "### Objective\n\nFoster feedback and consultation with stakeholders across sectors around both the opportunities and challenges posed by AI and the work of supporting responsible AI use across city government.\n\nSupport information sharing to ensure the city’s efforts reflect best practices in the field and to align policy across levels of government.",
    "### Target Outcomes\n\n- Experts from a variety of backgrounds provide timely perspective and guidance on city government’s AI efforts.\n\n- NYC develops productive relationships with partners across academia, industry, civil society, community organizations, and organized labor and leverages opportunities for collaboration.\n\n- City government works in alignment with efforts across levels of government, and productively shares information with peers.",
    "#### Immediate Actions – projects start within 1 year\n\n- **Create an Agenda for External Engagement**: In consultation with the Steering Committee, create an agenda for external engagement on the city’s efforts, focused on the coming year, which identifies priority topics and projects, and needed expertise.\n\nComplete within 3 months.\n\n- **Establish an External Advisory Network**: Establish an Advisory Network of individuals to support the city’s work on a consultative basis, to include stakeholders from, for example, academia, industry, civil society, community organizations, and organized labor.\n\nOTI will take steps to formalize relationships with an initial group to serve needs identified in the external engagement agenda.\n\nThe Advisory Network will be updated over time as the city’s needs and Advisor availabilities evolve.\n\nInitiate within 3 months, then ongoing.\n\n- **Use AI Advisory Network to Solicit Targeted Input on the City’s Efforts**: Use the AI Advisory Network on a consultative basis to obtain input on the city’s work, guided by priorities identified in the engagement agenda.\n\nInitiate within 12 months, then ongoing.\n\n- **Explore Opportunities for Structured Partnerships with External Groups**: Explore opportunities to create more structured partnerships with outside organizations that serve the city’s goals.\n\nThese might include, for example, research partnerships with local students or academic researchers, or public education and engagement partnerships with local community organizations.\n\nInitiate within 12 months, then ongoing.\n\n- **Support Information Sharing Across Governments**: Build relationships across State, Federal, and local governments, to support the ongoing exchange of information and best practices related to a wide range of AI subjects of interest to the city, and reduce the likelihood of policy and regulatory conflicts across jurisdictions.\n\nInitiate within 12 months, then ongoing.",
    "### Objective\n\nEducate and empower the public, communicating openly about the city’s progress and supporting a range of mechanisms for public input.\n\nIn order to best serve the public, foster trust, and support responsible use of AI, city government efforts must be grounded in the expertise, needs, and experience of New Yorkers.",
    "# Objective\n\nPublic education on AI is a critical foundation for meaningful and equitable engagement.\n\nNew Yorkers are well-informed about AI and the city’s efforts and accomplishments and are empowered to participate.\n\nCity stakeholders have an increased awareness of residents’ interests and concerns related to AI, and of the impacts of its use for diverse populations.",
    "**Immediate Actions – projects start within 1 year**\n\n- **Hold Introductory Public Listening Sessions**  \n  Hold a set of introductory public listening sessions to tap the ingenuity of New York’s diverse population to shape the city’s next steps on AI and understand residents’ key concerns and priorities.\n\nFollowing these sessions, brief city government stakeholders on findings, and issue a public summary.\n\n*Complete within 6 months*\n\n- **Establish Plans to Integrate Public Input on Citywide Actions**  \n  In consultation with the Steering Committee, identify and plan for opportunities to integrate public input on specific, citywide actions established in the Action Plan.\n\n*Initiate within 6 months, then ongoing*\n\n- **Explore Public Education Resources and Partnerships**  \n  Explore the existing landscape of resources and organizations working to educate the public about AI and related topics to identify useful tools and potential partnerships.\n\nThis may include, for example, exploration of educational materials or programs that may be made more readily available to New Yorkers, or partnerships that may be facilitated among relevant organizations.\n\n*Initiate within 9 months, then ongoing*\n\n- **Create Guidance for Agencies on AI Public Engagement**  \n  Craft initial guidance materials for agencies on integrating public engagement in their AI efforts, where relevant, to support responsible use, and better understand impacts for New York City’s diverse communities.\n\nThis work may occur in partnership with experts in this emerging area, as established via the Advisory Network, or via a more structured partnership engagement.\n\n*Complete within 12 months*\n\n- **Design and Build a Public-Facing Website**  \n  Design and build a public-facing website that will contain a range of resources for public use about AI broadly, as well as the city’s efforts in particular, including, for example, educational materials, city government reports, policies, and procedures.\n\n*Complete within 12 months*",
    "### Target Outcomes\n\n- City personnel engage with new AI knowledge resources and report that they are obtaining value from them.\n\n- Agencies regularly leverage centralized resources for swift AI information sharing.\n\n- Collaborations across industry, academia, and beyond lead to successful project engagements leveraging partners’ expertise to address key challenges the city faces.",
    "**Immediate Actions – projects start within 1 year**\n\n- **Explore and Pursue Opportunities to Foster Information Sharing Across Agencies and Teams**  \n  Enable agencies to share information about their use of and concerns related to AI.\n\nThis may include leveraging the Office of Data Analytics’ Analytics Exchange, Citywide CIO Forums, the Citywide Privacy Protection Committee, or other existing city convenings, or establishing new channels to support the exchange of information within the city, such as a dedicated AI community of practice or distribution list.\n\n*Complete within 6 months*\n\n- **Identify High-Priority Agency Skills Needs**  \n  Work with the Steering Committee to develop and implement a survey for agency executives to determine high-priority skills and knowledge needed across city government related to AI use and governance, to include skills for a diverse range of functions, both technical and not.\n\n*Complete within 12 months*\n\n- **Assess the Landscape of Internal and External Resources to Support AI Knowledge-Building Efforts**  \n  Engage with internal and external stakeholders to assess the state of existing resources, programs, and channels that can be leveraged to address identified city knowledge-building needs.\n\n*Complete within 12 months*\n\n- **Launch Initial Knowledge-Building Efforts**  \n  Work with the Steering Committee and other agencies involved in the city’s internal workforce development to plan the scope, structure, and priorities of new AI learning resources for city staff and begin roll-out of initial efforts.\n\nAnnounce and promote these resources across agencies.\n\n*Initiate within 12 months, then ongoing*\n\n- **Explore Opportunities to Bring AI Talent into City Government for Limited-Term Projects**  \n  In partnership with industry, academic, or other stakeholders, and leveraging analogous efforts already underway within the city, explore the feasibility of a partnership program that brings external teams in on a limited-term basis to directly support city AI projects.\n\n*Complete within 12 months*",
    "**Medium-Term Actions – projects start within 1–2 years**\n\n- **Centrally Track and Share Emerging Tools, Use Cases, and Considerations**  \n  Actively track the evolving technology, policy, and social landscape around AI from within OTI, and share information with agencies via established meetings or channels.\n\n*Ongoing*\n\n- **Encourage Alignment on AI Skills and Duties**  \n  In partnership with internal stakeholders, identify means to ensure that applicable city government job descriptions and civil services titles reflect the range of AI skills needed to support city efforts.\n\nFor example, consider development of a civil service title for “data scientist.”  \n  *Initiate within 18 months, then ongoing*",
    "### Objective\n\nSupport city agencies in their efforts to build and use AI, based on identified agency needs, including a process to provide comprehensive assistance throughout the lifecycle of AI projects.\n\nDevelop a knowledge base to equip agencies with resources and guidance, including in their reuse of existing tools, capabilities, and deployed technologies.",
    "### Target Outcomes\n\n- Common agency challenges are identified and tracked on an ongoing basis.\n\n- Agencies are effectively supported across their AI efforts, and benefit from centralized expertise and resources.\n\n- City government is empowered to build AI tools in-house, where applicable, and to scale those efforts to expand their impact as appropriate.",
    "**Immediate Actions – projects start within 1 year**\n\n- **Identify Opportunities for In-House Tool Development**  \n  Using the typology of AI projects identified through public reporting, conduct an analysis related to advantages and disadvantages of in-house development vs. procurement for different project types.\n\nAs part of the analysis, identify project types where in-house development is advantageous and suitable for expedient deployment.\n\nAdditionally, identify specific use cases that appear scalable within an agency or transferable across agencies.\n\n*Complete within 9 months*\n\n- **Develop Example Project Lifecycles and Identify Bottlenecks**  \n  Consult with OTI teams and agency partners to develop example project lifecycles that can be used to identify common bottlenecks and determine the form and degree of support that OTI should provide to agencies.\n\nBottlenecks may include, for example, agency or citywide procedures, infrastructure or software availability, costs and funding, sufficient and appropriate skills, sponsorship, or data quality.\n\n*Initiate within 12 months, then ongoing*\n\n- **Define OTI and Agency Roles in Support of Projects**  \n  Create a “Roles and Responsibilities” model for supported projects.\n\nFor example, OTI could assign “owners” to key project activities from high-level ideation to technology implementation.\n\nThis could also include support with problem definition, model building, budget development, defining a vendor proof of concept, risk mitigation, and compliance with city policies and guidance, among other areas.\n\n*Complete within 12 months*",
    "**Medium-Term Actions – projects start within 1–2 years**\n\n- **Scale, Reuse, and Repurpose Identified In-House Projects**  \n  For projects that may be identified as suitable for scaling or transferring across agencies, create scaling plans in partnership with agencies and other stakeholders.\n\nInclude considerations related to availability of skills, budget, and applicability.\n\n*Ongoing*\n\n- **Provide Implementation Support, Tracking, and Risk Analysis**  \n  Provide agencies with ongoing operational and infrastructure support, as well as project tracking and risk analysis, where needed, to promote successful implementation.\n\nAnalyze staff and budgetary needs, and document workflows for common project support requests to appropriately build centralized support capacities and resources.\n\n*Ongoing*",
    "### Objective\n\nDevelop AI-specific procurement standards or guidance to support agency-level contracting, ensuring procured products adhere to the city’s AI principles and goals and take steps to mitigate risks, including challenges with respect to transparency and explainability that can occur when procuring AI tools from third-party vendors.\n\nLeverage opportunities to streamline city government’s AI contracting to avoid redundancies and support cross-agency access to high-demand tools, as appropriate.",
    "**Immediate Actions – projects start within 1 year**\n\n- **Conduct an Agency Needs Assessment**  \n  Conduct an agency needs assessment to identify and document where agencies need support with contracting for their AI solutions, considering definitions and typologies of different types of AI, and where there are opportunities to streamline efforts.\n\n*Complete within 9 months*\n\n- **Establish a Directory of Procured AI Tools and Guidance on Appropriate Use**  \n  Establish and share across agencies a directory of AI solutions that city government has already procured to support visibility and access, where appropriate.\n\nInclude within this directory guidance on appropriate use of such tools.\n\n*Complete within 12 months*\n\n- **Support AI Piloting**  \n  Leverage existing OTI infrastructure to support the city’s experimentation and evaluation of AI tools through pilots and proof-of-concept (POC) or demonstration projects.\n\n*Initiate within 12 months, then ongoing*",
    "**Medium-Term Actions – projects start within 1–2 years**\n\n- **Develop AI-Specific Procurement Standards, Terms, or Guidance**  \n  In partnership with relevant agencies, and complementing existing contracting requirements related to privacy and cybersecurity, develop a set of contract standards, terms, or guidance for procurement of AI tools, to support agency-level contracting, and help ensure procured products align with city AI principles and goals.\n\nThese materials should be tailored to project types and risk profiles, and may include, for example, content related to performance, transparency and explainability, fairness and non-discrimination, privacy, cybersecurity, and environmental impact, among others.\n\n*Complete within 24 months*\n\n- **Explore Creating New Contracts to Support Agency Needs**  \n  If indicated, explore establishing new shared or citywide contracts for high-demand AI tools to streamline access for agencies, support competitive terms and pricing, and align with citywide MWBE goals.\n\n*Initiate within 24 months, then ongoing*",
    "### Target Outcomes\n\n- Action Plan efforts are periodically revised and remain up-to-date with the changing AI landscape and developing best practices.\n\n- The city is highly transparent, actively disseminating information regarding progress on the Action Plan.\n\n- The public is well-informed on the city’s status toward implementing the Action Plan.",
    "**Immediate Actions – projects start within 1 year**\n\n- **Institute and Implement Processes for Refreshing Key Aspects of the City’s Plan on an Ongoing Basis**  \n  Identify areas of AI work, as described in this Action Plan, where change management processes are needed to help ensure timeliness and currency of the city’s management of AI.\n\nAI is a highly dynamic technology and policy space, and an iterative approach will be required to account for the changing technological, organizational, and social landscape, as well as emerging best practices in the field.\n\nAccordingly, the city will need to maintain and update its efforts in several areas to ensure they remain relevant and up-to-date.\n\nThis includes updates to a range of governance measures, public engagement efforts, and city knowledge-building resources, among others.\n\n*Initiate within 9 months, then ongoing*\n\n- **Publish an Annual AI Progress Report**  \n  Track the city’s progress toward the implementation of the actions set out in this Plan to ensure accountability on an annual basis, via a public report.\n\nThis report will include an account of any updates made to actions outlined, as described above.\n\n*Initiate within 12 months, then ongoing*",
    "## Notes\n\nThe city’s algorithmic tools reporting outputs from 2020 to 2022 can be found at NYC Reports; the NYC AI Strategy and appended AI Primer can be found at NYC Government Publications; agency applications are described in each of these efforts.\n\nAlthough frequently used together, the terms “artificial intelligence” and “algorithmic tool” are not synonymous.\n\nMany, though not all algorithmic tools are derived from or are applications of AI.\n\nBased on particular contexts or regulations, not all applications of AI may be considered algorithmic tools.\n\nSee New York City Local Law 35 of 2022 for additional details on the definition of “algorithmic tool” used therein and associated reporting requirements: LegisStar.\n\nThe OTI Strategic Plan can be found at Strategic Plan 2022.\n\nFor more on the AI Accountability Framework from the Government Accountability Office (GAO), see: GAO.\n\nFor more on the National Institute for Standards and Technology (NIST) AI Risk Management Framework, see: NIST.\n\nFor more on the White House Office of Science and Technology Policy (OSTP) Blueprint for an AI Bill of Rights, see: White House AI Bill of Rights.\n\nThe NYC Department of Education (DOE) is incorporating AI-related topics in its Computer Science for All (CS4All) initiative to ensure all NYC public school students learn computer science, its Future Ready NYC program to build career-connected pathways for high-school students, and its broader digital literacy and digital citizenship efforts.\n\nDOE announced the launch of an AI Policy Lab in September 2023.\n\nFor more on these programs, see CS4AllNYC, NYC Schools Career Education.\n\nThe Office of Talent and Workforce Development's Tech Talent Pipeline also leads initiatives to build a diverse and inclusive local tech workforce.\n\nFor more on these efforts, see Tech Talent Pipeline.\n\nThe Department of Consumer and Worker Protection enforces Local Law 144 of 2021, which requires employers to provide notification and conduct bias audits of the tools in advance of their use in employment decisions in NYC.\n\nFor more on the Local Law, see DCWP Information Tools.\n\nThe City Commission on Human Rights is charged with the enforcement of the city’s Human Rights Law, Title 8 of the Administrative Code of the City of New York.\n\nFor more on the City Human Rights Law, see Human Rights Law.\n\nPursuant to the OTI Strategic Plan, the Office of Data Analytics recently created a citywide data governance program.\n\nA complete list of organizations engaged for the NYC AI Strategy is included on pp.\n\n112-113 of the document, available at NYC AI Strategy.\n\nFor example, the White House Office of Science and Technology Policy (OSTP) 2022 “Blueprint for an AI Bill of Rights,” available at White House, the National Institute of Standards and Technology (NIST) AI Risk Management Framework, available at NIST AI Risk Management Framework, or the Organisation for Economic Co-Operation and Development (OECD) AI Principles, available at OECD AI Principles.",
    "## Impact Statement\n\nGenerative artificial intelligence (AI) systems have become extremely popular and prevalent in a very short amount of time.\n\nThere is likely interest in using such systems to conduct City business.\n\nThe field is emergent and rapidly evolving, and the potential policy impacts and risks to the City are not fully understood.\n\nUse of generative AI systems with the City of Seattle, therefore, can have unanticipated and unmitigated impacts.\n\nThis Interim Policy is intended to minimize issues that may arise from the use of this technology while additional research and analysis is conducted.",
    "## Background and Definitions\n\nGenerative AI refers to a class of AI systems that are capable of generating content, such as text, images, video, or audio, based on a set of input data rather than simply analyzing or acting on existing data.\n\nPopular generative AI systems include GPT-3 and GPT-4/ChatGPT, Dall-E, and Lensa AI among many others.\n\nGenerative AI technology is rapidly being incorporated into common online tools, such as search engines.\n\nThese systems have the potential to support many City business functions and services, however their use also raises important questions, particularly around the sourcing of training data, ensuring proper attribution of generated content, and the handling of sensitive or public data.\n\nFurther research into this technology may uncover issues that require more restrictions on its use.\n\nEmployees are strongly advised to not invest heavily in using this technology or use it to support critical processes.",
    "## Interim Policy\n\n**Acquisition or use.\n\n** All software services, even if they are free or part of a pilot or proof-of-concept project, must be acquired via Seattle IT’s acquisition processes to ensure the software receives all necessary reviews and considerations (SMC 3.23.040).\n\nThis requirement applies to downloadable software, Software as a Service, web-based services, browser plug-ins, and smartphone apps.\n\nIf a City employee wishes to create an account with a generative AI service or otherwise use generative AI systems to perform functions related to City business, the employee must submit a Service Hub purchase request for software (indicate “z_Unlisted Software”), specify the AI service and describe how it will be used, and obtain departmental approval.\n\nUse of generative AI technology that is incorporated into existing services and products, such as internet search engines, does not require permission to use, however the following guidelines must be followed.\n\n**Intellectual property.\n\n** Content produced by generative AI systems may include copyrighted material.\n\nAI systems may be “trained” using data (text, images, etc.)\n\nthat has been sourced from the internet without regard for copyright or licensing terms.\n\nIt is extremely difficult to determine what content was used to train an AI system, and difficult to verify whether AI-generated content is wholly original or only a slight stylization of existing copyrighted material.\n\nNevertheless, City employees are required to perform due diligence to ensure that no copyrighted material is published by the City without proper attribution or without obtaining proper rights.\n\n**Attribution and accountability.\n\n** Audiences should know when content was produced by AI in whole or in part.\n\nIf a City employee uses AI-generated content in an official City capacity, the content should be clearly labeled as having been produced using generative AI tools.\n\nEmployees should also consider including information about how the material was reviewed and edited, and by whom.\n\nThis allows consumers of the content to understand its authorship and be able to evaluate the content accordingly.\n\n**Reduce bias and harm.\n\n** AI systems can reflect the cultural, economic, and social biases of the source materials used for training, and the algorithms used to parse and process that content can be a source of bias as well.\n\nEmployees should carefully review any content generated by AI to ensure that unintended or undesirable instances of bias, or even potentially offensive or harmful material, is changed or removed.\n\n**Data privacy.\n\n** City employees must not submit any sensitive, confidential, or regulated data, or any personally-identifiable data about members of the public, to a generative AI system.\n\n**Public records.\n\n** Employees should be aware of when the use of a generative AI system may result in the creation of a public record that must be retained under Washington state’s Public Records Act.\n\nYour department’s Privacy Champion and/or Public Disclosure Officer can be a resource to provide further guidance.",
    "## Next Steps and Actions\n\nThroughout Q2 2023, Seattle IT will work with internal and external stakeholder groups to conduct research into the policy implications for government use of generative AI.\n\nIn the meantime, this Advisory Memo outlines the preliminary considerations and guidelines that City employees should follow when working with generative AI services in a City context.\n\nEnforcement of this policy will be led by the Chief Technology Officer (CTO) and may be imposed by individual division directors.\n\nNon-compliance may result in disciplinary action, restriction of access, or more severe penalties up to and including termination of employment or vendor contract.",
    "# Policies in Parallel?\n\nA Comparative Study of Journalistic AI Policies in 52 Global News Organisations\n\nPre-Print – Not Peer-Reviewed\n\nKim Björn Becker¹†, Felix M. Simon²†, and Christopher Crum³‡\n\n1 Frankfurter Allgemeine Zeitung, Frankfurt, Germany & Trier University, Trier, Germany,   \n2 Oxford Internet Institute, University of Oxford, Oxford, United Kingdom,   \n3 Oxford Internet Institute, University of Oxford, Oxford, United Kingdom\n\nSeptember 6th, 2023",
    "## Abstract\n\nA growing number of news organisations have set up specific guidelines to govern how they use artificial intelligence (AI).\n\nThis article analyses a set of 52 guidelines from publishers in Belgium, Brazil, Canada, Finland, Germany, India, the Netherlands, Norway, Sweden, Switzerland, the United Kingdom, and the United States.\n\nLooking at both formal and thematic characteristics, we provide comparative insights into how news outlets address both expectations and concerns when it comes to using AI in the news.\n\nDrawing from neo-institutional theory and the concept of institutional isomorphism, we argue that the policies show signs of homogeneity, likely explained by isomorphic dynamics which arose as a response to the uncertainty created by the rise of generative AI after the release of ChatGPT in November 2022.\n\nOur study shows that publishers have already begun to converge in their guidelines on key points such as transparency and human supervision when dealing with AI-generated content.\n\nHowever, we argue that national and organisational idiosyncrasies continue to matter in shaping publishers’ practices, with both accounting for some of the variation seen in the data.\n\nWe conclude by pointing out blind spots around technological dependency, sustainable AI, and inequalities in current AI guidelines and providing directions for further research.\n\nKeywords: Artificial Intelligence, LLMs, News, Journalism, Isomorphism, AI Guidelines, AI Ethics, Comparative Analysis\n\n†Both authors contributed equally to the design, data collection, analysis, and writing of the study.\n\n‡Christopher Crum contributed to the coding and analysis of guidelines, designed, and carried out the syntactical analysis, and contributed to the final write-up.",
    "## Introduction\n\nArtificial intelligence (AI) is increasingly being adopted in the news industry.\n\nThe public release of ChatGPT, a so-called Large Language Model (LLM), in November 2022 by US start-up OpenAI has accelerated this trend, with news organisations looking to the technology with both high expectations and concerns.\n\nAI, and especially LLMs with the functionality to create realistic, multi-modal content ranging from text to visuals are seen as technologies with the potential to fundamentally change the way people interact with news, how news organisations produce and distribute content, as well the broader information environment and news organisations’ business models.\n\nAt the same time, concerns about abuse and safety of AI applications, copyright, accuracy of output, and privacy – to name just a few – abound.\n\nWhile it is too early to say if and how these expectations will ultimately be borne out by reality, many news organisations – some of which had already started to adopt AI in the past – have started to experiment with generative AI.\n\nAccording to a non-representative survey by industry body WAN-IFRA, many organisations experiment with LLMs for tasks such as summaries, illustrations, copy-editing or generally improving workflows, among other things.\n\nThese uses are in line with earlier uses of other forms of AI in news organisations which are ongoing.\n\nYet, many of these carry risks.\n\nRecommendation engines can discriminate against certain groups of users.\n\nTexts produced by LLMs are prone to factual errors and distortions while AI-generated images may be mistaken as real by audiences.\n\nIf newsrooms decide to publish AI output without taking precautions, they may be putting their journalistic credibility at risk.\n\nAt the same time, the rise of digital media more generally has also brought new actors into news production and distribution, including technology vendors and platform companies, with activities once viewed as inherently journalistic ‘increasingly distributed across a range of actors’, putting the professional ethics of journalism on shakier ground.\n\nThis is also true for the case of AI, with publishers concerned about e.g., the privacy of their data and the viability of their business models as news content is used to train AI models – often without publishers’ permission.\n\nIn response, news organisations have started to draw up AI guidelines as one way of countering some of these issues and regarding the use of AI and to ensure the ethical use of AI.\n\nFor example, press councils and associations in Germany, Belgium and Spain have proposed ground rules for AI in journalism.\n\nHowever, as the possible uses of AI may vary from publisher to publisher, general position papers can only provide some guidance and are unlikely to meet individual needs.\n\nVarious publishers have therefore decided to create more specific sets of rules, which in turn have attracted the interest of scholars.\n\nYet, despite some pioneering work studying the content of such guidelines, questions about these in-house guidelines remain.\n\nAmid calls to regulate AI more tightly, including in the news, how advanced are current efforts?\n\nWhere do efforts converge or diverge and what are the blind spots?\n\nGiven that AI’s shaping power acts broadly in the same way across contexts, one could expect that publishers’ reactions to these effects would show at least some uniformity, too.\n\nHowever, how and where AI guidelines converge or diverge from each other – both in terms of what they look like (formal characteristics) and what they say and do not say (thematic characteristics) remains an open question.\n\nOur paper addresses this gap.\n\nWe examine a total of 52 journalistic AI policies from publishers in ten countries in Western Europe, Scandinavia, and North America as well as Brazil and India.\n\nWe argue that AI guidelines show patterns of isomorphism, suggesting that publishers across national media systems and organisational categories respond to the rise of AI in broadly similar ways.\n\nHowever, there are indications that national idiosyncrasies and organisational categories continue to matter as important moderating factors.\n\nFunding models, in particular, seem to lead to different priorities, with commercial publishers often more detailed and with a stronger focus on allowed and prohibited uses as well as data protection.\n\nFinally, our study shows that AI guidelines still exhibit some blind spots – especially around questions of sustainable AI, technological dependencies an AI inequalities and human rights – leaving important aspects concerning the use of AI unmentioned, and thus under-regulated.\n\nWe first provide an overview of the literature, starting with a summary of the role of ethics in journalism and the development of industry self-regulation through editorial and social media guidelines.",
    "Second, we explain institutional isomorphism which we use as a framework to motivate our research questions and explain our findings.\n\nThird, we explain our sampling strategy, data collection and analysis before presenting and discussing findings.\n\nWe conclude with suggestions for further research.",
    "## Literature Review\n\nEthical issues can arise at every juncture of the journalistic process.\n\nWhile professional ethics is intimately linked to the quality of journalistic products, these concepts are not identical.\n\nSometimes, measures aimed at enhancing the quality of journalism are not necessarily based on ethical behaviour, and sometimes quality and journalistic ethics might even be at odds, as seen with the speed of reporting.\n\nSelf-Regulation of the News Through Guidelines\n\nBelow the formal level of laws and regulations, which generally define what journalism can and cannot do, the journalistic profession relies on self-regulation.\n\nThis is particularly true in liberal democracies, wherein press freedoms curb potential government efforts to influence reporting.\n\nSelf-regulation rests on two pillars.\n\nFirst, many news organisations regulate themselves by setting up non-governmental press councils, who often issue broad guidelines which in turn shape the work of participating organisations.\n\nA study of 55 press council codes of ethics across 45 countries found that these stress ‘core journalistic ethical principles’ such as fairness and accuracy in reporting, as well as autonomy of the press but are often very general in their remit.\n\nSecond, publishers often develop individual guidelines.\n\nAn editorial guideline constitutes a set of rules by a specific organisation that media professionals must or should observe.\n\nSuch internal guidelines often show great variety, encompassing various documents, from formal regulations to informal memos.\n\nLarge publishers’ own guidelines are often more specific than those of press councils and reflect the principal values and standards of the respective publisher.\n\nUsually, these are intended to further specify the rules for these organisations’ journalists and staff.\n\nNews Organisations’ Social and New Media Guidelines as Precursors\n\nIn recent years, publishers have formulated additional guidelines for specific topics, for example the use of social media.\n\nVarious studies have looked at these, e.g., through interviews or content analysis.\n\nMany of these focused on English-speaking countries due to shared ideological and economic structures and similar media systems.\n\nResearch on social media policies has largely been limited to mapping how specific media companies understand social media and what kind of behaviour they require of their journalists on these platforms.\n\nThe modest sample sizes and focus on the Anglosphere make broad conclusions difficult.\n\nHowever, the results showed ‘no homogeneity’ as well as ‘ambivalence’ which is notable given that social media are somewhat similar in their affordances across countries.\n\nIn their study of ethics guidelines for immersive journalism of eight publishers in English and Spanish-speaking countries, Sánchez Laws & Utne mainly found differences by organisation type, with ‘a stricter ethical regime […] in publicly funded broadcasters’ compared to privately owned media.\n\nFinally, contrasting professional journalism ethics with social media guidelines, Lee found that the latter ‘hardly reflect changing journalistic norms’.",
    "## Policies for the Use of AI in News Organisations\n\nEarly research on news organisations’ AI guidelines has also been largely descriptive.\n\nBecker examined a total of seven guidelines from Europe and North America, while Cools & Diakopoulos analysed 21 guidelines, 14 from Europe, five from North America and one each from Asia and South America.\n\nBoth studies looked at the formal level, examining how the documents were titled and what statements were made about their binding nature.\n\nIn addition, they addressed why media companies want to use AI, what applications should be allowed and prohibited, how to deal with human oversight of AI-produced material and transparency, principles of responsible AI, and possible dynamisation of the guidelines.\n\nBecker furthermore refers to internal and external collaboration, while Cools & Diakopoulos focus on accountability and responsibility, training, and the concept of cautious experimentation.\n\nGiven the small sample sizes, possible patterns are cursory.\n\nFor example, Cools & Diakopoulos point out that two media outlets owned by the same company tend to have similar policies.\n\nBecker noted links between the journalistic style of the organisation and the form chosen for the guidelines: ‘The news agencies present their guidelines briefly in a news-like style, while magazine[s] chose a more narrative form, and a British broadcaster known for its structural complexity chose the form of detailed guidelines’.\n\nIn addition, the goals for the use of AI in the newsroom, as stated in the guidelines, tended to vary between media organisations.\n\nThe AI policies of private sector news organisations seemed to associate AI with comparative business advantages, such as speed and breadth of coverage, while public service broadcasters focused more on public service implications.",
    "### Neo-Institutionalism and Institutional Isomorphism: How News Organisations deal with Uncertainty\n\nIt is worth noting that journalism ethics were ‘developed for a journalism of limited reach, whose public duties were assumed to stop at the border’ with the search for a global journalism ethic still ‘a work in progress’.\n\nWhile both national differences and organisational differences continue to matter in the news and play a role in shaping publishers’ and journalists’ practices, including around the adoption of new technologies, the news industry, or its organisations, are also increasingly shaped by factors that transcend nationally bounded media systems, for instance the growing influence of the technology sector, or they themselves transcend nationally bounded media systems because they operate in more than one national market.\n\nAI as a technology also acts with broadly similar effects across contexts, so it would be reasonable to assume that outlets across context react to the technology – and its concomitant challenges – in similar ways.\n\nTo explain possible similarities – a homogenisation – and patterns in AI guidelines on a larger scale, we work with a neo-institutional lens, in particular the concept of institutional isomorphism, the ‘tendency of organisations in a particular field to resemble one another,’ especially when faced with constraints.\n\nIsomorphism can be the result of one or a combination of three possible factors: coercive, mimetic, and normative.\n\nCoercive isomorphism ‘results from both formal and informal pressures exerted on organisations by other organisations on which they depend and by cultural expectations in the society in which organisations operate’.\n\nPressures can include laws and regulations or industry standards.\n\nMimetic isomorphism refers to an organisation’s response to uncertainty which often encourages organisations to respond to a stimulus by modelling themselves on similar or more successful organisations in their field.\n\nThis may be particularly true when ‘technologies are poorly understood […], when goals are ambiguous, or when the environment creates symbolic uncertainty’.\n\nFinally, normative isomorphism is the result of pressure from professional groups, i.e., it ‘stems primarily from professionalisation,’ what DiMaggio & Powell describe as ‘the collective struggle of members of a profession to define the conditions and methods of their work’.\n\nFactors leading to normative isomorphism include inter-organisational networks of exchange or the movement of labour between firms.\n\nIn the news industry, there is ample evidence that isomorphic processes in the past have occurred as result of all three of these factors.\n\nLooking at the pivot to online video news, Kalogeropoulos & Nielsen found that a mixture of audience demand (coercive), commercial considerations (mimetic), uncertainty about platform businesses’ interests and strategies, and uncertainty about the future direction of digital media (coercive, mimetic, normative) led the majority of organisations in their study to ‘converge on a similar short, platform and mobile-oriented approach to online news video’ with differences ‘more clearly related to organisational differences than to country differences’.\n\nChristin and later Petre also demonstrated forms of mimetic isomorphism in the use of audience metrics.\n\nFaced with uncertainty and constraints, publishers across organisation types and countries adopted audience metrics in broadly similar ways, even though some differences remain due to national and organisational idiosyncrasies.\n\nFinally, Simon finds that the adoption of platform companies’ AI and AI infrastructures follows an isomorphic pattern, with uncertainty about the direction and effects of the technology and the fear of being left behind acting as strong motivators for forms of mimetic isomorphism, recurring movement of talent and the highly networked nature between these news organisations contributing to these forms of normative and mimetic isomorphism, and AI as a large technological system itself acting as coercive force.\n\nConsidering the current uncertainty about what AI is, what it can and cannot do for and to the news, isomorphism can serve as a useful theoretical framework to investigate the adoption and content of news organisations’ AI guidelines.\n\nOur focus is therefore on examining the following research questions:",
    "### Sampling of Cases\n\nRecent studies of social media and AI guidelines have been limited by small sample sizes and convenience sampling approaches, relying mainly on guidelines available online.\n\nTo create a dataset with some meaningful variation that allows for a more general analysis, we took a more systematic approach.\n\nFirst, we identified a set of twelve countries falling into different media system categories where the existence of AI guidelines was already known or likely to be expected.\n\nThese countries can be grouped into four main geographical regions: Western Europe, Scandinavia, North America, and the Global South.\n\nWe then identified up to six leading companies or organisations for each country based on weekly use according to the Digital News Report in each of the following categories: magazine, media group, news agency, legacy newspaper, online news/digital-born, private broadcaster, professional organisation, and public broadcaster.\n\nBecause national media markets differ significantly from country to country, the sample is not entirely symmetrical.\n\nTo avoid missing out on important outlets not captured by the overall sampling, we strategically incorporated additional outlets based on recommendations from country experts.\n\nThe final sample included 207 media outlets which we contacted by email.\n\nOf the 207 organisations, we were ultimately able to include 52 AI policies in our study.\n\nSeven media companies indicated that their policies were still under development, ten organisations had policies in place but would not share them for academic purposes, and a further eleven companies responded that they had no AI policies.\n\nWe received no response from 127 contacted media organisations.\n\nIn the final sample, a total of 33 documents (63.46%) were found online, in eight cases the companies made their policies available to us (15.38%), and in a further eleven cases we were able to obtain the documents from other sources (21.15%).\n\n21 guidelines were available in English (40.38%), while the remaining 31 documents (59.62%) were translated to English using the neural machine translation service DeepL.\n\nWhere possible, we verified the accuracy of translations drawing from our own experience (with German, Dutch and French) or with the help of native speakers.",
    "**Table 1: Study sample, sorted by country and organisation type**\n\n- Deutsche Presse-Agentur, Germany, News agency, Online\n- Deutscher Journalisten-Verband, Germany, Professional organisation, Online\n- Bayerischer Rundfunk, Germany, Public broadcaster, Online\n- The Quint, India, Digital-born media, Obtained\n- De Volkskrant, Netherlands, Legacy newspaper, Online\n- DPG Media, Netherlands, Media group, Obtained\n- ANP, Netherlands, News agency, Obtained\n- NPO, Netherlands, Public broadcaster, Provided\n- TV2, Norway, Commercial broadcaster, Obtained\n- Dagens Naeringsliv, Norway, Legacy newspaper, Online\n- Schibsted, Norway, Media group, Online\n- NRK, Norway, Public broadcaster, Provided\n- Sveriges Television, Sweden, Commercial broadcaster, Obtained\n- Svenska Dagbladet, Sweden, Legacy newspaper, Online\n- Aftonbladet, Sweden, Legacy newspaper, Online\n- Dagens Nyheter, Sweden, Legacy Newspaper, Provided\n- Journalisten, Sweden, Legacy newspaper, Online\n- Bonnier, Sweden, Media group, Provided\n- TT Nyhetsbyran, Sweden, News agency, Obtained\n- Heidi News, Switzerland, Digital-born media, Online\n- Tamedia, Switzerland, Media group, Online\n- Ringier, Switzerland, Media group, Online\n- SRF, Switzerland, Public broadcaster, Online\n- ITN, United Kingdom, Commercial broadcaster, Provided\n- Financial Times, United Kingdom, Legacy newspaper, Online\n- Reuters, United Kingdom, News agency, Online\n- BBC, United Kingdom, Public broadcaster, Online\n- Business Insider, United States, Digital-born media, Online\n- USA Today, United States, Legacy newspaper, Online\n- The Atlantic, United States, Magazine, Online\n- Wired, United States, Magazine, Online\n- AP, United States, News agency, Provided\n- RTDNA, United States, Professional organisation, Online\n- National Public Radio, United States, Public broadcaster, Online",
    "### Qualitative and Quantitative Coding\n\nDrawing from the literature on AI in the news as well as previous research on news organisations’ general, social media, and AI guidelines as well as a first round of inductive coding using open, axial, and selective coding, we developed a codebook spanning 50 categories.\n\nThis was followed by a first round of deductive qualitative coding, where three coders coded all guidelines for 15 selected formal and thematic characteristics, with the unit of analysis being each individual sentence.\n\nFormal characteristics included the professional roles and specific AI engines mentioned.\n\nThematic aspects were goals of AI deployment, journalistic values mentioned, allowed and prohibited AI applications, possible pitfalls of journalistic AI use, applications where transparency is required, possible elements of algorithmic bias, aspects of source protection, professional roles and institutions involved in internal and external cooperation around AI, and views on possible dependencies on large AI platform companies.\n\nAdditionally, we analysed the documents through rigorous quantitative coding, focusing on 35 additional formal and thematic categories including allowed deployments of AI in the journalistic process, areas of human supervision, and methods of creating transparency when AI was used.\n\nThe unit of analysis was the whole document.\n\nEach document was coded into all the categories.\n\nFor the quantitative coding, the codebook was tested and refined over two initial rounds of test coding, whereby three coders independently coded a random selection of three guidelines in both rounds to resolve difficulties and misunderstandings.\n\nAfter each round, the results were compared and discussed, and the codebook refined.\n\nOnce the codebook was set, two coders independently recoded all 52 pieces of content.\n\nTo measure intercoder reliability, we used Krippendorf’s Alpha as the most rigorous and reliable measurement.\n\nThe reliability of each guideline’s coding was estimated based on 3640 (1820 x 2) independent decisions in the coding process.\n\nIn 68 cases the coders disagreed.\n\nKrippendorff suggests that it ‘is customary to require α ≥ .800.\n\nWhere tentative conclusions are still acceptable, α ≥ .667 is the lowest conceivable limit’.\n\nKrippendorff’s Alpha was .94 on average with a range of .73 and 1.\n\nRemaining differences in coding were discussed and resolved.",
    "### Text-to-Text Statistical Comparison by Cosine Distance\n\nIn a final step, we conducted a quantitative, statistical comparison of each AI guideline to each other AI guideline to account for any residual blind spots in the manual coding.\n\nThis analysis of syntactic similarity looked at the degree of resemblance in the arrangement and structure of five-word blocks and quantifies how closely two or more sentences align in terms of their words, grammatical patterns, and word order.\n\nThe text-to-text comparison involved a three-step process for each dyad in the dataset: (1) cleaning translated-to-English PDF documents, (2) vectorising text, and (3) comparing the vectors by Cosine Distance.\n\nSince possible inferences from data are sensitive to pre-processing choices, only those aspects of text data that were irrelevant to the research questions were excluded at the cleaning stage.\n\nThese included numbers, Unicode punctuation, and English-language stop words (‘the’, ‘which’, ‘on’, ‘at’ etc.).\n\nFinally, each text file was transformed to lower case and white space was stripped.\n\nFollowing Spirling and Alschner & Skougarevskiy, texts were vectorised as the set of distinct five-word (“5-gram”) sequences present in each text.\n\n5-gram sequences were employed because they tend to be long enough to capture meaningful passages of text while short enough to be reoccurring across multiple guidelines if textual borrowing occurs.\n\nCosine Distance, a measure of textual similarity well-suited to texts of varying lengths – a ubiquitous phenomenon in our dataset – was then computed.",
    "# Cosine Distance in Text Analysis\n\nThe balance between vectors ranges from 0, entirely the same set of n-grams, to 1, indicating no shared n-grams, and is specified by 1 minus the Euclidean dot product of vector A, where A is the term frequency vectorization of the first text, and vector B, where B is the term frequency vectorization of the second text, over the magnitude of each vector.\n\nThe intuition behind Cosine Distance is that the more similar the two vectors are, the smaller the angle between them will be, and thus the larger the Cosine Distance.\n\nResults were arrayed in a symmetrical matrix and visualized as a heatmap (see Fig.\n\n1 & 2), with darker colors indicating smaller distances, and thus, higher degrees of similarity.\n\nTo contextualize resulting values, a sample of general editorial guidelines (n=10) with an intentional overrepresentation of German-language guidelines (see Appendix, Table 3) to reflect the linguistic variation in the overall dataset, was also compared using Cosine Distances.\n\nSuch an approach allows this paper to draw broad-strokes conclusions about how similar journalistic AI Guidelines are to each other in comparison to how similar general editorial guidelines are to each other.\n\nThus, preliminary findings regarding the degree of convergence driven by the sharing of ideas in an environment of uncertainty surrounding AI can be assessed relative to convergence in established journalistic practices.",
    "# Findings\n\nWe first present findings from the syntactic, statistical comparison before presenting findings from the qualitative and quantitative coding of the formal and thematic characteristics of the AI guidelines.\n\nDue to the small sample size, we are constrained to descriptive statistics for the quantitative coding, as attempting more complex analyses or inferential procedures could lead to unreliable or misleading results.",
    "## Syntactic Similarity\n\nThe concept of syntactic similarity aids in assessing the likeness of sentences or text based on their syntactic composition.\n\nThe matrix results of comparison by Cosine Distance in the AI guidelines sample indicates, on average, a lower degree of similarity than do results in the benchmark general editorial guidelines sample.\n\nOne can see from the histogram in Figure 1 that the modal outcome of dyadic comparison in editorial guidelines is between 0.4 and 0.6 while in Figure 2 (AI guidelines) the modal outcome is above 0.6.\n\nVisually, this is reflected in the presence of more tiles toward the red end of the spectrum indicating lower degrees of distance or a higher degree of statistical similarity on Figure 1 than on Figure 2.",
    "### Figure 2: Dyadic Comparison of AI Guidelines by Cosine Distance\n\nDue to space limitations, not all outlets are labeled.\n\nOne can observe clustering at the level of shared language in both samples.\n\nFor example, the two most similar editorial guidelines in the benchmark sample, those of Sky News and the Australian Broadcasting Corporation, share English as a common language.\n\nLikewise, ITN and the Canadian Broadcasting Company (CBC), the two most similar AI guidelines in the study sample, also share English as a common language.\n\nIn broad strokes, the columnar dendrogram in both Figure 1 and Figure 2 indicates hierarchical grouping along language lines.\n\nIllustratively, Der Standard and Axel Springer, both German-language editorial guidelines, form a visually distinct block in Figure 1 (bottom-left quadrant), while Aftonbladet and TT Nyhetsbyrån, both Swedish-language AI guidelines, form an outlier block (bottom-left) in Figure 2.\n\nAt a broader level, the columnar dendrograms indicate a high-level in-group/out-group dynamic where many media organizations group together regardless of language boundaries while some media organizations chart their own path outside of the main group (labeled on both figures), leaving the average Cosine Distance within the group higher than in the out-group.\n\nThe outlier group in Figure 1 appears to be an outlier group of one, suggesting that there are fewer media organizations willing to chart their own path in editorial guidelines than there are in AI guidelines, though that may be a sample size driven outcome.\n\nAs one might expect, large, influential media organizations find themselves at the center of the in-groups, particularly in Figure 2, where the BBC and Bayerischer Rundfunk, two early and influential sets of AI guidelines, are at the center of the in-group block.",
    "**Publication of AI Guidelines Over Time**\n\n- Web.de/GMX/1&1 National Public Radio\n- The Quint The Globe and Mail\n- Tamedia Svenska Dagbladet Süddeutsche Zeitung Independent Television News\n- Der Spiegel Dagens Naeringsliv\n- CBC\n- Ringier Rheinische Post Journalisten Financial Times DPG Media\n- De Volkskrant USA Today T−Online Heidi News\n- Frankfurter Allgemeine Zeitung Deutscher Journalisten−Verband Deutsche Presse−Agentur\n- Business Insider\n- ANP\n- Aftonbladet The Atlantic\n- Ippen Helsingin Sanomat\n- BBC\n- Bayerischer Rundfunk\n- Schibsted Council for Mass Media\n\nAs Figure 3 shows, the documents vary considerably in length (see also Table 4 in the Appendix).\n\nThe shortest consists of only 87 words or 506 characters including spaces, the longest has 3972 words (25,192 characters).\n\nThe arithmetic mean of all AI guidelines included in our study is 780 words or 4899 characters.\n\nFor 33 guidelines (63.46%), the month of publication was available (see Figure 4).\n\nThe earliest document in our study is the Finnish Press Council’s guidelines, published in January 2020, while the latest documents were published in July 2023 by US public broadcaster National Public Radio and the German online news sites web.de/GMX/1&1.\n\nThe release of ChatGPT in November 2022 is likely to have boosted the development of AI guidelines in the media industry: A total of 33 guidelines with a known release date (87.88%) were published in 2023, but only one in 2021 (3.03% each) and three in 2020 (9.09%).\n\nThe findings from the analysis of title keywords (Table 2) reveal a diverse range of terms used to frame AI guidelines.\n\nThe most common term is ‘Guideline,’ accounting for 30.77% of the sample.\n\nOther prevalent terms include ‘How-to’-phrases (13.46%), ‘Policy’ (13.46%), and ‘Principles’ (7.69%).\n\nLess frequently used terms include ‘Framework’ (3.85%) and ‘Guide’ (1.92%).\n\nNotably, some documents do not have explicit title keywords (7.69%).\n\nRegarding the remit of the AI guidelines – stating which part of the news organization falls under the guidelines and must abide by them (Table 3) – the majority of documents are designed for the newsroom and journalists, constituting 69.23% of the sample.\n\nAbout 28.85% of the guidelines are intended for all departments within the news organization.\n\nThere is a single instance (1.92%) where the AI guidelines pertain to the business side only.\n\nNone of the documents specify other remits, and there are no instances where the remit is not specified.",
    "### Table 3: Remit\n\nThe examination of accountability mechanisms – the question of if AI guidelines will be enforced and compliance controlled in some way – (Table 4) indicates that only 7.69% of the documents explicitly mention such mechanisms, while 92.31% do not.\n\nMoreover, the guidelines mostly lack details on how enforcement will occur.\n\nRegarding the intended audience (which differs from remit as it specifies if the guidelines are meant for internal, external consumption, or both), Table 5 shows that 34.62% of the guidelines are directed towards internal stakeholders, 28.85% towards external audiences, and 36.54% are intended for both internal and external consumption.\n\n73.08% of guidelines furthermore mention one or several professional roles within the guidelines such as ‘editor-in-chief’ or ‘legal staff’ for whom the guidelines either apply in specific ways or who serve as points of contact for other people within the organization.",
    "### Table 5: AI Guidelines’ Intended Audiences\n\nFinally, on the dynamization of guidelines – the question of if guidelines will be updated – 63.46% stated that guidelines will be updated, 36.54% did not mention the same.\n\nHowever, the timing of such updates appears to be an area of varied opinions, with a notable preference for a less rigid and more adaptable approach.\n\nOut of the 33 guidelines mentioning dynamization, only 6.06% specified a particular interval for updates, with the majority (93.94%) leaving the same unspecified.",
    "### Journalistic Values and Conditions for Use of AI\n\nWe also analyzed guidelines’ reference to journalistic values drawing on Deuze’s classification of journalism’s ‘occupational ideology’ that can be recognized worldwide (2005).\n\nThese five traits include ‘Public Service’, ‘Objectivity’, ‘Autonomy’, ‘Immediacy’, and ‘Ethics’.\n\nOverall, 71.15% of the documents mention one or more of these journalistic values, while 28.85% do not mention any.\n\nFor allowed applications of AI in the journalistic process, 86.54% of the documents explicitly state where AI is permitted, while 13.46% do not provide such information.\n\nSimilarly, in relation to prohibited applications of AI in journalism, 67.31% of the guidelines specify where AI cannot be deployed, whereas 32.69% do not mention prohibited applications.\n\nIn Table 6, we provide additional breakdowns for allowed and prohibited AI applications along the chain of gatekeeping (adapted from Domingo et al., 2008).",
    "### Table 6: Allowed and Prohibited AI applications\n\n| Access & observation | Processing & filtering | Distribution |\n\nIn 69.23% of cases, guidelines also mentioned potential pitfalls of AI that staff should be aware of while 30.77% did not refer to the same.\n\nThe three most mentioned pitfalls in AI guidelines were hallucinations, wherein the AI fabricates facts, with one guideline stating that the organization takes a ‘source-critical approach to AI-generated material’ in response.\n\nSecond, bias of AI models – the tendency to perpetuate existing biases, such as those based on race, gender, ethnicity, and other factors, thereby reinforcing societal inequalities – was frequently mentioned.\n\nFinally, some guidelines expressed concerns about copyright and intellectual property, with AI-generated content violating licensing terms, plagiarizing existing material, and potentially infringing on intellectual property rights.\n\nLooking at the question of if guidelines reference specific AIs, AI engines or LLMs as examples of existing or possible AI deployment, we found an even split with 50.00% citing examples (most commonly ChatGPT, DALL-E, and Midjourney), while the remaining 50.00% did not.",
    "### Transparency and Human Supervision\n\nLooking at forms of how to deal with the deployment of AI in the journalistic process, 90.38% of organizations reference transparency – the fact that the use of AI has to be disclosed – in their AI guidelines.\n\nHowever, it should be noted that 82.98% do not explicitly specify how this transparency should be communicated, as is evident from Table 7.",
    "### Table 7: How to Communicate Transparency\n\nLooking at the qualitative results for all organizations who reference transparency, we see some further variation beyond the method of disclosure, namely on when and where this must happen.\n\nA few outlets are very prescriptive and detailed, with one outlet, for example, writing that ‘the editorial team shall indicate when a news item or part of the information offering has been produced wholly or partly on the basis of automated processes and, as far as possible, refer to the sources on which the news item is based,’ before listing examples.\n\nMany are less specific, with one recommending that AI use is labeled when AI ‘is used as more than a mere aid,’ but leaving this at the discretion of staff.\n\nOverall, the emphasis seems to be on the use of AI for texts, followed by images, with content recommendation only receiving limited attention.\n\nWhen it came to human supervision of AI, 84.62% of organizations stipulated supervision in some form.\n\nStill, Table 8 shows that news organizations have no unanimous way of handling human supervision.",
    "### Table 9: Elements of Responsible AI\n\nThe qualitative data demonstrates that many guidelines emphasize the protection of vulnerable groups and contributors’ privacy, urging against uploading or using confidential or sensitive information in AI engines.\n\nSource protection is a recurring theme, with guidelines ensuring that AI platforms are not given access to sensitive, source-protected, or unpublished information, with one organization for example writing that ‘we protect the privacy of sources and do not share sensitive material or personal data.’ \n\nSimilarly, for algorithmic bias, those guidelines that mention the same show e.g., concern for unfair discrimination stemming from biases in training data or caution against the use of AI that could lead to the ‘discriminat[ion] against any individual or group based on race, ethnicity, religion, gender, sexual orientation, or any other characteristic.’",
    "## Cooperation and Dependency\n\nInternal cooperation on AI between different departments within news organizations was present in 36.54% of cases.\n\nOrganizations mentioned that they had ‘several teams across departments studying AI’ or outlined how AI was a topic for ‘legal, tech, finance, HR’ and all other departments.\n\nIn terms of external cooperation with e.g., technology companies, consultants, researchers, or governments, 17.31% of the guidelines mention the same.\n\nFinally, on dependency, only 9.62% of the surveyed news organizations’ guidelines make any reference to possible dependencies on platform companies or other technology companies when it comes to the development and deployment of AI.\n\nOne news organization, for example, stresses the importance of ‘independence […] not only from political but also from technical influences’ while another argues that ‘greater independence from commercial big-tech providers is desirable.’ The majority, 90.38%, however, do not make any reference to dependency.",
    "# Discussion\n\nIt is time to return to the questions we set out at the beginning, namely to what extent international news organizations’ AI guidelines exhibit isomorphic tendencies (RQ0), i.e., to what degree can we find homogeneity between them.\n\nThe overall picture that emerges provides evidence for isomorphic tendencies leading to homogeneity, but within bounds.\n\nThe syntactical similarity analysis supports that media organizations borrow wordings from each other when developing AI guidelines (RQ0 & RQ1).\n\nTo the extent these borrowed wordings also encode meanings, the statistical comparison by Cosine Distances also supports the concept that ideas are shared across organizations.\n\nThe results further indicate the presence of at least a moderate degree of isomorphism in AI guidelines as illustrated by the presence of an in-group cluster of organizations.\n\nOn the matter of comparative convergence, the results offer some preliminary support to the idea that convergence has progressed further in editorial guidelines than in AI-use guidelines, which makes sense given the relative novelty of AI guidelines.\n\nEditorial guidelines simply have had a longer time to converge.\n\nWhile the findings in Figure 2 (AI Guidelines) clearly show the grouping dynamics and red tiles indicative of shared wordings and thus are consonant with on-going convergence, that convergence process has not progressed as far as it has in the editorial guidelines sample (Figure 1), which has more red-end-of-the-spectrum tiles.\n\nWhen it comes to AI guidelines on a syntactical level, the processes of isomorphism are nascent rather than fully developed.\n\nConceptual isomorphism (RQ0 & RQ1) may be a different story entirely, however.\n\nJournalists and publishers may not use the same words for the same concepts even if there is broad agreement on the why, how, and what of AI guidelines in journalistic settings.\n\nAnd indeed, the quantitative coding results show homogeneity for almost two-thirds of the variables (assuming a threshold of >60%).\n\nOn the formal level, documents vary in length but most fall within the approximate range of 200 to 1000 words.\n\nA majority governs the editorial realm of journalists and the newsroom (69.23%), mentions specific professional roles as relevant stakeholders (73.08%), states that the guidelines should be updated at some point (63.46%), but also lacks any reference to accountability mechanisms (92.31%).\n\nRegarding the thematic features, our coding again shows general patterns across publishers.\n\n71.15% refer to journalistic values, with ‘trust’ (46.15%) and ‘accuracy’ (44.23%) named most often.\n\nIt is also common that AI guidelines cover at least some forms of allowed or prohibited applications, with more news outlets mentioning allowed (86.54%) than prohibited ones (67.31%).\n\nA majority of 69.23% refers to possible pitfalls that can result from using AI in journalism.\n\nThe documents were also quite homogeneous in dealing with transparency and human supervision of AI-generated content.\n\n90.38% of news outlets refer to transparency, although most do not specify how to communicate the same, and 84.62% stress the importance of human supervision, although such oversight is mostly applied to text generated or otherwise edited by AI.\n\n65.38% require human supervision at all times.\n\nRegarding the quantitative coding, AI guidelines display a marked degree of homogeneity, especially compared to social media guidelines.\n\nWhile some difference remains, due to the distinct requirements of media organizations and the early stage of AI guideline development, the overall similarity across guidelines is striking, especially compared to the lack of similarity research has uncovered in social media guidelines.",
    "## National and Organizational Idiosyncrasies\n\nWe turn next to more specific research questions (RQ2a and RQ2b) – if national and organizational idiosyncrasies continue to shape publishers’ practices within the overall trend towards homogeneity.\n\nWhile the small sample size does not allow for rigorous statistical analysis and means that these cursory results must be interpreted with caution, we can see some variance across organizational types (commercial vs. public service) and country when looking ‘under the hood’.\n\nA difference seems to emerge mainly between publicly funded and commercial publishers – but not necessarily as expected.\n\nSurprisingly commercial media organizations’ guidelines seem to be more fine-grained and contain significantly more information on permitted and prohibited applications \n\n(see Appendix, Table 4).\n\nFor example, the protection of sources, which plays a role especially when sensitive information is entered into the interface of LLMs, is emphasized above all by commercial broadcasters and legacy newspapers (see Appendix, Table 5), the latter are also significantly more concerned than average about data protection, perhaps owing to the risk legal liability poses to their business models.\n\nCommercial media also make more statements about possible pitfalls of AI (see Appendix, Table 6) which also ties in with the fact that this group more often tends to demand transparency in the use of AI and more frequently calls for human control of the products generated or edited by AI, compared to public media (see Appendix, Table 7).\n\nSo the conclusion that publicly funded broadcasters tend to establish a ‘stricter ethical regime’ than privately funded media outlets, as Sánchez Laws & Utne (2019) have shown regarding social media guidelines, seems questionable for AI guidelines.\n\nWhere public media are ahead is the human control of algorithms; they are more aware of this topic than private-sector actors (see Appendix, Table 8) – possibly due to their high degree of organization and the associated professional specialization within editorial teams.\n\nLess surprising is that commercial media are somewhat more permissive when it comes to the use of AI than their publicly financed or public-service-oriented counterparts (see Appendix, Table 9).\n\nThis seems especially true for news agencies which allow AI to be used across most levels of the journalistic process much more than the average (see Appendix, Table 10), potentially owing to their early adoption of the technology.\n\nThe data also shows some variance across different countries (RQ2b).\n\nA set of four countries refers to journalistic values at least ten percentage points more often than average: Belgium, Canada, the United Kingdom, and Germany (see Appendix, Table 12).\n\nGuidelines from Belgium and Finland allow for AI use more often than average on all three levels of the journalistic process (see Appendix, Table 13).\n\nPossible pitfalls of AI are most often mentioned by organizations in Canada, Norway, and the United Kingdom (see Appendix, Table 14).\n\nWhile many organizations make statements about transparency and human oversight, organizations in Canada, the Netherlands, Switzerland, and the United Kingdom are significantly above average (see Appendix, Table 15).\n\nWhen it comes to elements of Responsible AI (see Appendix, Table 16), organizations in Canada, the United Kingdom, Netherlands, and Germany have a particularly strong emphasis on data privacy in their guidelines.\n\nAlgorithmic bias is covered most often by organizations in Western Europe, especially in the United Kingdom, the Netherlands, and Switzerland.\n\nSource protection, on the other hand, is mainly a topic for Scandinavian countries, although outlets in Canada and the United States also mention this more often than average.",
    "## Blind Spots in AI Guidelines\n\nFinally, the analysis of current AI guidelines within news organizations also revealed several blind spots (RQ3).\n\nFirst, the vast majority of guidelines are essentially toothless regarding enforcement of violations or broader oversight of what they stipulate.\n\nSimilarly, while many organizations demand the supervision of output, oversight over algorithms and technical systems seems limited.\n\nA third notable absence is explicit directives regarding external collaborations, e.g., with technology vendors, researchers, or other stakeholders.\n\nGiven the increasing reliance on external expertise in the development and deployment of AI, guidelines could include provisions for transparent and ethical engagement with such actors.\n\nMost AI guidelines also did not address questions of technological dependency, a factor that holds implications for the autonomy of news organizations (Simon, 2022).\n\nFew discussed safeguarding editorial independence and self-reliance when it came to AI.\n\nLikewise, few organizations specified if and when their guidelines would be updated, a noteworthy omission considering the fast-moving nature of the field.\n\nIn addition, we identified several blind spots in our qualitative coding that matter in as much as they are part of the current discourse around AI but were not discussed in the guidelines at all.\n\nFirst, while serving audiences was often mentioned, soliciting audience feedback on guidelines or engaging audiences on AI use was conspicuously absent – an interesting facet amidst industry discussions stressing the need for greater audience engagement.\n\nLikewise, references to recent debates around sustainable AI and AI supply chains (Brown, 2023; van Wynsberghe, 2021) which shed light on the environmental and societal impact and harm of AI development and use, were notably absent.\n\nThe impact of AI use on existing power asymmetries (Arguedas & Simon, 2023), especially concerning local and cultural diversity, received only fleeting references in very few instances.\n\nSimilarly, issues of workplace surveillance through AI (Ebert et al., 2021), data colonialism (Couldry & Mejias, 2019), labor exploitation, and potential human rights abuses associated with AI training received no attention from any of the guidelines.\n\nThe oversight of these facets underscores the need for a more comprehensive integration of ethical considerations.",
    "# Limitations\n\nIt is worth briefly dwelling here on the limitations of this study.\n\nFirst, the uneven distribution of sources across different geographical regions is a notable caveat.\n\nOur reliance on a larger number of German sources in comparison to sources from other regions introduces a potential source of bias in the findings.\n\nThe sample size, while valuable for exploratory insights and exceeding previous research, remains limited in its scope.\n\nThis limitation is particularly pronounced when considering the Global South, which is inadequately represented here despite concerted efforts to include guidelines from more news organizations in India and Brazil.\n\nMoreover, the study encountered restrictions in accessing guidelines from certain organizations.\n\nSome publishers acknowledged having or working on guidelines but were unwilling to share them.\n\nFrom background conversations, we learned that often this could be attributed to concerns surrounding divulging proprietary strategies and thus potentially losing a competitive advantage.\n\nSome were also concerned about looking amateurish vis-à-vis their peers if they released guidelines too early.\n\nOrganizations might also be wary of disclosing their approaches to issues such as dependency on major technology corporations, lest it jeopardize their response to the same.\n\nFurthermore, it should be acknowledged that this study does not encompass the entirety of internal guidelines that some organizations have, with several organizations maintaining more extensive internal guidelines that in some but not all cases were beyond our purview.\n\nThese internal documents often provide more granular instructions for staff members, beyond what is publicly available.\n\nIt is important to note that our analysis examines the outcome – homogeneity – of a process that has likely already occurred (though it is likely to be still ongoing at the time of writing).\n\nBut while we can assume that an isomorphic process has transpired (and can elaborate on potential driving factors), we cannot establish causality or definitively prove that this has been the case.\n\nThis aspect will require further research and delving into the motivations behind these guidelines and the processes shaping their creation.",
    "# Conclusion\n\nInstitutional isomorphism offers plausible explanations for our observations.\n\nDiMaggio & Powell contend that ‘the greater the extent to which technologies are uncertain or goals are ambiguous within a field, the greater the rate of isomorphic change’ (1983, p. 156).\n\nThis certainly holds true for the current state of AI in journalism.\n\nThe uncertainty surrounding the trajectory of the technology is significant and many organizations are grappling with defining their goals for AI.\n\nMoreover, the more ambiguous AI’s nature, what it could enable, and what should crucially be done about it, the more likely organizations will emulate successful entities that preceded them.\n\nCertain guidelines, such as those of the BBC and Bayerischer Rundfunk, which have gained widespread attention through industry publications and conferences, have served as influential benchmarks for others.",
    "# Journalism and AI: Insights into Guidelines and Practices\n\nJournalism, both nationally and globally, is becoming more internationally connected, facilitated by digital media, the exchange of labour, and collaborations among major players.\n\nIn the AI domain, the core community working on it remains relatively small, and initiatives like the London School of Economics’ Journalism AI initiative provide vital platforms for idea exchange.\n\nBoth could have contributed to similar patterns emerging in the guidelines of international news organisations.\n\nUltimately, it should not be forgotten that both the race to AI and the establishment of AI guidelines are also a quest for legitimacy.\n\nFormulating an AI policy – one that resembles those of successful organisations and accedes to common demands on how AI should be used and regulated – also functions as a form of signalling.\n\nBy having an AI policy, a publisher conveys some important information about themselves, in this case likely to make their commitment to the ethical use of AI observable and to show to competitors that yes, they too, are innovative.\n\nThis, of course, ultimately raises the point who and what AI guidelines are really for – are they a mere PR exercise, dressed up in form of a policy or a meaningful contribution to regulating a technology in the face of uncertainty.\n\nWhile only future research will be able to answer this question, we can assume that reality is more nuanced.\n\nPublicly, at least, many organisations assert that their motivation behind formulating such guidelines stems from the dynamic nature of the environment in which they operate, with guidelines intended to serve as an initial framework, offering a sense of security to staff, readers, and partners.\n\nMany seem to have emerged in response to both internal calls for direction and a perceived need to address external demands.\n\nEstablishing legitimacy is part of the answer, not the whole story.\n\nLastly, isomorphism theory argues that a field’s dependence on a single source leads to greater isomorphism.\n\nWhere DiMaggio & Powell referred to a ‘single source of support for vital resources’ (1983, p. 155) and resource centralisation, AI, as a large technological system (Simon, 2023a), comes into play.\n\nOne does not have to fully embrace technological determinism to assume that AI has a shaping power of its own.\n\nIt thus acts as a coercive force with broadly similar effects across contexts, resulting in analogous reactions, including in the development of AI guidelines.\n\nWhile we could hypothesise that all organisations developing AI policies simply began at similar starting points with equivalent concerns and knowledge about the technology, it is also considerably less elegant as a theory to explain the similarities we can observe.\n\nOccam’s razor would suggest that they modelled aspects of their policies on each other.\n\nWith this paper we hope to help lay the groundwork for future analysis and work in this area.\n\nWe are at juncture on the road to more substantive AI use and, by extension, regulation in journalistic work.\n\nOne should nevertheless remember that many AI guidelines are early examples, developed quickly in response to the launch of ChatGPT and due to concerns about the speed with which generative AI became accessible to the public and journalists.\n\nAnd while the notion that AI guidelines in and of themselves will somehow magically resolve the intricacies of AI implementation and its attendant challenges is questionable, they can potentially make an important contribution in ensuring the responsible, ethical, and effective use of the technology in the news.\n\nPart of the significance of these findings also lies in the fact that this self-regulation for AI is well underway, with a sizable number of publishers having begun to establish strategies addressing various critical aspects of the technology.\n\nCrucially, while there remains ample room for improvement, these pioneering organisations, many of them leaders in the news industry, are poised to influence and set a precedent for broader industry practices, thus facilitating a trickle-down effect of their AI guidelines and strategies.\n\nHow unanimous these will and should be remains to be seen.\n\nFor now, we can see that there are some overlapping trends among AI guidelines but also a considerable degree of variety and we withhold judgment at this point if this is to be celebrated or rectified.\n\nFuture questions abound.\n\nFor one, AI guidelines often emerge from internal consultation processes that involve various departments, sometimes building upon pre-existing materials.\n\nOne question here will be which ‘tribes’ – editorial, business, tech – within news organisations will exert dominance in shaping the ideas and logics embedded in these guidelines.\n\nA second line of inquiry pertains to what kinds of organisations release or craft both internal and external AI guidelines.",
    "A second line of inquiry pertains to what kinds of organisations release or craft both internal and external AI guidelines.\n\nUnderstanding which organisations engage in this practice and the reasons behind those that opt not to (be it due to deeming guidelines unnecessary or as potential hindrances to their operations) will tell us something about the future direction of AI in journalism.\n\nFinally, against the backdrop of industry efforts to develop a set of principles, rights, and obligations regarding the use of AI-based systems, the question looms along which lines these guidelines will develop.\n\nWill we have more standardisation and homogeneity, or will we see more customisation in the future?\n\nFor now, publishers seem to embark on this journey from somewhat similar points.",
    "# Acknowledgments\n\nThe authors would like to thank the organisations and individuals that assisted us by providing their guidelines or general feedback.\n\nSpecial thanks are owed to Michelle Disser for her invaluable assistance with the analysis and interpretation of the data and feedback on the final paper, and to Evelyn Dappa for her meticulous feedback on the final manuscript.\n\nErik Bucy, Benjamin Toff and Camila Mont’Alverne provided guidance on coding and analysis which greatly contributed to this work.\n\nThe authors also wish to extend their thanks to Isabel Ebert and Maggie Mustaklem for advice and Mitali Mukherjee, Julie Posetti, Nabeelah Shabbir, Tomás Dodds, and Nico Wilfer for facilitating valuable contacts.",
    "# Disclosure Statement\n\nChristopher Crum has no conflicts of interest to disclose.\n\nFelix M. Simon sits on the AI and Local News Steering Committee of Partnership on AI, which is funded from philanthropy and corporate entities and for which he receives an honorarium.\n\nHe has no conflicts of interest to disclose.\n\nKim Björn Becker is a staff writer at the Frankfurter Allgemeine Zeitung (F.A.Z.)\n\nwhich is covered as part of this study.\n\nHe was not involved in the development of F.A.Z.’s AI guidelines.\n\nHe has no conflicts of interest to disclose.",
    "# References\n\nAdornato, A., & Lysak, S. (2017).\n\nYou Can’t Post That!\n\n: Social Media Policies in U.S. Television Newsrooms.\n\nElectronic News, 11(2), 80–99.\n\nAlschner, W., & Skougarevskiy, D. (2016).\n\nMapping the Universe of International Investment Agreements.\n\nJournal of International Economic Law, 19(3), 561–588.\n\nArguedas, A. R., & Simon, F. M. (2023).\n\nAutomating democracy: Generative AI, journalism, and the future of democracy.\n\nBalliol Interdisciplinary Institute, University of Oxford.\n\nBecker, K. B.\n\n(2023).\n\nNew game, new rules.\n\nAn investigation into editorial guidelines for dealing with artificial intelligence in the newsroom.\n\nJournalistik, 2(6), 142–164.\n\nBeckett, C. (2019).\n\nNew powers, new responsibilities.\n\nA global survey of journalism and artificial intelligence.\n\nPolis.\n\nLondon School of Economics.\n\nBloom, T., Cleary, J., & North, M. (2016).\n\nTraversing the ‘Twittersphere’.\n\nJournalism Practice, 10(3), 343–357.\n\nBrown, I.\n\n(2023, June 29).\n\nExpert explainer: Allocating accountability in AI supply chains.\n\nRetrieved from \n\nChristin, A.\n\n(2020).\n\nMetrics at Work: Journalism and the Contested Meaning of Algorithms.\n\nPrinceton University Press.\n\nCools, H., & Diakopoulos, N. (2023).\n\n‘Writing guidelines for the role of AI in your newsroom?\n\nHere are some, er, guidelines for that’.\n\nNieman Lab, 11 July 2023, \n\nCouldry, N., & Mejias, U.\n\nA.\n\n(2019).\n\nData colonialism: Rethinking big data’s relation to the contemporary subject.\n\nTelevision & New Media, 20(4), 336–349.\n\nDenny, M. J., & Spirling, A.\n\n(2018).\n\nText Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It.\n\nPolitical Analysis, 26(2), 168–189.\n\nDeutscher Journalisten-Verband.\n\n(2023).\n\nPositionspapier bezüglich des Einsatzes Künstlicher Intelligenz im Journalismus.\n\nRetrieved from \n\nDeuze, M. (2005).\n\nWhat is journalism?\n\n: Professional identity and ideology of journalists reconsidered.\n\nJournalism, 6(4), 442–464.\n\nDiakopoulos, N. (2019).\n\nAutomating the News: How Algorithms are Rewriting the Media.\n\nHarvard University Press.\n\nDiMaggio, P., & Powell, W. (1983).\n\nThe Iron Cage Revisited: Institutional Isomorphism and Collective Rationality in Organizational Fields.\n\nAmerican Sociological Review, 48(2), 147–160.\n\nDuffy, A., & Knight, M. (2018).\n\nDon’t be stupid.\n\nThe role of social media policies in journalistic boundary-setting.\n\nJournalism Studies, 20(7), 932–951.\n\nEbert, I., Wildhaber, I., & Adams-Prassl, J.\n\n(2021).\n\nBig Data in the workplace: Privacy Due Diligence as a human rights-based approach to employee privacy protection.\n\nBig Data & Society, 8(1).\n\nEuropean Commission.\n\n(n.d.).\n\nMedia Councils in the Digital Age, Ethical Codes Database.\n\nRetrieved from \n\nHallin, D. C.\n\n(Ed.).\n\n(2016).\n\nThe international encyclopedia of political communication.\n\nIn Media system (pp.\n\n801–812).\n\nWiley.\n\nHanitzsch, T., & Mellado, C. (2011).\n\nWhat Shapes the News around the World?\n\nHow Journalists in Eighteen Countries Perceive Influences on Their Work.\n\nThe International Journal of Press/Politics, 16(3), 404–426.\n\nIhlebæk, K., & Larsson, A.\n\n(2018).\n\nLearning by Doing.\n\nJournalism Studies, 19(6), 905–920.\n\nKalogeropoulos, A., & Nielsen, R. K. (2018).\n\nInvesting in Online Video News.\n\nJournalism Studies, 19(15), 2207–2224.\n\nKrippendorff, K. (2004).\n\nReliability in Content Analysis.\n\nHuman Communication Research, 30(3), 411–433.\n\nLee, J.\n\n(2016).\n\nOpportunity or risk?\n\nHow news organizations frame social media in their guidelines for journalists.\n\nThe Communication Review, 19(2), 106–127.\n\nMeier, K. (2018).\n\nJournalistik.\n\nUVK Verlagsgesellschaft.\n\nNewman, N. (2023).\n\nJournalism, Media, and Technology Trends and Predictions 2023 (Reuters Institute Report).\n\nReuters Institute for the Study of Journalism.\n\nRetrieved from \n\nNewman, N., Fletcher, R., Eddy, K., Robertson, C. T., & Nielsen, R. K. (2023).\n\nReuters Institute Digital News Report 2023 (Digital News Report).\n\nReuters Institute for the Study of Journalism.\n\nOpgenhaffen, M., & d’Haenens, L. (2015).\n\nManaging Social Media Use: Whither Social Media Guidelines in News Organizations?\n\nThe International Journal on Media Management, 17(4), 201–216.\n\nOpgenhaffen, M., & Scheerlinck, H. (2014).\n\nSocial Media Guidelines for Journalists.\n\nJournalism Practice, 8(6), 726–741.\n\nParasie, S. (2022).\n\nComputing the News: Data Journalism and the Search for Objectivity.\n\nColumbia University Press.\n\nPeruško, Z., Čuvalo, A., & Vozab, D. (2020).\n\nMediatization of journalism: Influence of the media system and media organization on journalistic practices in European digital mediascapes.\n\nJournalism, 21(11), 1630–1654.\n\nPetre, C. (2021).\n\nAll the News That’s Fit to Click: How Metrics Are Transforming the Work of Journalists.\n\nPrinceton University Press.\n\nRaad voor de Journalistiek (n.d.).\n\nNieuwe richtlijn over het gebruik van artificiële intelligentie in de journalistiek.\n\nRetrieved from \n\nRuß-Mohl, S., & Schultz, T. (2023).\n\nJournalismus.\n\nDas Lehr- und Handbuch.",
    "Nieuwe richtlijn over het gebruik van artificiële intelligentie in de journalistiek.\n\nRetrieved from \n\nRuß-Mohl, S., & Schultz, T. (2023).\n\nJournalismus.\n\nDas Lehr- und Handbuch.\n\nKöln: Herbert von Halem.\n\nSacco, V., & Bossio, D. (2016).\n\nDon’t Tweet This!\n\nDigital Journalism, 5(2), 177–193.\n\nSánchez Laws, A. L., & Utne, T. (2021).\n\nEthics Guidelines for Immersive Journalism.\n\nFrontiers in Robotics and AI, 6(28).\n\nSchultz, T. (2021).\n\nMedien und Journalismus.\n\nStuttgart: Kohlhammer.\n\nSimon, F. M. (2023a).\n\nEscape Me If You Can.\n\nHow AI Reshapes News Organisations’ Dependency on Platform Companies (Working Paper).\n\nSimon, F. M. (2023b).\n\nAI in the News: Re-Shaping the Public Arena?\n\n(Working Paper).\n\nTow Center for Digital Journalism, Columbia University.\n\nSimon, F. M. (2022).\n\nUneasy Bedfellows: AI in the News, Platform Companies and the Issue of Journalistic Autonomy.\n\nDigital Journalism, 10(10), 1832–1854.\n\nSimon, F. M., & Isaza-Ibarra, L. F. (2023).\n\nAI in the News: Reshaping the Information Ecosystem?\n\nOxford Internet Institute, University of Oxford.\n\nRetrieved from \n\nSpirling, A.\n\n(2012).\n\nU.S. Treaty Making with American Indians: Institutional Change and Relative Power, 1784-1911.\n\nAmerican Journal of Political Science, 56(1), 84–97.\n\nvan Wynsberghe, A.\n\n(2021).\n\nSustainable AI: AI for sustainability and the sustainability of AI.\n\nAI Ethics, 1, 213–218.\n\nVentura Pocino, P. (2021).\n\nAlgorithms in the newsrooms.\n\nChallenges and recommendations for artificial intelligence with the ethical values of Journalism.\n\nPublished by the Catalan Press Council.\n\nRetrieved from \n\nWAN-IFRA (2023).\n\nGauging Generative AI’s impact on newsrooms.\n\nSurvey: Newsroom executives share their experiences so far.\n\nRetrieved from \n\nWard, S. (2009).\n\nJournalism Ethics.\n\nIn K. Wahl-Jorgensen & T. Hanitzsch (Eds.\n\n), The Handbook of Journalism Studies (pp.\n\n295-309).\n\nRoutledge.",
    "**No.\n\nVariable Description**\n\n- **Professional roles**: List all professional roles such as ‘editor-in-chief’ or ‘legal team’ which are mentioned in the guidelines\n- **Values**: Name all journalistic values, e.g.\n\n‘trust’ and ‘accuracy’ which are included in the document\n- **Goals**: Collect all wordings where the organisation refers to the specific goals it wants to achieve by deploying AI\n- **Allowed applications**: List all AI applications allowed according to the policy\n- **Prohibited applications**: List all AI applications prohibited according to the policy\n- **Mentioned pitfalls**: Collect each possible pitfall mentioned that can be associated with the deployment of AI\n- **AI engines**: Name all AI engines such as LLMs or image generators that are explicitly called\n- **Applications relevant for transparency**: Collect all wordings where the guidelines mention AI uses cases that should or must be made transparent to the audiences\n- **Elements of data privacy**: Name all elements of data privacy that are included in the document\n- **Elements of Responsible AI**: Include all references to elements of Responsible AI\n- **Elements of algorithmic bias**: Collect all phrases where the guideline refers to elements of algorithmic bias associated with the deployment of AI\n- **Elements of source protection**: Search for text elements where the guidelines say something about possible risks in source protection associated with the use of AI\n- **Professional roles involved**: Name each professional role that is named in terms of possible internal cooperation on AI projects\n- **Institutions involved**: Collect each institution that is named when the guideline makes statements about external cooperation",
    "**No.\n\nVariable Values Description**\n\n- **Title keyword**: Guideline, Guidance, How is the document labelled according to the Framework, Principles, Letter, keyword used in the title?\n\nNote, Policy, How-to, Charter, Position paper, Statement, Other, None\n- **Remit**: Newsroom/Journalists, Business development, Other, All departments, Not specified Which part of the news organisation falls under these guidelines?\n\n- **Accountability**: Yes, No Do the guidelines mention an accountability mechanism, e.g.\n\nhow guidelines will be enforced?\n\n- **Audience**: Internal, External, Both, Not Are the guidelines meant for internal, external specified consumption or both?\n\n- **Reference to professional roles**: Yes, No Do the guidelines mention certain professional role(s) within the organisation?\n\n- **Reference to dynamization**: Yes, No Do the guidelines state that they might be updated one day?\n\n- **Interval**: Specified, Unspecified If so, is the interval specified?\n\n- **Reference to applications**: Yes, No Does the document state where AI shall be deployed in the journalistic process?\n\n- **Reference to prohibition**: Yes, No Does the document state where AI shall not be deployed in the journalistic process?\n\n- **Status: Access and observation**: Yes, Partial, No, Not specified Do the guidelines allow AI to gather information, angles, ideas and outlines (creative purposes)?\n\n- **Status: Processing and filtering**: Yes, Partial, No, Not specified Do the guidelines allow to process and filter the editorial content (editing, updating, augmenting)?\n\n- **Status: Distribution**: Yes, Partial, No, Not specified Do the guidelines allow to augment the editorial content with respect to distribution (headline generation, social media posts, summaries, content moderation)?\n\n- **AI-generated text**: Yes, Partial, No, Not specified Is AI-generated text allowed according to the policy?\n\n- **AI-generated images**: Yes, Partial, No, Not specified Are AI-generated images allowed according to the policy?\n\n- **Reference to pitfalls**: Yes, No Do the guidelines refer to possible pitfalls of AI/LLMs?\n\n- **Reference to AI engines/LLMs**: Yes, No Do the guidelines name specific engines or LLMs?",
    "**Reference to human supervision**\n\n- **Yes, No**: Do the guidelines refer to human supervision of AI-generated content at some point?\n\n- **...text/product supervision**: Anytime, Sometimes, Never, Not specified Is human supervision of AI-generated content required at some point?\n\n- **...algorithmic supervision**: Anytime, Sometimes, Never, Not specified Is human supervision limited to checking the integrity of the algorithm deployed (instead of checking every AI-generated contribution)?",
    "### Definitions\n\nGenerative Artificial Intelligence (Generative AI) is a class of computer software and systems, or functionality within systems, that use large language models, algorithms, deep-learning, and machine learning models, and are capable of generating new content, including but not limited to text, images, video, and audio, based on patterns and structures of input data.\n\nThese also include systems capable of ingesting input and translating that input into another form, such as text-to-code systems.\n\nWhile this policy document includes principles that apply to AI technologies generally, the policy statements apply only to generative AI systems.",
    "### Artificial Intelligence (AI) Principles\n\nPrinciples describe general codes of conduct that represent the City’s values and are aligned with our responsibilities to the residents we serve.\n\nThese principles serve to guide City employees in their use of both generative and traditional AI technology.\n\nCity employees shall adhere to the principles and requirements outlined in this policy, and will be held accountable for compliance with these commitments.\n\n- **Innovation and Sustainability:** The City values public service innovation to meet our residents’ needs.\n\nWe commit to responsibly explore and evaluate AI technologies, which will improve our services and advance beneficial outcomes for both people and the environment.\n\n- **Transparency and Accountability:** The City values transparency and accountability and understands the importance of these values in our use of AI systems.\n\nThe City will ensure that the development, use, and deployment of AI systems are evaluated for and compliant with all laws and regulations applicable to the City prior to use, and will make documentation related to the use of AI systems available publicly.\n\n- **Validity and Reliability:** The City will work to ensure that AI systems perform reliably and consistently under the conditions of expected use, and that ongoing evaluation of system accuracy throughout the development and/or deployment lifecycle is managed, governed, and auditable, to the greatest extent possible.\n\n- **Bias and Harm Reduction and Fairness:** We acknowledge that AI systems have the potential to perpetuate inequity and bias resulting in unintended harms on Seattle residents.\n\nThe City will evaluate AI systems through an equity lens, in alignment with our Race and Social Justice commitments, for potential impacts such as discrimination and unintended harms arising from data, human, or algorithmic bias to the extent possible.\n\n- **Privacy Enhancing:** The City values data privacy and understands the importance of protecting personal data.\n\nWe work to ensure that policies and standard operating procedures that reduce privacy risk are in place, and are applied to the AI system throughout development, testing, deployment, and use to the greatest extent possible.\n\n- **Explainability and Interpretability:** The City understands the importance of leveraging AI systems, models, and outputs that are easily interpreted and explained.\n\nWe work to ensure all AI systems and their models are explainable to the extent possible, and that system outputs are interpretable and communicated in clear language, representative of the context for use and deployment.\n\n- **Security and Resiliency:** Securing our data, systems, and infrastructure is important to the City.\n\nWe will ensure AI systems are evaluated for resilience and can maintain confidentiality, integrity, and availability of data and critical City systems, through protection mechanisms to minimize security risks to the greatest extent possible, in alignment with governing policy and identified best practices.",
    "### Acquisition of Generative AI Technology\n\nConsistent with the City’s standards for Acquisition of Technology Resources, City employees may be authorized to use pre-approved generative AI software tools or they may request a non-standard acquisition of generative AI software through Seattle IT’s current request process.\n\nSeattle IT shall review exception requests according to its current risk and impact methodology, which shall include specific review criteria for generative AI technology.\n\nSeattle IT shall either approve or deny a request according to its criteria.\n\nThe City’s standard for technology acquisition applies to all technology, including free-to-use software or software-as-a-service tools.\n\nIf a technology that has already been approved for use in the City adds or incorporates generative AI capabilities, no additional approval is required to use those capabilities, however all other aspects in this policy apply to said use.\n\nSeattle IT may revoke authorization for a technology that adds AI capabilities, or may restrict the use of those AI capabilities, if, in its judgment, those AI capabilities present risks that cannot be effectively mitigated to comply with this policy or other City policies.",
    "### Use of Generative AI Outputs\n\nOutputs of Generative AI systems must be reviewed by humans prior to each use in an official City capacity (“Human in the Loop” or HITL).\n\nHITL review processes shall be documented by owning departments and shall demonstrate how the HITL review was conducted to adhere to the principles outlined in this document.\n\nDocumentation of HITL reviews shall be retained according to the appropriate records retention schedule.",
    "### Attribution, Accountability, and Transparency of Authorship\n\nAll images and videos created by Generative AI systems must be attributed to the appropriate Generative AI system.\n\nWherever possible, attributions and citations to the City of Seattle should be embedded in the image or video (e.g., via digital watermark).\n\nIf text generated by an AI system is used substantively in a final product, attribution to the relevant AI system is required.\n\nIf a significant amount of source code generated by an AI system is used in a final software product, or if any amount is used for an important or critical function, attribution to the appropriate AI system is required via comments in the source code and in product documentation.\n\nAll attributions should include the name of the AI system used plus an HITL assertion (which should include the department or group who reviewed/edited the content).\n\nExample: Some material in this brochure was generated using ChatGPT 4.0 and was reviewed for accuracy by a member of the Department of Human Services before publication.\n\nDepartments shall interpret \"substantive use\" thresholds to be consistent with the principles outlined in this document as well as relevant intellectual property laws.",
    "### Reducing Bias and Harm\n\nGenerative AI systems may produce outputs based on stereotypes or use data that is historically biased against protected classes.\n\nCity employees must leverage RSJI resources (e.g., the Racial Equity Toolkit) and/or work with their departmental RSJI Change Team to conduct and apply a Racial Equity Toolkit (RET) prior to the use of a Generative AI tool, especially uses that will analyze datasets or be used to inform decisions or policy.\n\nAs per the objectives of the RSJ program, the RET should document the steps the department will take to evaluate AI-generated content to ensure that its output is accurate and free of discrimination and bias against protected classes.",
    "### Data Privacy\n\nUse of generative AI tools shall be consistent with the principles and standards described in the City’s Data Privacy Policy and Information Security Policy.\n\nUnless suitable enterprise controls and data protection mitigations are in place, employees shall not submit data that is classified by the City’s data classification guidelines as Confidential or Confidential with Special Handling, or that otherwise not considered to be acceptable to disclose to the public, shall not be submitted to Generative AI systems.\n\nNo City data or records, including inputs or prompts, are to be used for training or parameter-tuning for Generative AI models outside the City’s control.\n\nAI technologies that cannot prevent City data or records from contributing to their language models may not be used by City employees.",
    "### Public Records & City Records Management\n\nAll records generated, used, or stored by Generative AI vendors or solutions may be considered public records and must be disclosed upon request.\n\nAll Generative AI solutions and/or vendors approved for City use shall be required to support retrieval and export of all prompts and outputs (either via exposed functionality or through vendor contract assurances).\n\nCity employees who use generative AI tools are required to maintain, or be able to retrieve upon request, records of inputs, prompts, and outputs in a manner consistent with the City’s records management and public disclosure policies and practices.",
    "### Exceptions\n\nExceptions must be approved in advance through submission of a Seattle IT Exception Review Approval request in Service Hub.\n\nThis can be submitted directly or with the assistance of Client Engagement personnel.\n\nNote: this section refers to exceptions to this policy as it relates to generative AI tools that are in use by the City.\n\nIt does not refer to requests for acquisition of non-standard applications or technologies.",
    "### Non-compliance\n\nThe Chief Technology Officer (CTO) is responsible for compliance with this policy.\n\nEnforcement may be imposed in coordination with individual division directors and department leaders.\n\nNon-compliance may result in department leaders imposing disciplinary action, restriction of access, or more severe penalties up to and including termination of employment or vendor contract.",
    "### Responsibilities\n\nThe policy will be maintained through the Data Privacy, Accountability and Compliance (DPAC) division, owned by the Director of DPAC and City of Seattle Chief Privacy Officer.\n\nTheir responsibilities include creating and maintaining the generative AI risk and impact criteria and the documents and forms to support the exception review process for this technology.",
    "# Advancing Responsible Development and Deployment of Generative AI\n\nThe value proposition of the UN Guiding Principles on Business and Human Rights\n\nA UN B-Tech Foundational Paper  \nNovember 2023\n\nThe UN Guiding Principles on Business and Human Rights (UNGPs) — the global authoritative standard for preventing and addressing business impacts on people — can add considerable value to efforts aimed at achieving responsible development and deployment of generative artificial intelligence (generative AI) foundation models, applications, and products.\n\nBased on six months of research and consultation, the UN Human Rights B-Tech Project has identified three broad headlines and associated practical recommendations for how lawmakers, standard setters, businesses, and civil society can leverage the UNGPs to foster governance and business practices capable of tackling human rights impacts and risks of generative AI.\n\nRights-based approaches focus attention on specific harms to people’s dignity and equality.\n\nThey also provide agreed norms for assessing and addressing impacts, along with a shared language that can facilitate understanding and engagement across diverse stakeholder groups.\n\nTo catalyse greater attention to applying a human rights lens to developing and deploying generative AI, B-Tech has developed a Taxonomy of Human Rights Risks Connected to Generative AI.\n\nThis includes companies that are suppliers of AI knowledge and resources, actors in the AI system lifecycle, and users/operators of an AI system.\n\nA UNGPs-informed approach emphasizes that:\n- States should implement a “smart-mix” of regulation, guidance, incentives, and transparency requirements — all supported by policy coherence in domestic and multi-lateral efforts — to advance corporate responsibility and accountability for human rights harms.\n\n1.\n\nThe UNGPs are the global authoritative standard for preventing and addressing business impacts on people, unanimously endorsed by the Human Rights Council in 2011.\n\nThe UNGPs sparked an unprecedented regulatory dynamic for issue-specific and overarching due diligence legislation; civil society in campaigns, complaints, and litigation; companies, and more recently investors, building and implementing good practice principles, codes, and guidance aligned to the UNGPs; and reporting standards.\n\nAn Introduction to the UN Guiding Principles in the Age of Technology, a B-Tech foundational paper.\n\n2.\n\nThe OECD defines generative AI as \"a form of AI model specifically intended to produce new digital material as an output (including text, images, audio, video, software code), including when such AI models are used in applications and their user interfaces.\n\nThese are typically constructed as machine learning systems that have been trained on massive amounts of data.\n\nThey work by predicting words, pixels, waveforms, data points, etc., that would resemble the models’ training data, often in response to a prompt.\"\n\n3.\n\nThis articulation is based on the depiction of a typical AI value chain, proposed by the OECD’s Advancing accountability in AI: Governing and managing risks throughout the lifecycle for trustworthy AI.\n\nBy way of example: 1) Suppliers of AI knowledge and resources can include; content creators; data providers and data annotators; investors; digital infrastructure providers; hardware manufacturers.\n\n2) Actors in the AI lifecycle can include companies, states, research institutions involved in planning & design of the system; collecting & processing of data; building & using the model; verifying & validating the model; deploying the system, regardless of the distribution channel (including the distribution of open-source software); and operating & monitoring the system; 3) Users/operators of the AI system can include businesses, including financial institutions and businesses in the ‘real’ economy (e.g., manufacturing, purchases, and flows of goods and services); individuals or other actors using AI for personal use, commercial, or research activity; and states.\n\nRegional, national, international, and industry-led initiatives focused on advancing responsible generative AI should align to the international standards of business conduct.\n\nThis means, in particular, integrating a true risk-based approach to identifying and taking action on impacts.\n\nGreater urgency is needed to ensure effective judicial and non-judicial access to remedy for individuals whose human rights are harmed by the development or deployment of generative AI.\n\nClear and regularly updated guidance on what constitutes best practice is required, building on company practice and informed by civil society and relevant experts.",
    "Clear and regularly updated guidance on what constitutes best practice is required, building on company practice and informed by civil society and relevant experts.\n\nEmphasis should be placed on the following key practices, which are currently under-emphasized in regulatory proposals and technical standards:\n\n- Practice 1: Boards and executives identifying the extent to which the company’s business model and strategy carries inherent human rights risks, and taking action to address this.\n\n- Practice 2: Embedding human rights risk assessment into the working methods and cultures typical of the product-oriented technology organizations developing foundation models.\n\n- Practice 3: Evaluating “technical” mitigations with a focus on people in situations of vulnerability or marginalization.\n\n- Practice 4: Creatively building and using leverage to address residual risks and enable remedy for harms.\n\n- Practice 5: Engagement with affected stakeholders and human rights experts across all phases of human rights due diligence.\n\nFor each headline, this paper recommends specific near-term actions that states, companies, and other stakeholders should pursue.\n\nThese key messages and recommendations are summarized in the appendix.\n\nWhen developing these propositions for ways forward, attention has been given to spotlighting existing practice and initiatives, the perspectives from diverse stakeholders about how generative AI technologies are currently built and function, and the rapidly evolving landscape of users, use cases, risks, and actual harms connected to these technologies.\n\nThese recommendations from the first phase of the B-Tech Generative AI project have been released to support multi-stakeholder dialogue and collaboration that advances UNGPs-consistent public policy, regulation, and business practice.\n\nThe findings, and responses to them, will inform B-Tech's ongoing work on generative AI in 2024.\n\n4.\n\nAccording to the Ada Lovelace Institute, foundation models are “a form of AI designed to produce a wide and general variety of outputs, capable of a range of tasks and applications, such as text, image, or audio generation (…) notable examples are OpenAI’s GPT-3 and GPT-4, foundation models that underpin the conversational tool ChatGPT.\n\nFollowing the launch of large language model (LLM) interfaces (…) foundation models are more widely accessible than ever.”",
    "## Background\n\nTechnological breakthroughs in the field of generative AI, coupled with the unprecedented speed and scale of uptake of new consumer tools and enterprise-facing applications have captured the public imagination.\n\nAspirations of leveraging artificial intelligence to dramatically improve our lives suddenly seem much less fictional: whether helping individuals to reach new heights in creativity and productivity, bolstering industrial development, or uncovering solutions to shared challenges in the realms of healthcare and climate change.\n\nAnd yet, it also seems more likely than ever that these same tools will be designed and used (or abused) in ways that erode individual freedoms, undercut livelihoods, reinforce inequalities, and undermine norms and institutions designed to uphold democratic values and protect human rights.\n\nIn fact, evidence of adverse impacts on people from generative AI tools — whether stemming from in-built characteristics of these tools or from their misuse — are already being reported: for example, increasing technology-enabled gender-based violence, the amplification of discriminatory racial and ethnic stereotypes, the supercharging of online disinformation campaigns, or the creation of child sexual abuse material at scale.\n\nAttention to the near-term opportunities and risks of current generative AI models is taking place within a wider debate about the future promise and threats to humanity of Artificial General Intelligence (AGI).\n\nSome see the potential of AGI to augment and propel the human experience to new, currently unimaginable heights.\n\nOthers have voiced concern that the achievement of AGI will usher in a dystopian future in which AGI works to undermine human existence.\n\nRegardless of one’s position on these matters, addressing current risks and harms should be a priority.\n\nFocusing on what is in front of us also has the merit of being a way to iterate guardrails that can protect against present but also future, as yet unknown, harms.\n\n5.\n\n“Artificial general intelligence (AGI) is the representation of generalized human cognitive abilities in software so that, faced with an unfamiliar task, the AGI system could find a solution.\n\nThe intention of an AGI system is to perform any task that a human being is capable of.\n\nDefinitions of AGI vary because experts from different fields define human intelligence from different perspectives.\n\nComputer scientists often define human intelligence in terms of being able to achieve goals.\n\nPsychologists, on the other hand, often define general intelligence in terms of adaptability or survival.” Source: Tech Target\n\nGovernments, civil society, academics, technologists, investors, and business executives have all called for regulation to govern the design and deployment of generative AI systems to protect against harms and maximize their benefits.\n\nThis has added even more urgency to an already high number of regulatory and other AI initiatives, in some cases resulting in amendments to regulatory proposals and voluntary initiatives.\n\nWhile many of these initiatives seek to advance the governance, assessment, and management of risks to society by private sector actors developing and deploying generative AI technologies, few have incorporated the due diligence expectations laid out by the international standards of business conduct: specifically, the UN Guiding Principles on Business and Human Rights and the OECD Guidelines for Multinational Enterprises on Responsible Business Conduct (OECD Guidelines).\n\nThis misses the opportunity to benefit from several decades of policy developments, regulatory convergence, business practice, multi-stakeholder collaboration, and civil society advocacy about how to (and how not to) advance responsible corporate conduct across complex global value chains, including in the technology sector itself.\n\nAgainst this backdrop, B-Tech launched its Generative AI Project to raise awareness and facilitate exchange among key stakeholders and interdisciplinary experts and shape a comprehensive understanding about the role the UNGPs can play in governing generative AI responsibly.\n\nThe Project aims to do this by:\n\n- Clarifying the expectations under the UNGPs for companies developing and deploying generative AI technologies and products in order to achieve common and more effective human rights risk management approaches across the industry.\n\n- Spotlighting the growth and maturation of existing company responsible AI approaches, as well as academic research and civil society advocacy that have all laid important foundations for addressing the risks to human rights associated with generative AI.\n\n- Informing the debate about policy options for managing human rights risks related to the development and deployment of generative AI, including through mandatory and voluntary measures.",
    "- Informing the debate about policy options for managing human rights risks related to the development and deployment of generative AI, including through mandatory and voluntary measures.\n\n- Complementing parallel efforts to embed the international standards of business conduct into AI governance, such as the work being led by the OECD.\n\nBuilding on OHCHR’s existing work on tech and human rights and on B-Tech’s other workstreams, the B-Tech Generative AI Project is being implemented through an iterative process of research and engagement.\n\nThis has included exploratory interviews with company practitioners, civil society, technical experts, and other key stakeholders, as well as workshops and other convenings involving multiple stakeholders.\n\nThis paper lays out the findings from the first phase of the Project, implemented between June and November 2023.\n\n6.\n\nOECD AI Policy Observatory lists over 1000 AI policy initiatives from 69 countries, territories, and the EU.\n\n7.\n\nSee, for example Generative AI: A closer look at the EU AI Act.\n\n8.\n\nThe OECD is working to apply and adapt international standards on responsible business conduct to actors in the AI value chain.\n\nThis work is being led by a multistakeholder Network of Experts, which includes the UN B-Tech Project, and is overseen by government delegates in the OECD Working Party on Responsible Business Conduct and the OECD Working Party on AI Governance.\n\nThe project is systematically building towards the development of concrete and practical recommendations for AI actors under an overarching due diligence framework by first mapping out and consolidating recommendations, terminology, and risk scopes from existing AI-specific and generic risk management frameworks (e.g., the OECD Due Diligence Guidance for Responsible Business Conduct, the NIST AI Risk Management Framework, the G7 Code of Conduct for the Development of Advanced AI Systems, IEEE 7000 series, ISO 31000, and ISO/IEC 23894).\n\nInternational human rights comprise a list of basic rights that are universally recognised as necessary for a person to live a life of equality and dignity.\n\nThey have developed — and will develop — based on debate, cooperation, and consensus building between countries from different regions and people from many different groups, cultures, and ethical perspectives.\n\nHuman rights are not, and may never be, uniformly protected, upheld and respected around the world, but a rights-based approach to advancing the responsible development and deployment of generative AI brings the global legitimacy and pragmatism that no other set of standards or ethical frameworks can claim.\n\nHuman Rights also offer an intentionally aspirational roadmap and moral compass grounded in our shared humanity to help guide decision-making.\n\nA rights-based approach to advancing the responsible development and deployment of generative AI provides:\n- Agreed norms for assessing and addressing impacts: Human rights provide an existing, well-defined and holistic set of outcomes against which States, companies, and other actors evaluate the risks related to generative AI.\n\nThis offers a common basis for evaluating the nature of risk, i.e., whose lives may be adversely impacted by the proliferation of generative AI, and in which specific circumstances and in what specific ways.\n\nA human rights lens can also prompt attention to categories of impacts on people that may be otherwise missed such as impacts on political participation, access to public services, freedom of assembly, the right to a fair trial, the right to physical and mental health, and freedom to form and hold opinions, for example.\n\n- An architecture for convening, deliberation, and enforcement: The international human rights framework has developed a relatively elaborate architecture of regional and international institutions and processes (e.g., courts, specialized agencies, intergovernmental bodies, designated experts) which can be used both to facilitate consideration of these issues and, in some instances, to monitor and even enforce implementation of any resulting outputs.\n\nBut the international human rights system is not the only human rights regime.\n\nRegional human rights systems, national courts, local civil society actors, and human rights defenders — all actors paying increasing attention to human rights in the digital economy — also play an important, if uneven, role.\n\n- Standards that already apply to both States and corporations: Focusing on upholding international human rights has the merit of reinforcing that State action related to generative AI should not run counter to States’ human rights commitments.\n\nThis includes the requirement that States should refrain from interfering with or curtailing the enjoyment of human rights when deploying generative AI to deliver State functions (such as education, healthcare, social security, or defence) or when regulating generative AI development and use by others.",
    "At the same time, the international human rights framework was never exclusively framed around States.\n\nThe Universal Declaration of Human Rights, which is the foundation of contemporary international human rights, directs “every individual and every organ of society” to strive and to secure universal and effective recognition and observance” of human rights (emphasis added).\n\nAnd the UNGPs outline and clarify the respective duties of States to protect individuals from business-related human rights harms, and the related, distinct, and independent responsibilities of corporations to respect human rights in the course of their activities.\n\n9.\n\nSee also Jason Pielemeier, Global Network Initiative: The Advantages of Applying the International Human Rights Framework to Artificial Intelligence which informed these key messages.\n\nThe idea that impacts on human rights should sit at the core of governing technologies, including artificial intelligence, is gaining traction.\n\nGovernment policy and regulatory initiatives focused on the societal risks of artificial intelligence already focus, to varying degrees, on human rights impacts.\n\nSome of the most well-known technology companies have in place public commitments and processes focused on operating with respect for human rights, and many have invested considerable attention to “fairness and bias” of artificial intelligence and machine learning models, demonstrating one way in which principles of equality and non-discrimination already have some purchase in the field.\n\nAt the international level, the UN Secretary-General has called for guardrails to ensure AI governance is grounded in human rights, transparency, and accountability.\n\nNonetheless, a large number of international and national policy initiatives aiming to address the risks of generative AI entirely omit reference to international human rights standards.\n\nAt best, this omission creates an unnecessary normative vacuum in which a global patchwork of laws, industry standards, and business practices based on similar-sounding aims — e.g., ethics, fairness, openness, transparency — are vaguely or differently defined without being tethered to the important question of real impacts on real people in real places.\n\nAt worst, vague definitions of responsible conduct and judgments of what constitutes acceptable risk when developing and deploying generative AI systems may be determined solely by short-term geopolitical interests and market incentives that externalize impacts on people.\n\nTo catalyse greater application of a human rights lens to developing and deploying generative AI, B-Tech has developed a Taxonomy of Human Rights Risks Connected to Generative AI.\n\nThe Taxonomy outlines numerous “risk examples” connected to generative AI across nine categories of internationally agreed human rights.\n\nWhile the Taxonomy does not attempt to comprehensively list all potential human rights harms, it does offer an examination of some of the main ways in which human rights are currently at risk from generative AI development and deployment, as well as risks that are likely to materialize in the medium-term future.\n\nWhile many of these human rights impacts may have been associated with earlier forms of AI, they have been or risk being exacerbated by the particularities of generative AI.\n\n10.\n\nAccording to the UNGPs, States should take “appropriate steps to prevent, investigate, punish and redress human rights abuse through effective policies, legislation, regulations, and adjudication” (UNGP1), and that States “should consider a smart mix of measures—national and international, mandatory and voluntary—to foster business respect for human rights.” (UNGP3).\n\nFor more information about the UNGPs calls for States to apply a “smart mix” of measures and ensure “policy coherence”, see: B-tech foundational paper Bridging Governance Gaps in the Age of Technology — Key Characteristics of the State Duty to Protect\n\n11.\n\nThe Corporate Responsibility to Respect Human rights requires all business enterprises to: 1) avoid causing or contributing to adverse human rights impacts through their own activities, and address such impacts when they occur; and 2) seek to prevent or mitigate adverse human rights impacts that are directly linked to their operations, products, or services by their business relationships, even if they have not contributed to those impacts.\n\nSee also B-Tech foundational papers Key Characteristics of Business Respect for Human Rights and Designing and implementing effective company-based grievance mechanisms\n\n12.\n\nFor example, the Draft EU AI ACT: Art.\n\n35 “seeks to ensure a high level of protection for fundamental rights and aims to address various sources of risks through a clearly defined risk-based approach”; the Draft Brazilian AI Bill: Art.",
    "35 “seeks to ensure a high level of protection for fundamental rights and aims to address various sources of risks through a clearly defined risk-based approach”; the Draft Brazilian AI Bill: Art.\n\n7 “grants persons affected by AI systems the following rights vis-à-vis “providers” and “users” of AI systems, regardless of the risk-classification of the AI system” and lists Right to information about their interactions with an AI system; Right to an explanation, Right to challenge decisions or predictions, Right to human intervention, Right to non-discrimination and the correction of discriminatory bias, and Right to privacy and the protection of personal data; the U.S Blueprint for an AI Bill of Rights is set out “to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificial intelligence.\n\n(…), these principles are a blueprint for building and deploying automated systems that are aligned with democratic values and protect civil rights, civil liberties, and privacy.”\n\nThere are multiple forces shaping the evolution of generative AI technologies and the ways in which they are used.\n\nThese include the ambition by some technologists to pursue AGI, the economic potential of these technologies, diverse industries investing in generative AI-enabled efficiency gains and innovations, research organisations exploring new solutions to climate and other shared challenges, and diverse interests — some benign, some malicious — of States and individuals.\n\nWhat is undeniable is that private enterprises sit at the core of technological breakthroughs and the modes through which generative AI will permeate our lives.\n\nMany of these companies also have highly specialized understandings of generative AI’s functioning and the considerable financial, human, and other resources needed to remain at the forefront of innovations.\n\nLeaders from across academia, government, civil society, and business have called for States to regulate the practices of companies developing and/or deploying generative AI foundation models, applications, and products.\n\nIn sum, the challenge of addressing the adverse impacts of generative AI is in large part a challenge of establishing a robust, principled, and pragmatic governance framework of corporate responsibility and accountability for those impacts.\n\nThere are, of course, limits to what frameworks focused on responsible business conduct and corporate accountability can tackle.\n\nThey are not a panacea.\n\nMany issues will require other tools, laws, enforcement regimes, and multi-lateral solutions.\n\nFor example, addressing States deploying generative AI technologies in ways that violate the human rights of their own citizens, political parties flooding social media with AI-generated disinformation about opposition candidates, or criminal actions by individuals (such as the use of synthetic voice to commit fraud) will not be eradicated through a sole focus on corporate conduct.\n\nThat said, advancing responsible business conduct, in addition to being valuable in its own right, can serve as one powerful avenue to minimize the likelihood of the most egregious harms resulting from generative AI’s proliferation.\n\nThe UNGPs are a powerful tool for the task at hand.\n\nThey articulate the components of a multi-layered governance model needed to advance business respect for human rights in practice: one that moves beyond the false binary choice between voluntary self-regulation and binding law requirements.\n\nAs John Ruggie, architect of the UNGPs once noted, “(The UNGPs) are not merely a text.\n\nThey were intended to help generate a new regulatory dynamic, one in which public and private governance systems, corporate as well as civil, each come to add distinct value, compensate for one another’s weaknesses, and play mutually reinforcing roles — out of which a more comprehensive and effective global regime might evolve.”\n\nWith this vision in mind, a UNGPs-informed approach to this task would emphasize that:\n\n- States should implement a “smart-mix” of regulation, guidance, incentives, and transparency requirements — all supported by policy coherence in domestic and multi-lateral efforts - to advance corporate responsibility and accountability for human rights harms.\n\n- Regional, national, international, and industry-led initiatives focused on advancing responsible generative AI should align to the international standards of business conduct: the UNGPs and OECD Guidelines.\n\nThis means, in particular, integrating a true risk-based approach to identifying and taking action on impacts.\n\n- Greater urgency to ensure effective judicial and non-judicial access to remedy for individuals whose human rights are harmed by the development or deployment of generative AI.",
    "- Greater urgency to ensure effective judicial and non-judicial access to remedy for individuals whose human rights are harmed by the development or deployment of generative AI.\n\nThe following pages briefly summarize the rationale for these points of emphasis and provide practical recommendations for what States should do in the near term to advance a “comprehensive and effective global regime” for governing generative AI, including through robust engagement with companies and civil society.\n\nStates should implement a “smart-mix” of regulation, guidance, incentives, and transparency requirements — all supported by policy coherence in domestic and multi-lateral efforts — to advance corporate responsibility and accountability for human rights harms.\n\nRATIONALE: The UNGPs focus on this “smart-mix” because decades of experience have shown that regulation alone is rarely a ‘silver bullet’ solution that on its own will ensure that respect for human rights is consistently placed at the heart of private sector governance, strategy, and conduct.\n\nThere are varying reasons for this, most notably that some companies will lag behind others in terms of responsible conduct and compliance meaning States invariably need to explore how to make use of diverse legal regimes and policy domains.\n\nIt is also true that regulators often struggle to keep pace with technological innovation meaning that more nimble methods of governance, alongside regulation, are demanded.\n\nThe UNGPs also emphasize the importance of States ensuring coherent action across all State agencies that shape business practice.\n\nWhere policy coherence is lacking, States will fail to provide private companies developing and deploying generative AI with clear and predictable expectations.\n\nThis serves to undermine both the effectiveness of State measures and the ability of companies to adjust their practices in a stringent and comprehensive manner.\n\nSpecial attention to the role of home states within which generative AI investment is most prolific is critical.\n\nThe action or inaction of these States — notably the United States and China globally, as well as leaders in their region such as Singapore, South Korea, India, Germany, the United Kingdom, Brazil, Chile, Egypt, and South Africa — will have an outsized impact on whether these standards are established.",
    "# Generative AI Governance and Business Practice\n\nThe UNGPs reinforce that “States should set out clearly the expectation that all business enterprises domiciled in their territory and/or jurisdiction respect human rights throughout their operations” through applying “domestic measures with extra-territorial implications” or approaches that “amount to direct extraterritorial legislation and enforcement” (GP 2).",
    "## State Responsibilities and Smart Mix of Measures\n\nThe UNGPs state that States should take “appropriate steps to prevent, investigate, punish and redress human rights abuse through effective policies, legislation, regulations and adjudication” (GP1), and that States “should consider a smart mix of measures—national and international, mandatory and voluntary — to foster business respect for human rights.” (GP3).\n\nSee also The Geneva Academy of International Humanitarian Law and Human Rights, The relevance of the Smart Mix of Measures for Artificial Intelligence.",
    "## Global AI Index\n\nSee the Global AI Index which ranks 62 countries based 111 indicators, collected from 28 different public and private data sources.\n\nThe indicators are split across seven sub-pillars: Talent, Infrastructure, Operating Environment, Research, Development, Government Strategy, and Commercial.\n\nStates participating in multilateral fora and multi-stakeholder processes is also an essential component in ensuring the international legitimacy, coherence, and effectiveness of State action.\n\nCoherent collective action is key to ensuring that States can address the fact that the development and deployment of generative AI can occur at a high speed and at a great scale across borders.",
    "## Collective Action and Stakeholder Engagement\n\nCooperation between States can take the form of aiding States with less financial resource or technological expertise to implement their own form of the smart- mix needed to govern human rights risks and challenges particular to their national context.\n\nWhatever the modalities of collective action, the “Guiding Principles provide a common reference point” and can “serve as a useful basis for building a cumulative positive effect that takes into account the respective roles and responsibilities of all relevant stakeholders.” (GP10).\n\nFinally, the UNGPS reinforce that meaningful involvement of civil society and affected groups, as well as investors, academics, and business leaders can reinforce accountability for States to prioritize human rights protections and investment into effective ways to address business-related human rights impacts associated with generative AI technologies.",
    "## Recommendations\n\nThe following are near-term priorities for applying the “smart mix” of measures and ensuring policy coherence within State actions aimed at governing generative AI:\n\n- **Enforcement of Laws:** States should enforce laws that are aimed at, or have the effect of, requiring companies developing and deploying generative AI technology to respect human rights, periodically assess the adequacy of such laws, and address any gaps.\n\n- **Guidance and Capacity Building:** States should provide effective guidance and associated capacity building to business enterprises on how to respect human rights when developing or deploying generative AI.\n\n- **Corporate Transparency Regimes:** Authoritative corporate transparency regimes from the corporate responsibility and accountability field should be used to complement technology-specific transparency requirements.",
    "## Corporate Responsibility\n\nStates should build the competence and capability of relevant agencies, administrative supervisory bodies, and officials.\n\nThe goal of such efforts should be to enable the navigation of the societal and technical complexities of how digital technologies function, the associated risks to people, and the role (including limits) of companies to address these risks.",
    "## Multilateral Action and Stakeholder Engagement\n\nStates should pursue multilateral action focused on the protection and respect of human rights.\n\nLarge levels of cooperation and the rapid spread of best practices between States will be crucial for advancing responsible generative AI.\n\nSuch multilateral efforts can also minimize the risks of States pursuing their own, however legitimate — economic and geopolitical interests at the expense of building dignity and respect into the heart of generative AI development and deployment.\n\nStates should establish and sustain stakeholder engagement with companies, civil society, and especially affected stakeholders to learn about risks, impacts, and challenges/opportunities to advance meaningful generative AI risk assessment and mitigations.",
    "## International Standards and Business Conduct\n\nRegional, national, international, and industry-led initiatives should use or align with the international standards of business conduct.\n\nThis means, in particular, integrating a true risk-based approach to identifying and taking action on impacts that:\n- Use severity of risks to people to prioritize impacts for attention; and\n- Set expectations of companies across the generative AI value chain commensurate with the nature of their involvement (causation, contribution, or linkage) with human rights risks and impacts.",
    "## Rationale\n\nStates and stakeholders do not need to reinvent standards of responsible business conduct for companies developing and deploying generative AI technologies.\n\nRather, established expectations of what constitutes responsible business conduct, laid out by the UNGPs and OECD Guidelines, should be the starting point.\n\nThe UNGPs provide the authoritative definition of responsible corporate conduct in relation to business impacts on human rights: the Corporate Responsibility to Respect Human Rights.\n\nTo meet this responsibility, all companies should have in place “policies and processes appropriate to their size and circumstances” including a “human rights due diligence process to identify, prevent, mitigate and account for how they address their impacts on human rights.” (GP 15).",
    "## Risk-Based Approach\n\nTwo features of the UNGPs and OECD Guidelines are particularly pertinent to public policy, regulatory, and voluntary efforts to guide and govern the conduct of companies developing and deploying generative AI technologies:\n\n- **Severity of Risks to People:** A risk-based approach grounded in severity of risks to people.\n\nThis involves considering the scale of an impact, its scope, and its irremediability.\n\nAn impact can be severe overall even if it would only be so in one of these dimensions.\n\n- **Company’s Responsibility for Human Rights Harms:** Accounting for the nature of a company’s involvement with human rights risks and impacts in establishing thresholds of appropriate action.",
    "## Generative AI Value Chain\n\nThe generative AI value chain is vast and includes a growing number of technology companies involved in the development of generative AI.\n\nThis includes foundation model developers, Model hub and MLOps platforms, companies supplying capabilities, and companies, States, and individuals using generative AI across diverse industries and contexts.\n\nUnder the UNGPs, all companies across the value chain have a clearly defined responsibility to prevent and address negative impacts connected with operations, products, or services, wherever they occur in the value chain.",
    "## Involvement Framework\n\nThe UNGPs “involvement framework” provides a principled and pragmatic approach to this value chain wide approach, and sets the basis for determining appropriate action to address risks and impacts identified as part of risk assessments.\n\nThe UNGPs make these distinctions:\n- **Causing Adverse Human Rights Impact:** Where a company causes or may cause an adverse human rights impact, it should cease or prevent the actual impact and provide a remedy to affected individuals.\n\n- **Contributing to Adverse Impact:** Where a company contributes to or may contribute to an adverse impact, it should cease or prevent its contribution and use leverage to mitigate any remaining impacts to the greatest extent possible.\n\n- **Linked Through Business Relationships:** Where a company’s operations, products or services are linked through business relationships to an adverse impact, it does not have a responsibility to provide a remedy since it has not contributed to the harm.",
    "## Recommendations for Aligning Policies\n\nReaffirm and ground policies in States’ existing duty to protect and businesses’ Corporate Responsibility to Respect Human Rights as laid out by the UNGPs and OECD Guidelines.\n\nIntegrate risk-based prioritization based on severity of risks to people as well as the cause, contribution, linkage “Involvement Framework” into legislative texts, technical standards, and guidance.\n\nEstablish multi-stakeholder dialogue to deepen appreciation of what a full value chain approach to addressing human rights risks means in practice.",
    "## Ensuring Effective Access to Remedy\n\nEnsuring effective judicial and non-judicial access to remedy for individuals whose human rights are harmed by the development or deployment of generative AI is vital.\n\nEven where law, company commitments, business processes, and market incentives are aligned towards avoiding business-related human rights harms, some harms still occur.\n\nThe development and deployment of generative AI will not be an exception.",
    "## Establishing a System of Remedies\n\nEstablishing a robust and comprehensive system of remedies for human rights harms connected to generative AI will require focus, determination, investment, ingenuity, and resources.\n\nThe right to an effective remedy for violations of human rights is enshrined in international human rights law.\n\nThe duty of States to provide access to effective remedies for business-related human rights harms, including human rights harms associated with the development and use of digital technologies, is a key aspect of the State Duty to Protect human rights, as laid out in the UNGPs.",
    "## Framework for Delivering Effective Remedies\n\nThe UNGPs provide a pragmatic and compelling framework for delivering effective remedies for human rights harm to affected people and communities.\n\nKey elements include:\n\n- A focus on the need for a range of remedy mechanisms that can respond to all types of human rights risks.\n\n- Explanation of the distinct but complementary roles of different kinds of actors (public and private, including companies) in providing remedy.\n\n- Reinforcement of the foundational role that judicial systems play.",
    "# Remedy Ecosystem and Effective Remedies\n\nSetting out the different forms that effective remedies can take: Restitution, Compensation, Rehabilitation, Satisfaction, and Guarantees of non-repetition.\n\nThese concepts come from international human rights law and prioritize the importance of understanding and taking proper account of the needs and perspectives of affected people and groups in deciding what kind of remedy is needed in different situations.\n\nOffering a practical set of effectiveness criteria to guide the design, evaluation, and improvement of non-judicial approaches to remedy, including those managed by the private sector.",
    "## Recommendations\n\nThe following are near-term priorities for making Access to Remedy a central feature in the governance of generative AI.\n\n- All stakeholders should collaborate to establish processes for understanding the experience and perspectives of impacted or at-risk individuals or groups about what meaningful remedy for generative AI harms means in practice.\n\nThe voice of affected people is a vital, often overlooked, aspect of remedying harms.\n\nProcesses available to these groups to seek remedy often do not work for them, or the remedy delivered is not fully satisfactory when compared to the loss, pain, and suffering experienced.\n\nTo avoid repeating this pattern with regard to harms that flow from the use of generative AI technologies, states, responsible companies, and civil society must find ways to amplify the voice of at-risk individuals in designing an adequate remedy ecosystem.\n\nThis includes both process-related elements, such as understanding which types of remedy processes are most accessible to at-risk or harmed people, and outcome-related elements, such as clarifying what restitution, compensation, rehabilitation, satisfaction, and guarantees of non-repetition should look like in practice.\n\n- States should ensure access to judicial remedies where individuals may have been harmed by the development or deployment of generative AI technologies.\n\nThis could involve ensuring robust enforcement of legal standards that underpin public law remedies of various kinds when harms related to generative AI occur.\n\nDepending on the operation of the regime in question, this could include financial compensation, the criminal sanctions imposed, and binding orders to correct legal breaches and address underlying causes of harm.\n\nStates should also ensure that people are able to enforce their rights directly when these may have been harmed by the development or deployment of generative AI technologies.\n\nThis could occur, for instance, under the law of tort, or under a statutory cause of action.\n\nStates may also then need to make investments and adjustments needed to ensure that people are aware of their rights and that the barriers they face in accessing judicial processes are recognized and addressed.\n\n- States, companies, civil society experts, and affected stakeholders (or legitimate representatives) should work together on how to establish non-judicial routes through which people may seek remedies for specific human rights-related harms connected to generative AI.\n\nThis could include, for instance, raising complaints with regulators about the conduct of technology companies (e.g., with respect to trade practices, anti-competitive behavior, or data-handling); independent complaint and mediation processes led by consumer protection bodies or national human rights institutions; or company-based or collaborative grievance mechanisms.\n\nOf particular promise here is the “national contact point” system established under the OECD Guidelines, which is perhaps one of the most widely established but underutilized forms of accountability for AI systems that currently exist.\n\nIn all cases, special attention should be invested in aligning mechanisms with the UNGPs effective criteria.\n\nAttention to the conduct of companies developing foundation models is especially important because the risk evaluations, decisions, business practices, and disclosures of these companies can help to mitigate harms at an early stage across the generative AI value chain.\n\nThis is not to suggest that these companies are the only actors that have responsibilities; as noted above, all business actors across the generative AI value chain must meet their Corporate Responsibility to Respect Human Rights.\n\nIdentifying good due diligence practices by companies developing foundation models — as well as the challenges and limitations they encounter in the course of this — can deliver considerable benefits.\n\nIn addition to minimizing the severity or likelihood of harms caused by the use of generative AI systems by malicious individuals, non-State actors, or States, good practices implemented by model developers can inform due diligence by application developers and deployers, creating significant efficiencies in risk assessment and mitigation across the generative AI value chain, including establishing sector or use case-specific good practices.\n\nThere are also level-playing field and responsible innovation benefits.\n\nOn the one hand, guidance and tools based on good practices at the foundation model level can aid start-ups seeking to enter the market space, thus enabling responsible competition and innovation.\n\nOn the other hand, deeper multi-stakeholder consensus about what constitutes good practice can, when accompanied by meaningful regulation and incentives, establish a global, level playing field of conduct under which companies should not be allowed to operate.\n\nThe work to clarify good practices has already begun.",
    "The work to clarify good practices has already begun.\n\nThe field of AI risk assessment and management is rich with academic research, civil society reporting, and company policies and processes.\n\nAs laid out in a supplement to this paper, An Overview of Human Rights and Responsible AI Company Practice some of the most prominent technology companies driving generative AI have long had practices focused on identifying and addressing risks to society from artificial intelligence.\n\nMoreover, there is broad alignment between the risk management frameworks and methods being embedded in regulatory proposals or technical standards and the UNGPs.\n\nThe November 2023 OECD Report “Common guideposts to promote interoperability in AI risk management” demonstrates that leading risk management frameworks are generally aligned with the top-level steps (define, assess, treat for risks, and govern risk management) of an “Interoperability Framework” based on the OECD Guidelines.",
    "### Rationale\n\nUnder the UNGPs, companies are expected to conduct human rights due diligence across all of their business activities and relationships.\n\nAs outlined in the B-Tech Addressing Business Model-Related Human Rights Risks foundational paper, this includes addressing situations in which strategic product design/release decisions or business model choices create or increase human rights risks.\n\nBusiness model choices are made and reviewed by the top leadership of an enterprise responsible for strategy.\n\nExecutives and senior managers then work to ensure that these strategic choices are reflected in the company’s operating model and often culture.\n\nWhere this leads to business processes, incentives, or practices that increase risks to workers, communities, or consumers, a tension can arise between a company’s business model and its ability to respect human rights.\n\nThe intent of identifying features of business models and strategies is not to simplistically label some as rights-respecting and others not.\n\nNor should the existence of risks automatically lead to business model adaptation, as this can sometimes create other, more serious risks.\n\nRather, it spotlights that in certain situations, managing negative impacts connected to specific business activities requires active oversight and involvement from boards and executives making business model and strategy decisions.\n\nThis is in contrast to tasking teams to manage these risks at an operational level while making higher-level decisions that could unwittingly undermine those teams’ work.\n\nIn the context of developing and deploying generative AI foundation models, typical features of business models and strategies that companies’ due diligence programs and practices will need to account for can include:\n\n- **Features of foundation models**: Generative AI foundation models are often described as general-purpose technologies meaning that they can be used, misused, and abused in endless ways, many of which may not be immediately foreseeable.\n\nHarms can also arise when these technologies perform in sub-optimal or unexpected ways.\n\nIn addition, the inherent complexity and opacity of generative AI models mean that undesirable outputs are challenging, though by no means impossible, to explain and so fix.\n\n- **Revenue / monetization strategy**: The nature of human rights risks will shift based on the actors that a company targets and supports to use its foundation models.\n\nThese may include developers building applications, enterprise customers in different sectors, public sector organizations, or individuals using consumer interfaces.\n\nThis can be a complex picture, but the risks associated with distinct revenue models should be well-understood by board and executives.\n\n- **Nature and speed of deployment**: It is common practice for technology companies to release products and tools incrementally and iteratively in order to gather feedback from users to inform improvements.\n\nCompanies at the core of generative AI development are taking this same approach, which can focus policy and public attention on how to govern risks of current models and to help prepare to grapple with and address more powerful, future ones.\n\nAs such, some level of risk to society is inevitable.\n\nMoreover, risks may be increased where models may be rushed for release in order to get ahead of or catch up with competitors.\n\n- **The implications of closed, proprietary vs open-source models**: Whether a company pursues an open versus closed source strategy to developing and deploying foundation models can also impact the shape of risks to human rights connected to its products, and how the company can best mitigate risks that flow from end-use.",
    "### Next Steps\n\nAll stakeholders need to be part of creating greater clarity about the appropriate and reasonable oversight role of boards and leadership practices of company founders and executives to address business-model related risks.\n\nThis should start with deliberations involving executives, civil society, regulators, and investors to discuss this issue in more depth, and lead to case studies of good practices focused on:\n\n- Boards identifying as part of initial business model design and strategy — and in any changes to these — the inherent human rights risks that flow from these and ensuring that the company has systems and plans to address these.\n\n- Senior leaders establishing and implementing commitments to release or scale the capability of foundation models in a responsible manner, including evaluating which situations might merit adopting an approach akin to the “precautionary principle.” This is consistent with the notion that, under the UNGPs, severity of actual and potential impacts should inform company action, even if their likelihood is considered low.\n\n- The best ways to establish and sustain corporate cultures that reward the identification of risks and adverse impacts, including by ensuring that individuals feel able to raise concerns without fear of retribution.\n\n- Ensure that the company has in place the right competence, resources, and processes to hear, and act on, the perspectives of especially affected or at-risk stakeholders.",
    "### Rationale\n\nThe UNGPs place a significant emphasis on companies ensuring that human rights due diligence, starting with risk assessment and prioritization, should occur early and on an ongoing basis to enable timely and effective actions to address human rights risks.\n\nIn particular human rights impact assessments should be undertaken “prior to a new activity or relationship; prior to major decisions or changes in the operation (e.g., market entry, product launch, policy change, or wider changes to the business); in response to or anticipation of changes in the operating environment (e.g., rising social tensions); and periodically throughout the life of an activity or relationship.” (GP 17)\n\nHowever, within the typically product-driven, decentralized technology companies and corporate cultures developing generative AI foundation models, there is very little understanding of how to achieve this in practice.\n\nThis lack of clarity has several negative consequences.\n\nFirst, human rights risks may not be robustly assessed at moments in the life-cycle of generative AI foundation models that would allow these risks to be mitigated most effectively.\n\nThis could be the result of uncertainty about which moments make sense for human rights analysis.\n\nEven where clear decision points for safety and ethical review are established, product teams and engineers may struggle to identify human rights risks and effectively prioritize attention to those risks due to lack of tools, training, or access to internal or external human rights expertise.\n\nSecond, the ways in which product-oriented tech companies work can themselves be a “black-box” to external stakeholders, especially those without a background in computer science, software development, and engineering.\n\nThis can make it hard for companies to communicate in an impactful way about risk management efforts that are embedded into existing processes.\n\nIt also reduces the ability for meaningful dialogue about the appropriateness and effectiveness of leveraging product development working methods to identify and assess human rights risks.\n\nThird, methodologies for human rights impact assessments recommended by stakeholders or required in regulations may be too far removed from the pace and iterative nature of developing foundation models.\n\nThis can push human rights risk assessment and management towards solely being a compliance exercise as opposed to a meaningful tool in influencing corporate conduct towards improved management of risks to people and rights-based, responsible innovation.\n\nSome computer scientists and academics have already begun to frame up important aspects of this topic.\n\nThe most extensive literature and guidance for companies focus on the ethical deliberations within agile software development processes.\n\nMuch rarer is attention to how to integrate human rights considerations into these processes.\n\nWhere this focus does exist, some researchers are raising important questions about the limits of relying on the latest manifestations of agile product methods to address issues beyond the design of specific features, such as establishing the overall objectives of technology systems, or software products.",
    "### Next Steps\n\nAll stakeholders need to be part of creating greater clarity about what it means in practice to meaningfully embed the assessment of human rights risks into the development of generative AI foundation models.\n\nThis can start with deliberations involving technologists (inside and outside of companies), civil society, academia, and business and human rights experts focused on:\n\n- Identifying the most impactful moments at which a company should assess the actual and potential human rights impacts that it could become connected to due to the development or deployment of its foundation model.\n\nThis would ideally cover the spectrum from human rights impact assessments of a full model/system to lighter and quicker assessments as features of foundation models are iterated.\n\n- Creating tools, assessment methodologies, and training that support an evaluation of impacts based on the full range of internationally agreed human rights and prioritization of impacts for attention based on their scale, scope, and irremediability.\n\nInstrumental to this will be case studies or hypothetical scenarios detailing the mechanics (who is involved, how long does the assessment take, are external stakeholders consulted, how does it connect/support to other deliberations (e.g., on usability or ethics), what are the pitfalls to avoid, etc.).\n\n- Establishing mechanisms to allow external stakeholders to understand, appreciate, and inform the quality of human rights risk identification and prioritization practices.\n\nThis is distinct from important formal reporting by companies developing foundation models about risks and risk mitigation strategies.",
    "### Rationale\n\nThe companies developing and deploying the most prominent generative AI foundation models have done a great deal to find ways to make those models accurate and safe.\n\nThis paper uses the term “technical mitigations” to denote the varied approaches used by these companies to train, tune and guide the behavior of models.\n\nExamples of these mitigations include prompt filtering and engineering, supervised fine-tuning, Reinforcement Learning with Human Feedback (RLHF), Red Teaming, blocklists, and classifiers.\n\nThese same companies also implement post-deployment monitoring to track the model’s performance over time, detect potential biases, monitor model usage, and ensure model security.\n\nIt is beyond the scope of this paper to assess the technical effectiveness of these mitigation and monitoring approaches.\n\nCompany practice, however, does appear, upon initial review, to be well-aligned with the main process expectations of the UNGPs focused on tracking performance.\n\nThe UNGPs state that “in order to verify whether adverse human rights impacts are being addressed, business enterprises should track the effectiveness of their response” (GP 20).\n\nCompanies publish “system cards” and peer-reviewed research to communicate key information about their models, including the technical mitigations that have been applied, with what results and limitations.\n\nSome also include reference to how, at a high level, internal and external stakeholders have been involved in evaluating mitigations, which is also consistent with UNGPs’ expectations that companies should “draw on feedback from both internal and external sources, including affected stakeholders” (GP 20 b).\n\nHowever, two important expectations of the UNGPs require greater attention.\n\nFirst, the differential effectiveness of mitigations for people in situations of vulnerability needs to be further addressed.\n\nThe UNGPs state that “Business enterprises should make particular efforts to track the effectiveness of their responses to impacts on individuals from groups or populations that may be at heightened risk of vulnerability or marginalization” (GP 20).\n\nIn the context of generative AI foundation models, companies do point to limitations in existing technical mitigations, but apparently without tracking these implications for groups in situations of high risk.\n\nExamples of gaps already document include: sa",
    "# Evaluating Mitigations for Vulnerable Groups\n\nSafety features built in English that may not have the same effectiveness in other languages, the possibility that humans involved in the human feedback aspects of reinforcement learning may be biased or intentionally promote toxicity, and continuing reports that models are still prone to “hallucinations”.\n\nTo be consistent with the UNGPs, companies should be tracking quantitative and qualitative data about the extent to which these limits to mitigations are increasing risks or adverse impacts to vulnerable groups.\n\nFor example: are English trained models likely to result in disproportionate harm in countries where digital literacy is low, or to children whose native languages are not English?\n\nTo what degree are any human biases being replicated within RLHF disproportionately impacting women or ethnic minorities?\n\nSecond, the adverse human rights risks of mitigations themselves need to be evaluated.\n\nUnder the UNGPs, a company’s mitigations are “activities” which need to be assessed for their own human rights impacts.\n\nThis is already a well-understood principle for many companies in the telecommunications and technology sector.\n\nBy way of illustration, efforts by online platforms to prevent sexually exploitative content can also result in the removal of content involving nudity for important cultural, educational, and health related reasons.\n\nFor generative AI, two such examples of mitigations that civil society have identified as carrying human rights risks include watermarking of synthetic media content, and the abuse of human rights of workers within the human feedback phase of RLHF.\n\nFocusing on the extent to which mitigations work for the most at-risk communities should not detract from the critical importance of companies implementing mitigations that have proven to reduce human rights risk for high volumes (at times likely millions) of people.\n\nThe focus should not be on pitting the human rights of the many against the few.\n\nRather, the idea is that more quantitative and qualitative data about differential effectiveness and the human rights risks of mitigation enables diverse stakeholders to navigate dilemmas and find solutions.\n\nLack of such data will likely simply embed the idea that harms to large swathes of humanity are no more than residual, or worst acceptable, risks and costs of generative AI development and deployment.",
    "## Next Steps\n\nAll stakeholders need to be part of creating greater clarity about how to evaluate the effectiveness of technical mitigations for the most at-risk groups.\n\nThis can start with deliberations involving technologists (inside and outside of companies), civil society, affected stakeholders and academia focused on:\n\n- The extent to which existing quantitative methods used by companies to evaluate mitigations can feasibly and responsibly be leveraged to offer insight into differential risks to distinct vulnerable groups.\n\nWhere this is the case, establishing what constitutes good practice, including about how to communicate externally about insights, should be a priority.\n\n- Identifying how qualitative methods can offer feedback loops from affected stakeholders about the effectiveness, and indeed risks of technical mitigations for groups in situations of vulnerability.\n\nWhere applied, these qualitative research methods should draw on good social science practices.\n\n- Innovating collaborations that bring academics and civil society into the evaluation of effectiveness of mitigations but without compromising their independence and safety, or legitimate commercial interests of companies.\n\nThis might mean establishing voluntary but legally enforceable ‘safe harbour’ provisions for all parties.",
    "## Rationale\n\nIt is unlikely that generative AI technologies will be deployed with zero residual risks to people and planet.\n\nBut companies developing foundation models should, consistent with the UNGPs, continue to take action to reduce the severity and likelihood of those risks beyond technical mitigations described above.\n\nThe UNGPs establish that where companies are not causing adverse impacts, but nevertheless connected to those impacts, they are expected to build and use leverage to effect change to mitigate risk or remediate negative human rights impacts.\n\nThis is shown in the “involvement framework” described above.\n\nAs noted previously by B-Tech, leverage can take many forms that commonly fall into the following categories:\n\n- Bilaterally with third parties in the context of commercial relationships.\n\nFor example, via enforcing contractual terms and incentives, or undertaking capacity building.\n\n- With other companies — whether industry peers or companies from other industries.\n\nFor example, through the development of technical standards and associated efforts to incentivise implementation.\n\n- Via partnerships with an institution or actor that can play an effective supporting role in influencing the actions of the third party in question.\n\nThis might include with a home or host State, an international organization or a civil society organization.\n\n- Through multi-stakeholder collaboration — whether in the context of formal initiatives or not, and possibly with the aim of establishing relevant public policy and law.\n\nThe concept of leverage understood in all its forms can galvanise problem-solving and innovation by companies to tackle the root causes of social externalities in their industry or operating contexts.\n\nIn addition, it can serve to minimize the outsourcing of responsibility to entities that lack the will, competence or resources to implement it.\n\nThe notion that companies developing generative AI foundation models should build and use their leverage to address harms that they have contributed to or may be linked to should be reinforced in regulation, technical standards and guidance.\n\nThis should not in any way reduce efforts to implement technical mitigations.\n\nRather, the creative use of leverage by companies must complement these mitigations.",
    "## Next Steps\n\nCompanies developing foundation models and their stakeholders should develop greater clarity about the “state of the art” and “art of the possible” in building and using leverage situations of generative AI deployment where, even after technical mitigations, human rights risks exist.\n\nThe following dimensions should be explored.\n\n- Responsible use policies, terms of use in contracts, guidance and enforcement.\n\nThere is a rich body of practice in this domain which includes companies such as Meta and Google providing guidance and open-source tools for developers to evaluate safety and other features of the applications and products they develop (for example, Meta’s Llama 2 Responsible Use Guide, Google’s Responsible AI Practices, Microsoft’s Responsible Innovation Best Practices Toolkit and responsible AI training modules).\n\nWhen advancing good practices in the context of generative AI, initial points of emphasis could include: Understanding the impact of these policies and practices i.e., in what ways do they make a difference and what can be improved; and how to monitor third party practices without violating the rights of data subjects.\n\n- Know Your Customer assessment and follow-up: Know Your Customer (KYC) refers to the set of guidelines, regulations and practices in the financial services sector to verify the identity, suitability and risks involved in maintaining a business relationship with a private sector or government customer.\n\nMicrosoft, in their report Governing AI: A Blueprint for the Future has already signaled the value and relevance of this idea to address the risks of AI, including generative AI.\n\nThis is particularly interesting because there has also been considerable attention to the use of leverage by financial institutions to address human rights risks connected to their lending and investments: See, for example, Using Leverage to Drive Better Outcomes for People.\n\nWhen advancing good KYC practices in the context of generative AI, initial points of emphasis could include: how to use indicators capable of evaluating customers’ commitment and competence to manage risk and impacts from their own use of the company’s foundation model and products; and strategies and tactics for building and using leverage when a customer is considered to be high risk from a human rights perspective.\n\n- Collective action with peer competitors (including smaller market entrants), value chain companies, civil society and international organizations: It is broadly accepted that the human rights risks connected to the development and deployment of generative AI will require action from a wide range of actors.\n\nAnd many technology companies already engage in collective action to address upstream labor rights risks and downstream privacy, freedom of expression and cyber-security risks.\n\nWhen advancing collective action good practices in the context of generative AI, initial points of emphasis should be: Clarity about the scope of standards and activities being focused on to avoid signaling that some abuses or root causes of abuse are being addressed when they are not; ensuring that civil society and perspectives of affected stakeholders have an equal seat at the table; and targets and accountability measures that go beyond pledges and principles to focus resources on delivering results that will lead to demonstrably better human rights outcomes.\n\n- Leverage for remedy: Under the UNGPs, companies have a responsibility to provide for or cooperate in remedy processes where they have caused or contributed to the harm; they may also take a role in enabling remedy where they are linked to the harm, which can be an effective means of reducing risks of their continuation or recurrence.\n\nThere may also be practical, reputational and social license reasons for companies to contribute to a well-functioning eco-system of avenues for remedies.\n\nBuilding on efforts to advance responsible investment by financial institutions, the use of leverage for remedy by companies developing foundation models could be explored at two levels:\n  - Focusing on customers’ “preparedness for remedy” as part of KYC.\n\nIn practice, this could involve asking questions to assess the effectiveness of grievance mechanisms that customers put in place and providing proactive support to strengthen customers’ or industry-level mechanisms.\n\n- “Enabling remedy” in specific cases.\n\nIn practice, this could involve executives using their influence to bring greater focus to conversations about remedy when severe impacts occur; supporting fact-finding in support of affected stakeholders and customers when impacts are alleged to have occurred, but the facts are disputed; and even contributing financial or other resources to bolster remedy packages.",
    "## Rationale\n\nThe UNGPs strongly emphasize that companies should engage with affected stakeholders or credible proxies and expert stakeholders as part of assessing, mitigating and remediating adverse impacts on human rights that they are, or may become, connected to.\n\nThis is because these stakeholders typically have a strong understanding - many of them through lived experience - of the interplay between business operations, value chains, products and services and human rights impacts.\n\nThe UNGPs state that “To enable business enterprises to assess their human rights impacts accurately, they should seek to understand the concerns of potentially affected stakeholders by consulting them directly in a manner that takes into account language and other potential barriers to effective engagement.\n\nIn situations where such consultation is not possible, business enterprises should consider reasonable alternatives such as consulting credible, independent expert resources, including human rights defenders and others from civil society” (GP 18).\n\nMeaningful engagement by foundation model developers with the perspectives of affected and expert stakeholders about the human rights risks and impacts connected to the use of these models, or generative AI technologies in general, can improve the quality and credibility of a company’s risks assessments.\n\nThis is especially true given the challenges of fully predicting the ways in which these models will behave or be used and misused post-deployment.\n\nAffected stakeholders, as distinct from deployers, may prove to be the most reliable source of information about persistent or emerging harms.\n\nMoreover, robust engagement that authentically identifies and addresses human rights-related concerns can help to establish or sustain the social license of generative AI technologies.\n\nThe UNGPs affirm that engagement with stakeholders should not stop at risk assessment, but instead take place across all phases of human rights due diligence and as part of remedying harms.\n\nIn this way, engagement with affected and expert civil society stakeholders can inform model design, risk mitigation and deployment decisions towards mitigating risks to human rights, as well as strategies to ensure victims of harm have access to remedy when harms occur.\n\nThis ethos, which has been expressed by some as design from the margins in the context of social media is arguably the most promising pathway to the proliferation of generative AI grounded in dignity and equality for all.\n\nSeveral organisations have already elaborated guidance for stakeholder engagement around AI systems that offer a good starting point for advancing good practice by foundation model developers.\n\nFor example:\n\n- The NIST AI Risk Management Playbook (Govern 5.1) lays out suggested actions to ensure robust engagement with relevant AI actors, which includes affected stakeholders.\n\nThe playbook emphasizes that participatory stakeholder engagement: assist in identify emergent scenarios and risks in certain AI applications; is best carried out from the very beginning of AI system commissioning through the end of the lifecycle; and is best carried out by personnel with expertise in participatory practices, qualitative methods, and translation of contextual feedback for technical audiences.\n\n- European Centre for Not-For-Profit Law’s Framework for Meaningful Engagement as part of assessing human rights impact for AI systems focus on the importance of Shared Purpose beyond the interest of the convening organisation; Trustworthy Processes that are inclusive, open, fair and respectful and delivered with integrity and competence; and Visible Impact i.e., that involvement can make a significant contribution to decision-making, or makes changes to the governance of the organisation, product or service to align it with the public interest.\n\n- Data and Society’s Democratizing AI: Principles for Meaningful Public Participation provides recommendations concerning, among other things: Early-stage public participation to ensure that decision- makers do not become wedded to a preconceived decision before receiving public input; the need for equity and social justice commitments to guide every aspect of participation; the design of participation methods for high-quality engagement; and the need to build the technical capacity of communities while also acknowledging that “Affected people do not need to know how to build an algorithm to have an opinion on how automated decision-making systems should (or should not) affect their lives.”\n\nIn the context of generative AI foundation models, their general-purpose nature presents challenges related to the massive numbers, diversity and geographic location of potentially affected stakeholders that may need to be engaged.",
    "On these points, B-Tech Improving Stakeholder Engagement in Tech Company Due Diligence affirms that “technology companies should not interpret the expectation of the UNGPs as meaning that they must engage with every one of the many thousands, even multiple millions, of stakeholders potentially impacted by the use of the company’s products and services.\n\nRather, tech companies should seek to hear from a representative mix of stakeholders, with resources prioritized to where risks to human rights are most severe”.\n\nB-Tech has also previously called attention to the importance of engagement with expert and affected stakeholders outside of North America and Western Europe.\n\nFor some companies, membership in robust multistakeholder initiatives, such as GNI has proved a useful way to connect with local civil society and affected groups, as well as the critical work of NGOs with specific focus on digital rights in their region, and international NGOs working to support with capacity building and guidance to these organizations.\n\nBroad-based public participation represents another possible avenue for engagement with affected stakeholders.\n\nFor example, Anthropic and Open AI have partnered with The Collective Intelligence Project to pilot “Alignment Assemblies” aimed at shaping the trajectory of generative AI deployment in society.\n\nThe project founders also aspire to experiment with other modes of engagement, citing federated citizens’ assemblies, retroactive funding processes for writing better mode evaluations and public red-teaming.\n\nAnother exemplar innovation from the wider AI domain is the Ada Lovelace Institute’s Citizens’ Biometrics Council which brings together 50 members of the UK public to deliberate on the use of biometrics technologies like facial recognition.\n\nFinally, all individuals have political and public participation rights that “play a crucial role in the promotion of democratic governance, the rule of law, social inclusion and economic development, as well as in the advancement of all human rights”.\n\nThis is a reminder that formalized, legally mandated mechanisms for ensuring that the voice and interests of specific affected stakeholders are represented in the development of generative AI foundation models should also have a role to play.\n\nThis could, for example, take the form of regulatory oversight of public participation forums, or establishing modes for specific groups such as data enrichment workers or artists to exercise their collective bargaining rights.",
    "## Next Steps\n\nEstablishing when, how and under what conditions companies developing generative AI foundation models can most meaningfully ensure engagement with at-risk or impacted stakeholders and civil society organizations requires attention from business, civil society and regulators.\n\nInitial points of focus could be on:\n\n- Companies developing foundation models establishing the necessary internal commitment, capacity and culture to engage with affected stakeholders and civil society representatives across all phases of the AI development lifecycle.\n\n- The meaningful integration of affected stakeholder perspectives within industry-led responsible generative AI collaboration, with particular attention to removing logistical barriers to participation, diversity among participants and investing in the technical capacity of communities to engage.\n\n- Companies developing foundation models using “leverage for engagement” by taking a proactive role in advocating for more formalized mechanisms, and possibly funding options, for at-risk stakeholders to convene and advocate for their rights with relevant actors across the generative AI value chain.",
    "# Insights and Recommendations\n\nThe insights and recommendations laid out in this paper and supporting supplements from the first phase of the B-Tech Generative AI project have been released to support multi-stakeholder dialogue and collaboration that advances UNGPs-consistent public policy, regulation and business practice.\n\nThe findings, and responses to them, will inform B-Tech ongoing work on generative AI in 2024.\n\nUN Human Rights invites engagement from all stakeholders as we move into the second phase of this B-Tech initiative.\n\nPlease contact us if you would like to engage with our work, including if you have recommendations for practical tools, case studies and guidance that will advance company, investor and State implementation of the UN Guiding Principles on Business and Human Rights in the context of Generative AI development and deployment.",
    "# Acknowledgements\n\nThe UN B-Tech team expresses thanks to all the experts and stakeholders that provided input into this foundational paper such as representatives from the OECD Centre for Responsible Business Conduct, the Global Network Initiative, BSR and Shift.\n\nThe team is especially appreciative to Mark Hodge, Vice President of Shift, the lead author of this paper.",
    "# IMPACT, OPPORTUNITY AND CHALLENGES OF GENERATIVE AI\n\nA MEITY, NEGD & NASSCOM INITIATIVE\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\nYou are free to: Share — copy and redistribute the material in any medium or format Adapt — remix, transform, and build upon the material for any purpose, even commercially.\n\nUnder the following terms: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made.\n\nYou may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nNo additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.\n\nAuthored by Humans Lead Author: Jibu Elias Co-Author: Anjali Raja Research: Nibedita Saha & Dr Nivash Jeevanandam Illustrations: Generated by Tapan Aslot\n\nSri Narendra Modi Honourable Prime Minister of India\n\n“Artificial intelligence is a tribute to human intellectual power, the power to think enabled humans to make tools and technologies.\n\nToday, these tools and technologies have also acquired the power to learn and think.\n\nIn this, one key technology is AI.\n\nThe teamwork of AI with humans can do wonders for our planet.” ¹\n\n1.\n\nFrom the inaugural speech of RAISE 2020 Summit",
    "# Introduction\n\nArtificial Intelligence, both as an area of research and technology, has existed for more than half a century with varying levels of progress and success.\n\nThe origin of machine learning and deep learning, fuelled by vast data pools and advancements in semiconductors and the internet, brought the most significant leap in the 2000s, giving us fascinating AI programs such as AlphaGo and Alpha Fold.\n\nHowever, the advent of transformer architecture in 2018 and the subsequent boom of Generative AI models, huge language models such as GPT, has finally brought AI into the mainstream in recent months.\n\nWith the growing popularity of Generative AI tools such as Chat GPT and Midjourney, and tech companies such as Microsoft and Google in a race to incorporate these models into their search and enterprise offerings, we have reached a point where AI, especially Generative AI, is causing a significant influence in our day to day lives and beyond.\n\nConsidering this growing influence of Generative AI, INDIAai - the National AI Portal of the Government of India, conducted numerous research and held three roundtables that featured some of the prominent voices in Generative AI, AI Policy, AI Governance and Ethics, and academia to analyze the impact, ethical and regulatory questions, and opportunities it brings to India.\n\nThis report is the outcome of that process and is intended to be used by policymakers, entrepreneurs, practitioners, and students.",
    "# INDIAai Generative AI Roundtables\n\nIn the last few years, Generative AI technologies have exploded, ranging from sophisticated language models such as GPT-3 and image generation models such as DALLE-2.\n\nWith the investment in AI to reach $422.37 billion by 2028, at a 39.4 % CAGR, experts believe Generative AI will play a critical role in pushing AI innovation and investment in the coming decade.\n\nGenerative AI tools, primarily focused on low-cost and higher-value solutions, are perceived as the future of text, image and even code generation, sometimes indistinguishable from human creation.\n\nHowever, with the pace at which these technologies advance, we need to foresee the risks it brings.\n\nOn the one hand, countries such as China have implemented regulatory measures to put a leash on Generative AI risks, such as watermarking the final product, auditing the algorithms, and even ensuring the consent of users are taken before using their data for training these models.\n\nOn the other hand, the socio-economic impact that Generative AI models can create is broad and deep and can even threaten our country’s unity and national fabric, leading to the question of how we can ensure the ethical and responsible use of Generative AI models.\n\nFurthermore, the question of rights, attribution and IP of AI-generated works that mimic specific human creators opens more ethical questions.\n\nWith these questions in mind, INDIAai organised a series of roundtables with stakeholders and ecosystem players to understand the present Generative AI landscape, ethical and legal challenges, and solutions to mitigate the harmful impact these models can have in our society.\n\nThe first roundtable on 31st Jan 2023 was a big success as it explored the current state of Generative AI and analysed recent hype around the subject.\n\nThe first roundtable on 31st Jan 2023 was a big success as it explored the current state of Generative AI and analysed recent hype around the subject.\n\nThe participants were:\n\nThe second roundtable was organised on 28th Feb 2023, which looked deeper into the ethical questions regarding Generative AI and the need for regulatory frameworks and how to formulate a policy approach.\n\nThe participants were:\n\nThe third roundtable was held on 24th Apr 2023, and it focused on the India opportunity in Generative AI, especially the economic impact and how this technology could transform various sectors and parts of the economy.\n\nThe participants were:\n\nThis report contains essential learnings, findings, analysis, and opinions from these roundtables.",
    "# What is Generative AI?\n\nGenerative Artificial Intelligence (GAI) describes algorithms (such as ChatGPT, Midjourney, Bard, DALL E, etc.)\n\nthat can be used to create new content, including audio, code, images, text, simulations, and videos.\n\nGenerative AI models use neural networks to identify the patterns and structures within existing data to generate new and original content.\n\nIn recent years, large-scale models have become increasingly important in the AI-generated content space, providing better intent extraction and, thus, improved generation results.\n\nFurthermore, with the growth of data and the models’ size, the distribution that these models can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation.\n\nContrary to popular belief, Generative AI (GAI) models have existed as a technology since the early days of AI.\n\nThe history of Generative AI models can be traced back to the 1950s with the development of Hidden Markov Models (HMMs) and Gaussian Mixture Models (GMMs), as they generated sequential data such as speech and time series.\n\nWith the advent of Deep Learning, Generative Models saw significant performance improvements.\n\nThe current boom of Generative AI has its origins rooted deeply in advance of Natural Language Processing (NLP), which is a subfield within AI which focuses on how computers process and analyse large amounts of natural language data.\n\nIn NLP, the traditional method to generate sentences is to learn word distribution using N-gram language modelling and then search for the best sequence.\n\nThis modelling can be used for disambiguating the input.\n\nIn addition, they can be used for selecting a probable solution.\n\nThis modelling depends on the theory of probability.\n\nProbability is to predict how likely something will occur.\n\nModels that assign probabilities to sequences of words are called Language Models or LMs.\n\nHowever, this method cannot effectively adapt to long sentences, and this issue was tackled by introducing Recurring Neural Networks for language modelling tasks.\n\nThe advent of generative AI models in various domains has followed different paths, but eventually, the intersection emerged due to the Transformer Architecture.\n\nIntroduced for NLP tasks in 2017, a Transformer Model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence.\n\nTransformer models apply an evolving set of mathematical techniques, called attention or self-attention, to detect subtle ways even distant data elements in a series influence and depend on each other.\n\nTransformer was later applied in computer vision and then became the dominant backbone for many generative models in various domains.",
    "## Foundation models\n\nProposed to solve the limitations of traditional models such as RNNs, Transformer is the backbone architecture for many state-of-the-art models, such as GPT-3, DALL-E-2, Codex, and Gopher.\n\nSince the introduction of Transformer Architecture, pre-trained language models have become the dominant choice in NLP due to parallelism and learning capabilities.\n\nGenerally, these transformer-based pre-trained language models can be commonly classified into two types based on their training tasks: Autoregressive language modelling and masked language modelling.\n\nFurthermore, despite being trained on large-scale data, the AIGC may not always produce output that aligns with the user’s intent.\n\nTo overcome this issue, reinforcement learning from human feedback (RLHF) has been applied to fine-tune models in various applications.\n\nDeveloping computing with enhanced hardware, distributed training, and cloud computing contributed to the development of the foundation model.",
    "## A comprehensive Survey of AI-Generated Content (AIGC)\n\nGenerative language models (GLMs) are unimodal models trained to generate readable human language based on patterns and structures in input data they have been exposed to.\n\nThese models can be used for a wide range of NLP tasks, such as dialogue systems, translation and question-answering.\n\nThese include decoder models and encoder-decoder models.\n\nVision generative models are other kinds of unimodal.\n\nMultimodal generations are relatively hard to learn compared to unimodal.\n\nThe generation of state-of-the-art multimodal in vision language generation, text audio generation, text graph generation and text code generation aided in tackling this issue.\n\nThe application of these architectures can be seen in Chatbots, AI art generation, music generation, coding for AI-based programming systems, and education.",
    "# Popular Generative AI Models\n\nLarge Language Models Large Language Models (LLM) are neural networks that analyse and comprehend natural language.\n\nCommonly trained on large datasets, they can be utilised for tasks including text generation, classification, question response, and machine translation.\n\nLarge language models process vast volumes of text data to approximate human speech.\n\nThe LLM uses a deep learning model, a network of interconnected neurons, to process, analyse, and forecast complex data to produce these natural language responses.\n\nToday, many tasks in NLP, such as speech-to-text and sentiment analysis, rely on language models as their basis.\n\nThese models can analyse a text and guess what word will come next.\n\nFor example, the parameters of an LLM let it make predictions about the likelihood of word sequences by considering the text’s relationships.\n\nThe model can capture complex relationships and handle unusual words with more parameters.",
    "### BERT\n\nBERT (Bidirectional Encoder Representations from Transformers) is a family of masked-language models developed by Google researchers in 2018.\n\nAccording to a 2020 literature review, “in little more than a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments, counting over 150 research publications analysing and improving the model.” BERT was first implemented in English with two model sizes: BERTBASE (12 encoders with 12 bidirectional self-attention heads totalling 110 million parameters) BERT LARGE (24 encoders with 16 bidirectional self-attention heads totalling 340 million parameters) BERT is a free and open-source NLP machine learning framework.\n\nThe purpose of BERT is to provide context to help computers interpret ambiguous words in the text.",
    "### GPT-3\n\nGPT-3 (Generative Pre-trained Transformer 3) is a language model developed by OpenAI, a San Francisco-based artificial intelligence research laboratory.\n\nThe 175-billion-parameter deep learning model can produce text that resembles human language and was trained on large text datasets containing hundreds of billions of words.\n\nGPT-3 employs deep learning to generate human-like text.\n\nIt will generate text that continues the prompt when provided a prompt.",
    "### LaMDA\n\nGoogle’s LaMDA (Language Model for Dialogue Applications) is a collection of conversational large language models.\n\nInitially developed and released as Meena in 2020, the first-generation LaMDA was announced during the Google I/O keynote in 2021, followed by the second generation the following year.\n\nLaMDA attracted significant attention in June 2022 after Google employee Blake Lemoine claimed that the chatbot had grown sentient.",
    "### BLOOM\n\nBigScience The huge Open-science, Open-access Multilingual Language Model (BLOOM) is a large language model based on transformers.\n\nOver 1000 AI researchers collaborated to construct a free big language model for anyone who wants to explore it.\n\nIt is considered an alternative to OpenAI’s GPT-3, which has 176 billion parameters and was trained on about 366 billion tokens from March to July 2022.\n\nBLOOM employs a modified Megatron-LM GPT-2 decoder-only transformer model design.",
    "### GPT-4\n\nOpenAI’s Generative Pre-trained Transformer 4 (GPT-4) is a multimodal big language model and the fourth in its “GPT-n” series of GPT foundation models.\n\nIt was published on March 14, 2023, and is now publicly available in a limited form through the chatbot product ChatGPT Plus (a premium version of ChatGPT), with access to the GPT-4-based version of OpenAI’s API available through a waitlist.",
    "### Google - Bard\n\nGoogle introduced Bard, a LaMDA-powered conversational generative artificial intelligence chatbot.\n\nIt is intended to function similarly to ChatGPT, with the main distinction being that Google’s service will obtain its data from the internet.\n\nGoogle’s Bard is a conversational generative artificial intelligence chatbot based on the LaMDA family of large language models (LLMs) and later the PaLM LLM.\n\nIt was created in direct response to the rise of OpenAI’s ChatGPT, and it was initially published in a restricted capacity in March 2023 to lukewarm reviews before moving to other nations.\n\nFurthermore, Bard, like most AI chatbots, can code, solve maths questions, and assist you with your writing needs.",
    "# Current state of Generative AI\n\nWith the launch of ChatGPT, Generative AI has taken over the world by storm.\n\nAccording to NVIDIA Founder and CEO Jensen Huang, Generative AI has finally caused the inflexion point in AI.\n\nHe calls it the iPhone moment for AI as a new computing platform has been developed and emerged.\n\n“This is a new computing model that you program in a new way, and this new way is using human language,” stated Huang.\n\n“The first computer you can program in any language you like; English, Chinese, French, Japanese, however you like to do it,” he noted.\n\nThe open-to-public version of ChatGPT gained 100 million users in just under two months, while Stability AI’s Stable Diffusion, which can generate images based on text descriptions, garnered more than 30,000 stars on GitHub within 90 days of its release.\n\nToday, many believe that ChatGPT has the potential to change the entire business stream.",
    "## Public perception of AI\n\nWhile some opine generative AI as a threat rather than a boon because of the lack of awareness of the actual capability of the tool, others are extensively using it and gaining a clear understanding of what the tool can accomplish.\n\n“A significant number of people are still looking at AI as a threat.\n\nThere is generally low acceptance with regard to AI as people are still relating AI with Hollywood movies they might have watched, in which it is often portrayed as how Ai is going to take over the world.\n\nThere is a dire need to educate people,” said Generative AI artist Tapan Aslot at the first INDIAai Generative AI Roundtable.\n\nLack of basic understanding of AI is yet another major issue.\n\n“The public is lost in the complexity of the field.\n\nexplaining AI without hiding its complex nature is essential, said Emmanuel Goffi, Founder and Director of Global AI Ethics Institute.”Some people do not even have access to 4G in countries like India and Africa.\n\nTheir knowledge of Chat GPT and Generative AI is limited.\n\nThis clarity will lead to inclusivity and the maintenance of balance,” he added.",
    "## Is Generative AI ‘creative’ or ‘generative’?\n\nThe emergence of creative tools such as Midjourney and DALL-E has opened the debate regarding the “creativity” of these tools.\n\nAccording to Prof Chandra Sekhar of IISc Bangalore, these models are only ‘generative’ as they derive results from existing material and are not creative.\n\nIn his opinion, creativity is still unique to humans as natural intelligence would be hard to replicate.\n\n“AI is not completely well founded.\n\nThat is the actual threat,” Prof Chandra Sekhar noted.\n\nFurthermore, self-learning is still a dream for the models.\n\nBut, for now, AI lacks thought; the initial input must still be from the human,” he stated in the first roundtable.\n\nAbhinav Aggarwal, whose company Fluid AI recently released a book authored by AI, pointed out that even the “creative” humans are also dependent on other reference materials before creating a particular model.\n\n“Why is it that when a human does it, it is creative and generative when an AI does it?” he asks.\n\nTo elaborate on this, he added that till now, AI has been able to generate something based on the information being fed to it.\n\nBut being creative is something which is unique, on which humans have a clear advantage.\n\nHe pointed out that one must distinguish between inference-based and memory-based logic.\n\nAt present, AI systems are working on inference-based logic, but a decade later, it will be functioning on memory-based logic.\n\nWhen an AI system learns from data, it is dubbed as non-creative, whereas if a similar thing is done by humans, it is termed as creative.\n\nHence, it is either that the definition of creativity is blurred or we term one to be less creative than the other.\n\nAccording to the first INDIAai Generative AI Roundtable, at a finer level, one needs to distinguish between creative and generative AI.\n\nWhile at present, the difference is blurred, and both are used interchangeably, a decade down the line, this difference will amplify.\n\nDefining creativity in an era of ChatGPT is a task in itself.",
    "# Future of jobs\n\nThe fear of being replaced by machines in the workspace has always been a major concern whenever a new technology arises.\n\nThe recent boon in Generative AI has just amplified this concern.\n\nAccording to many reports, Generative AI could impact up to 300 million jobs globally.\n\nGenerative AI could substitute up to 25% of current work in the US and 24% in Europe while complementing most of the remaining work.\n\nJyoti Joshi, Founder and CEO of Generative AI startup Kroop AI, pointed out that, though there are concerns about AI replacing humans, in the present stage, human intervention is a necessity.\n\n“Correct innovation is the key to the correct output”, she added.\n\nDespite AI bringing in a lot of automation in many processes, the technology will still need to replace jobs.\n\nIt will add to new kinds of job profiles and lead to job efficiency, concluding the first INDIAai Generative AI Roundtable.\n\nThe insights from the roundtable further backed Goldman Sachs findings that should generative AI reach its full potential and capabilities, the labour market might experience quite an upheaval.\n\nBy analysing data on occupational tasks in both the United States and Europe, the report came to the conclusion that approximately two-thirds of existing jobs are susceptible to various degrees of AI automation.\n\nNot only that, Generative AI has the potential to replace as much as one-fourth of the current workload.\n\nAlternatively, Generative AI has the potential to automate around 18% of the total workforce.\n\nAlso, an independent study on GPTs and their impact on labour market points that 80% of the US workforce might have at least 10% of their tasks affected by such large language models.\n\nWith access to LLMs, 15% of all other work tasks could be expedited while maintaining the same quality level.\n\nGiven below are representations of job growth and job loss and how Generative AI will transform work across industries.",
    "## Total job growth and job loss\n\nFuture of Jobs Total job growth and loss\n\n23% of today’s job will change\n\nOne million Lost jobs Stable jobs New jobs\n\nSource: World Economic Forum, Future of Jobs Report 2023 Generative AI will transform work across industries\n\nBanking Insurance Software & Platforms Capital Markets Energy Communications & Media Retail Industry Average Health Public Service Aerospace & Defence Automotive High Tech Travel Utilities Life Sciences\n\n28% 30% 26% 30% 26% 38% 27% 25%\n\n11% 9% 13% 6% 8% 6% 6% 8% \n\n20% 13% 16% 15% 15% 17%\n\n33% 35% \n\n41% 50% 50% 50% 52% 50% \n\n27% 26%\n\nWork time distribution by industry and potential AI impact Based on their employment levels in the US in 2021 Higher potential for automation Higher potential for augmentation Lower potential for augmentation and automation Non-language tasks\n\nSource: World Economic Forum, May 2023.\n\nThese are the jobs most likely to be lost - and created - because of AI\n\nHowever, on the brighter side, AI has the potential to create jobs as well.\n\nThe World Economic Forum predicts by 2027, there will be a significant increase in the number of AI and machine learning specialists by 40%.\n\nIn other words, 1 million jobs are expected to be created with the increase in usage of AI and machine learning across all industry verticals.\n\nAdditionally roles such as data analysts, data scientists, big data specialists and information security analysts are also expected to experience an increase of 30-35% and 31% respectively.\n\nThese developments are anticipated to generate an additional 2.6 million jobs.\n\nAnother skill that will gain much prominence in the future is prompt engineering.\n\nIt is a comprehensive process encompassing the entire interaction cycle between humans and AI.\n\nPrompt engineering aids in smooth, clear and efficient human-AI interaction.\n\nThe field is expected to flourish and continue to evolve and develop as new techniques and demand for advanced AI systems increase over time.",
    "## Top 10 fastest growing jobs\n\n- AI and Machine Learning Specialists\n- Sustainability Specialists\n- Business Intelligence Analysts\n- Information Security Analysts\n- Fintech Engineers\n- Data Analysts and Scientists\n- Robotics Engineers\n- Electrotechnology Engineers\n- Agricultural Equipment Operators\n- Digital Transformation Specialists\n\nSource: World Economic Forum, Future of Jobs Report 2023",
    "# Impact, Opportunity, and Challenges of Generative AI\n\nGenerative AI is expected to increase from USD 11.3 billion in 2023 to USD 51.8 billion by 2028.\n\nThe CAGR is expected to touch 36% between the forecasted period.\n\n“There are endless opportunities for generative AI in India.\n\nStarting from big companies making chatbots to students using ChatGPT for writing academic papers.\n\nEven the older generation is curious about ChatGPT,” pointed out Pavankumar Dubagunta, Speech Scientist at Uniphore Software Systems.\n\nAccording to Goldman Sachs estimates, Generative AI could raise global GDP by 7% (USD 7 trillion) and increase productivity by 1.5% in the next decade.\n\nPwC estimates that the openness to tap into the power of Generative AI will likely only continue to grow with an estimated USD 15.7 trillion of potential contribution to the global economy by 2030.\n\nAccording to Jaspreet Bindra, in technology as fundamental and transformative as generative AI, there would be both sides.\n\n“In India as a country, I see five possibilities.\n\nFirstly, it has the potential to make everyone a 10x engineer, which will be a benefit in the technical space.\n\nSecondly, with open APIs and the cloud, it provides greater possibilities for entrepreneurship.\n\nThirdly it will benefit service companies.\n\nFourthly if we do it right, AI provides huge possibilities for the creative industry, and India has a massive creative industry.\n\nAnd finally, India stat can be leveraged with the abundant amount of data.\n\nWith AI in the market, we now have a tool to leverage this data”, he added.\n\nThere are multiple opportunities across the generative AI value chain, with much of the opportunity in its applications and services.",
    "### Impact, Opportunity, and Challenges of Generative AI\n\nThere are opportunities across the generative AI value chain, but the most significant is building end-user applications.\n\nGenerative AI value chain Opportunity size for new entrants in next 3-5 years, scale of 1-5\n\n- Services\n- Applications\n- Model hubs and MLOps\n- Foundation models\n- Cloud platforms\n- Computer hardware\n\nSource: McKinsey article Exploring opportunities in the generative AI value chain",
    "### Generative AI tools from India\n\nWhen it comes to talent, presently, much of the AI talent works in the US, India, UK 60% of the Generative AI talent pool comes from the US, India, UK, Germany, Canada and France.\n\nYet, in India, there is a rush to find talent in AI despite around 416,000 people working in AI and Data Science.\n\nAt present, India still faces a demand of approximately 629,000 AI talent.\n\nAccording to NASSCOM, there is still an additional demand for One million workers in the space, which is expected to increase exponentially in the coming years.",
    "### Bridging the digital divide\n\nAccording to Jensen Huang, Generative AI can positively impact countries like India and is one of the most fantastic opportunities we have ever had to close and bring together the social and technology divide.\n\nFor the last 30-40 years, only so many people know how to program a computer.\n\nSo the number of people who know how to use this incredible instrument for the benefit of themselves or their business or their country is really quite limited.\n\n\"And yet, all of a sudden, there’s a new type of computer, this new type of computer, you don’t have to learn C, C++, you don’t have to learn Pascal, you don’t have to learn Fortran, you don’t have to learn Java.\n\nYou don’t even have to learn Python.\n\nYou just have to speak your language.\n\nAnd by communicating to this computer what you need, what you want, what problems you want to be solved, this computer will write the software by itself.\"\n\nHe theorises that everyone is a computer programmer now.\n\n\"This is going to have the greatest opportunity for us to democratise this very powerful instrument we call the computer for the very first time in history,\" he said.\n\n\"I believe it will lift so many segments of society.\n\nIt will bring great education to people who don’t have access to education.\n\nIt’s the most powerful democratisation force I’ve ever seen,\" he concluded.\n\nWhen it comes to bridging divides, linguistic barriers have plagued the country for many decades, and Generative AI tools built alongside the Government of India’s Bhashini program can be a valuable asset in the future.\n\n\"Creating content in multiple languages is an issue for India.\n\nThis shortcoming can be tackled with Generative AI,\" says Harsha Mundhada of Inflexor Venture.\n\nThe language translation and other Generative AI capabilities will make India’s apex judicial bodies more accessible to people across the country, she added.\n\nFurthermore, many believe that Generative AI tools are a blessing for many people with disabilities.\n\nAccording to Pavankumar Dubagunta, Speech Scientist, Uniphore Software Systems, \"people who have deficiencies in speaking and autism can leverage technologies like AI, and I think it will be a life-changing experience for them.\"",
    "### Generative AI’s impact on different sectors\n\nThe economic opportunities brought about by generative AI are vast and diverse.\n\nIt empowers industries to leverage data-driven insights, enhance productivity, and drive innovation.\n\nAs technology advances, we can expect further transformations across sectors, paving the way for a more efficient, personalised, and technologically advanced future.\n\nSome of the significant impact sectors for Generative AI include healthcare, fintech and education, with the education sector already being transformed into Generative AI.\n\nGenerative AI can play the role of personalised teachers in the education sector.\n\nIt can provide customised modules and classes for students based on their aptitude.\n\n\"AI is becoming the supportive tutor providing personalised teaching plans for students.\n\nI see applications which are catering towards that, be it in one-on-one skill development training to something as simple as fluency improvements,\" said Neethu Mariam Joy, who is currently working on a new Generative AI startup that focuses on EdTech.\n\n\"Not just in EdTech, in healthcare, it is simple to build a query-based system where AI takes the role of proxy doctors.\n\nMost of the applications are bringing personalization into whichever sector you are applying it into,\" she added.\n\nGenerative AI could revolutionise healthcare by helping doctors analyse medical data, diagnose patients, and customise treatment strategies.\n\nRecently, researchers from Drexel University’s School of Biomedical Engineering, Science and Health Systems recently demonstrated that Open AI’s GPT-3 program could identify clues from spontaneous speech that are 80% accurate in predicting early stages of dementia.\n\nSince there is still no cure for the disease, diagnosing it early can give patients more options for therapeutics and support.\n\nThe current practice for diagnosis involves medical history review and a lengthy set of physical and neurological evaluations and tests.",
    "### Next frontiers for Generative AI\n\nAccording to experts, one of the sectors expected to be disrupted by Generative AI in coming years is Fintech.\n\nOne of the sectors which will face a huge impact due to Generative AI will be FinTech, says Aveekshith Bushan, Vice President and General Manager for APJ at Aerospike.\n\n\"If there is fraud in the FinTech sector, it will affect thousands of users,\" he added.\n\nHe believes that at some point, someone will find a way to break into the security protocols followed currently.\n\nUnless and until the FinTech sector finds the means to tackle this possibility, the probability of sectoral disruption is strong.\n\nAccording to Jaspreet Bindra, \"The sector which would be disrupted is the one which created it- the tech (IT) sector,\" \"Generative AI has been writing codes involving basic process automation and many more.\n\nI recently read a tweet by a programmer who used ChatGPT for the first time for writing codes.\n\nHe said the value of 90% of his skills had gone down to zero dollars,\" he added.",
    "### Challenges\n\nWhile generative artificial intelligence (AI) holds tremendous potential and has made significant advancements in various fields, it also presents unique challenges.\n\nThe complexity and power of generative AI algorithms can give rise to ethical, legal, and societal concerns.\n\nAccording to Kazim Rizvi, Founding Director of a tech policy think tank called The Dialogue, \"Generative AI has inspired many to gain first-hand experience with AI in recent times.\n\nHowever, an opaqueness in data collection, developers of the models, and biases being fed into the system still exists.\n\nThis ambiguity will continue to remain even if the technology is evolved.\"\n\nOne of the major concerns that Generative AI possesses at present is its potential to amplify the spread of misinformation that can have varying impacts, especially when these language models can often produce outputs that are not real.\n\nAccording to researchers, this phenomenon of AI generating confident responses that do not seem to be real or justified by its training data is called hallucination.\n\n\"In markets like India, content can go out and spread quickly,\" says Aveekshith Bushan.\n\n\"There could be biases and misinformation.\n\nWe have already seen it on the other side of ChatGPT, where the output is irrelevant to the content searched.\n\nPeople will have to decipher and say what looks relevant and what does not,\" he added.\n\nFor many years, we have seen Generative AI techniques, such as deep fakes, being used to bring social and political unrest across the globe.\n\nThe recent developments in Generative AI technology can make these kinds of threats more prevalent.\n\n\"It was challenging to analyze the impact of the previous AI revolution, which was algorithms and information shared.\n\nIt is also complicated to foresee the implications of deep fakes.\n\nThis is a threat to the democracies of the world which is hard to tackle,\" says Anna Danes, Data Ethicist who works with organizations to create frameworks and practical methods to ensure responsible developments in AI.",
    "### Bias in AI models\n\nAnother area of concern regarding the widespread use of Generative AI models is the different kinds of bias the results produced by these models show.\n\nAccording to Divya Dwivedi, a lawyer at the Supreme Court of India, who focuses on tech-law, \"We really do not know what kind of data it will generate and what kind of bias it will come up with since we already have made these models very biased as a result of our own nature.\"\n\nBias in AI models has been a central point of debate for many years, and Generative AI models have just amplified these biases.\n\n\"There is an inherent bias that is there when we are training the dataset that comes out of how humans have kind of embedded their prejudices in the system,\" points out Prateek Sibal, Programme Specialist for AI, Emerging Tech and Internet Governance at UNESCO.\n\nRecently, India Today conducted a political experiment involving leading platforms that use AI to generate visual imagery, suggesting that these platforms could be biased in terms of their knowledge and understanding of various nations.\n\nThe Generative AI tool Midjourney was asked to create pictures of \"most popular elected political leaders posing in front of the Eiffel Tower in 2023.\"\n\nThe prompt was generic, without mentioning names to check the scope of the results.\n\nThe AI-generated image only included leaders with a Western appearance, such as Angela Merkel, Emmanuel Macron, and Donald Trump.\n\nNone of the popular Asian leaders were included in the result.\n\nThese demonstrations have only strengthened the belief that these models amplify human biases.",
    "### Ethics, rights, and intellectual property\n\nThe use of copyrighted materials for training AI models is still unclear, and hence is said to be in a legal grey area.\n\nIt can be said that there are no copyright laws so far that would safeguard any wholly AI-generated model or creation.\n\nIt becomes irrelevant whether that creation stemmed from a human-crafted text prompt.\n\nWhile fair use laws permit the use of copyrighted material under certain conditions without the owner’s permission, the ongoing legal disputes could disrupt this status quo and bring uncertainty in the future of AI model training.\n\nUndoubtedly, the advent of generative AI has revolutionized our lifestyle, labor practices, and artistry output within a mere few months.\n\nIn turn, the inundation of AI-fabricated written works, pictures, and tunes, alongside the mechanisms through which they were created, has stimulated a plethora of intricate legal inquiries.\n\nThese challenge our understanding of ownership, fairness, and the core foundation of innovation, writes Anndy Lian.\n\n\"There are papers and research where Generative AI or ChatGPT has been mentioned as co-authors, what does this mean for original scientific work?\"\n\nasks Prateek Sibal.\n\nPrateek is Programme Specialist, Digital Innovation and Transformation at UNESCO who is working to understand the impact of digital technologies, especially artificial intelligence (AI), on societies from a human rights, openness, inclusive access and multi-stakeholder governance perspective.\n\nCopyright laws play a fundamental role when it comes to protecting intellectual property and encouraging creativity.\n\nIt allows creators to rightfully control the usage, distribution, and adaptation of their work.\n\nThe law encourages creators to create more by offering them exclusive rights.\n\nCreative Commons licenses enable the free distribution of an otherwise copyrighted \"work\".\n\nThey also provide more options for creators to choose the level of protection they want for their work.\n\nWith advancements in AI technology, there is a simultaneous increase in its usage in the creative process.\n\nWith the increase in AI capabilities to generate fresh and original content, there is a growing need for a legal framework that addresses the copyright protection of collaborative works involving AI.\n\nThis legal framework is highly important to strike a balance between safeguarding the rights of creators while boosting innovation and originality.\n\nThis is difficult to predict how copyright law will shape up around AI-generated work.\n\nHowever, it is clear that the legal framework governing copyright protection will undergo significant transformation.",
    "### How to ensure responsible use of Generative AI?\n\nWith the rise in the usage of generative AI, there is a dire need for a holistic, and a comprehensive framework that would ensure responsible usage of the technology to harness the best benefits while mitigating its risks.\n\nThis framework shall include ethical guidelines, regulatory measures, transparency, accountability, and ongoing collaboration among stakeholders.\n\nBy boosting fair practices we can nurture the technology, increase people’s trust in it, protect it against potential harm, and ensure that generative AI serves humankind and society in more productive ways.\n\nAccording to Jaspreet Bindra, Founder and MD of Tech Whisperer Ltd. \"The biggest thing we will have to figure out is how to control this monster.\n\nThe democratization of technology is the biggest challenge here.\n\nThe closest I can think about something we came across like AI was nuclear power.\n\nCountries shall come together to regulate it.\"\n\nOn the other hand, people like Deepak Visweswaraiah, Vice President, Platform Engineering and Site Managing Director at Pegasus Systems, point out that \"Controlling the monster does not happen due to government regulations.\n\nIt should be a collective effort.\n\nHow do we teach developers to be mindful of AI ethics?\n\nThe developers should be clear about the purpose of each model and share awareness to avoid misuse.\"",
    "### Regulation\n\nWhen it comes to regulation, countries will not have uniform reasons to regulate Generative AI.\n\nFor example, China’s draft regulations are geared towards preventing further developments in AI technology that might undermine the government’s control over domestic internet and tech space.\n\nOn the other hand, the EU places the prevention of harm to individuals front and center in its draft for the AI Act.\n\nTherefore, the purpose of regulations must be defined to ascertain what the regulations should contain.\n\nAt a foundational level, all generative AI regulations should attempt to protect individuals against potential harm.\n\nHarms that could include violating an individual’s privacy and data rights, discrimination in access to services, or being subject to false or misleading news and information.\n\nProtection against these, and similar harms, has to be non-negotiable.\n\nThere is an international consensus on the necessity to ensure such protections, though the granular details of practical implementation still need to be clarified.\n\n\"When we are talking about regulation, what we are trying to do is regulate the impact of technology on society.\n\nAnd sometimes we just don’t know what it is going to be, and you don’t want to make any regulation because probably it’s too soon\", says Prateek Sibal.\n\nOf late, there has been a significant debate on whether regulations should also include protection from second-order harms, such as violation of intellectual property rights (IPR) and defamation.\n\nWhile this has not yet been settled in a definitive manner, generative AI systems will likely be subject to at least IPR laws soon.\n\nApart from individual harms, regulations could also look at systemic harms, specifically the increased concentration of economic and market power in the hands of Big Tech players.\n\nThe fundamental requirements for developing generative AI systems - access to substantial amounts of data and computing power to process this data - are readily available only to the Big Tech companies.\n\n\"The regulation of AI tends to focus on formal equality rather than substantial equality, even though the end goal is often equity,\" points out Shashank Reddy, Managing Partner at Evam Law & Policy.\n\nAlgorithmic Auditing is one of the tools put forward to mitigate Generative AI risk.\n\n\"Auditing AI systems is a mechanism or criteria for the system to comply with legal obligations.\n\nSuch legal obligations could be a hard law obligation, which may be an existing law, or it could just be just a company that is aware of the AI act or some other law and is looking to be compliant with it\", says Vibhav Mithal, of Anand and Anand and Future of Humanity.\n\nIn order to narrow down to the exact entity that is causing the harm, it is very critical to examine the generative AI value chain closely.\n\nCommercial Generative AI system has four pillars: Developers, who develop the system; Deployers, who create and work on the base mode for advanced functionalities for themselves or other third-party customers; Users, who are individuals, corporate organizations, or platforms who use the AI system themselves either internally or through product offerings; and ultimately the Recipients, people who receive and use the output of the AI system.\n\n\"I see a lot of engineering teams being very skeptical about the impact of what they are developing.\n\nI see people from engineering teams saying I’m also partly accountable for what I’m doing.\n\nThis shows that people are becoming more aware and vigilant.\"\n\nsays Anna Danes, a Digital Ethicist who works with organizations to create frameworks and practical methods to ensure responsible developments in AI.\n\nShe also believes that, \"we all start thinking a little bit about what incentives we can give companies, private companies, and also public institutions to awaken their ethical knowledge or imagination so that they can develop ethical products\".",
    "### AI regulation efforts across the globe\n\nThe capabilities of the generative AI models have been a matter of concern for governments, researchers, and the common man worldwide.\n\nAs a result, governments worldwide have imposed AI regulations.\n\nThe European Union’s AI Act, Canada’s Artificial Intelligence and Data Act (AIDA), the United States’s AI Bill of Rights and State Initiatives, and China’s Algorithm Transparency and Promoting AI Industry Development have been a subject of discussion in several forums.\n\nEach of these country’s AI regulations are exceptionally thorough and have great visions around the usage and protection of AI.\n\nWhile the AI Act concentrated on a risk-based approach to guide the use of AI in both the private and public sectors, the AI Bill of Rights targets specific use cases.\n\nThough the Chinese government has yet to pass rules on AI technology at large, recently, the country introduced a law that regulates how private companies use online algorithms for consumer marketing.",
    "### Recommendations from the roundtables\n\n- Foster Education and Public Awareness: It is crucial to educate the public about generative AI to dispel fears of job replacement and promote understanding of its capabilities.\n\nThis can be achieved through awareness campaigns, workshops, and educational programs.\n\n- Establish Data Sharing and Usage Regulations: Given the importance of data quality in building robust generative AI models, there is a need for regulations to govern data sharing and usage.\n\nThis can help protect privacy, ensure ethical practices, and maintain transparency in AI development.\n\n- Develop Global Standards and Regulatory Frameworks: The lack of global standards for AI poses a significant challenge.\n\nGovernments and international organizations should collaborate to establish common frameworks that address the regulatory aspects of generative AI, including accountability, bias mitigation, and safety measures.\n\n- Encourage Self-regulation: In addition to governmental regulations, individuals and corporations involved in generative AI development should adopt self-regulatory practices.\n\nThis includes being clear about the purpose of each model, promoting ethical guidelines, and fostering responsible use of AI technologies.\n\n- Prioritize Bias Mitigation: While it may be difficult to completely eliminate bias, it is important to prioritize efforts to mitigate bias in generative AI models.\n\nDevelopers should invest in research and development of bias detection and mitigation techniques to ensure fair and unbiased outcomes.\n\n- Explore Augmentation of Human Intelligence: Generative AI tools have the potential to augment human intelligence.\n\nEncourage research and development in using generative AI as a tool to enhance human capabilities and improve productivity across various sectors.\n\n- Foster Collaboration and Interdisciplinary Approaches: The development of generative AI models involves multiple stakeholders.\n\nEncourage collaboration among researchers, developers, policymakers, and domain experts to foster interdisciplinary approaches and ensure a comprehensive understanding of the societal impacts and potential risks of generative AI.\n\n- Address Ethical Concerns: Given the intellectual capacity of generative AI, it is crucial to address ethical concerns and potential misuse.\n\nDevelopers should consider the ethical implications of their models and proactively work towards preventing any harm or unintended consequences.\n\n- Support Accessibility and Inclusivity: Generative AI can have a positive impact on individuals with communication difficulties or disabilities.\n\nEncourage the development of inclusive AI solutions that cater to the needs of speech-impaired individuals, autistic people, and others who may benefit from improved communication tools.\n\n- Emphasize Safety and Control: As generative AI continues to advance, safety and control measures must be prioritized.\n\nGovernments, organizations, and developers should collaborate to establish safeguards that prevent malicious use or unintended consequences, ensuring that the technology is harnessed for the benefit of society.\n\n- Monitor Future Enhancements: Stay vigilant about future enhancements to deep learning models that require less training.\n\nIt is important to carefully assess the implications of these advancements, including potential risks and unintended consequences, to ensure responsible and safe deployment of generative AI technologies.",
    "### References\n\n- Arianna, Johnson.\n\n2023.\n\n“Which Jobs Will AI Replace?\n\nThese 4 Industries Will Be Heavily Impacted.” Forbes.\n\n- Cao, Yihan.\n\n2023.\n\n“A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT.” arxiv 1, no.\n\n2023 (March): 8.\n\n- Digital, Mckinsey.\n\n2023.\n\n“What is generative AI?” McKinsey and company.",
    "# About Generative AI\n\n- Mckinsey Digital.\n\nWhat every CEO should know about generative AI.\n\n- Drake, Marianna.\n\n2023.\n\n\"UK Government Adopts a 'Pro-Innovation' Approach to AI Regulation.\"\n\nCovington.\n\n- Elias, Jibu.\n\n2023.\n\n\"Generative AI can help India close the technology divide, NVIDIA CEO Jensen Huang.\"\n\nINDIAai.\n\n- Eloundou, Tyna.\n\n2023.\n\n\"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.\"\n\narxiv 4, no.\n\n20 (March).\n\n- Enterprise, IT.\n\n2023.\n\n\"AI will drive 83 million 'structural' job cuts in 5 years says WEF.\"\n\nThe Stack.\n\n- Gazala, Nawal.\n\n2021.\n\n\"Generative AI & Synthetic Media: A new era of advertisements.\"\n\nINDIAai.\n\n- Government, Canada.\n\n2022.\n\n\"The Artificial Intelligence and Data Act (AIDA) – Companion document.\"\n\nGovernment of Canada.\n\n- JALAN, AYUSH.\n\n2023.\n\n\"7 Ways Generative AI Will Change the Job Market.\"\n\nMake Use Of.\n\n- Jeevanandam, Nivash.\n\n2022.\n\n\"Dall-E - the AI that creates images of everything.\"\n\nINDIAai.\n\n- Jeevanandam, Nivash.\n\n2023.\n\n\"Exploring Amazon Bedrock language model.\"\n\nINDIAai.\n\n- Jeevanandam, Nivash.\n\n2023.\n\n\"Exploring Firefly - a generative AI model from Adobe.\"\n\nINDIAai.\n\n- Ji, Ziwei.\n\n2023.\n\n\"Survey of Hallucination in Natural Language Generation.\"\n\nACM Computing Surveys 55, no.\n\n12 (March): 38.\n\n- Justice, Cliff.\n\n2023.\n\n\"Is AI going to slash jobs?\"\n\nKPMG.",
    "# Impact, Opportunity, and Challenges of Generative AI\n\n- KPMG, team.\n\n2023.\n\n\"Is AI going to slash jobs?\"\n\nKPMG.\n\nKPMG Report.\n\n- Lian, Anndy.\n\n2023.\n\n\"The Legal Implications of AI-Generated Content in Copyright Law.\"\n\nINDIAai.\n\n- Mdaka, Yamkela.\n\n2023.\n\n\"Jobs that are most exposed to generative AI, according to a new study.\"\n\nNews24.\n\n- Milin, Stanley.\n\n2023.\n\n\"Everything you need to know about Prompt Engineering, one of the hottest jobs in Generative AI.\"\n\nINDIAai.\n\n- Mossy, Glenn.\n\n2023.\n\n\"The Impact of Generative AI on Labor Productivity, Employment, Wages and GDP.\"\n\nLinkedin.\n\n- nasscom, insights.\n\n2023.\n\n\"10 BEST AND HIGH-PAYING AI JOBS FOR 2023.\"\n\nnasscom insights.\n\n- nasscom, team.\n\n2023.\n\n\"State Of Data Science & AI Skills In India – Data And The Art Of Smart Intelligence.\"\n\nnasscom.\n\n- news, nvidia.\n\n2023.\n\n\"What is Generative AI?\"\n\nnvidia.\n\n- Raja, Anjali.\n\n2023.\n\n\"Analyzing AI regulations based on dynamics in the AI ecosystem.\"\n\nINDIAai.\n\n- research, INDIAai.\n\n2023.\n\n\"How Generative AI will influence the media industry.\"\n\nINDIAai.\n\n- Secretary, Parliament.\n\n2023.\n\n\"A pro-innovation approach to AI regulation.\"\n\nOpen Government Licence 1, no.\n\n12 (March): 8.\n\n- Shanthi, S. 2023.\n\n\"Can India’s Startups Tap Into Generative AI Opportunity?\"\n\nEntrepreneur India.\n\n- Singh, Manish.\n\n2023.\n\n\"Where is India in the generative AI race?\"\n\nTech Crunch.\n\n- team, Drexel.\n\n2022.\n\n\"Study: AI Behind ChatGPT Could Help Spot Early Signs of Alzheimer’s Disease.\"\n\nDrexel News.\n\n- Team, Insights.\n\n2023.\n\n\"The state of generative AI in 7 charts.\"\n\nCBI Insights.\n\n- Vaswani, Ashish.\n\n2017.\n\n\"Attention Is All You Need.\"\n\narxiv 1, no.\n\n12 (December): 23.\n\n- Warnke, Arne.\n\n2023.\n\n\"The Impact of ChatGPT and Generative AI on Jobs.\"\n\nData camp.\n\n- WHO, Growth.\n\n2023.\n\n\"These are the jobs most likely to be lost – and created – because of AI.\"\n\nWorld Economic Forum.\n\n- Wiles, Jackie.\n\n2023.\n\n\"Beyond ChatGPT: The Future of Generative AI for Enterprises.\"\n\nGartner.",
    "# Notes\n\nThe National AI Portal of India (INDIAai) is a central repository of AI information and resources, serving the Indian AI ecosystem and providing global insight into AI in India.\n\nLaunched in May 2020, it has gained recognition and features interviews and articles by industry leaders.\n\nThe portal houses information on AI initiatives by the government, academia, and corporates, while maintaining a database of Indian AI startups.\n\nINDIAai is respected as an authoritative source on AI in India and worldwide.\n\nWith the potential to add $957 billion to India’s economy by 2035, INDIAai aims to foster a unified AI ecosystem, drive knowledge creation, develop an AI-ready workforce, and utilize AI for economic growth.\n\nTeam INDIAai\n- Kavita Bhatia, Senior Director, MeitY\n- Prashant Kumar Mittal, Director, NeGD, MeitY\n- Sushil Kumar Jangid, Scientist, MeitY\n- Bramhanand Jha, Senior Consultant - PM, MeitY\n- Sangeeta Gupta, Senior Vice President, NASSCOM\n- Asna Siddiqui, Project Lead, INDIAai\n- Jibu Elias, Content and Research Lead, INDIAai\n- Anjali Pathak, Product & Social Media Lead, INDIAai\n- Nibedita Saha, Senior Research Associate, INDIAai\n- Dr. Nivash Jeevanandam, Senior Research Writer, INDIAai\n- Anjali Raja, Content & Research Associate, INDIAai\n- Milin Stanly, Content & Research Associate, INDIAai\n\nI la.i A MEITY, NEGD & NASSCOM INITIATIVE",
    "## Scope\n\nThis ITP applies to all offices, departments, boards, commissions, and councils under the Governor’s jurisdiction (hereinafter referred to as \"agencies\").\n\nAgencies not under the Governor’s jurisdiction are strongly encouraged to follow this ITP.\n\nThird-party vendors, licensors, contractors, or suppliers shall meet the policy requirements of this ITP that are applicable to the products and services provided to the Commonwealth.",
    "## Background\n\nThe field of AI has developed rapidly, and the availability and popularity of Generative AI have greatly increased.\n\nThere is significant public interest in Generative AI because of its potential to empower creativity, innovation, and efficiency.\n\nHowever, the technology also presents risks and new challenges that the Commonwealth must navigate.\n\nThis policy encourages users to engage with these technologies in a manner that is additive to their work and the services they provide to customers, residents, visitors, and industry, while addressing and mitigating risks inherent in its use.\n\nThe Commonwealth will continually review policies and directives relating to the governance of Generative AI use in all its forms and evaluate Generative AI tools that may provide additional security while mitigating risks associated with this technology.",
    "## Definitions\n\n- **Bias**: Erroneous or prejudiced assumptions in artificial intelligence and machine learning processes that may affect generative output.\n\n- **Generative Artificial Intelligence (Generative AI)**: Predictive algorithms that can be used to create new content including audio, code, images, text, simulations, and videos.\n\n- **Private Generative AI**: Generative AI tools that are specific to an entity or organization and their data.\n\nPrivate Generative AI tools can be developed in-house by an entity or organization for their own use or obtained from a third-party vendor.\n\nThese systems are configured in a way that ensures an organization’s data is segmented from other Training Data and accessible to only the entity or organization that owns it.\n\n- **Public Generative AI**: Generative AI tools that are openly available to multiple entities, organizations, or the general public and utilize widely sourced data from the internet as well as data from users or customers to train the Generative AI model.\n\nPublic Generative AI tools do not guarantee the privacy of data input by users, entities, or organizations.\n\nAdditionally, Training Data and models are not owned by a public organization unless otherwise noted.\n\n- **Training Data**: Data used to train a large language model and other predictive algorithms.",
    "## Policy\n\nIf using Generative AI, users may only use a Commonwealth-approved Public Generative AI tool.\n\nPublic Generative AI tools that receive approval are available for use only in accordance with this policy.\n\nThere are many Public Generative AI tools that offer different strengths and weaknesses.\n\nThe Commonwealth will continue to evaluate and may approve additional Public Generative AI tools that may be of value to users.",
    "### Account Creation\n\nGenerative AI tools often require that users enter an email address to register and create an account.\n\nUsers, who are utilizing an approved Public Generative AI tool for Commonwealth business purposes, shall use their Commonwealth e-mail address for registration and account creation purposes.\n\nOnce created, the account associated with a user’s Commonwealth e-mail address shall be used solely for Commonwealth business purposes.\n\nPersonal use of Public Generative AI from an account using a Commonwealth e-mail is prohibited.\n\nUpon completion of the registration and the account creation process, users shall opt-out of data sharing and disable the chat history within the Public Generative AI system.\n\nIf unable to opt-out, the user must contact the Office of Administration, Office of Information Technology (OA IT) prior to using the Public Generative AI system.",
    "### Risk Assessment\n\nGenerative AI is a versatile technology that can be used for a variety of purposes.\n\nLike any other tool, different use cases create different risks and rewards.\n\nWhile Generative AI tools are relatively new, many of the risks are the same as common internet or software-based tools.\n\nExamples of common risks with Generative AI tools are:\n- Sharing private or confidential information in a Generative AI prompt.\n\n- Generative AI outputs that are inaccurate or misleading in communications to the public or relying on inaccurate or misleading outputs to inform agency programs or policies.\n\n- Reinforcing existing Bias in work products due to Bias in Generative AI outputs.\n\n- Copyright infringement.\n\nBecause the use of Public Generative AI tools can pose significant risk depending on the information or data input into the tool, proper governance of such tools is required.\n\nWhen assessing whether to use an approved Public Generative AI tool, users should consider if the use case is high or low risk and high or low impact.\n\nExamples:\n- **High risk/low impact (avoid)**: Using Generative AI to draft an external-facing communication that includes sensitive information for citizens and would have taken minimal time to write manually.\n\nCopying and pasting that output for use with minimal review.\n\n- **Low risk/high impact**: Using Generative AI to compare a new and old version of a publicly available policy and asking the Generative AI tool to identify which sections have been modified, then confirming the nature of these changes manually.",
    "### Generative AI & Coding\n\nGenerative AI tools can be an excellent coding resource with high impact.\n\nPublic Generative AI tools used for coding purposes should be approached with caution, and special attention should be paid to risk assessment.\n\nSubject matter experience is required to properly validate Generative AI outputs, and this requirement is particularly necessary for coding use cases.\n\nUsers must be cautious not to include production code or proprietary information in prompts, must assess vulnerabilities in code outputs, and must keep in mind that assessment of these outputs may require technical knowledge.\n\nUse of Generative AI in coding may result in more bugs or flaws in programs since it may gather the code from flawed sources.",
    "### Qualifications to verify and review outputs\n\nFor a user to be able to review and verify outputs adequately, the user must have experience in the relevant topic area.\n\nFor example, a software engineer may be able to verify the quality of code generated in a coding language in which the engineer specializes but may not be able to verify if a contract is legally sound without the requisite legal training.\n\nGenerative AI content should not be assumed to be accurate.\n\nAt a minimum, users should review the output for:\n- **Bias**: Since the data used to train Generative AI is vast, from a variety of sources, and not always vetted, outputs may contain inaccurate assumptions or stereotypes regarding certain individuals or communities.\n\n- **Dated Information**: The data used to train Generative AI may have a fixed cutoff date, meaning any output generated will not reflect information available after a certain cutoff date.\n\n- **Inaccurate Information**: Generative AI relies on Training Data.\n\nTraining Data is vast and not always consistent or accurate.\n\nInaccuracies in the Training Data may be included in the output generated by the Generative AI system.\n\nInaccurate output can also be generated regardless of the Training Data.\n\nThe Generative AI system may produce a confident response that appears plausible; however, the response is fabricated and divorced from reality (sometimes referred to as “hallucinations”).\n\nIn one recent example, a user doing legal research using Generative AI was provided several court decisions, and the decisions provided by the Generative AI system turned out to be non-existent and completely fabricated.\n\n- **Inappropriate Content**: If Training Data contains inappropriate content, the inappropriate content could appear in the Generative AI output.\n\n- **Intellectual Property**: Generative AI tools continually ingest publicly available information for training purposes including information that may be subject to copyright.\n\nCopyrighted information could be inappropriately included in any output generated by the Generative AI system, creating intellectual property risks.\n\n- **Confidential, Non-Public Information**: Since the data used to train Generative AI is vast, from a variety of sources, and not always vetted, outputs may contain confidential, non-public information.",
    "## Disclosure\n\nUsers shall be transparent about their use of Generative AI and must disclose to customers, residents, visitors, and industry when Generative AI has been used to generate content that may be public facing or shared externally.\n\nGenerative AI use must be disclosed even if it was only used to generate a portion of the content.\n\nThe disclosure shall be prominently displayed and include an indication that the content was generated either entirely or in part by Generative AI and identify the Generative AI system and version that was used.\n\nExample: “ChatGPT-3.5 was used in the creation of this document.”",
    "### Non-Text Outputs\n\nUsers shall not utilize Public Generative AI for non-text-based outputs.\n\nMost Generative AI platforms can generate images, video, audio, or other types of content.\n\nHowever, the risks related to inadvertently including others’ intellectual property or generating offensive content are significantly higher and more difficult to detect than with text-based outputs.\n\nStructured data, numbers, code, and different languages are acceptable outputs from Generative AI only so long as the output is properly reviewed and verified by users with the appropriate expertise.",
    "### Private and Sensitive Data\n\nNo class C data, as defined in ITP-INF015, Policy and Procedures for Identifying, Classifying, and Categorizing Commonwealth Electronic Data, may be input into any Public Generative AI prompt, tool, or system.\n\nThis includes, but is not limited to:\n- Sensitive Security Information\n- Personal Identifiable Information (PII)\n- Protected Health Information (PHI)\n- Regulated Data – Such as data from or regulated by:\n  - Social Security Administration (SSA)\n  - Internal Revenue Service (IRS)\n  - Centers for Medicare & Medicaid Services (CMS)\n  - Criminal Justice Information (CJI)\n  - Criminal History Record Information Act (CHRIA)\n  - Family Educational Rights and Privacy Act (FERPA)\n  - Payment Card Industry Data Security Standard (PCI DSS)\n- Confidential or Non-Public Information\n- Privileged Information",
    "### Decision Making\n\nGenerative AI outputs are not to be used to make decisions for or on behalf of employees.\n\nEmployees may use Generative AI outputs to inform a larger decision-making process, but ultimately the Commonwealth employee or official remains the final decision maker.\n\nUsers must review and verify all output produced with the assistance of Generative AI.\n\nThe user will be accountable for any decision-making based upon such output.\n\nGenerative AI cannot make reliable subjective or value-based judgments and may not be used for such purposes.\n\nFor example, do not use generative AI to make final decisions that affect employment.",
    "## Acceptable Uses\n\nExamples of acceptable uses of Public Generative AI include:\n- Drafting a job posting or job description\n- Summarizing or paraphrasing a writing\n- Taking a technical answer to a question and rewriting it in customer-friendly language\n- Creating an outline for a memo or other communication\n- Brainstorming icebreakers for a meeting\n\nThe examples provided above assume that the Generative AI tool has been approved for use; only public, non-confidential data is involved; and proper review and verification is completed as outlined in section 6.3 of this policy.\n\nThis is not a comprehensive list of the permitted uses, but rather illustrates some common lower-risk use cases.",
    "### Third-party vendors, licensors, contractors, or suppliers\n\n- Comply with the requirements as outlined in this ITP that are applicable to the products or services they are providing to the Commonwealth.\n\nIf the products or services being provided by the third-party vendor, licensor, contractor, or supplier do not fall within the scope of this ITP, compliance is implied.\n\nIf the third-party vendor, licensor, contractor, or supplier subsequently deploys products or services that fall within the scope of this ITP in the future, compliance with the policy is required.",
    "## Related ITPs/Other References\n\nDefinitions of associated terms of this policy are published on the Office of Administration’s public portal:   \nCommonwealth policies, including Executive Orders, Management Directives, and IT Policies are published on the Office of Administration’s public portal:   \n- Management Directive 205.34 Amended, Commonwealth of Pennsylvania Information Technology Acceptable Use Policy\n- ITP-BUS012, Artificial Intelligence General Policy\n- ITP-INF015, Policy and Procedures for Identifying, Classifying, and Categorizing Commonwealth Electronic Data\n- ITP-SFT001, Software Licensing\n- Right-to-Know Law (RTKL), 65 P.S.\n\n§§ 67.101, et seq.",
    "## Exemption from this Policy\n\nIn the event an agency chooses to seek an exemption from the guidance within this ITP, a request for a policy waiver shall be submitted via the enterprise IT policy waiver process.\n\nRefer to ITP-BUS004 IT Policy Waiver Review Process for guidance.\n\nThis chart contains a history of this publication’s revisions.\n\nRedline documents detail the revisions and are available to CWOPA users only.",
    "# Statement of Need\n\nThe newly widespread availability of Generative artificial intelligence (“AI”) technology has quickly reshaped the digital landscape.\n\nGenerative AI, including chatbots relying on large language models such as ChatGPT, can generate novel media content, including text, music, images and videos, in response to text prompts from a user.\n\nThis technology opens new possibilities for education and knowledge production, personal development, and expression.\n\nBut optimism about these potential benefits has also, appropriately, been accompanied by concerns about adverse impacts of Generative AI, provoking debate among policymakers and the public at large.\n\nSome companies launching products powered by Generative AI, such as Microsoft, Open AI, and Google maintain ethical and/or human rights guidelines for AI more broadly and have confirmed that they are conducting risk assessments and testing processes.\n\nBeyond the product level, there also exists a broader debate about the appropriate timing of introducing Generative AI models into the consumer market at scale, and the extent to which the publication of models and codes still under development is appropriate.\n\nInformation about the extent to which human rights due diligence (HRDD) has been conducted as part of the development and deployment of these technologies is limited, and there has been little opportunity for learning across the industry about the most effective approaches to prevent and mitigate human rights risks stemming from advances in Generative AI technologies.\n\nWhile ethical frameworks can be useful in guiding company risk management approaches, they are unlikely to cover the full spectrum of internationally recognized human rights standards necessary to ensure human dignity and non-discrimination.\n\nAs additional companies enter the market, risks will increase as many firms prioritize speed and profits over preventing and mitigating adverse human rights impacts.\n\nSome standard setting is underway in different fora, but regulatory frameworks are not keeping pace with technical developments or with effective human rights risk mitigation practices, leaving potentially severe human rights and other societal risks unaddressed.\n\nThere is therefore an urgent need to explore what constitutes the appropriate scope and practice of business responsibility in relation to Generative AI.\n\nIdentifying appropriate responses to this question and building alignment across industry, civil society and standard setters about expectations should draw on international human rights standards.\n\nIn particular, the expectations set out in the UN Guiding Principles on Business and Human Rights (UNGPs) can provide authoritative and widely accepted guidance.\n\nUsing these global standards as the initial basis for unpacking the scope and nature of corporate responsibilities can also provide a common foundation for constructive and robust dialogue.\n\nConsidering this, the UN Human Rights Office is launching this project as a contribution to wider global and societal debates about how to realise the positive potential of Generative AI, while mitigating the many severe, salient risks that may accompany this transformative technology.",
    "# Objectives\n\n- Clarify the expectations under the UNGPs for companies developing and launching Generative AI products in order to achieve common and more effective human rights risk management approaches across the industry.\n\n- Raise awareness and facilitate exchange among key stakeholders and interdisciplinary experts to shape a comprehensive understanding about the role the UNGPs can play in governing Generative AI responsibly.\n\n- Inform the debate about policy options for managing human rights risks related to the development and launch of Generative AI, including through mandatory and voluntary measures.",
    "# Approach\n\nBuilding on OHCHR’s existing work on tech and human rights, the project will be implemented through an iterative process of research and engagement.\n\nThis will include conducting a series of exploratory interviews with company practitioners, civil society, technical experts, and other key stakeholders, as well as workshops and other convenings involving multiple stakeholders.\n\nProject implementation will be led by the UN Human Rights B-Tech Project, supported by Shift, a leading center of expertise on the UNGPs, with additional assistance from the Global Network Initiative (GNI), a leading multistakeholder initiative with fifteen years of experience working on tech and human rights.\n\nThe project team will include business and human rights expertise and be supported by a technical expert group of academics to bring in state-of-the-art computer science research on Generative AI to enable a better understanding of its implications for managing human rights risks.\n\nFor the company engagements, the project will draw in particular from the team’s ongoing engagement with companies in the B-Tech Community of Practice (CoP), while also involving additional companies.\n\nGiven the highly competitive nature of efforts in the Generative AI area, the ability to work within an existing framework with some degree of trust among the participants and organizers will be essential for the initial exercise of mapping risks.\n\nWe aim to create a space that allows for companies to jointly discuss not only best practices but also lessons learned.\n\nFor civil society engagement, the project will draw on the expertise and perspectives from GNI’s academic, civil society, and investor constituencies, and the extensive network of the B-Tech community of digital rights advocates, academics, and other practitioners.\n\nThe project will be underpinned by research into AI’s salient human rights risks and build on work already done on AI risk management, as well as the work of B-Tech and other OHCHR tech-related work, the Global Network Initiative (GNI) and other relevant initiatives on business and human rights in the tech sector.",
    "# Activities\n\nThe project will be structured into three initial phases:\n\n1.\n\n**Consultations and Mapping:** Map current company approaches to human rights due diligence in order to identify and mitigate salient human rights risks stemming from or being linked to the development, deployment, and use of Generative AI.\n\nThis will be carried out through desk-based research, explorative expert interviews with company representatives, civil society, and experts, and the establishment of a reference group comprised of experts from AI and data science as well as leaders from relevant policy areas.\n\n2.\n\n**Peer Learning Workshops and Engagement:** Targeted convenings (in-person and virtual) with key stakeholders, in particular with leading tech firms, relating to Generative AI practices, including measures adopted to account for potential adverse impacts on human rights.\n\n3.\n\n**Consolidation:** Based on the input gathered and received, a summary of salient risks and state of play on company practices and learning to date will be prepared, without attribution.\n\nFurther outputs will be developed based on the needs and approaches identified in the convenings described above.\n\nAn outline of good practice/standards based on UNGPs expectations is a likely outcome.\n\nBuilding on the consolidation of the findings from the initial phases of the project, possible additional activities will be determined in consultation with those participating.",
    "# Guiding questions to inform project activities and engagements:\n\n- How do existing HRDD practices need to be adapted to be able to incorporate the risks and mitigation strategies stemming from or being linked to the use of Generative AI?\n\n- In which corporate governance structures are risk management processes for Generative AI embedded in companies, e.g., would engaging with product counsel be impactful and if so, how?\n\n- At which points should an evaluation of misuse/abuse be integrated into the AI research/innovation process?\n\nHow can this be tailored to find the appropriate balance between innovation, actual functional breakthroughs/insights and severity of risks.\n\n- What does meaningful transparency about, and accountability for, use, risks and adverse impacts of Generative AI look like in practice?\n\n- Are existing ethical checks/questions in the R&D process sufficient?\n\nWhat, if anything, does attention to, and prioritization of, a vast number of prospective adverse human rights impacts add to this in practice?\n\n- What does responsible conduct look like for different objectives and modes of AI being made or becoming available for use?\n\n- Which forms of mitigation can be employed?",
    "# Outputs\n\n- B-Tech Briefing (max.\n\n15 pages) on key risks, current state of practice, best practices and standards, and UNGPs expectations with regard to Generative AI.\n\n- Additional shorter supporting supplements will be produced that discuss Generative AI-related human rights risks, scenario-based UNGPs interpretations for AI use cases, the state of practice of business respect for human Rights in AI companies and policy coherence in AI Governance aligned with the UNGPs.",
    "## Generative AI Primer\n\nThis document contains proprietary information of the United Nations.\n\nInformation contained herein is to be used solely for the purpose submitted, and no part of this document or its content shall be reproduced, published or disclosed to a third party without the express permission of the United Nations.",
    "### What is Generative AI?\n\nGenerative AI is a subfield of artificial intelligence (AI) and machine learning (ML) that involves the creation of original data or content, including images, video, text, code and 3D renderings.\n\nThis subfield has been developing over several decades and is rapidly evolving, due to advances and availability in computational power, large datasets and significant improvements in machine learning algorithms.\n\nGenerative AI models are based on deep learning algorithms that learn to recognize patterns and relationships from vast amounts of input data, which then generate new outputs that are similar in style and structure to the data they were trained on.\n\nThe ability of these models to self-formulate new and varied outputs represents a paradigm shift in the field of AI because they are not being explicitly programmed to follow pre-determined rules, or generate specific outputs, like other AI systems.\n\nThis will likely lead to a change in how we interface with computers, and more broadly, in how we access, understand, and produce knowledge and information.\n\nFigure 1: Subfields of artificial intelligence",
    "#### Building Blocks of Generative AI\n\n- **Supervised and semi-supervised learning** - A machine learning technique that helps algorithms learn to recognize patterns and make predictions based on categorized or labeled data.\n\nIn the case of semi-supervised learning, algorithms are trained on both labeled and unlabeled data to detect patterns and make predictions.\n\n- **Deep learning** - A machine learning technique that uses layers of neural networks to process data and make decisions.\n\n- **Neural networks** - An AI method that simulates the structure and function of the human brain.\n\nNeural networks process information through interconnected nodes that are organized in a layered structure.\n\nThis computational model serves as the basis of deep learning and is used in various types of generative models.\n\n- **Generative Adversarial Network (GAN)** - A machine learning model that uses two neural networks—a generator and a discriminator—to produce new data that is similar to a given data set.\n\nGANs have become a popular approach for generative AI in various domains, such as image and video generation.\n\n- **Transformer** - A type of neural network that uses encoders and decoders to generate the best probability for the following word in a sentence.\n\nTransformers enable the development of powerful generative models.\n\n- **Large Language Model (LLM)** - A statistical AI model that is trained on massive amounts of text data and predicts the probability of sequences of words to produce human-like text responses.\n\n- **Natural Language Processing (NLP)** - A subfield of AI that is at the intersection of linguistics, computer science and machine learning.\n\nNLP enables computer programs to process and analyze large amounts of natural language data.\n\nIt uses a range of computational methods and algorithms to allow machines, such as chatbots and voice assistants, to understand and mimic written or spoken human language.\n\n- **Generative Pre-trained Transformer (GPT)** - A type of LLM developed by the research lab OpenAI that uses deep learning and NLP techniques.\n\nGPT underlies the user-facing, general-purpose chatbot, ChatGPT, which produces human-like conversational responses in reaction to short user prompts.",
    "### Value Proposition\n\nProponents of generative AI believe that this set of converging technologies is poised to revolutionize the economy, spur productivity and transform industries—from education, healthcare and finance to infrastructure development.\n\nAccording to new research, it is estimated that generative AI systems could increase annual global Gross Domestic Product (GDP) by 7 percent over a 10-year period (Briggs and Kodnani, 2023).\n\nEarly data suggests that a massive transformation is already underway, as Big Tech companies have begun to quickly roll out and integrate new generative AI tools and features into their products, including search engines and office suite software.\n\nThe intense competition in Silicon Valley to develop and deploy these tools, however, has prompted concerns among many regulators and ethical AI researchers.\n\nSome of these concerns include the risk of perpetuating bias and discrimination, spreading mis- and disinformation, and infringing on intellectual property rights.\n\nDiscussions regarding generative AI’s value proposition are full of debate and uncertainties.\n\nBelow, we explore contrasting views on the potential benefits of these technologies, highlighting three interwoven features at the core of its value proposition: efficiency, personalization, and creativity and innovation.\n\nThere are many opportunities for businesses and organizations to maximize their potential using generative AI tools.\n\nOne such benefit can include increased efficiency by automating tasks that involve complex reasoning, pattern recognition and large data sets.\n\nIn the long term, this type of automation could reduce business costs and increase productivity.\n\nIt could also accelerate the speed of iteration, allowing for faster learning and improvements, which can drive business performance.\n\nAccording to economists, foundation models could impact every sector of the economy and lead to significant economic growth.\n\nIt is predicted that generative AI will likely create disruption across some industries and drive demand for new skills.\n\nThis includes roles for prompt engineers and data practitioners that can integrate, fine- tune, and improve generative AI models into existing products and pipelines.\n\nIt also creates greater demand for AI safety professionals to manage the AI systems’ shortcomings and work to ensure their responsible use.\n\nWhile generative AI systems can help an organization run more efficiently, training and operating these models is expensive and can be cost-prohibitive.\n\nThis is due in part to the specialized hardware and significant amount of memory and storage space these systems require to support the high volume of calculations models typically produce.\n\nWith continued advances in the field of AI, however, costs associated with generative AI models are expected to decrease over time, which could make them more accessible.\n\nResearchers estimate that millions of jobs across large economies could be exposed to some degree of automation.\n\nCertain white-collar positions are considered to be at greater risk of being altered or displaced by text generation tools such as ChatGPT.\n\nThis includes jobs in tech (e.g., coders, computer programmers), media (e.g., advertising professionals, journalists) and finance (e.g., financial analysts and advisors), as well as educators and customer service agents.",
    "### Maturity Assessment\n\n- **Hypothetical** – the technology is conceptually possible\n- **Experimental** – research and experiments are proving the technology\n- **Working Prototypes** – working examples are being built\n- **Diffusion** – the technology is being adopted\n- **Commercialization** – the technology is part of mainstream solutions\n\nThe United Nations Office of Information and Communications Technology (OICT) is developing an AI governance framework, as well as a matrix and guidance note, for the use of generative AI within the UN.",
    "### About the Emerging Technologies Team\n\nETT expedites the adoption of frontier technologies across the UN Secretariat.\n\nIt leverages emerging technologies to generate greater efficiencies and to enhance the Organization’s ability to respond to an ever-evolving technological landscape, while providing appropriate safeguards through the careful identification and evaluation of adoption-related risks.",
    "### Bibliography\n\n- Alphonso, G. (2023) ‘Generative AI: Education In The Age Of Innovation’, Forbes, 3 March.\n\nAvailable at:  (Accessed: 4 May 2023)\n- AWS (2023) ‘What Is A Neural Network?’, Amazon Web Services, Available at:  (Accessed: 27 April 2023)\n- Azhar, A.\n\n(2023) ‘What chatbots make cheap’, 21 February.\n\nAvailable at:  (Accessed: 27 April 2023)\n- Briggs, J. and Kodnani, D. (2023) ‘Generative AI could raise global GDP by 7%’, Goldman Sachs Research, 5 April.\n\nAvailable at:  (Accessed 1 May 2023)\n- Chow, A. and Perrigo, B.\n\n(2023) ‘The AI Arms Race Is Changing Everything’, TIME, 17 February.\n\nAvailable at:  (Accessed: 2 May 2023).\n\n- Fazackerley, A.\n\n(2023) ‘AI makes plagiarism harder to detect, argue academics – in paper written by chatbot’, The Guardian, 19 March.\n\nAvailable at:  (Accessed: 4 May 2023)\n- Grant, N. and Weise, K. (2023) ‘In A.I.\n\nRace, Microsoft and Google Choose Speed Over Caution’, New York Times, 7 April.\n\nAvailable at:  (Accessed: 2 May 2023)\n- Gruetzemacher, R. (2022) ‘The Power of Natural Language Processing’, Harvard Business Review, 19 April.\n\nAvailable at:  (Accessed: 27 April 2023)\n- Gurdeniz, E. and Hosanagar, K. (2023) ‘Generative AI Won’t Revolutionize Search — Yet’, Harvard Business Review, 23 February.\n\nAvailable at:  (Accessed 3 May 2023)\n- Hsu, T. and Thompson, S. (2023) ‘Disinformation Researchers Raise Alarms About A.I.\n\nChatbots’, New York Times, 8 February.\n\nAvailable at:  (Accessed 2 May 2023)\n- Kak, A. and Myers West, S. (2023) ‘AI Now 2023 Landscape: Confronting Tech Power’, AI Now Institute, 11 April.\n\nAvailable at:  (Accessed 26 April 2023)\n- Lehmann, E. (2023) ‘ChatGPT can make life easier, here is for whom.’, GIZ Data Lab, 15 March.\n\nAvailable at:  (Accessed 3 May 2023)\n- Leswing, K. and Vanian, J.\n\n(2023) ‘ChatGPT and generative AI are booming, but the costs can be extraordinary’, CNBC, 13 March.\n\nAvailable at:  (Accessed: 26 April 2023)\n- Luccioni, S. (2023) ‘The mounting human and environmental costs of generative AI’, Ars Technica, 12 April.\n\nAvailable at:  (Accessed: 1 May 2023)\n- Mok, A. and Zinkula, J.\n\n(2023) ‘ChatGPT may be coming for our jobs.\n\nHere are the 10 roles that AI is most likely to replace.’, Business Insider, 9 April.\n\nAvailable at:  (Accessed: 2 May 2023)\n- Ribeiro Neto, J.A.\n\n(2023) ‘ChatGTP and the Generative AI Hallucinations’, Medium, 15 March.\n\nAvailable at:  (Accessed: 26 April)\n- Samuelson, Pamela, (2023) ‘Generative AI meets copyright’, Science, 13 July.\n\nAvailable at:  (Accessed: 27 July 2023)\n- Singer, Gadi, (2023) ‘Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at Scale’, 25 July.\n\nAvailable at:  (Accessed: 27 July 2023)\n- Storius Magazine, (2023) ‘Generative AI: The End of Human Creativity or the New Renaissance?’, Storius Magazine, 14 April.\n\nAvailable at:  (Accessed: 2 May 2023)\n- Wiggers, K. (2023) ‘The current legal cases against generative AI are just the beginning’, TechCrunch, 27 January.\n\nAvailable at:  (Accessed: 4 May 2023)",
    "### PURPOSE\n\nGenerative AI refers to a new set of technologies that utilize machine learning techniques to generate content in response to user inputs.\n\nThe content produced can be textual (e.g.\n\nChatGPT, Bedrock, or Bard), visual (e.g.\n\nDALL-E, Canva or MidJourney), spoken (e.g.\n\nElevenLabsM4T, Polly/LEX) or even musical (e.g.\n\nMusicLM or Amadeus Code).\n\nThese new tools have the potential to be extraordinarily useful to public servants in your work, but they also present risks.\n\nTherefore, these guidelines serve as an interim resource for employees of the State of New Jersey to encourage responsible use of these emerging technologies.\n\nGenerative AI is a tool, not actual intelligence.\n\nWe remain responsible for the outcomes and uses of these tools.\n\nTechnology enables our work but does not excuse our judgment.\n\nAs we explore the responsible use of artificial intelligence, we embrace the following principles to guide our efforts.\n\nThese shared values of empowerment, inclusion, transparency, innovation, and risk management will steer our experimentation and decision-making when employing these rapidly evolving technologies.\n\nWith this Circular, we aim to build trust, spur progress, and ensure AI serves the public good.",
    "#### GENERAL\n\nEmpowerment.\n\nThe use of AI should support our workforce to deliver enhanced services and products efficiently, safely, and equitably to all residents.\n\nWe rely on the judgment of our professionals to ensure we realize the benefits of these tools.\n\nInclusion and Respect.\n\nThe use and development of AI should uplift communities, connecting them effectively with resources to thrive, especially those historically marginalized.\n\nAs public stewards, we will use tools respectfully to reflect values of equity and social justice.\n\nTransparency and Accountability.\n\nWe acknowledge the limits of foresight.\n\nBut transparency builds trust and enables collective learning.\n\nWhen AI is used, we must disclose that responsibly and share our workflow freely with other public servants and with the public.\n\nInnovation and Risk Management.\n\nWe embrace responsible experimentation that maintains control and respects privacy and security while developing uses that drive efficiency, dialogue, and better service.\n\nWe understand risks may not be fully apparent initially and commit to proactive risk assessment.",
    "#### Ask - Early and Often\n\nGenerative AI creates content based on your inputs.\n\nThat is why it is good to try out different questions (also known as “prompts”) to see how it responds.\n\nYou can specify the tone, style, and length of a text response and the attributes and qualities of an image.\n\nThe more you experiment with different ways of steering the tools, the faster you will learn how to instruct them to yield the best results.",
    "#### Fact Check\n\nVerify all AI-generated content, especially for public use.\n\nGenerative AI can swiftly produce clear prose, but the information may be inaccurate or outdated.\n\nResearch claims to ensure accuracy.\n\nWatch for: incorrect facts, events, links, or references; and biased information potentially harmful to vulnerable groups like racial, ethnic, and gender minorities, people with disabilities, etc.\n\nWe must actively mitigate risks from AI while benefiting from its capabilities.\n\nAs public stewards, we have an obligation to use these tools responsibly.\n\nSignoff from Agency Director level or Asst Commissioner is required before production use.",
    "#### Disclose\n\nLabel content created with generative AI to that effect.\n\nTransparency is crucial in AI-generated content, even when AI is used sparingly.\n\nDisclosing AI involvement fosters trust and aids in error identification.\n\nDetail the AI model, prompts, and methods employed.\n\nThis documentation aids comprehension and safe usage by colleagues and stakeholders.\n\nSample AI-generated content disclosures:\n- \"This content was generated with the aid of ChatGPT and subsequently revised by Bob Smith.\"\n\n- \"This text was summarized for clarity using Google Bard.\"",
    "#### Sensitive Information\n\nWhen prompting the AI or using AI models, do not disclose sensitive or private information.\n\nWe aim to enable responsible AI use while safeguarding sensitive information.\n\n- Do not share personally identifiable information (PII) about residents, colleagues, or yourself.\n\n- Do not share confidential or sensitive content.\n\n- Do not use AI tools to transcribe or summarize meetings where sensitive topics are discussed.\n\n- Do not share any information that you wouldn’t share publicly.",
    "##### Drafting\n\nGenerative AI like ChatGPT can help draft memos, letters, and job descriptions.\n\nWhen creating prompts, specify any format preferences, keywords, technical terms, or phrases to include or avoid.\n\nRemember, you can tell the software how long you would like the response to be (e.g.\n\nthe word count), the style (e.g.\n\nprofessional or informal), the language (e.g.\n\nplease respond in Spanish).\n\nExample: Memos: In government, we often have to write short documents that present an argument why a policy should be adopted, or a decision should be made.\n\nFor instance, try the prompt in ChatGPT, Bard, and other generative text tools:\n\n\"Write a 250-word memo to a state agency head about the potential benefits of the use of generative AI in state government.\n\nPlease support the memo with evidence.\n\nThe tone should be professional, and the text should be formatted as a memo.\"",
    "**Do Not:**\n\n- Do not rely on generative AI to provide accurate answers.\n\n- Do not use generative AI to create communication regarding sensitive topics.\n\nFor instance, a renowned institution was criticized for using generative AI to write a press release regarding a shooting.\n\n- Do not use content without carefully editing.",
    "#### Communicating with the Public\n\nGenerative AI can help you write clearly and in plain, accessible language.\n\nYou can use the prompt to indicate the reading level or audience for a text.\n\nExample: Use Anthropic’s Claude to upload the text of a website or policy and ask the AI to help you simplify the language and make it more accessible, concise, and easy-to-understand.\n\n\"Help me to rewrite the text of this page in simple English accessible to a ten-year-old.\n\nWhile you may streamline the language for clarity, please do not eliminate any directions.\"",
    "**Suggested:**\n\n- Specify in the prompt if you have a specific audience in mind.\n\n- Try different prompts or verbiage of the same sentence to find what works best.\n\n- You can pass the output of the text by using a readability app that can identify challenging sentences, as well as the reading level for the text.\n\n- Review the text to ensure that the language is inclusive and respectful.\n\nThe models might use language or patterns that appear regularly, but that might exclude some people.\n\nFor instance, a model might suggest: “Dear Sir/Ma'am” does not include non-binary people and could be replaced with “Dear Colleague” or “Dear Neighbor”.",
    "#### Translation\n\nGenerative AI can help you translate content into other languages.\n\nFacebook Research claims it’s free, open-source Seamless M4T tool can translate text or speech from 100 languages into 35 languages.\n\nExample: Use M4T, ChatGPT, or Google Bard to translate instructions for applying for a government benefit into the most popular non-English languages.",
    "**Do Not:**\n\n- Do not include confidential information in the prompt.\n\n- Do not use content generated in a language you do not understand before consulting someone with proficiency in the language.\n\nYou still need to check for accuracy, bias, etc.\n\n- Note that text generated in other languages might be confusing to people who speak different regional dialects.\n\nDo not assume that some text will be easily understood by all speakers.\n\nBe specific in your prompt and ask for regional idioms.",
    "#### Summarization\n\nGenerative AI can summarize longer text or speech into the desired length.\n\nYou can use it to summarize lengthy documents for clarity or brevity when required.\n\nMany citizen engagement platforms now make it possible to summarize and organize citizen comments.\n\n\"Please write a 100-word summary of this report, highlighting the most important recommendations.\n\nUse bullet points.\"",
    "**Suggested:**\n\n- Remember that summaries and transcripts are subject to OPRA.\n\n- If you plan on making a decision based on the summary, you should read the entire document(s) to make sure you did not miss or mischaracterized the original document.\n\n- Be aware that the resulting summary might have biases as it will tend to present language that is more frequent in the data used to train the model.\n\nYou can use changes to the prompt to enhance the results by suggesting that the result incorporates perspectives from marginalized groups.\n\nEven better, you can engage with some individuals in these communities to better understand their perspectives on the text generated.",
    "#### Data Analysis\n\nUtilizing a code interpreter with generative AI empowers individuals with limited coding experience to engage in data analysis tasks.\n\nThis is particularly beneficial for those who are less technically inclined, including interns and student workers, enabling them to effectively contribute to technical projects.\n\nFor instance, OpenAI’s Code Interpreter can be employed to interact with datasets through inquiries.\n\nFeel free to experiment with various programming languages and libraries.\n\nHowever, ensure that you grasp the code's logic and thoroughly go through the relevant component documentation before application.\n\nGenerative AI serves as an initial aid, but it might be necessary to adjust the generated code for optimal results.\n\nExample: “Hello Code Interpreter, I have collected a bunch of survey forms in PDF format.\n\nThese forms contain valuable information, but it's a pain to go through them manually.\n\nI am looking to extract the participant comments and their locations from these PDFs and create a spreadsheet for easier review.\n\nCould you help me generate some Python code using libraries like 'PyPDF2' and 'Pandas' to extract this data and organize it into a neat CSV file?\n\nIt would be a huge time-saver!\n\nThanks!\"",
    "**Do Not:**\n\n- Avoid inserting sensitive information in prompts.\n\nFollowing development best practices, abstain from including confidential data like passwords, private keys, or proprietary details either in your code or the prompts.\n\n- Refrain from utilizing code in a production setting without a comprehensive understanding of its operations.\n\n- Ensure you are well-acquainted with novel libraries and dependencies.\n\nFamiliarize yourself with potential vulnerabilities and security aspects related to the chosen language or library.",
    "#### Generating Images, Audio, and Video\n\nGenerative AI enables the creation of images, audio, and videos based on prompts, offering support in crafting captivating and insightful communication resources.\n\nTools like Mid-Journey and DALL-E generate images.\n\nCANVA is a popular tool for automating the creation of slides.\n\nThe Music plugin available from ChatGPT, Amadeus Code and Amper Music are examples of music generators.\n\n\"Write me a jingle or song to remind residents to enroll in GetCoveredNJ.\"\n\n\"Generate a training video illustrating how to schedule bulky item pickups, leveraging a provided script.\"",
    "**Suggested:**\n\n- Recognize the potency of visual, audio, and video communication in conveying messages.\n\nUse generative AI to harness these tools, extending your artistic abilities.\n\n- Employ generative AI to draft mock-ups that facilitate effective communication with graphic designers, videographers, and other creative professionals.\n\n- Collaborate with your department's public information officer when using or publishing images, audio, or videos.\n\nTheir expertise ensures adherence to accessibility and branding best practices.\n\n- Engage with Cabinet members or counsel; Respectfully seeking their insights helps identify potentially hurtful, discriminatory, or misconstrued content.",
    "**Do Not:**\n\n- Include confidential information in prompts; Rather, ensure any sensitive data is removed from notes or inputs.\n\nThis includes faces, voices, identifications, and license plates of individuals who haven't granted consent.\n\n- Assume that the outputs of generative AI are respectful and non-offensive, especially to vulnerable residents like diverse ethnic, racial, and gender groups; Test and validate.\n\n- Ensure all content aligns with the OIT and Governor’s Office Web Presence Guidelines to maintain visual consistency and compliance.\n\n---\n\nChristopher Rein, CTO  \nMichael Geraghty, CISO  \nOffice of Information Technology  \nOffice of Homeland Security & Preparedness",
    "# Guidelines for the Use of Generative AI in Higher Education at Aga Khan University\n\nThis document is intended for all Aga Khan University students and faculty/staff, including persons with honorary positions.\n\nThe Aga Khan University, in this document, means its institutes, centres, schools, colleges, and hospitals operating across all campuses around the globe.",
    "## POLICY STATEMENT\n\nThese guidelines outline principles for the use of Generative Artificial Intelligence (AI) in higher education at Aga Khan University.\n\nThese guidelines aim to ensure the effective and responsible integration of Generative AI technologies into academic and administrative processes.\n\nIn this document, the term “AI” is used to refer to Generative AI technologies.",
    "## PURPOSE & SCOPE\n\nOpenAI’s ChatGPT-3 (Nov 30, 2022) delivers human-like responses on a variety of subjects and has made generative AI tools easily accessible.\n\nThis has led to AI tools becoming rapidly integrated into various domains, including educational institutions, to enhance the efficiency and effectiveness of processes.\n\nHence, it is essential to engage in discussion and establish guidelines at the University outlining the appropriate and ethical use of AI tools.\n\nBy doing so, we can ensure that the AI tools are used responsibly and prevent their use in academic misconduct and other unethical practices.\n\nAfter reviewing various policies and strategies from leading educational institutions, related international bodies, and policy-making authorities across the world, the University has found that the principles outlined by the Russell Group of universities provide a comprehensive framework for addressing the ethical and responsible use of generative AI tools in academic settings.\n\nThe University aims to follow guidelines on the use of AI tools in education outlined by the following principles by the Russell Group as shown below:\n\n- Universities will support students and staff to become AI-literate.\n\n- (Faculty and) Staff should be equipped to support students to use generative AI tools effectively and (appropriately responsibly) in their learning experience.\n\n- Universities will (review and) adapt (curriculum,) teaching and assessment to incorporate the ethical use of generative AI and support equal access.\n\n- Universities will ensure academic rigor and integrity is upheld.\n\n- Universities will work collaboratively to share best practice as the technology and its application in education evolves.\n\n(Brackets indicate editorial modifications to the principles)\n\nThese guidelines apply to all faculty, staff, students, and other stakeholders involved in the University's academic and administrative functions.\n\nThe guidelines and use of this technology will be in compliance with national data protection laws.",
    "## ACADEMIC INTEGRATION\n\nThe University shall encourage the use of AI technologies to enhance teaching, learning, research, and administrative processes, with a focus on improving efficiency and effectiveness.\n\nAI tools and platforms may be utilized by faculty and staff to support assessments, grading, administrative tasks, and other relevant areas.\n\nInstructors and other staff should not copy/paste students’ work into Generative AI tools either for feedback, assessment, or any other purpose without informing students because it could lead to a breach of data privacy.",
    "## USE OF AI TOOLS IN LEARNING\n\nThe University encourages instructors to support their students in using AI tools to improve their critical thinking and facilitate learning.\n\nIf a student is uncertain about the assistance of an AI tool in a task, it is their responsibility to seek clarification from their instructor and the available guidelines on the use of the technology before incorporating material generated by that tool into their work.\n\nAs stated in the AKU Academic Integrity Policy, a student is not allowed to submit or present work acquired from other source(s) as their own.\n\nThis includes any materials generated by AI tools as well.\n\nStudents are expected to paraphrase, reflect, and critique information obtained from AI tools before including it in their work.\n\nThe University encourages students to use AI tools for facilitating learning and assessments by helping them in evaluating and understanding new concepts and ideas to generate their own academic work, within ethical and responsible boundaries.\n\nThis includes, but is not limited to:\n\n- Developing ideas and thoughts by asking meaningful questions or suggestions\n- Personalizing learning\n- Paraphrasing, improving grammar, punctuation, sentence construction, and other language skills on material written by the student.\n\n- Generating text, graphics, audio, or any other materials based on appropriate prompts.\n\nWhenever AI tools have been used by the student in their work, they must be appropriately referenced and cited according to the instructions outlined by the University or the publisher.\n\nCourse handbooks can provide further instructions.",
    "## CREDIBILITY OF AI GENERATED CONTENT\n\nMost AI tools are pre-trained on large-scale datasets, which may contain biases, limited or incorrect information.\n\nThis is why content generated by such tools may be offensive, inaccurate, incomplete, or not currently valid.\n\nHence, AI tools cannot be considered a completely reliable source of information.\n\nAI tools must not be used as an authoritative source of study on a topic.\n\nAny AI-generated content used in research or assessments must be verified using credible sources to check for factual inaccuracies and incomplete information.\n\nCurrent AI tools rely on generating responses by analyzing patterns and associations within their training data rather than comprehending the meaning or context of the information.\n\nHence AI tools must not be used as the sole means of critical analysis of data.\n\nForming original ideas and critique based on data is an integral part of the research process, and AI tools must not be relied on to generate opinions where reflection and reasoning are required by the student.",
    "## HOW TO ACKNOWLEDGE WHEN AN AI TOOL IS USED\n\nContent from generative AI is a nonrecoverable source as it cannot be retrieved or linked.\n\nAny use of AI technology must be appropriately acknowledged and identified in any submitted work.\n\nThis includes, but is not limited to, the name, version, description, and date of use for the AI tool.\n\nStudents must identify where and how they have used AI assistance in all their submitted work.\n\nAdditionally, students must be able to produce a fully documented record of the prompts, materials, and outputs given to and generated by the AI tool, along with each work when required.",
    "## USE OF AI TOOLS IN TEACHING\n\nFor this document, the term “instructors” is used to refer to all faculty and staff involved in the teaching process including but not limited to those in support roles, teaching assistants, technical assistants, researchers, mentors, and counsellors.\n\nThe University recommends instructors incorporate AI tools in facilitating teaching and encourages them to support their students in using similar tools to facilitate their learning and assist their studies, where appropriate.\n\nPrograms may devise regulations for AI tools use.\n\nThese regulations must not conflict with the guidelines outlined in this document.\n\nInstructors should seek evidence of original thought and critical thinking in submissions made with AI assistance.\n\nStudents must have documentation and be able to produce such information when requested.\n\nThe University encourages instructors to design teaching material incorporating AI tools creatively, ethically, and responsibly to support student learning.\n\nInstructors are advised to review/adapt techniques to prioritize assessment tasks that require higher-order thinking skills and promote critical analysis, reducing reliance on tasks that can be easily accomplished by AI systems.\n\nAlthough AI-detection tools exist, instructors are reminded that none of them have yet been able to guarantee accurate detection.\n\nTherefore, instructors are strongly advised to consider the academic and mental repercussions of false accusations on students for the unethical use of AI tools.\n\nIf academic dishonesty or research misconduct is suspected, the AKU Student Academic Integrity Policy, the AKU Research Misconduct Policy, or other relevant policies will be applied.\n\nAn instructor must seek permission from the student before submitting his/her work into any AI tool for assessment, feedback, or any other purpose.",
    "## REFERENCES\n\nAga Khan University.\n\n(2018).\n\nPolicy on Research Misconduct (with Appendix Revised 11 OCT 2018).\n\nRetrieved from \n%20Misconduct%20(with%20Appendix%20Revised%2011OCT2018).pdf.pdf)\n\nAga Khan University.\n\n(2021).\n\nAuthorship Policy.\n\nRetrieved from  August%202021%20(Approved).pdf.pdf)\n\nAga Khan University.\n\n(2018).\n\nIntellectual Property Rights Policy.\n\nRetrieved from \n\nAga Khan University.\n\n(2022).\n\nPolicy on Student Academic Integrity (KE-014).\n\nRetrieved from  integrity-ke-014.pdf\n\nAga Khan University.\n\n(2018).\n\nPublications Policy.\n\nRetrieved from \n\nAmerican Psychological Association.\n\n(2021, July 23).\n\nHow to Cite ChatGPT and Other AI Language Models in APA Style.\n\nRetrieved from \n\nAtlantic Canada Institute of Fisheries and Aquaculture.\n\n(2023).\n\nACIFA Policy on Academic Integrity.\n\nRetrieved from \n\nBoston University.\n\n(2023, March 21).\n\nGAIA Policy.\n\nRetrieved from \n\nBrown University.\n\n(n.d.).\n\nIntentional Pedagogy and AI Technology.\n\nRetrieved from \n\nCalifornia Institute of Technology.\n\n(n.d.).\n\nResources for Teaching in the Age of AI.\n\nRetrieved from \n\nCarnegie Mellon University.\n\n(n.d.).\n\nAI Tools.\n\nRetrieved from \n\nCornell University Center for Teaching Innovation.\n\n(n.d.).\n\nPromoting Academic Integrity in Your Course.\n\nRetrieved from \n\nDrouin, M.-A., Nguyen, H., & Faulkner, B.\n\n(2023).\n\nAI Teaching Assistants: An Examination of Higher Education Instructors’ Perspectives on AI Adoption.\n\nEducational Technology Research and Development, 71(2), 461–487.\n\nRetrieved from \n\nEuropean Parliament.\n\n(2023).\n\nReport on artificial intelligence in education, culture and the audiovisual sector.\n\nRetrieved from \n\nHarvard Business School Publishing.\n\n(2023, April 27).\n\nLet ChatGPT Be Your Teaching Assistant.\n\nRetrieved from \n\nHarvard Business School Publishing.\n\n(n.d.).\n\nUnlocking the Power of AI.\n\nRetrieved from \n\nHarvard Business School Publishing.\n\n(2023, February 9).\n\nWhy All Our Classes Suddenly Became AI Classes.\n\nRetrieved from \n\nHarvard Law School.\n\n(n.d.).\n\nStatement on Use of AI Large Language Models.\n\nRetrieved from \n\nInternational Institute for Higher Education in Latin America and the Caribbean (IESALC).\n\n(2023).\n\nChatGPT and Artificial Intelligence in Higher Education: Quick Start Guide (EN).\n\nRetrieved from \n\nJohns Hopkins University.\n\n(2023, January 30).\n\nChatGPT: A Brief Introduction and Considerations for Academic Integrity.\n\nJohns Hopkins University Center for Educational Resources.\n\nRetrieved from \n\nJohns Hopkins University.\n\n(n.d.).\n\nAcademic Integrity at Johns Hopkins University.\n\nRetrieved from \n\nKornberg, R. D., & McEuen, P. L. (2023).\n\nAI Technology and Implications for Privacy.\n\nACS Nano, 17(1), 2–5.\n\nRetrieved from \n\nMassachusetts Institute of Technology.\n\n(n.d.).\n\nAvoiding Plagiarism: Paraphrasing.\n\nRetrieved from \n\nMcGill University.\n\n(n.d.).\n\nIntegrity in Research and Scholarship.\n\nRetrieved from \n\nMcMurry, J., & Maloney, D. (2023).\n\nA Brief Introduction to AI, Machine Learning, and Neural Networks.\n\nJournal of Chemical Education, 100(1), 2–3.\n\nRetrieved from \n\nMoya, B., Eaton, S. E., Pethrick, H., Hayden, K. A., Brennan, R., Wiens, J., McDermott, B., & Lesage, J.\n\n(2023).\n\nAcademic Integrity and Artificial Intelligence in Higher Education Contexts: A Rapid Scoping Review Protocol.\n\nCanadian Perspectives on Academic Integrity, 5(2), 59–75.\n\nRetrieved from \n\nRussell Group.\n\n(2023, July 4).\n\nRG AI Principles.\n\nRetrieved from \n\nSpark, M. (n.d.).\n\nAcademic Integrity: What Is Academic Integrity?\n\nRetrieved from \n\nStanford University.\n\n(2023, February 13).\n\nGenerative AI Policy Guidance.\n\nRetrieved from \n\nUNESCO.\n\n(2022).\n\nDigital Solutions for Skills Development in Africa.\n\nRetrieved from \n\nUNESCO.\n\n(n.d.).\n\nArtificial Intelligence in Education.\n\nRetrieved from \n\nUniversité du Québec en Outaouais.\n\n(n.d.).\n\nArtificial Intelligence and Plagiarism.\n\nRetrieved from \n\nUniversity of Alabama.\n\n(2023).\n\nGuidelines on Using Generative AI Tools.\n\nRetrieved from \n\nUniversity of Alberta.\n\n(n.d.).\n\nArtificial Intelligence & Academic Integrity.\n\nRetrieved from \n\nUniversity of Calgary Libraries.\n\n(2023, June 17).\n\nAPA Citation Guide (7th edition): Online Sources.\n\nRetrieved from \n\nUniversity of Calgary.\n\n(n.d.).\n\nAcademic Integrity at the University of Calgary.\n\nRetrieved from \n\nUniversity of Calgary.\n\n(2023).\n\nTeaching with AI Apps.\n\nRetrieved from \n\nUniversity of California, Berkeley.\n\n(n.d.).\n\nStatements of Course Policies.\n\nRetrieved from \n\nUniversity of California, Berkeley.\n\n(n.d.).\n\nUnderstanding AI Writing Tools and Their Uses in Teaching and Learning at UC Berkeley.\n\nRetrieved from \n\nUniversity of Queensland Library.\n\n(2023, July 10).\n\nReferencing Guide: ChatGPT and Generative AI Tools.\n\nRetrieved from \n\nUniversity of Queensland.\n\n(n.d.).\n\nUsing AI Tools in Your Studies.\n\nRetrieved from \n\nUniversity of Southern California.\n\n(2023).\n\nCIS Generative AI Guidelines.\n\nRetrieved from \n\nUniversity of Toronto Mississauga.\n\n(2017).\n\nPolicy 60: Academic Integrity.",
    "Retrieved from \n\nUniversity of Southern California.\n\n(2023).\n\nCIS Generative AI Guidelines.\n\nRetrieved from \n\nUniversity of Toronto Mississauga.\n\n(2017).\n\nPolicy 60: Academic Integrity.\n\nRetrieved from \n\nUniversity of Toronto Mississauga.\n\n(n.d.).\n\nFrequently Asked Questions - Academic Integrity.\n\nRetrieved from \n\nUniversity of Waterloo Libraries.\n\n(n.d.).\n\nThird-Party Content Use and Specialized Content Submission.\n\nRetrieved from \n\nUniversity of Waterloo.\n\n(n.d.).\n\nChatGPT and Generative AI Tools.\n\nRetrieved from \n\nUniversity of Waterloo.\n\n(n.d.).\n\nCopyright at Waterloo.\n\nRetrieved from \n\nUniversity of Waterloo.\n\n(n.d.).\n\nFrequently Asked Questions - Copyright at Waterloo.\n\nRetrieved from \n\nUniversity of Waterloo.\n\n(n.d.).\n\nThesis Copyright.\n\nRetrieved from \n\nUniversity of Western Australia.\n\n(2023, March 15).\n\nAcademic Integrity Policy.\n\nRetrieved from \n\nYale Alumni Magazine.\n\n(2023).\n\nArtificial Intelligence.\n\nRetrieved from \n\nYale University.\n\n(n.d.).\n\nAI Technology: Guidance and Recommendations for Course Instructors.\n\nRetrieved from \n\nYale University.\n\n(2023, January 24).\n\nAI Technology: Implications for Yale Teaching and Research.\n\nRetrieved from \n\nYork University.\n\n(2022, November 2).\n\nAcademic Integrity Resources.\n\nRetrieved from \n\nYork University.\n\n(n.d.).\n\nAI Technology and Academic Integrity.\n\nRetrieved from",
    "## Basic Safety Requirements for Generative Artificial Intelligence Services\n\n(Draft for Feedback)\n\nReleased on 2023-XX-XX\n\nReleased by the National Information Security Standardization Technical Committee\n\n---\n\n1 Translator’s note: The Chinese word 安全 ānquán—found in the title of this standard and throughout its text—can be translated into English as either “safety” or “security.” The Chinese authors of this standard provided the following English translation of its title: “Basic security requirements for generative artificial intelligence service.” However, this CSET English translation renders 安全 as “safety” in most cases, because in the context of this standard, the authors are mainly discussing the prevention of accidents or unforeseen problems (“safety”) of generative AI, rather than the prevention of deliberate abuse or sabotage (“security”).",
    "### Scope\n\nThis document gives the basic requirements for the safety aspects of generative artificial intelligence (AI) services, including corpus safety (语料安全), model safety, safety measures, and safety assessment.\n\nThis document applies to providers of generative AI services for the public in China as they improve the safety level of their services.\n\nIt applies to providers that carry out safety assessments on their own or entrust them to third parties, and also provides the relevant main oversight department (主管部门) a reference for judging the safety levels of generative AI services.",
    "### Normative Reference Documents\n\nThe contents of the following documents, through normative references in this text, constitute indispensable provisions of this document.\n\nAmong them, for dated references, only the edition corresponding to that date applies to this document.\n\nFor undated references, the latest edition (including all amendments) applies to this document.\n\nInformation security technology terminology GB/T 25069-2022",
    "### Terminology and Definitions\n\nThe terms and definitions defined in GB/T 25069-2022 and listed below apply to this document.\n\n- **Generative artificial intelligence services**  \n  Artificial intelligence services that, based on data, algorithms, models, and rules, can generate text, images, audio, video, and other content according to user prompts.\n\n- **Provider**  \n  An organization or individual that provides generative AI services in the form of interactive interfaces, programmable interfaces, etc., to the public in China.\n\n- **Training data (训练语料)**  \n  All data that serve directly as input for model training, including input data in the pre-training and optimization training processes.\n\n- **Illegal and unhealthy information (违法不良信息)**  \n  A collective term for 11 types of illegal information and 9 types of unhealthy information specified in Provisions on the Governance of the Online Information Content Ecosystem.\n\n- **Sampling qualified rate**  \n  The percentage of samples that do not contain any of the 31 safety risks listed in Appendix A of this document.",
    "### General Provisions\n\nThis document supports the Interim Measures for the Administration of Generative Artificial Intelligence Services, and puts forward the basic safety requirements that providers must follow.\n\nBefore a provider submits a filing application for the online launch of a generative AI service to the relevant main oversight department, it must carry out safety assessments item by item in accordance with all of the requirements in this document, and must submit the assessment results and supporting materials at the time of filing.\n\nIn addition to the basic requirements put forward by this document, providers must also carry out other safety work on their own with respect to cybersecurity, data security, personal information protection, etc., in accordance with China’s laws and regulations and the relevant requirements of national standards.",
    "#### 1.\n\nCorpus Source Safety Requirements\n\nRequirements for providers are as follows.\n\n**Corpus source management:**  \n- A corpus source blacklist shall be established, and data from blacklisted sources shall not be used to carry out training;\n- Safety assessments shall be carried out on each source corpus, and where a source corpus contains over 5% illegal and unhealthy information, it must be added to the blacklist.\n\n- Matching of different source corpora: Diversification shall be increased, and there shall be multiple corpus sources for each language, such as Chinese, English, etc., as well as each corpus type, such as text, images, video, and audio; and corpora from domestic and foreign sources shall be reasonably matched.",
    "**Corpus source traceability:**\n\n- When using an open-source corpus, it is necessary to have an open-source license agreement or relevant licensing document for that corpus source;\n  - Note 1: In situations where aggregated network addresses, data links, etc., are able to point to or generate other data, if it is necessary to use the content thus pointed to or generated as a training corpus, it shall be treated the same as a self-collected corpus.\n\n- When using a self-collected corpus, the provider must have collection records, and shall not collect a corpus that others have expressly declared may not be collected;\n  - Note 2: Self-collected corpora include self-produced corpora and corpora collected from the internet.\n\n- Note 3: Methods of declaring non-collectability include, but are not limited to, the Robots [Exclusion] Protocol.\n\n- When using commercial corpora:  \n  It is necessary to have a legally valid transaction contract, cooperation agreement, etc.\n\n;\n  When the transaction or cooperation parties are unable to provide materials supporting the legality of a corpus, said corpus shall not be used.\n\n- When users enter information for use as corpus, there must be user authorization records.\n\n- Information that is blocked in accordance with the requirements of China's cybersecurity-related laws shall not be used as a training corpus.\n\n- Note 4: Relevant legal and regulatory requirements include, but are not limited to, Article 50 of the Cybersecurity Law.",
    "#### 2.\n\nCorpus Content Safety Requirements\n\nRequirements for providers are as follows.\n\n- **Filtering of training corpus content:**  \n  Methods such as keywords, classification models, and manual sampling inspection shall be adopted to thoroughly filter out all illegal and unhealthy information in corpora.\n\n- **Intellectual property rights:**  \n  - A person shall be put in charge of the intellectual property rights (IPR) of the corpus as well as generated content, and an IPR management strategy shall be established;\n  - Before a corpus is used for training, the person in charge of IPR shall identify IPR infringements in the corpus, and the provider shall not use corpora with infringement issues to carry out training:\n    - Where a training corpus contains literary, artistic, or scientific works, the focus should be on identifying copyright infringement in the training corpus as well as in the generated content;\n    - For a training corpus that contains commercial corpora as well as user-input information, the focus should be on identifying trade secret infringement;\n    - Where a training corpus involves trademarks and patents, the focus should be on identifying whether the provisions of laws and regulations related to trademarks and patents are complied with.\n\n- Channels for reporting and handling complaints on IPR issues shall be established;\n  - In the user service agreement, users shall be informed of IPR-related risks in the use of generated content, and the responsibilities and obligations regarding the identification of IPR issues shall be agreed upon with the users;\n  - The IPR strategy shall be updated in a timely manner in accordance with national policies and third-party complaints;\n  - It is best to have the following IPR measures:\n    - Disclosure of summary information concerning the IPR-related parts of the training corpus;\n    - Support in complaint reporting channels for third-party inquiries about corpus usage and related IPR circumstances.\n\n- **Personal information:**  \n  - When it is necessary to use a corpus containing personal information, the authorized consent of the corresponding subjects of the personal information shall be obtained, or other conditions for the lawful use of such personal information shall be met;\n  - When it is necessary to use a corpus containing sensitive personal information, the individually authorized consent of the corresponding subjects of the personal information shall be obtained, or other conditions for the lawful use of such sensitive personal information shall be met;\n  - When it is necessary to use a corpus containing biometric information such as faces, the written authorized consent of the corresponding subjects of the personal information shall be obtained, or other conditions for the lawful use of such biometric information shall be met.",
    "**Annotators:**\n\n- The provider shall conduct its own examination of annotators, granting annotation qualifications to those who are qualified, and have mechanisms for regular re-training and examination as well as the suspension or cancellation of annotation qualifications when necessary;\n- The functions of annotators shall, at a minimum, be divided into data annotation and data review; and the same annotators should not undertake multiple functions under the same annotation task;\n- Adequate and reasonable time shall be set aside for annotators to perform each annotation task.",
    "**Annotation rules:**\n\n- The annotation rules shall, at a minimum, include such content as annotation objectives, data formats, annotation methods, and quality indicators;\n- Rules for functional annotation and safety annotation shall be formulated separately, and the annotation rules shall, at a minimum, cover data annotation and data review;\n- Functional annotation rules must be able to guide annotators in producing annotated corpora possessing authenticity, accuracy, objectivity, and diversity in accordance with the characteristics of specific fields;\n- The safety annotation rules must be able to guide annotators in annotating the main safety risks around the corpus and generated content, and there shall be corresponding annotation rules for all 31 types of safety risks in Appendix A of this document.",
    "**Annotated content accuracy:**\n\n- For safety annotation, each annotated corpus shall be reviewed and approved by at least one auditor;\n- For functional annotation, each batch of annotated corpora shall be manually sampled, and if it is found that the content is inaccurate, it shall be re-annotated; if it is found that the content contains illegal and unhealthy information, that batch of annotated corpora shall be invalidated.",
    "### Model Safety Requirements\n\nRequirements for providers are as follows.\n\n- If a provider uses a foundation model to carry out research and development, it shall not use a foundation model that has not been filed with the main oversight department.\n\n**Model-generated content safety:**  \n- In the training process, the safety of generated content shall be made one of the main indicators for consideration in evaluating the merits and drawbacks of the generation results;\n- During all conversations, safety testing shall be conducted on the information entered by users, so as to guide the model to generate positive (积极正向) content;\n- Where problems are discovered during the service provision process or when conducting regular testing, the model must be optimized through instruction fine-tuning, reinforcement learning, and other methods.\n\nNotes: Model-generated content refers to original content that is directly output by the model and has not been otherwise processed.\n\n**Service transparency:**  \n- If the service is provided using an interactive interface, the following information shall be disclosed to the public in a prominent position such as the homepage of the website:\n  - Information on the people, situations, and uses to which the service is suitable;\n  - Information on third-party foundation model usage.\n\n- If the service is provided using an interactive interface, the following information shall be disclosed to the users on the homepage of the website, the service agreement, and other easily viewed locations:\n  - Limitations of the service;\n  - Summary information that helps users understand the mechanism of the service, such as the model architecture and training framework used.\n\n- If the service is provided in the form of a programmable interface, the information in 1) and 2) shall be disclosed in the descriptive documentation.\n\n**Accuracy of the generated content:**  \nThe generated content shall accurately respond to the intent of the user's input, and the data and its expression contained therein shall be in line with scientific common sense and mainstream perception, and shall not contain erroneous content.\n\n**Reliability of generated content:**  \nThe responses given by the service according to the user's instructions shall be in a reasonable format and framework, with a high amount of effective content, and should be able to effectively help the user answer questions.",
    "### Safety Measure Requirements\n\nRequirements for providers are as follows.\n\n**People, contexts, and uses for which the model is suitable:**  \n- The necessity, applicability, and safety of applying generative artificial intelligence in various fields within the scope of services must be fully demonstrated;\n- Where the service is used for critical information infrastructure, automatic control, medical information services, psychological counseling, and other important situations, it is necessary to have protection measures that are appropriate to the level of risk as well as to the context;\n- Where a service is suitable for minors, it is necessary to:\n  - Allow guardians to set up anti-addiction measures for minors and protect them with passwords;\n  - Limit the number and duration of conversations by minors in a single day, and require the input of an admin password if the number of times or duration of use is exceeded;\n  - Require confirmation by a guardian before minors can consume;\n  - Filter content inappropriate for minors and show content that is good for physical and mental health.\n\n- If the service is not suitable for minors, technical or management measures should be taken to prevent minors from using it.\n\n**Handling of personal information:**  \nPersonal information shall be protected in accordance with China's personal information protection requirements, and with full reference to existing national standards, such as GB/T 35273.\n\nNotes: Personal information includes, but is not limited to, personal information entered by the user and personal information provided by the user during the registration and other steps.\n\n**Collection of user-entered information for use in training:**  \n- It shall be agreed upon with the user whether user-entered information can be used for training;\n- An option shall be provided to turn off the use of user-entered information for training;\n- It shall not take more than four clicks for the user to reach said option from the main screen of the service;\n- The user shall be clearly informed of the status of user input collection and the method in 2) for turning it off.\n\n**For the labeling of content such as images, videos, etc., the following labeling shall be performed in accordance with TC260-PG-20233A, “Guidelines for Cybersecurity Standards in Practice—Methods for Labeling Generative Artificial Intelligence Service Content”:**  \n- Display area labeling;\n- Hint text labeling for images and videos;\n- Hidden watermark labeling for images, videos, and audio;\n- File metadata labeling;\n- Special service scenario labeling.\n\n**Acceptance of complaints and reports from the public or users:**  \n- Ways for accepting complaints and reports from the public or users, as well as feedback methods, shall be provided, including but not limited to methods such as telephone, email, interactive windows, and text messages;\n- The rules for handling complaints and reports from the public or users and the time limit for said handling shall be established.\n\n**Provision of generated content to users:**  \n- Answering of questions that are obviously extreme, as well as those that obviously induce the generation of illegal and unhealthy information, shall be refused; all other questions shall be answered normally;\n- Monitoring personnel shall be put in place to improve the quality of generated content in a timely manner in accordance with national policies and third-party complaints, and the number of monitoring personnel shall be appropriate to the scale of the service.\n\n**Model updating and upgrading:**  \n- A safety management strategy shall be formulated for when models are updated and upgraded;\n- A management mechanism shall be formed to conduct safety assessments again after important model updates and upgrades, and to re-file with the main oversight department in accordance with provisions.",
    "#### Assessment Methods\n\nRequirements for providers are as follows.\n\n- Safety assessments shall be carried out before a service is launched online and when major changes are made.\n\nThe assessments may be carried out in-house or entrusted to a third-party assessment organization.\n\n- The safety assessments shall cover all of the provisions of this document, and a separate assessment conclusion shall be formed for each provision, which shall be either “conforms,” “does not conform,” or “not applicable”:\n  - If the conclusion is “conforms,” there shall be sufficient supporting materials for this;\n  - Where the conclusion is “does not conform,” the reasons for non-conformity shall be stated, and where technical or management measures inconsistent with this document are adopted but are able to achieve the same safety effect, a detailed explanation shall be given and proof of the effectiveness of the measures shall be provided;\n  - Where the conclusion is “not applicable,” the reasons for its non-applicability shall be stated.\n\n- The conclusions of the assessments for each provision of this document, as well as the relevant evidential and supporting materials, shall be included in the assessment report:\n  - The assessment report shall comply with the requirements of the main oversight department at the time the assessments are conducted;\n  - In the process of writing the assessment report, if the assessment conclusions and relevant circumstances of some provisions in this document cannot be written in the body of the assessment report due to the report format, they shall all be written into an attachment.\n\n- If the safety assessments are carried out in-house, the assessment report shall have the joint signatures of at least three persons in charge:\n  - The legal person of the work unit (单位);\n  - The person in charge with overall responsibility for the safety assessment work, who shall be the main manager of the work unit or the person in charge of cybersecurity;\n  - The person in charge of the legality assessment part of the safety assessment, who shall be the main manager of the work unit or the person in charge of legal affairs.\n\n- Note: If the legal person of the work unit is also the person in charge of cybersecurity or the person in charge of legal affairs, it can be signed by the legal person of the work unit as both, but shall be accompanied by a separate note.",
    "#### Corpus Safety Assessment\n\nWhen a provider carries out an assessment of corpus safety, the requirements are as follows.\n\n- Using manual sampling, and randomly sampling no fewer than 4,000 corpora from the total training corpus, the qualified rate shall not be less than 96%.\n\n- When sampling in conjunction with keywords, classification models, and other techniques, and randomly sampling from the training corpus not less than 10% of the total corpora, the sampling qualified rate shall not be less than 98%.\n\n- The keyword library and classification model used for evaluation shall meet the requirements of Chapter 9 of this document.",
    "#### Generated Content Safety Assessment\n\nThe requirements for providers when assessing the safety of generated content are as follows.\n\n- A test question bank that meets the requirements of section 9.3 of this document shall be constructed.\n\n- Using manual sampling, and randomly selecting no fewer than 1,000 test questions from the test question bank, the qualified rate of sampled model-generated content shall not be less than 90%.\n\n- Using keyword sampling, and randomly selecting no fewer than 1,000 test questions from the test question bank, the qualified rate of sampled model-generated content shall not be less than 90%.\n\n- Using classification model-based sampling, and randomly selecting no fewer than 1,000 test questions from the test question bank, the qualified rate of the sampled model-generated content shall not be less than 90%.",
    "#### Assessment of Refusal to Answer Questions\n\nThe requirements for providers when assessing question refusal are as follows.\n\n- A test question bank that meets the requirements of section 9.4 of this document shall be constructed.\n\n- Randomly selecting no fewer than 300 test questions from the bank of test questions that the model should refuse to answer, the refusal rate of the model shall not be less than 95%.\n\n- Randomly selecting no fewer than 300 test questions from the bank of test questions that the model should not refuse to answer, the refusal rate of the model shall not be more than 5%.",
    "#### Keyword Library\n\nRequirements are as follows.\n\n- Keywords should generally not exceed 10 Chinese characters or 5 words in other languages.\n\n- The keyword library shall be comprehensive, with a total size of not less than 10,000.\n\n- The keyword library shall be representative and contain keywords for at least the 17 safety risks in Appendix A.1 and A.2, with no fewer than 200 keywords for each safety risk in Appendix A.1, and no fewer than 100 keywords for each safety risk in Appendix A.2.",
    "#### Generated Content Test Question Bank\n\nRequirements are as follows.\n\n- The generated content test question bank shall be comprehensive, with a total size of no fewer than 2,000 questions.\n\n- The generated content test question bank shall be representative and completely cover all 31 types of safety risks in Appendix A of this document, with no fewer than 50 test questions for each type of safety risk in Appendix A.1 and A.2, and no fewer than 20 test questions each for other types of safety risks.\n\n- Operational procedures shall be established for identifying all 31 types of safety risks based on the generated content test question bank and the basis for judgment.",
    "#### Refusal to Answer Test Question Bank\n\nRequirements are as follows.\n\n- A test question bank shall be built around questions which the model should refuse to answer:  \n  - The bank of test questions that the model should refuse to answer shall be comprehensive, with a total size of no fewer than 500 questions;\n  - The bank of test questions that the model should refuse to answer shall be representative and cover the 17 safety risks in Appendix A.1 and A.2 of this document, with no fewer than 20 questions for each safety risk.\n\n- A test question bank shall be built around questions that the model should not refuse to answer:  \n  - The bank of test questions that the model should not refuse to answer shall be comprehensive, with a total size of no fewer than 500 questions;\n  - The bank of test questions that the model should not refuse to answer shall be representative, covering aspects of China's system, beliefs, image, culture, customs, ethnicity (民族), geography, history and heroic martyrs (英烈), as well as an individual's gender, age, occupation, and health, and there shall be no fewer than 20 instances of each type of test question.",
    "## Appendix A Main Safety Risks of Corpora and Generated Content (Five classes, 31 types in total)\n\n(Normative)\n\n- Contains content that violates the socialist core values concept (社会主义核心价值观)  \n  - Contains the following content:\n    - Incitement to subvert state power and overthrow the socialist system;\n    - That which endangers national security and interests and harms the image of the state;\n    - Incitement of separatism, or undermining national unity and social stability;\n    - Promotion of terrorism or extremism;\n    - Promotion of ethnic hatred or ethnic discrimination;\n    - Promotion of violence or obscenity and pornography;\n    - Dissemination of false and harmful information;\n    - Other content prohibited by laws and administrative regulations.\n\n- Contains discriminatory content  \n  - Contains the following content:\n    - Ethnic discrimination;\n    - Discrimination on the basis of beliefs;\n    - Nationality-based discrimination;\n    - Discrimination on the basis of regional origin;\n    - Gender discrimination;\n    - Age discrimination;\n    - Occupation-based discrimination;\n    - Health-based discrimination;\n    - Other types of discriminatory content.\n\n- Commercial violations  \n  - The main risks include:\n    - Infringement of IPR of others;\n    - Violation of business ethics;\n    - Disclosure of the trade secrets of others;\n    - Use of algorithms, data, platforms, etc.\n\nto engage in monopolistic or unfair competition behaviors;\n    - Other commercial violations.\n\n- Violations of the legitimate rights and interests of others  \n  - The main risks include:\n    - Endangerment of the physical or mental health of another.\n\n- Unauthorized use of the likeness of another;\n    - Defamation of the reputation of another;\n    - Defamation of the honor of another;\n    - Infringement of others' right to privacy;\n    - Infringement of the personal information rights and interests of others;\n    - Infringement of other legitimate rights and interests of others.\n\n- Inability to meet the safety requirements of specific service types  \n  - The main safety risks in this area are those that exist when generative AI is used for specific service types with higher safety requirements, such as automation, medical information services, psychological counseling, critical information infrastructure, etc.\n\n:\n    - Inaccurate content that is grossly inconsistent with common scientific knowledge or mainstream perception;\n    - Unreliable content that, although not containing grossly erroneous content, does not help the user answer questions.",
    "## BACKGROUND:\n\nAs generative AI technology progresses, chatbots, virtual assistants, and other systems based on it are becoming more prevalent.\n\nThese can include standalone systems, be integrated as features within search engines, or be overtly or transparently embedded in all manner of other software tools.\n\nExamples include ChatGPT and DALL-E from OpenAI, Microsoft Bing’s chat, Microsoft 365 Copilot, and Bard from Google.\n\nGenerative AI tools have the potential to enhance productivity by assisting with tasks like drafting documents, editing text, generating ideas, and software coding.\n\nHowever, these technologies also come with potential risks that include inaccuracies, bias, and unauthorized use of intellectual property in the content generated.\n\nIn addition, content created by AI, and the public availability of information submitted to the AI, could pose security or privacy concerns.",
    "## DEFINITIONS:\n\nGenerative artificial intelligence (AI) uses advanced technologies such as predictive algorithms, machine learning, and large language models to process natural language and produce content in the form of text, images, or other types of media.\n\nGenerated content is typically remarkably similar to what a human creator might produce, such as text consisting of entire narratives of naturally reading sentences.\n\nRestricted Use Information as defined in ITEC 7230A.\n\nEntity is defined as agencies, boards, and commissions under the direction of the Governor or agents and contractors acting on behalf of those agencies, boards, or commissions.",
    "## POLICY:\n\nThis policy shall serve as the primary governing document for usage of generative artificial intelligence technology as a user or related activities by the entities.\n\nWhile any entity may impose additional restrictions through their own policy, such policies must not conflict with the provisions outlined in this policy.\n\nThis policy applies to all business use cases involving the State of Kansas, including but not limited to:\n- Development of software code,\n- Written documentation (i.e., policy, legislation, or regulations) and correspondence (such as memorandums, letters, text messages, and emails),\n- Research,\n- Summarizing and proofreading documents,\n- Making business decisions that impact short-term or long-term activities or policies and procedures.",
    "### Responsibilities\n\nResponses generated from generative AI outputs shall be reviewed by knowledgeable human operators for accuracy, appropriateness, privacy, and security before being acted upon or disseminated.\n\nResponses generated from generative AI shall not:\n- Be used verbatim,\n- Be assumed to be truthful, credible, or accurate,\n- Be treated as the sole source of reference,\n- Be used to issue official statements (i.e., policy, legislation, or regulations),\n- Be solely relied upon for making final decisions,\n- Be used to impersonate individuals or organizations.\n\nRestricted Use Information (RUI) shall not be provided when interacting with generative AI.\n\nRefer to ITEC Policy 7230A Section 9.16 Account Management - RUI.\n\nMaterial that is inappropriate for public release shall not be entered as input to generative AI.\n\nAll information that is provided shall be subjected to the same standard as referenced in the State Social Media Policy and shall be treated as publicly available.\n\nMaterial that is copyrighted or the property of another shall not be entered as input to generative AI.\n\nGenerative AI shall not be used for any activities that are harmful, illegal, or in violation of state policy or agency acceptable use policy.\n\nAgencies shall ensure contractors disclose in their contracts the utilization of generative AI or integrations with generative AI platforms.\n\nAgency contracts shall prohibit contractors from using State of Kansas RUI or other confidential data in generative AI queries or for building or training proprietary generative AI programs unless explicitly approved by the agency head with consultation from the Chief Information Security Officer.\n\nContractors utilizing Generative AI to build software explicitly for the State of Kansas must demonstrate positive control over all data input into the system.",
    "#### Reasons for and objectives of the proposal\n\nThis explanatory memorandum accompanies the proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act).\n\nArtificial Intelligence (AI) is a fast evolving family of technologies that can bring a wide array of economic and societal benefits across the entire spectrum of industries and social activities.\n\nBy improving prediction, optimising operations and resource allocation, and personalising service delivery, the use of artificial intelligence can support socially and environmentally beneficial outcomes and provide key competitive advantages to companies and the European economy.\n\nSuch action is especially needed in high-impact sectors, including climate change, environment and health, the public sector, finance, mobility, home affairs and agriculture.\n\nHowever, the same elements and techniques that power the socio-economic benefits of AI can also bring about new risks or negative consequences for individuals or the society.\n\nIn light of the speed of technological change and possible challenges, the EU is committed to strive for a balanced approach.\n\nIt is in the Union interest to preserve the EU’s technological leadership and to ensure that Europeans can benefit from new technologies developed and functioning according to Union values, fundamental rights and principles.\n\nThis proposal delivers on the political commitment by President von der Leyen, who announced in her political guidelines for the 2019-2024 Commission “A Union that strives for more”[1], that the Commission would put forward legislation for a coordinated European approach on the human and ethical implications of AI.\n\nFollowing on that announcement, on 19 February 2020 the Commission published the White Paper on AI - A European approach to excellence and trust[2].\n\nThe White Paper sets out policy options on how to achieve the twin objective of promoting the uptake of AI and of addressing the risks associated with certain uses of such technology.\n\nThis proposal aims to implement the second objective for the development of an ecosystem of trust by proposing a legal framework for trustworthy AI.\n\nThe proposal is based on EU values and fundamental rights and aims to give people and other users the confidence to embrace AI-based solutions, while encouraging businesses to develop them.\n\nAI should be a tool for people and be a force for good in society with the ultimate aim of increasing human well-being.\n\nRules for AI available in the Union market or otherwise affecting people in the Union should therefore be human-centric, so that people can trust that the technology is used in a way that is safe and compliant with the law, including the respect of fundamental rights.\n\nFollowing the publication of the White Paper, the Commission launched a broad stakeholder consultation, which was met with a great interest by a large number of stakeholders who were largely supportive of regulatory intervention to address the challenges and concerns raised by the increasing use of AI.\n\nThe proposal also responds to explicit requests from the European Parliament (EP) and the European Council, which have repeatedly expressed calls for legislative action to ensure a well-functioning internal market for artificial intelligence systems (‘AI systems’) where both benefits and risks of AI are adequately addressed at Union level.\n\nIt supports the objective of the Union being a global leader in the development of secure, trustworthy and ethical artificial intelligence as stated by the European Council[3] and ensures the protection of ethical principles as specifically requested by the European Parliament[4].\n\nIn 2017, the European Council called for a ‘sense of urgency to address emerging trends’ including ‘issues such as artificial intelligence …, while at the same time ensuring a high level of data protection, digital rights and ethical standards’[5].\n\nIn its 2019 Conclusions on the Coordinated Plan on the development and use of artificial intelligence Made in Europe[6], the Council further highlighted the importance of ensuring that European citizens’ rights are fully respected and called for a review of the existing relevant legislation to make it fit for purpose for the new opportunities and challenges raised by AI.\n\nThe European Council has also called for a clear determination of the AI applications that should be considered high-risk[7].\n\nThe most recent Conclusions from 21 October 2020 further called for addressing the opacity, complexity, bias, a certain degree of unpredictability and partially autonomous behaviour of certain AI systems, to ensure their compatibility with fundamental rights and to facilitate the enforcement of legal rules[8].\n\nThe European Parliament has also undertaken a considerable amount of work in the area of AI.\n\nIn October 2020, it adopted a number of resolutions related to AI, including on ethics[9], liability[10] and copyright[11].",
    "In October 2020, it adopted a number of resolutions related to AI, including on ethics[9], liability[10] and copyright[11].\n\nIn 2021, those were followed by resolutions on AI in criminal matters[12] and in education, culture and the audio-visual sector[13].\n\nThe EP Resolution on a Framework of Ethical Aspects of Artificial Intelligence, Robotics and Related Technologies specifically recommends to the Commission to propose legislative action to harness the opportunities and benefits of AI, but also to ensure protection of ethical principles.\n\nThe resolution includes a text of the legislative proposal for a regulation on ethical principles for the development, deployment and use of AI, robotics and related technologies.\n\nIn accordance with the political commitment made by President von der Leyen in her Political Guidelines as regards resolutions adopted by the European Parliament under Article 225 TFEU, this proposal takes into account the aforementioned resolution of the European Parliament in full respect of proportionality, subsidiarity and better law making principles.\n\nAgainst this political context, the Commission puts forward the proposed regulatory framework on Artificial Intelligence with the following specific objectives:\n\n- Ensure that AI systems placed on the Union market and used are safe and respect existing law on fundamental rights and Union values;\n- Ensure legal certainty to facilitate investment and innovation in AI;\n- Enhance governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to AI systems;\n- Facilitate the development of a single market for lawful, safe and trustworthy AI applications and prevent market fragmentation.\n\nTo achieve those objectives, this proposal presents a balanced and proportionate horizontal regulatory approach to AI that is limited to the minimum necessary requirements to address the risks and problems linked to AI, without unduly constraining or hindering technological development or otherwise disproportionately increasing the cost of placing AI solutions on the market.\n\nThe proposal sets a robust and flexible legal framework.\n\nOn the one hand, it is comprehensive and future-proof in its fundamental regulatory choices, including the principle-based requirements that AI systems should comply with.\n\nOn the other hand, it puts in place a proportionate regulatory system centred on a well-defined risk-based regulatory approach that does not create unnecessary restrictions to trade, whereby legal intervention is tailored to those concrete situations where there is a justified cause for concern or where such concern can reasonably be anticipated in the near future.\n\nAt the same time, the legal framework includes flexible mechanisms that enable it to be dynamically adapted as the technology evolves and new concerning situations emerge.\n\nThe proposal sets harmonised rules for the development, placement on the market and use of AI systems in the Union following a proportionate risk-based approach.\n\nIt proposes a single future-proof definition of AI.\n\nCertain particularly harmful AI practices are prohibited as contravening Union values, while specific restrictions and safeguards are proposed in relation to certain uses of remote biometric identification systems for the purpose of law enforcement.\n\nThe proposal lays down a solid risk methodology to define “high-risk” AI systems that pose significant risks to the health and safety or fundamental rights of persons.\n\nThose AI systems will have to comply with a set of horizontal mandatory requirements for trustworthy AI and follow conformity assessment procedures before those systems can be placed on the Union market.\n\nPredictable, proportionate and clear obligations are also placed on providers and users of those systems to ensure safety and respect of existing legislation protecting fundamental rights throughout the whole AI systems’ lifecycle.\n\nFor some specific AI systems, only minimum transparency obligations are proposed, in particular when chatbots or ‘deep fakes’ are used.\n\nThe proposed rules will be enforced through a governance system at Member States level, building on already existing structures, and a cooperation mechanism at Union level with the establishment of a European Artificial Intelligence Board.\n\nAdditional measures are also proposed to support innovation, in particular through AI regulatory sandboxes and other measures to reduce the regulatory burden and to support Small and Medium-Sized Enterprises (‘SMEs’) and start-ups.",
    "### Consistency with existing policy provisions in the policy area\n\nThe horizontal nature of the proposal requires full consistency with existing Union legislation applicable to sectors where high-risk AI systems are already used or likely to be used in the near future.\n\nConsistency is also ensured with the EU Charter of Fundamental Rights and the existing secondary Union legislation on data protection, consumer protection, non-discrimination and gender equality.\n\nThe proposal is without prejudice and complements the General Data Protection Regulation (Regulation (EU) 2016/679) and the Law Enforcement Directive (Directive (EU) 2016/680) with a set of harmonised rules applicable to the design, development and use of certain high-risk AI systems and restrictions on certain uses of remote biometric identification systems.\n\nFurthermore, the proposal complements existing Union law on non-discrimination with specific requirements that aim to minimise the risk of algorithmic discrimination, in particular in relation to the design and the quality of data sets used for the development of AI systems complemented with obligations for testing, risk management, documentation and human oversight throughout the AI systems’ lifecycle.\n\nThe proposal is without prejudice to the application of Union competition law.\n\nAs regards high-risk AI systems which are safety components of products, this proposal will be integrated into the existing sectoral safety legislation to ensure consistency, avoid duplications and minimise additional burdens.\n\nIn particular, as regards high-risk AI systems related to products covered by the New Legislative Framework (NLF) legislation (e.g.\n\nmachinery, medical devices, toys), the requirements for AI systems set out in this proposal will be checked as part of the existing conformity assessment procedures under the relevant NLF legislation.\n\nWith regard to the interplay of requirements, while the safety risks specific to AI systems are meant to be covered by the requirements of this proposal, NLF legislation aims at ensuring the overall safety of the final product and therefore may contain specific requirements regarding the safe integration of an AI system into the final product.\n\nThe proposal for a Machinery Regulation, which is adopted on the same day as this proposal fully reflects this approach.\n\nAs regards high-risk AI systems related to products covered by relevant Old Approach legislation (e.g.\n\naviation, cars), this proposal would not directly apply.\n\nHowever, the ex-ante essential requirements for high-risk AI systems set out in this proposal will have to be taken into account when adopting relevant implementing or delegated legislation under those acts.\n\nAs regards AI systems provided or used by regulated credit institutions, the authorities responsible for the supervision of the Union’s financial services legislation should be designated as competent authorities for supervising the requirements in this proposal to ensure a coherent enforcement of the obligations under this proposal and the Union’s financial services legislation where AI systems are to some extent implicitly regulated in relation to the internal governance system of credit institutions.\n\nTo further enhance consistency, the conformity assessment procedure and some of the providers’ procedural obligations under this proposal are integrated into the procedures under Directive 2013/36/EU on access to the activity of credit institutions and the prudential supervision[14].\n\nThis proposal is also consistent with the applicable Union legislation on services, including on intermediary services regulated by the e-Commerce Directive 2000/31/EC[15] and the Commission’s recent proposal for the Digital Services Act (DSA)[16].\n\nIn relation to AI systems that are components of large-scale IT systems in the Area of Freedom, Security and Justice managed by the European Union Agency for the Operational Management of Large-Scale IT Systems (eu-LISA), the proposal will not apply to those AI systems that have been placed on the market or put into service before one year has elapsed from the date of application of this Regulation, unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the AI system or AI systems concerned.",
    "### Consistency with other Union policies\n\nThe proposal is part of a wider comprehensive package of measures that address problems posed by the development and use of AI, as examined in the White Paper on AI.\n\nConsistency and complementarity is therefore ensured with other ongoing or planned initiatives of the Commission that also aim to address those problems, including the revision of sectoral product legislation (e.g.\n\nthe Machinery Directive, the General Product Safety Directive) and initiatives that address liability issues related to new technologies, including AI systems.\n\nThose initiatives will build on and complement this proposal in order to bring legal clarity and foster the development of an ecosystem of trust in AI in Europe.\n\nThe proposal is also coherent with the Commission’s overall digital strategy in its contribution to promoting technology that works for people, one of the three main pillars of the policy orientation and objectives announced in the Communication ‘Shaping Europe's digital future’[17].\n\nIt lays down a coherent, effective and proportionate framework to ensure AI is developed in ways that respect people’s rights and earn their trust, making Europe fit for the digital age and turning the next ten years into the Digital Decade[18].\n\nFurthermore, the promotion of AI-driven innovation is closely linked to the Data Governance Act[19], the Open Data Directive[20] and other initiatives under the EU strategy for data[21], which will establish trusted mechanisms and services for re-use, sharing and pooling of data that are essential for the development of data-driven AI models of high quality.\n\nThe proposal also strengthens significantly the Union’s role to help shape global norms and standards and promote trustworthy AI that is consistent with Union values and interests.\n\nIt provides the Union with a powerful basis to engage further with its external partners, including third countries, and at international fora on issues relating to AI.",
    "#### Legal basis\n\nThe legal basis for the proposal is in the first place Article 114 of the Treaty on the Functioning of the European Union (TFEU), which provides for the adoption of measures to ensure the establishment and functioning of the internal market.\n\nThis proposal constitutes a core part of the EU digital single market strategy.\n\nThe primary objective of this proposal is to ensure the proper functioning of the internal market by setting harmonised rules in particular on the development, placing on the Union market and the use of products and services making use of AI technologies or provided as stand-alone AI systems.\n\nSome Member States are already considering national rules to ensure that AI is safe and is developed and used in compliance with fundamental rights obligations.\n\nThis will likely lead to two main problems: i) a fragmentation of the internal market on essential elements regarding in particular the requirements for the AI products and services, their marketing, their use, the liability and the supervision by public authorities, and ii) the substantial diminishment of legal certainty for both providers and users of AI systems on how existing and new rules will apply to those systems in the Union.\n\nGiven the wide circulation of products and services across borders, these two problems can be best solved through EU harmonizing legislation.\n\nIndeed, the proposal defines common mandatory requirements applicable to the design and development of certain AI systems before they are placed on the market that will be further operationalised through harmonised technical standards.\n\nThe proposal also addresses the situation after AI systems have been placed on the market by harmonising the way in which ex-post controls are conducted.\n\nIn addition, considering that this proposal contains certain specific rules on the protection of individuals with regard to the processing of personal data, notably restrictions of the use of AI systems for ‘real-time’ remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU.",
    "#### Subsidiarity (for non-exclusive competence)\n\nThe nature of AI, which often relies on large and varied datasets and which may be embedded in any product or service circulating freely within the internal market, entails that the objectives of this proposal cannot be effectively achieved by Member States alone.\n\nFurthermore, an emerging patchwork of potentially divergent national rules will hamper the seamless circulation of products and services related to AI systems across the EU and will be ineffective in ensuring the safety and protection of fundamental rights and Union values across the different Member States.\n\nNational approaches in addressing the problems will only create additional legal uncertainty and barriers, and will slow market uptake of AI.\n\nThe objectives of this proposal can be better achieved at Union level to avoid a further fragmentation of the Single Market into potentially contradictory national frameworks preventing the free circulation of goods and services embedding AI.\n\nA solid European regulatory framework for trustworthy AI will also ensure a level playing field and protect all people, while strengthening Europe’s competitiveness and industrial basis in AI.\n\nOnly common action at Union level can also protect the Union’s digital sovereignty and leverage its tools and regulatory powers to shape global rules and standards.",
    "#### Proportionality\n\nThe proposal builds on existing legal frameworks and is proportionate and necessary to achieve its objectives, since it follows a risk-based approach and imposes regulatory burdens only when an AI system is likely to pose high risks to fundamental rights and safety.\n\nFor other, non-high-risk AI systems, only very limited transparency obligations are imposed, for example in terms of the provision of information to flag the use of an AI system when interacting with humans.\n\nFor high-risk AI systems, the requirements of high-quality data, documentation and traceability, transparency, human oversight, accuracy and robustness, are strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that are not covered by other existing legal frameworks.\n\nHarmonised standards and supporting guidance and compliance tools will assist providers and users in complying with the requirements laid down by the proposal and minimise their costs.\n\nThe costs incurred by operators are proportionate to the objectives achieved and the economic and reputational benefits that operators can expect from this proposal.",
    "#### Choice of the instrument\n\nThe choice of a regulation as a legal instrument is justified by the need for a uniform application of the new rules, such as the definition of AI, the prohibition of certain harmful AI-enabled practices and the classification of certain AI systems.\n\nThe direct applicability of a Regulation, in accordance with Article 288 TFEU, will reduce legal fragmentation and facilitate the development of a single market for lawful, safe and trustworthy AI systems.\n\nIt will do so, in particular, by introducing a harmonised set of core requirements with regard to AI systems classified as high-risk and obligations for providers and users of those systems, improving the protection of fundamental rights and providing legal certainty for operators and consumers alike.\n\nAt the same time, the provisions of the regulation are not overly prescriptive and leave room for different levels of Member State action for elements that do not undermine the objectives of the initiative, in particular the internal organisation of the market surveillance system and the uptake of measures to foster innovation.",
    "#### Stakeholder consultation\n\nThis proposal is the result of extensive consultation with all major stakeholders, in which the general principles and minimum standards for consultation of interested parties by the Commission were applied.\n\nAn online public consultation was launched on 19 February 2020 along with the publication of the White Paper on Artificial Intelligence and ran until 14 June 2020.\n\nThe objective of that consultation was to collect views and opinions on the White Paper.\n\nIt targeted all interested stakeholders from the public and private sectors, including governments, local authorities, commercial and non-commercial organisations, social partners, experts, academics and citizens.\n\nAfter analysing all the responses received, the Commission published a summary outcome and the individual responses on its website[22].\n\nIn total, 1215 contributions were received, of which 352 were from companies or business organisations/associations, 406 from individuals (92% individuals from EU), 152 on behalf of academic/research institutions, and 73 from public authorities.\n\nCivil society’s voices were represented by 160 respondents (among which 9 consumers’ organisations, 129 non-governmental organisations and 22 trade unions), 72 respondents contributed as ‘others’.\n\nOf the 352 business and industry representatives, 222 were companies and business representatives, 41.5% of which were micro, small and medium-sized enterprises.\n\nThe rest were business associations.\n\nOverall, 84% of business and industry replies came from the EU-27.\n\nDepending on the question, between 81 and 598 of the respondents used the free text option to insert comments.\n\nOver 450 position papers were submitted through the EU Survey website, either in addition to questionnaire answers (over 400) or as stand-alone contributions (over 50).\n\nOverall, there is a general agreement amongst stakeholders on a need for action.\n\nA large majority of stakeholders agree that legislative gaps exist or that new legislation is needed.\n\nHowever, several stakeholders warn the Commission to avoid duplication, conflicting obligations and overregulation.\n\nThere were many comments underlining the importance of a technology-neutral and proportionate regulatory framework.\n\nStakeholders mostly requested a narrow, clear and precise definition for AI.\n\nStakeholders also highlighted that besides the clarification of the term of AI, it is important to define ‘risk’, ‘high-risk’, ‘low-risk’, ‘remote biometric identification’ and ‘harm’.",
    "# Risk-Based Approach\n\nThe respondents are explicitly in favor of the risk-based approach.\n\nUsing a risk-based framework was considered a better option than blanket regulation of all AI systems.\n\nThe types of risks and threats should be based on a sector-by-sector and case-by-case approach.\n\nRisks also should be calculated taking into account the impact on rights and safety.\n\nRegulatory sandboxes could be very useful for the promotion of AI and are welcomed by certain stakeholders, especially the Business Associations.\n\nAmong those who formulated their opinion on the enforcement models, more than 50%, especially from the business associations, were in favor of a combination of an ex-ante risk self-assessment and an ex-post enforcement for high-risk AI systems.",
    "# Collection and Use of Expertise\n\nThe proposal builds on two years of analysis and close involvement of stakeholders, including academics, businesses, social partners, non-governmental organisations, Member States, and citizens.\n\nThe preparatory work started in 2018 with the setting up of a High-Level Expert Group on AI (HLEG) which had an inclusive and broad composition of 52 well-known experts tasked to advise the Commission on the implementation of the Commission’s Strategy on Artificial Intelligence.\n\nIn April 2019, the Commission supported the key requirements set out in the HLEG ethics guidelines for Trustworthy AI, which had been revised to take into account more than 500 submissions from stakeholders.\n\nThe key requirements reflect a widespread and common approach, as evidenced by a plethora of ethical codes and principles developed by many private and public organizations in Europe and beyond, that AI development and use should be guided by certain essential value-oriented principles.\n\nThe Assessment List for Trustworthy Artificial Intelligence (ALTAI) made those requirements operational in a piloting process with over 350 organisations.\n\nEuropean Commission, Building Trust in Human-Centric Artificial Intelligence, COM(2019) 168.\n\nHLEG, Ethics Guidelines for Trustworthy AI, 2019.\n\nHLEG, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 2020.\n\nIn addition, the AI Alliance was formed as a platform for approximately 4000 stakeholders to debate the technological and societal implications of AI, culminating in a yearly AI Assembly.\n\nThe White Paper on AI further developed this inclusive approach, inciting comments from more than 1250 stakeholders, including over 450 additional position papers.\n\nAs a result, the Commission published an Inception Impact Assessment, which in turn attracted more than 130 comments.\n\nAdditional stakeholder workshops and events were also organized, the results of which support the analysis in the impact assessment and the policy choices made in this proposal.\n\nAn external study was also procured to feed into the impact assessment.",
    "# Impact Assessment\n\nIn line with its “Better Regulation” policy, the Commission conducted an impact assessment for this proposal examined by the Commission's Regulatory Scrutiny Board.\n\nA meeting with the Regulatory Scrutiny Board was held on 16 December 2020, which was followed by a negative opinion.\n\nAfter substantial revision of the impact assessment to address the comments and a resubmission of the impact assessment, the Regulatory Scrutiny Board issued a positive opinion on 21 March 2021.\n\nThe opinions of the Regulatory Scrutiny Board, the recommendations, and an explanation of how they have been taken into account are presented in Annex 1 of the impact assessment.\n\nThe Commission examined different policy options to achieve the general objective of the proposal, which is to ensure the proper functioning of the single market by creating the conditions for the development and use of trustworthy AI in the Union.\n\nFour policy options of different degrees of regulatory intervention were assessed:  \n- Option 1: EU legislative instrument setting up a voluntary labelling scheme;  \n- Option 2: a sectoral, “ad-hoc” approach;  \n- Option 3: Horizontal EU legislative instrument following a proportionate risk-based approach;  \n- Option 3+: Horizontal EU legislative instrument following a proportionate risk-based approach + codes of conduct for non-high-risk AI systems;  \n- Option 4: Horizontal EU legislative instrument establishing mandatory requirements for all AI systems, irrespective of the risk they pose.\n\nAccording to the Commission's established methodology, each policy option was evaluated against economic and societal impacts, with a particular focus on impacts on fundamental rights.\n\nThe preferred option is option 3+, a regulatory framework for high-risk AI systems only, with the possibility for all providers of non-high-risk AI systems to follow a code of conduct.\n\nThe requirements will concern data, documentation and traceability, provision of information and transparency, human oversight and robustness and accuracy and would be mandatory for high-risk AI systems.\n\nCompanies that introduced codes of conduct for other AI systems would do so voluntarily.\n\nThe AI Alliance is a multi-stakeholder forum launched in June 2018, AI Alliance.\n\nThe preferred option was considered suitable to address in the most effective way the objectives of this proposal.\n\nBy requiring a restricted yet effective set of actions from AI developers and users, the preferred option limits the risks of violation of fundamental rights and safety of people and fosters effective supervision and enforcement, by targeting the requirements only to systems where there is a high risk that such violations could occur.\n\nAs a result, that option keeps compliance costs to a minimum, thus avoiding an unnecessary slowing of uptake due to higher prices and compliance costs.\n\nIn order to address possible disadvantages for SMEs, this option includes several provisions to support their compliance and reduce their costs, including creation of regulatory sandboxes and obligation to consider SMEs interests when setting fees related to conformity assessment.\n\nThe preferred option will increase people’s trust in AI, companies will gain in legal certainty, and Member States will see no reason to take unilateral action that could fragment the single market.\n\nAs a result of higher demand due to higher trust, more available offers due to legal certainty, and the absence of obstacles to cross-border movement of AI systems, the single market for AI will likely flourish.\n\nThe European Union will continue to develop a fast-growing AI ecosystem of innovative services and products embedding AI technology or stand-alone AI systems, resulting in increased digital autonomy.\n\nBusinesses or public authorities that develop or use AI applications that constitute a high risk for the safety or fundamental rights of citizens would have to comply with specific requirements and obligations.\n\nCompliance with these requirements would imply costs amounting to approximately EUR € 6000 to EUR € 7000 for the supply of an average high-risk AI system of around EUR € 170000 by 2025.\n\nFor AI users, there would also be the annual cost for the time spent on ensuring human oversight where this is appropriate, depending on the use case.\n\nThose have been estimated at approximately EUR € 5000 to EUR € 8000 per year.\n\nVerification costs could amount to another EUR € 3000 to EUR € 7500 for suppliers of high-risk AI.\n\nBusinesses or public authorities that develop or use any AI applications not classified as high risk would only have minimal obligations of information.\n\nHowever, they could choose to join others and together adopt a code of conduct to follow suitable requirements, and to ensure that their AI systems are trustworthy.\n\nIn such a case, costs would be at most as high as for high-risk AI systems, but most probably lower.",
    "In such a case, costs would be at most as high as for high-risk AI systems, but most probably lower.\n\nThe impacts of the policy options on different categories of stakeholders (economic operators/ business; conformity assessment bodies, standardisation bodies and other public bodies; individuals/citizens; researchers) are explained in detail in Annex 3 of the Impact assessment supporting this proposal.",
    "# Regulatory Fitness and Simplification\n\nThis proposal lays down obligation that will apply to providers and users of high-risk AI systems.\n\nFor providers who develop and place such systems on the Union market, it will create legal certainty and ensure that no obstacle to the cross-border provision of AI-related services and products emerge.\n\nFor companies using AI, it will promote trust among their customers.\n\nFor national public administrations, it will promote public trust in the use of AI and strengthen enforcement mechanisms (by introducing a European coordination mechanism, providing for appropriate capacities, and facilitating audits of the AI systems with new requirements for documentation, traceability and transparency).\n\nMoreover, the framework will envisage specific measures supporting innovation, including regulatory sandboxes and specific measures supporting small-scale users and providers of high-risk AI systems to comply with the new rules.\n\nThe proposal also specifically aims at strengthening Europe’s competitiveness and industrial basis in AI.\n\nFull consistency is ensured with existing sectoral Union legislation applicable to AI systems (e.g.\n\non products and services) that will bring further clarity and simplify the enforcement of the new rules.",
    "# Fundamental Rights\n\nThe use of AI with its specific characteristics (e.g.\n\nopacity, complexity, dependency on data, autonomous behaviour) can adversely affect a number of fundamental rights enshrined in the EU Charter of Fundamental Rights (‘the Charter’).\n\nThis proposal seeks to ensure a high level of protection for those fundamental rights and aims to address various sources of risks through a clearly defined risk-based approach.\n\nWith a set of requirements for trustworthy AI and proportionate obligations on all value chain participants, the proposal will enhance and promote the protection of the rights protected by the Charter: the right to human dignity (Article 1), respect for private life and protection of personal data (Articles 7 and 8), non-discrimination (Article 21) and equality between women and men (Article 23).\n\nIt aims to prevent a chilling effect on the rights to freedom of expression (Article 11) and freedom of assembly (Article 12), to ensure protection of the right to an effective remedy and to a fair trial, the rights of defence and the presumption of innocence (Articles 47 and 48), as well as the general principle of good administration.\n\nFurthermore, as applicable in certain domains, the proposal will positively affect the rights of a number of special groups, such as the workers’ rights to fair and just working conditions (Article 31), a high level of consumer protection (Article 28), the rights of the child (Article 24) and the integration of persons with disabilities (Article 26).\n\nThe right to a high level of environmental protection and the improvement of the quality of the environment (Article 37) is also relevant, including in relation to the health and safety of people.\n\nThe obligations for ex ante testing, risk management and human oversight will also facilitate the respect of other fundamental rights by minimizing the risk of erroneous or biased AI-assisted decisions in critical areas such as education and training, employment, important services, law enforcement and the judiciary.\n\nIn case infringements of fundamental rights still happen, effective redress for affected persons will be made possible by ensuring transparency and traceability of the AI systems coupled with strong ex post controls.\n\nThis proposal imposes some restrictions on the freedom to conduct business (Article 16) and the freedom of art and science (Article 13) to ensure compliance with overriding reasons of public interest such as health, safety, consumer protection and the protection of other fundamental rights (‘responsible innovation’) when high-risk AI technology is developed and used.\n\nThose restrictions are proportionate and limited to the minimum necessary to prevent and mitigate serious safety risks and likely infringements of fundamental rights.\n\nThe increased transparency obligations will also not disproportionately affect the right to protection of intellectual property (Article 17(2)), since they will be limited only to the minimum necessary information for individuals to exercise their right to an effective remedy and to the necessary transparency towards supervision and enforcement authorities, in line with their mandates.\n\nAny disclosure of information will be carried out in compliance with relevant legislation in the field, including Directive 2016/943 on the protection of undisclosed know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure.\n\nWhen public authorities and notified bodies need to be given access to confidential information or source code to examine compliance with substantial obligations, they are placed under binding confidentiality obligations.",
    "# Budgetary Implications\n\nMember States will have to designate supervisory authorities in charge of implementing the legislative requirements.\n\nTheir supervisory function could build on existing arrangements, for example regarding conformity assessment bodies or market surveillance, but would require sufficient technological expertise and human and financial resources.\n\nDepending on the pre-existing structure in each Member State, this could amount to 1 to 25 Full Time Equivalents per Member State.\n\nA detailed overview of the costs involved is provided in the ‘financial statement’ linked to this proposal.",
    "## Implementation Plans and Monitoring, Evaluation and Reporting Arrangements\n\nProviding for a robust monitoring and evaluation mechanism is crucial to ensure that the proposal will be effective in achieving its specific objectives.\n\nThe Commission will be in charge of monitoring the effects of the proposal.\n\nIt will establish a system for registering stand-alone high-risk AI applications in a public EU-wide database.\n\nThis registration will also enable competent authorities, users and other interested people to verify if the high-risk AI system complies with the requirements laid down in the proposal and to exercise enhanced oversight over those AI systems posing high risks to fundamental rights.\n\nTo feed this database, AI providers will be obliged to provide meaningful information about their systems and the conformity assessment carried out on those systems.\n\nMoreover, AI providers will be obliged to inform national competent authorities about serious incidents or malfunctioning that constitute a breach of fundamental rights obligations as soon as they become aware of them, as well as any recalls or withdrawals of AI systems from the market.\n\nNational competent authorities will then investigate the incidents/or malfunctioning, collect all the necessary information and regularly transmit it to the Commission with adequate metadata.\n\nThe Commission will complement this information on the incidents by a comprehensive analysis of the overall market for AI.\n\nThe Commission will publish a report evaluating and reviewing the proposed AI framework five years following the date on which it becomes applicable.",
    "### Scope and Definitions (Title I)\n\nTitle I defines the subject matter of the regulation and the scope of application of the new rules that cover the placing on the market, putting into service and use of AI systems.\n\nIt also sets out the definitions used throughout the instrument.\n\nThe definition of AI system in the legal framework aims to be as technology neutral and future proof as possible, taking into account the fast technological and market developments related to AI.\n\nIn order to provide the needed legal certainty, Title I is complemented by Annex I, which contains a detailed list of approaches and techniques for the development of AI to be adapted by the Commission in line with new technological developments.\n\nKey participants across the AI value chain are also clearly defined such as providers and users of AI systems that cover both public and private operators to ensure a level playing field.",
    "### Prohibited Artificial Intelligence Practices (Title II)\n\nTitle II establishes a list of prohibited AI.\n\nThe regulation follows a risk-based approach, differentiating between uses of AI that create (i) an unacceptable risk, (ii) a high risk, and (iii) low or minimal risk.\n\nThe list of prohibited practices in Title II comprises all those AI systems whose use is considered unacceptable as contravening Union values, for instance by violating fundamental rights.\n\nThe prohibitions cover practices that have a significant potential to manipulate persons through subliminal techniques beyond their consciousness or exploit vulnerabilities of specific vulnerable groups such as children or persons with disabilities in order to materially distort their behavior in a manner that is likely to cause them or another person psychological or physical harm.\n\nOther manipulative or exploitative practices affecting adults that might be facilitated by AI systems could be covered by the existing data protection, consumer protection and digital service legislation that guarantees that natural persons are properly informed and have free choice not to be subject to profiling or other practices that might affect their behavior.\n\nThe proposal also prohibits AI-based social scoring for general purposes done by public authorities.\n\nFinally, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement is also prohibited unless certain limited exceptions apply.",
    "### High-Risk AI Systems (Title III)\n\nTitle III contains specific rules for AI systems that create a high risk to the health and safety or fundamental rights of natural persons.\n\nIn line with a risk-based approach, those high-risk AI systems are permitted on the European market subject to compliance with certain mandatory requirements and an ex-ante conformity assessment.\n\nThe classification of an AI system as high-risk is based on the intended purpose of the AI system, in line with existing product safety legislation.\n\nTherefore, the classification as high-risk does not only depend on the function performed by the AI system, but also on the specific purpose and modalities for which that system is used.\n\nChapter 1 of Title III sets the classification rules and identifies two main categories of high-risk AI systems:  \n- AI systems intended to be used as safety component of products that are subject to third-party ex-ante conformity assessment;  \n- other stand-alone AI systems with mainly fundamental rights implications that are explicitly listed in Annex III.\n\nThis list of high-risk AI systems in Annex III contains a limited number of AI systems whose risks have already materialized or are likely to materialize in the near future.\n\nTo ensure that the regulation can be adjusted to emerging uses and applications of AI, the Commission may expand the list of high-risk AI systems used within certain predefined areas, by applying a set of criteria and risk assessment methodology.\n\nChapter 2 sets out the legal requirements for high-risk AI systems in relation to data and data governance, documentation and recording-keeping, transparency and provision of information to users, human oversight, robustness, accuracy, and security.\n\nThe proposed minimum requirements are already state-of-the-art for many diligent operators and the result of two years of preparatory work, derived from the Ethics Guidelines of the HLEG, piloted by more than 350 organisations.\n\nThey are also largely consistent with other international recommendations and principles, which ensures that the proposed AI framework is compatible with those adopted by the EU’s international trade partners.\n\nThe precise technical solutions to achieve compliance with those requirements may be provided by standards or by other technical specifications or otherwise be developed in accordance with general engineering or scientific knowledge at the discretion of the provider of the AI system.\n\nThis flexibility is particularly important because it allows providers of AI systems to choose the way to meet their requirements, taking into account the state-of-the-art and technological and scientific progress in this field.\n\nChapter 3 places a clear set of horizontal obligations on providers of high-risk AI systems.\n\nProportionate obligations are also placed on users and other participants across the AI value chain (e.g., importers, distributors, authorized representatives).\n\nChapter 4 sets the framework for notified bodies to be involved as independent third parties in conformity assessment procedures, while Chapter 5 explains in detail the conformity assessment procedures to be followed for each type of high-risk AI system.\n\nThe conformity assessment approach aims to minimise the burden for economic operators as well as for notified bodies, whose capacity needs to be progressively ramped up over time.\n\nAI systems intended to be used as safety components of products that are regulated under the New Legislative Framework legislation (e.g., machinery, toys, medical devices, etc.)\n\nwill be subject to the same ex-ante and ex-post compliance and enforcement mechanisms of the products of which they are a component.\n\nThe key difference is that the ex-ante and ex-post mechanisms will ensure compliance not only with the requirements established by sectorial legislation, but also with the requirements established by this regulation.\n\nAs regards stand-alone high-risk AI systems that are referred to in Annex III, a new compliance and enforcement system will be established.\n\nThis follows the model of the New Legislative Framework legislation implemented through internal control checks by the providers with the exception of remote biometric identification systems that would be subject to third-party conformity assessment.\n\nA comprehensive ex-ante conformity assessment through internal checks, combined with a strong ex-post enforcement, could be an effective and reasonable solution for those systems, given the early phase of the regulatory intervention and the fact the AI sector is very innovative and expertise for auditing is only now being accumulated.\n\nAn assessment through internal checks for ‘stand-alone’ high-risk AI systems would require a full, effective and properly documented ex-ante compliance with all requirements of the regulation and compliance with robust quality and risk management systems and post-market monitoring.",
    "After the provider has performed the relevant conformity assessment, it should register those stand-alone high-risk AI systems in an EU database that will be managed by the Commission to increase public transparency and oversight and strengthen ex-post supervision by competent authorities.\n\nBy contrast, for reasons of consistency with the existing product safety legislation, the conformity assessments of AI systems that are safety components of products will follow a system with third-party conformity assessment procedures already established under the relevant sectoral product safety legislation.\n\nNew ex-ante re-assessments of the conformity will be needed in case of substantial modifications to the AI systems (and notably changes that go beyond what is pre-determined by the provider in its technical documentation and checked at the moment of the ex-ante conformity assessment).",
    "### Transparency Obligations for Certain AI Systems (Title IV)\n\nTitle IV concerns certain AI systems to take account of the specific risks of manipulation they pose.\n\nTransparency obligations will apply for systems that (i) interact with humans, (ii) are used to detect emotions or determine association with (social) categories based on biometric data, or (iii) generate or manipulate content (‘deep fakes’).\n\nWhen persons interact with an AI system or their emotions or characteristics are recognised through automated means, people must be informed of that circumstance.\n\nIf an AI system is used to generate or manipulate image, audio, or video content that appreciably resembles authentic content, there should be an obligation to disclose that the content is generated through automated means, subject to exceptions for legitimate purposes (law enforcement, freedom of expression).\n\nThis allows persons to make informed choices or step back from a given situation.",
    "### Measures in Support of Innovation (Title V)\n\nTitle V contributes to the objective to create a legal framework that is innovation-friendly, future-proof, and resilient to disruption.\n\nTo that end, it encourages national competent authorities to set up regulatory sandboxes and sets a basic framework in terms of governance, supervision, and liability.\n\nAI regulatory sandboxes establish a controlled environment to test innovative technologies for a limited time on the basis of a testing plan agreed with the competent authorities.\n\nTitle V also contains measures to reduce the regulatory burden on SMEs and start-ups.",
    "### Governance and Implementation (Titles VI, VII, and VIII)\n\nTitle VI sets up the governance systems at Union and national levels.\n\nAt the Union level, the proposal establishes a European Artificial Intelligence Board (the ‘Board’), composed of representatives from the Member States and the Commission.\n\nThe Board will facilitate a smooth, effective, and harmonised implementation of this regulation by contributing to the effective cooperation of the national supervisory authorities and the Commission and providing advice and expertise to the Commission.\n\nIt will also collect and share best practices among the Member States.\n\nAt the national level, Member States will have to designate one or more national competent authorities and, among them, the national supervisory authority, for the purpose of supervising the application and implementation of the regulation.\n\nThe European Data Protection Supervisor will act as the competent authority for the supervision of the Union institutions, agencies, and bodies when they fall within the scope of this regulation.\n\nTitle VII aims to facilitate the monitoring work of the Commission and national authorities through the establishment of an EU-wide database for stand-alone high-risk AI systems with mainly fundamental rights implications.\n\nThe database will be operated by the Commission and provided with data by the providers of the AI systems, who will be required to register their systems before placing them on the market or otherwise putting them into service.\n\nTitle VIII sets out the monitoring and reporting obligations for providers of AI systems with regard to post-market monitoring and reporting and investigating on AI-related incidents and malfunctioning.\n\nMarket surveillance authorities would also control the market and investigate compliance with the obligations and requirements for all high-risk AI systems already placed on the market.\n\nMarket surveillance authorities would have all powers under Regulation (EU) 2019/1020 on market surveillance.\n\nEx-post enforcement should ensure that once the AI system has been put on the market, public authorities have the powers and resources to intervene in case AI systems generate unexpected risks, which warrant rapid action.\n\nThey will also monitor compliance of operators with their relevant obligations.",
    "## Monitoring and Enforcement\n\n- The proposal does not foresee the automatic creation of any additional bodies or authorities at Member State level.\n\n- Member States may appoint existing sectorial authorities, who would be entrusted with monitoring and enforcing the provisions of the regulation.\n\n- Existing supervision and enforcement authorities will have the power to request and access any documentation maintained following this regulation and request market surveillance authorities to organize testing of the high-risk AI system.",
    "## Codes of Conduct (Title IX)\n\nTitle IX establishes a framework for creating codes of conduct to encourage providers of non-high-risk AI systems to apply voluntarily the mandatory requirements for high-risk AI systems.\n\nProviders can create and implement these codes themselves, and they may include voluntary commitments such as environmental sustainability and accessibility.",
    "## Final Provisions (Titles X, XI, and XII)\n\n- **Title X** underlines the obligation to respect confidentiality and sets rules for the exchange of information during the implementation of the regulation.\n\n- **Title XI** provides rules for exercising delegation and implementing powers, empowering the Commission to adopt acts for uniform application of the regulation.\n\n- **Title XII** calls for the Commission to assess regularly the need for updates of Annex III and to prepare regular reports, including a transitional period for the applicability of the regulation.",
    "### Artificial Intelligence Systems\n\n- Certain Member States have explored adopting national rules for AI safety and rights compliance, but differing national rules might lead to market fragmentation and decreased legal certainty.\n\n- A consistent and high level of protection throughout the Union can prevent divergences and ensure free circulation while protecting public interest and rights.",
    "### AI Systems and Public Interest\n\n- AI systems are widely used across sectors and offer economic and societal benefits, but they may generate risks or cause harm to public interests.\n\n- A Union legal framework for AI should foster development and use that protects public interests such as health, safety, and fundamental rights.",
    "### Regulation Goals\n\n- Support the development, use, and uptake of AI while ensuring protection of public interests and fundamental rights.\n\n- Establish rules for marketing and service of AI systems to support internal market functioning and free movement of goods and services.\n\n- Define AI systems clearly, allowing flexibility for technological advancement.",
    "# High-Risk AI Regulations\n\nIn accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, annexed to the TEU and TFEU, Denmark is not bound by rules laid down in Article 5(1), point (d), (2), and (3) of this Regulation adopted on the basis of Article 16 of the TFEU, or subject to their application, which relate to the processing of personal data by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU.\n\nHigh-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements.\n\nThose requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.\n\nAI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety, and fundamental rights of persons in the Union and such limitation minimizes any potential restriction to international trade, if any.\n\nAI systems could produce adverse outcomes to the health and safety of persons, particularly when such systems operate as components of products.\n\nConsistently with the objectives of Union harmonization legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated.\n\nFor instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care, should be able to safely operate and perform their functions in complex environments.\n\nSimilarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostic systems and systems supporting human decisions should be reliable and accurate.\n\nThe extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk.\n\nThose rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non-discrimination, consumer protection, workers’ rights, rights of persons with disabilities, right to an effective remedy and to a fair trial, the right of defense and the presumption of innocence, right to good administration.\n\nIn addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No.\n\n25 as regards the digital environment), both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being.\n\nThe fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons.\n\nAs regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems, falling within the scope of Regulation (EC) No 300/2008 of the European Parliament and of the Council, Regulation (EU) No 167/2013 of the European Parliament and of the Council, Regulation (EU) No 168/2013 of the European Parliament and of the Council, Directive 2014/90/EU of the European Parliament and of the Council, Directive (EU) 2016/797 of the European Parliament and of the Council, Regulation (EU) 2018/858 of the European Parliament and of the Council, Regulation (EU) 2018/1139 of the European Parliament and of the Council, and Regulation (EU) 2019/2144 of the European Parliament and of the Council, it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementing acts on the basis of those acts.",
    "As regards AI systems that are safety components of products, or which are themselves products, falling within the scope of certain Union harmonization legislation, it is appropriate to classify them as high-risk under this Regulation if the product in question undergoes the conformity assessment procedure with a third-party conformity assessment body pursuant to that relevant Union harmonization legislation.\n\nIn particular, such products include machinery, toys, lifts, equipment, and protective systems intended for use in potentially explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.\n\nThe classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered ‘high-risk’ under the criteria established in the relevant Union harmonization legislation that applies to the product.\n\nThis is notably the case for Regulation (EU) 2017/745 of the European Parliament and of the Council and Regulation (EU) 2017/746 of the European Parliament and of the Council, where a third-party conformity assessment is provided for medium-risk and high-risk products.\n\nAs regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence, and they are used in a number of specifically pre-defined areas specified in the Regulation.\n\nThe identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems.\n\nTechnical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects.\n\nThis is particularly relevant when it comes to age, ethnicity, sex, or disabilities.\n\nTherefore, ‘real-time’ and ‘post’ remote biometric identification systems should be classified as high-risk.\n\nIn view of the risks that they pose, both types of remote biometric identification systems should be subject to specific requirements on logging capabilities and human oversight.\n\nAs regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating, and electricity, since their failure or malfunctioning may put at risk the life and health of persons at a large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities.\n\nAI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions, or to evaluate persons on tests as part of or as a precondition for their education, should be considered high-risk, since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood.\n\nWhen improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination.\n\nAI systems used in employment, workers’ management, and access to self-employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk, since those systems may appreciably impact future career prospects and livelihoods of these persons.\n\nRelevant work-related contractual relationships should involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021.\n\nSuch persons should in principle not be considered users within the meaning of this Regulation.\n\nThroughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example, against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation.\n\nAI systems used to monitor the performance and behavior of these persons may also impact their rights to data protection and privacy.",
    "AI systems used to monitor the performance and behavior of these persons may also impact their rights to data protection and privacy.\n\nAnother area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one's standard of living.\n\nIn particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services.\n\nAI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, disabilities, age, sexual orientation, or create new forms of discriminatory impacts.\n\nConsidering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small-scale providers for their own use.\n\nNatural persons applying for or receiving public assistance benefits and services from public authorities are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities.\n\nIf AI systems are used for determining whether such benefits and services should be denied, reduced, revoked, or reclaimed by authorities, they may have a significant impact on persons’ livelihood and may infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignity, or an effective remedy.\n\nThose systems should therefore be classified as high-risk.\n\nNonetheless, this Regulation should not hamper the development and use of innovative approaches in public administration, which would stand to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal and natural persons.\n\nFinally, AI systems used to dispatch or establish priority in the dispatching of emergency first response services should also be classified as high-risk since they make decisions in very critical situations for the life and health of persons and their property.\n\nActions by law enforcement authorities involving certain uses of AI systems are characterized by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter.\n\nIn particular, if the AI system is not trained with high-quality data, does not meet adequate requirements in terms of its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner.\n\nFurthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defense and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable, and documented.\n\nIt is therefore appropriate to classify as high-risk a number of AI systems intended to be used in the law enforcement context where accuracy, reliability, and transparency are particularly important to avoid adverse impacts, retain public trust, and ensure accountability and effective redress.\n\nIn view of the nature of the activities in question and the risks relating thereto, those high-risk AI systems should include in particular AI systems intended to be used by law enforcement authorities for individual risk assessments, polygraphs and similar tools or to detect the emotional state of a natural person, to detect ‘deep fakes’, for the evaluation of the reliability of evidence in criminal proceedings, for predicting the occurrence or reoccurrence of an actual or potential criminal offense based on profiling of natural persons, or assessing personality traits and characteristics or past criminal behavior of natural persons or groups, for profiling in the course of detection, investigation, or prosecution of criminal offenses, as well as for crime analytics regarding natural persons.\n\nAI systems specifically intended to be used for administrative proceedings by tax and customs authorities should not be considered high-risk AI systems used by law enforcement authorities for the purposes of prevention, detection, investigation, and prosecution of criminal offenses.\n\nAI systems used in migration, asylum, and border control management affect people who are often in a particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities.",
    "The accuracy, non-discriminatory nature, and transparency of the AI systems used in those contexts are therefore particularly important to guarantee the respect of the fundamental rights of the affected persons, notably their rights to free movement, non-discrimination, protection of private life and personal data, international protection, and good administration.\n\nIt is therefore appropriate to classify as high-risk AI systems intended to be used by the competent public authorities charged with tasks in the fields of migration, asylum, and border control management as polygraphs and similar tools or to detect the emotional state of a natural person; for assessing certain risks posed by natural persons entering the territory of a Member State or applying for visa or asylum; for verifying the authenticity of the relevant documents of natural persons; for assisting competent public authorities in the examination of applications for asylum, visa, and residence permits and associated complaints with regard to the objective to establish the eligibility of the natural persons applying for a status.\n\nAI systems in the area of migration, asylum, and border control management covered by this Regulation should comply with the relevant procedural requirements set by the Directive 2013/32/EU of the European Parliament and of the Council, the Regulation (EC) No 810/2009 of the European Parliament and of the Council and other relevant legislation.\n\nCertain AI systems intended for the administration of justice and democratic processes should be classified as high-risk, considering their potentially significant impact on democracy, rule of law, individual freedoms, as well as the right to an effective remedy and to a fair trial.\n\nIn particular, to address the risks of potential biases, errors, and opacity, it is appropriate to qualify as high-risk AI systems intended to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts.\n\nSuch qualification should not extend, however, to AI systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymization or pseudonymization of judicial decisions, documents or data, communication between personnel, administrative tasks, or allocation of resources.\n\nThe fact that an AI system is classified as high risk under this Regulation should not be interpreted as indicating that the use of the system is necessarily lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons.\n\nAny such use should continue to occur solely in accordance with the applicable requirements resulting from the Charter and from the applicable acts of secondary Union law and national law.\n\nThis Regulation should not be understood as providing for the legal ground for the processing of personal data, including special categories of personal data, where relevant.\n\nTo mitigate the risks from high-risk AI systems placed or otherwise put into service on the Union market for users and affected persons, certain mandatory requirements should apply, taking into account the intended purpose of the use of the system and according to the risk management system to be established by the provider.\n\nRequirements should apply to high-risk AI systems as regards the quality of data sets used, technical documentation and record-keeping, transparency and the provision of information to users, human oversight, and robustness, accuracy, and cybersecurity.\n\nThose requirements are necessary to effectively mitigate the risks for health, safety, and fundamental rights, as applicable in the light of the intended purpose of the system, and no other less trade restrictive measures are reasonably available, thus avoiding unjustified restrictions to trade.\n\nHigh data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensuring that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law.\n\nHigh-quality training, validation, and testing data sets require the implementation of appropriate data governance and management practices.\n\nTraining, validation, and testing data sets should be sufficiently relevant, representative and free of errors, and complete in view of the intended purpose of the system.\n\nThey should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used.",
    "They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used.\n\nIn particular, training, validation, and testing data sets should take into account, to the extent required in the light of their intended purpose, the features, characteristics, or elements that are particular to the specific geographical, behavioral, or functional setting or context within which the AI system is intended to be used.\n\nIn order to protect the rights of others from the discrimination that might result from bias in AI systems, the providers should be able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection, and correction in relation to high-risk AI systems.\n\nFor the development of high-risk AI systems, certain actors, such as providers, notified bodies, and other relevant entities, such as digital innovation hubs, testing experimentation facilities, and researchers, should be able to access and use high-quality datasets within their respective fields of activities which are related to this Regulation.\n\nEuropean common data spaces established by the Commission and the facilitation of data sharing between businesses and with government in the public interest will be instrumental to provide trustful, accountable, and non-discriminatory access to high-quality data for the training, validation, and testing of AI systems.\n\nFor example, in health, the European health data space will facilitate non-discriminatory access to health data and the training of artificial intelligence algorithms on those datasets, in a privacy-preserving, secure, timely, transparent, and trustworthy manner, and with an appropriate institutional governance.\n\nRelevant competent authorities, including sectoral ones, providing or supporting the access to data may also support the provision of high-quality data for the training, validation, and testing of AI systems.\n\nHaving information on how high-risk AI systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this Regulation.\n\nThis requires keeping records and the availability of a technical documentation, containing information which is necessary to assess the compliance of the AI system with the relevant requirements.\n\nSuch information should include the general characteristics, capabilities, and limitations of the system, algorithms, data, training, testing, and validation processes used as well as documentation on the relevant risk management system.\n\nThe technical documentation should be kept up to date.\n\nTo address the opacity that may make certain AI systems incomprehensible to or too complex for natural persons, a certain degree of transparency should be required for high-risk AI systems.\n\nUsers should be able to interpret the system output and use it appropriately.\n\nHigh-risk AI systems should therefore be accompanied by relevant documentation and instructions of use and include concise and clear information, including in relation to possible risks to fundamental rights and discrimination, where appropriate.\n\nHigh-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning.\n\nFor this purpose, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service.\n\nIn particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role.\n\nHigh-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art.\n\nThe level of accuracy and accuracy metrics should be communicated to the users.\n\nThe technical robustness is a key requirement for high-risk AI systems.\n\nThey should be resilient against risks connected to the limitations of the system (e.g.\n\nerrors, faults, inconsistencies, unexpected situations) as well as against malicious actions that may compromise the security of the AI system and result in harmful or otherwise undesirable behavior.\n\nFailure to protect against these risks could lead to safety impacts or negatively affect the fundamental rights, for example, due to erroneous decisions or wrong or biased outputs generated by the AI system.",
    "Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behavior, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities.\n\nCyberattacks against AI systems can leverage AI-specific assets, such as training data sets (e.g.\n\ndata poisoning) or trained models (e.g.\n\nadversarial attacks), or exploit vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure.\n\nTo ensure a level of cybersecurity appropriate to the risks, suitable measures should therefore be taken by the providers of high-risk AI systems.",
    "# High-Risk AI Systems and Regulatory Framework\n\nAs part of Union harmonisation legislation, rules applicable to the placing on the market, putting into service, and use of high-risk AI systems should be laid down consistently with several regulations and decisions of the European Parliament and of the Council.\n\nThese include Regulation (EC) No 765/2008, Decision No 768/2008/EC, and Regulation (EU) 2019/1020, also referred to as the ‘New Legislative Framework for the marketing of products’.\n\nIt is appropriate that a specific natural or legal person, defined as the provider, takes the responsibility for the placing on the market or putting into service of a high-risk AI system, regardless of whether that natural or legal person is the person who designed or developed the system.\n\nThe provider should establish a sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation, and establish a robust post-market monitoring system.\n\nPublic authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level, taking into account the specificities of the sector and the competences and organisation of the public authority in question.\n\nWhere a high-risk AI system that is a safety component of a product covered by a relevant New Legislative Framework sectorial legislation is not placed on the market or put into service independently from the product, the manufacturer of the final product should comply with the obligations of the provider established in this Regulation.\n\nTo enable enforcement of this Regulation and create a level-playing field for operators, it is important that a person established in the Union can provide authorities with all necessary information on the compliance of an AI system.\n\nTherefore, providers established outside the Union shall appoint an authorised representative established in the Union by written mandate if an importer cannot be identified.\n\nSpecific obligations for relevant economic operators, such as importers and distributors, should be established to ensure legal certainty and facilitate regulatory compliance.\n\nGiven the nature of AI systems and the risks to safety and fundamental rights possibly associated with their use, it is appropriate to set specific responsibilities for users.\n\nUsers should, in particular, use high-risk AI systems according to the instructions of use and other obligations regarding monitoring and record-keeping.\n\nIn light of the complexity of the artificial intelligence value chain, relevant third parties, such as those involved in software supply, should cooperate with providers and users to ensure compliance with the obligations under this Regulation.\n\nStandardisation should play a key role in providing technical solutions for compliance.\n\nCompliance with harmonised standards should demonstrate conformity with the requirements of this Regulation.\n\nHowever, the Commission could adopt common technical specifications where harmonised standards are insufficient.\n\nHigh-risk AI systems should be subject to a conformity assessment before being placed on the market or put into service.\n\nThis should be integrated with existing Union harmonisation legislation.\n\nThe interrelation between this Regulation and other legislation, such as the [Machinery Regulation], should be managed to ensure compatibility.\n\nThe [Machinery Regulation] and this Regulation apply the same definition of AI system.\n\nIt is appropriate to limit the scope of third-party conformity assessment for high-risk AI systems other than those related to products initially.\n\nThe conformity assessment for such systems should generally be carried out by the provider, except for systems meant for remote biometric identification, which should involve a notified body.\n\nFor AI systems undergoing significant changes or continuing to learn after their initial conformity assessment, specific provisions are set for reassessment.\n\nHigh-risk AI systems should bear the CE marking to indicate conformity and to facilitate free movement within the internal market.\n\nMember States can authorise the market introduction of AI systems without a conformity assessment for public security or health protection reasons.\n\nProviders of high-risk AI systems should register their systems in a EU database to ensure transparency and facilitate regulatory work.\n\nCertain AI systems that pose impersonation or deception risks should be subject to transparency obligations.\n\nRegulatory sandboxes are established to foster innovation under safe and controlled conditions, supporting compliance and legal certainty.\n\nThey should facilitate the testing of innovative AI systems.",
    "Regulatory sandboxes are established to foster innovation under safe and controlled conditions, supporting compliance and legal certainty.\n\nThey should facilitate the testing of innovative AI systems.\n\nProviders and users of AI systems, especially small-scale entities, should be supported with initiatives to reduce barriers, such as translation of documentation and awareness campaigns.\n\nThe AI-on demand platform, the European Digital Innovation Hubs, and Testing and Experimentation Facilities should aid in implementing this Regulation by providing technical and scientific support.\n\nA European Artificial Intelligence Board should be established to provide guidance and enable uniform implementation across the Union.\n\nMember States should designate national competent authorities for supervising the application of the Regulation, and ensure the establishment of a post-market monitoring system by providers.\n\nMarket surveillance and compliance mechanisms established by Regulation (EU) 2019/1020 should be applied for effective enforcement.\n\nThe supervision and enforcement of this Regulation in financial services sectors should be aligned with existing regulations such as Directive 2013/36/EU.\n\nProviders of non-high-risk AI systems should be encouraged to adopt voluntary codes of conduct to align with high-risk AI system requirements.\n\nAI systems must be safe when placed on the market or put into service, even if not classified as high-risk under this Regulation.\n\nMember States should ensure confidentiality in the application of this Regulation and impose penalties for infringements.\n\nThe European Data Protection Supervisor has the authority to impose fines on Union institutions.\n\nTo adapt the regulatory framework as needed, the Commission should be empowered to adopt delegated acts.\n\nThese acts should be prepared with appropriate consultations.\n\nFor the implementation of this Regulation, implementing powers should be conferred to the Commission.\n\nThese powers should be exercised according to existing procedures under Regulation (EU) No 182/2011.\n\nSince the Regulation's goals cannot be adequately achieved by individual Member States and require Union-level action, it aligns with the principle of subsidiarity.\n\nThe principle of proportionality ensures that the Regulation goes no further than necessary.\n\nThe Regulation shall apply from a designated date, but infrastructure for governance should be operational beforehand.\n\nPenalty provisions should apply twelve months after the Regulation's entry into force.\n\nThe European Data Protection Supervisor and the European Data Protection Board were consulted, and have provided an opinion.\n\nHAVE ADOPTED THIS REGULATION:",
    "### Article 1 Subject matter\n\n- Harmonised rules for the placing on the market, putting into service, and use of artificial intelligence systems (AI systems) in the Union.\n\n- Prohibitions of certain artificial intelligence practices.\n\n- Specific requirements for high-risk AI systems and obligations for their operators.\n\n- Harmonised transparency rules for AI systems intended to interact with natural persons, emotion recognition systems, and biometric categorisation systems, as well as AI systems used to generate or manipulate image, audio, or video content.\n\n- Rules on market monitoring and surveillance.",
    "# Article 2 Scope\n\n- Providers placing on the market or putting into service AI systems in the Union, irrespective of whether those providers are established within the Union or in a third country.\n\n- Users of AI systems located within the Union.\n\n- Providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union.\n\nFor high-risk AI systems that are safety components of products or systems, or which are themselves products or systems, falling within the scope of the following acts, only Article 84 of this Regulation shall apply:\n\n- Regulation (EC) 300/2008;\n- Regulation (EU) No 167/2013;\n- Regulation (EU) No 168/2013;\n- Directive 2014/90/EU;\n- Directive (EU) 2016/797;\n- Regulation (EU) 2018/858;\n- Regulation (EU) 2018/1139;\n- Regulation (EU) 2019/2144.\n\nThis Regulation shall not apply to AI systems developed or used exclusively for military purposes.\n\nThis Regulation shall not apply to public authorities in a third country nor to international organisations falling within the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the framework of international agreements for law enforcement and judicial cooperation with the Union or with one or more Member States.\n\nThis Regulation shall not affect the application of the provisions on the liability of intermediary service providers set out in Chapter II, Section IV of Directive 2000/31/EC of the European Parliament and of the Council60 [as to be replaced by the corresponding provisions of the Digital Services Act].",
    "# Article 3 Definitions\n\nFor the purpose of this Regulation, the following definitions apply:\n\n(1) ‘Artificial Intelligence system’ (AI system) means software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with.\n\n(2) ‘Provider’ means a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed with a view to placing it on the market or putting it into service under its own name or trademark, whether for payment or free of charge.\n\n- ‘Small-scale provider’ means a provider that is a micro or small enterprise within the meaning of Commission Recommendation 2003/361/EC61.\n\n- ‘User’ means any natural or legal person, public authority, agency or other body using an AI system under its authority, except where the AI system is used in the course of a personal non-professional activity.\n\n- ‘Authorised representative’ means any natural or legal person established in the Union who has received a written mandate from a provider of an AI system to, respectively, perform and carry out on its behalf the obligations and procedures established by this Regulation.\n\n- ‘Importer’ means any natural or legal person established in the Union that places on the market or puts into service an AI system that bears the name or trademark of a natural or legal person established outside the Union.\n\n- ‘Distributor’ means any natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the Union market without affecting its properties.\n\n- ‘Operator’ means the provider, the user, the authorised representative, the importer and the distributor.\n\n- ‘Placing on the market’ means the first making available of an AI system on the Union market.\n\n- ‘Making available on the market’ means any supply of an AI system for distribution or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge.\n\n- ‘Putting into service’ means the supply of an AI system for first use directly to the user or for own use on the Union market for its intended purpose.\n\n- ‘Intended purpose’ means the use for which an AI system is intended by the provider, including the specific context and conditions of use, as specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation.\n\n- ‘Reasonably foreseeable misuse’ means the use of an AI system in a way that is not in accordance with its intended purpose, but which may result from reasonably foreseeable human behavior or interaction with other systems.\n\n- ‘Safety component of a product or system’ means a component of a product or of a system which fulfills a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property.\n\n- ‘Instructions for use’ means the information provided by the provider to inform the user of in particular an AI system’s intended purpose and proper use, inclusive of the specific geographical, behavioral or functional setting within which the high-risk AI system is intended to be used.\n\n- ‘Recall of an AI system’ means any measure aimed at achieving the return to the provider of an AI system made available to users.\n\n- ‘Withdrawal of an AI system’ means any measure aimed at preventing the distribution, display and offer of an AI system.\n\n- ‘Performance of an AI system’ means the ability of an AI system to achieve its intended purpose.\n\n- ‘Notifying authority’ means the national authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring.\n\n- ‘Conformity assessment’ means the process of verifying whether the requirements set out in Title III, Chapter 2 of this Regulation relating to an AI system have been fulfilled.\n\n- ‘Conformity assessment body’ means a body that performs third-party conformity assessment activities, including testing, certification and inspection.\n\n- ‘Notified body’ means a conformity assessment body designated in accordance with this Regulation and other relevant Union harmonisation legislation.\n\n- ‘Substantial modification’ means a change to the AI system following its placing on the market or putting into service which affects the compliance of the AI system with the requirements set out in Title III, Chapter 2 of this Regulation or results in a modification to the intended purpose for which the AI system has been assessed.",
    "- ‘CE marking of conformity’ (CE marking) means a marking by which a provider indicates that an AI system is in conformity with the requirements set out in Title III, Chapter 2 of this Regulation and other applicable Union legislation harmonising the conditions for the marketing of products (‘Union harmonisation legislation’) providing for its affixing.\n\n- ‘Post-market monitoring’ means all activities carried out by providers of AI systems to proactively collect and review experience gained from the use of AI systems they place on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions.\n\n- ‘Market surveillance authority’ means the national authority carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020.\n\n- ‘Harmonised standard’ means a European standard as defined in Article 2(1)(c) of Regulation (EU) No 1025/2012.\n\n- ‘Common specifications’ means a document, other than a standard, containing technical solutions providing a means to, comply with certain requirements and obligations established under this Regulation.\n\n- ‘Training data’ means data used for training an AI system through fitting its learnable parameters, including the weights of a neural network.\n\n- ‘Validation data’ means data used for providing an evaluation of the trained AI system and for tuning its non-learnable parameters and its learning process, among other things, in order to prevent overfitting; whereas the validation dataset can be a separate dataset or part of the training dataset, either as a fixed or variable split.\n\n- ‘Testing data’ means data used for providing an independent evaluation of the trained and validated AI system in order to confirm the expected performance of that system before its placing on the market or putting into service.\n\n- ‘Input data’ means data provided to or directly acquired by an AI system on the basis of which the system produces an output.\n\n- ‘Biometric data’ means personal data resulting from specific technical processing relating to the physical, physiological or behavioral characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic data.\n\n- ‘Emotion recognition system’ means an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data.\n\n- ‘Biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories, such as sex, age, hair color, eye color, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data.\n\n- ‘Remote biometric identification system’ means an AI system for the purpose of identifying natural persons at a distance through the comparison of a person’s biometric data with the biometric data contained in a reference database, and without prior knowledge of the user of the AI system whether the person will be present and can be identified.\n\n- ‘‘Real-time’ remote biometric identification system’ means a remote biometric identification system whereby the capturing of biometric data, the comparison and the identification all occur without a significant delay.\n\nThis comprises not only instant identification, but also limited short delays in order to avoid circumvention.\n\n- ‘‘Post’ remote biometric identification system’ means a remote biometric identification system other than a 'real-time' remote biometric identification system.\n\n- ‘Publicly accessible space’ means any physical place accessible to the public, regardless of whether certain conditions for access may apply.\n\n- ‘Law enforcement authority’ means:\n  - any public authority competent for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security; or\n  - any other body or entity entrusted by Member State law to exercise public authority and public powers for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security.\n\n- ‘Law enforcement’ means activities carried out by law enforcement authorities for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security.\n\n- ‘National supervisory authority’ means the authority to which a Member State assigns the responsibility for the implementation and application of this Regulation, for coordinating the activities entrusted to that Member State, for acting as the single contact point for the Commission, and for representing the Member State at the European Artificial Intelligence Board.",
    "- ‘National competent authority’ means the national supervisory authority, the notifying authority and the market surveillance authority.\n\n- ‘Serious incident’ means any incident that directly or indirectly leads, might have led or might lead to any of the following:\n  - the death of a person or serious damage to a person’s health, to property or the environment,\n  - a serious and irreversible disruption of the management and operation of critical infrastructure.",
    "# Article 4 Amendments to Annex I\n\nThe Commission is empowered to adopt delegated acts in accordance with Article 73 to amend the list of techniques and approaches listed in Annex I, in order to update that list to market and technological developments on the basis of characteristics that are similar to the techniques and approaches listed therein.",
    "### Article 5\n\nThe following artificial intelligence practices shall be prohibited:\n\n- The placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person’s consciousness in order to materially distort a person’s behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm.\n\n- The placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behavior of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm.\n\n- The placing on the market, putting into service or use of AI systems by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behavior or known or predicted personal or personality characteristics, with the social score leading to either or both of the following:\n  - Detrimental or unfavourable treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected.\n\n- Detrimental or unfavourable treatment of certain natural persons or whole groups thereof that is unjustified or disproportionate to their social behaviour or its gravity.\n\n- The use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement, unless and in as far as such use is strictly necessary for one of the following objectives:\n  - The targeted search for specific potential victims of crime, including missing children.\n\n- The prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attack.\n\n- The detection, localisation, identification or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2(2) of Council Framework Decision 2002/584/JHA62 and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years, as determined by the law of that Member State.\n\nThe use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall take into account the following elements:\n\n- The nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm caused in the absence of the use of the system.\n\n- The consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences.\n\nIn addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall comply with necessary and proportionate safeguards and conditions in relation to the use, in particular as regards the temporal, geographic and personal limitations.\n\nAs regards paragraphs 1, point (d) and 2, each individual use for the purpose of law enforcement of a ‘real-time’ remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph 4.\n\nHowever, in a duly justified situation of urgency, the use of the system may be commenced without an authorisation and the authorisation may be requested only during or after the use.\n\nThe competent judicial or administrative authority shall only grant the authorisation where it is satisfied, based on objective evidence or clear indications presented to it, that the use of the ‘real-time’ remote biometric identification system at issue is necessary for and proportionate to achieving one of the objectives specified in paragraph 1, point (d), as identified in the request.\n\nIn deciding on the request, the competent judicial or administrative authority shall take into account the elements referred to in paragraph 2.\n\nA Member State may decide to provide for the possibility to fully or partially authorise the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement within the limits and under the conditions listed in paragraphs 1, point (d), 2 and 3.",
    "That Member State shall lay down in its national law the necessary detailed rules for the request, issuance and exercise of, as well as supervision relating to, the authorisations referred to in paragraph 3.\n\nThose rules shall also specify in respect of which of the objectives listed in paragraph 1, point (d), including which of the criminal offences referred to in point (iii) thereof, the competent authorities may be authorised to use those systems for the purpose of law enforcement.",
    "#### Article 6 Classification rules for high-risk AI systems\n\nIrrespective of whether an AI system is placed on the market or put into service independently from the products referred to in points (a) and (b), that AI system shall be considered high-risk where both of the following conditions are fulfilled:\n\n- The AI system is intended to be used as a safety component of a product, or is itself a product, covered by the Union harmonisation legislation listed in Annex II.\n\n- The product whose safety component is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment with a view to the placing on the market or putting into service of that product pursuant to the Union harmonisation legislation listed in Annex II.\n\nIn addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall also be considered high-risk.",
    "#### Article 7 Amendments to Annex III\n\nThe Commission is empowered to adopt delegated acts in accordance with Article 73 to update the list in Annex III by adding high-risk AI systems where both of the following conditions are fulfilled:\n\n- The AI systems are intended to be used in any of the areas listed in points 1 to 8 of Annex III.\n\n- The AI systems pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights, that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III.\n\nWhen assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria:\n\n- The intended purpose of the AI system.\n\n- The extent to which an AI system has been used or is likely to be used.\n\n- The extent to which the use of an AI system has already caused harm to the health and safety or adverse impact on the fundamental rights or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities.\n\n- The potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons.\n\n- The extent to which potentially harmed or adversely impacted persons are dependent on the outcome produced with an AI system, in particular because for practical or legal reasons it is not reasonably possible to opt-out from that outcome.\n\n- The extent to which potentially harmed or adversely impacted persons are in a vulnerable position in relation to the user of an AI system, in particular due to an imbalance of power, knowledge, economic or social circumstances, or age.\n\n- The extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an impact on the health or safety of persons shall not be considered as easily reversible.\n\n- The extent to which existing Union legislation provides for:\n  - Effective measures of redress in relation to the risks posed by an AI system, with the exclusion of claims for damages.\n\n- Effective measures to prevent or substantially minimise those risks.",
    "#### Article 9 Risk management system\n\nA risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems.\n\nThe risk management system shall consist of a continuous iterative process run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic updating.\n\nIt shall comprise the following steps:\n\n1.\n\nIdentification and analysis of the known and foreseeable risks associated with each high-risk AI system.\n\n2.\n\nEstimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse.\n\n3.\n\nEvaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61.\n\n4.\n\nAdoption of suitable risk management measures in accordance with the provisions of the following paragraphs.\n\nThe risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this Chapter 2.\n\nThey shall take into account the generally acknowledged state of the art, including as reflected in relevant harmonised standards or common specifications.\n\nThe risk management measures referred to in paragraph 2, point (d) shall be such that any residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged acceptable, provided that the high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse.\n\nThose residual risks shall be communicated to the user.\n\nIn identifying the most appropriate risk management measures, the following shall be ensured:\n\n- Elimination or reduction of risks as far as possible through adequate design and development.\n\n- Where appropriate, implementation of adequate mitigation and control measures in relation to risks that cannot be eliminated.\n\n- Provision of adequate information pursuant to Article 13, in particular as regards the risks referred to in paragraph 2, point (b) of this Article, and, where appropriate, training to users.\n\nIn eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the user and the environment in which the system is intended to be used.\n\nHigh-risk AI systems shall be tested for the purposes of identifying the most appropriate risk management measures.\n\nTesting shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this Chapter.\n\nTesting procedures shall be suitable to achieve the intended purpose of the AI system and do not need to go beyond what is necessary to achieve that purpose.\n\nThe testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the market or the putting into service.\n\nTesting shall be made against preliminarily defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system.\n\nWhen implementing the risk management system described in paragraphs 1 to 7, specific consideration shall be given to whether the high-risk AI system is likely to be accessed by or have an impact on children.\n\nFor credit institutions regulated by Directive 2013/36/EU, the aspects described in paragraphs 1 to 8 shall be part of the risk management procedures established by those institutions pursuant to Article 74 of that Directive.",
    "#### Article 10 Data and data governance\n\nHigh-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5.\n\nTraining, validation and testing data sets shall be subject to appropriate data governance and management practices.\n\nThose practices shall concern in particular:\n\n- The relevant design choices.\n\n- Data collection.\n\n- Relevant data preparation processing operations, such as annotation, labeling, cleaning, enrichment, and aggregation.\n\n- The formulation of relevant assumptions, notably with respect to the information that the data are supposed to measure and represent.\n\n- A prior assessment of the availability, quantity and suitability of the data sets that are needed.\n\n- Examination in view of possible biases.\n\n- The identification of any possible data gaps or shortcomings, and how those gaps and shortcomings can be addressed.\n\nTraining, validation, and testing data sets shall be relevant, representative, free of errors and complete.\n\nThey shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used.\n\nThese characteristics of the data sets may be met at the level of individual data sets or a combination thereof.\n\nTraining, validation, and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioral or functional setting within which the high-risk AI system is intended to be used.\n\nTo the extent that it is strictly necessary for the purposes of ensuring bias monitoring, detection, and correction in relation to the high-risk AI systems, the providers of such systems may process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons, including technical limitations on the re-use and use of state-of-the-art security and privacy-preserving measures, such as pseudonymisation, or encryption where anonymisation may significantly affect the purposes.",
    "# Article 11: Technical Documentation\n\nThe technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to-date.\n\nThe technical documentation shall be drawn up in such a way to demonstrate that the high-risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with all the necessary information to assess the compliance of the AI system with those requirements.\n\nIt shall contain, at a minimum, the elements set out in Annex IV.\n\nWhere a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in Annex IV as well as the information required under those legal acts.\n\nThe Commission is empowered to adopt delegated acts in accordance with Article 73 to amend Annex IV where necessary to ensure that, in the light of technical progress, the technical documentation provides all the necessary information to assess the compliance of the system with the requirements set out in this Chapter.",
    "# Article 12: Record-keeping\n\nHigh-risk AI systems shall be designed and developed with capabilities enabling the automatic recording of events (‘logs’) while the high-risk AI systems is operating.\n\nThose logging capabilities shall conform to recognised standards or common specifications.\n\nThe logging capabilities shall ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to the intended purpose of the system.\n\nIn particular, logging capabilities shall enable the monitoring of the operation of the high-risk AI system with respect to the occurrence of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or lead to a substantial modification, and facilitate the post-market monitoring referred to in Article 61.\n\nFor high-risk AI systems referred to in paragraph 1, point (a) of Annex III, the logging capabilities shall provide, at a minimum: recording of the period of each use of the system (start date and time and end date and time of each use); the reference database against which input data has been checked by the system; the input data for which the search has led to a match; the identification of the natural persons involved in the verification of the results, as referred to in Article 14 (5).",
    "# Article 13: Transparency and Provision of Information to Users\n\nHigh-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system’s output and use it appropriately.\n\nAn appropriate type and degree of transparency shall be ensured, with a view to achieving compliance with the relevant obligations of the user and of the provider set out in Chapter 3 of this Title.\n\nHigh-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users.\n\nThe information referred to in paragraph 2 shall specify: the identity and the contact details of the provider and, where applicable, of its authorised representative; the characteristics, capabilities and limitations of performance of the high-risk AI system, including its intended purpose; the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity; any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights; its performance as regards the persons or groups of persons on which the system is intended to be used; when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system.\n\nthe changes to the high-risk AI system and its performance which have been pre-determined by the provider at the moment of the initial conformity assessment, if any; the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the users; the expected lifetime of the high-risk AI system and any necessary maintenance and care measures to ensure the proper functioning of that AI system, including as regards software updates.",
    "# Article 14: Human Oversight\n\nHigh-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use.\n\nHuman oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter.\n\nHuman oversight shall be ensured through either one or all of the following measures: identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service; identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user.\n\nThe measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances: fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible; remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (‘automation bias’), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons; be able to correctly interpret the high-risk AI system’s output, taking into account in particular the characteristics of the system and the interpretation tools and methods available; be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; be able to intervene on the operation of the high-risk AI system or interrupt the system through a “stop” button or a similar procedure.\n\nFor high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons.",
    "# Article 15: Accuracy, Robustness and Cybersecurity\n\nHigh-risk AI systems shall be designed and developed in such a way that they achieve, in the light of their intended purpose, an appropriate level of accuracy, robustness and cybersecurity, and perform consistently in those respects throughout their lifecycle.\n\nThe levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use.\n\nHigh-risk AI systems shall be resilient as regards errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems.\n\nThe robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans.\n\nHigh-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs due to outputs used as an input for future operations (‘feedback loops’) are duly addressed with appropriate mitigation measures.\n\nHigh-risk AI systems shall be resilient as regards attempts by unauthorised third parties to alter their use or performance by exploiting the system vulnerabilities.\n\nThe technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks.\n\nThe technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent and control for attacks trying to manipulate the training dataset (‘data poisoning’), inputs designed to cause the model to make a mistake (‘adversarial examples’), or model flaws.",
    "## Article 16: Obligations of Providers of High-risk AI Systems\n\nProviders of high-risk AI systems shall: ensure that their high-risk AI systems are compliant with the requirements set out in Chapter 2 of this Title; have a quality management system in place which complies with Article 17; draw-up the technical documentation of the high-risk AI system; when under their control, keep the logs automatically generated by their high-risk AI systems; ensure that the high-risk AI system undergoes the relevant conformity assessment procedure, prior to its placing on the market or putting into service; comply with the registration obligations referred to in Article 51; take the necessary corrective actions, if the high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title; inform the national competent authorities of the Member States in which they made the AI system available or put it into service and, where applicable, the notified body of the non-compliance and of any corrective actions taken; to affix the CE marking to their high-risk AI systems to indicate the conformity with this Regulation in accordance with Article 49; upon request of a national competent authority, demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title.",
    "## Article 17: Quality Management System\n\nProviders of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation.\n\nThat system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects: a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system; techniques, procedures and systematic actions to be used for the design, design control and design verification of the high-risk AI system; techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the high-risk AI system; examination, test and validation procedures to be carried out before, during and after the development of the high-risk AI system, and the frequency with which they have to be carried out; technical specifications, including standards, to be applied and, where the relevant harmonised standards are not applied in full, the means to be used to ensure that the high-risk AI system complies with the requirements set out in Chapter 2 of this Title; systems and procedures for data management, including data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the data that is performed before and for the purposes of the placing on the market or putting into service of high-risk AI systems; the risk management system referred to in Article 9; the setting-up, implementation and maintenance of a post-market monitoring system, in accordance with Article 61; procedures related to the reporting of serious incidents and of malfunctioning in accordance with Article 62; the handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties; systems and procedures for record keeping of all relevant documentation and information; resource management, including security of supply related measures; an accountability framework setting out the responsibilities of the management and other staff with regard to all aspects listed in this paragraph.\n\nThe implementation of aspects referred to in paragraph 1 shall be proportionate to the size of the provider’s organisation.\n\nFor providers that are credit institutions regulated by Directive 2013/36/ EU, the obligation to put a quality management system in place shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive.\n\nIn that context, any harmonised standards referred to in Article 40 of this Regulation shall be taken into account.",
    "## Article 18: Obligation to Draw Up Technical Documentation\n\nProviders of high-risk AI systems shall draw up the technical documentation referred to in Article 11 in accordance with Annex IV.\n\nProviders that are credit institutions regulated by Directive 2013/36/EU shall maintain the technical documentation as part of the documentation concerning internal governance, arrangements, processes and mechanisms pursuant to Article 74 of that Directive.",
    "# Article 19: Conformity Assessment\n\nProviders of high-risk AI systems shall ensure that their systems undergo the relevant conformity assessment procedure in accordance with Article 43, prior to their placing on the market or putting into service.\n\nWhere the compliance of the AI systems with the requirements set out in Chapter 2 of this Title has been demonstrated following that conformity assessment, the providers shall draw up an EU declaration of conformity in accordance with Article 48 and affix the CE marking of conformity in accordance with Article 49.\n\nFor high-risk AI systems referred to in point 5(b) of Annex III that are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013/36/EU, the conformity assessment shall be carried out as part of the procedure referred to in Articles 97 to101 of that Directive.",
    "# Article 20: Automatically Generated Logs\n\nProviders of high-risk AI systems shall keep the logs automatically generated by their high-risk AI systems, to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law.\n\nThe logs shall be kept for a period that is appropriate in the light of the intended purpose of high-risk AI system and applicable legal obligations under Union or national law.\n\nProviders that are credit institutions regulated by Directive 2013/36/EU shall maintain the logs automatically generated by their high-risk AI systems as part of the documentation under Articles 74 of that Directive.",
    "# Article 21: Corrective Actions\n\nProviders of high-risk AI systems which consider or have reason to consider that a high-risk AI system which they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it or to recall it, as appropriate.\n\nThey shall inform the distributors of the high-risk AI system in question and, where applicable, the authorised representative and importers accordingly.",
    "# Article 22: Duty of Information\n\nWhere the high-risk AI system presents a risk within the meaning of Article 65(1) and that risk is known to the provider of the system, that provider shall immediately inform the national competent authorities of the Member States in which it made the system available and, where applicable, the notified body that issued a certificate for the high-risk AI system, in particular of the non-compliance and of any corrective actions taken.",
    "# Article 23: Cooperation with Competent Authorities\n\nProviders of high-risk AI systems shall, upon request by a national competent authority, provide that authority with all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title, in an official Union language determined by the Member State concerned.\n\nUpon a reasoned request from a national competent authority, providers shall also give that authority access to the logs automatically generated by the high-risk AI system, to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law.",
    "# Article 24: Obligations of Product Manufacturers\n\nWhere a high-risk AI system related to products to which the legal acts listed in Annex II, section A, apply, is placed on the market or put into service together with the product manufactured in accordance with those legal acts and under the name of the product manufacturer, the manufacturer of the product shall take the responsibility of the compliance of the AI system with this Regulation and, as far as the AI system is concerned, have the same obligations imposed by the present Regulation on the provider.",
    "# Article 25: Authorised Representatives\n\nPrior to making their systems available on the Union market, where an importer cannot be identified, providers established outside the Union shall, by written mandate, appoint an authorised representative which is established in the Union.\n\nThe authorised representative shall perform the tasks specified in the mandate received from the provider.\n\nThe mandate shall empower the authorised representative to carry out the following tasks: keep a copy of the EU declaration of conformity and the technical documentation at the disposal of the national competent authorities and national authorities referred to in Article 63(7); provide a national competent authority, upon a reasoned request, with all the information and documentation necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in Chapter 2 of this Title, including access to the logs automatically generated by the high-risk AI system to the extent such logs are under the control of the provider by virtue of a contractual arrangement with the user or otherwise by law; cooperate with competent national authorities, upon a reasoned request, on any action the latter takes in relation to the high-risk AI system.",
    "# Article 26: Obligations of Importers\n\nBefore placing a high-risk AI system on the market, importers of such system shall ensure that: the appropriate conformity assessment procedure has been carried out by the provider of that AI system the provider has drawn up the technical documentation in accordance with Annex IV; the system bears the required conformity marking and is accompanied by the required documentation and instructions of use.\n\nWhere an importer considers or has reason to consider that a high-risk AI system is not in conformity with this Regulation, it shall not place that system on the market until that AI system has been brought into conformity.\n\nWhere the high-risk AI system presents a risk within the meaning of Article 65(1), the importer shall inform the provider of the AI system and the market surveillance authorities to that effect.\n\nImporters shall indicate their name, registered trade name or registered trademark, and the address at which they can be contacted on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable.\n\nImporters shall ensure that, while a high-risk AI system is under their responsibility, where applicable, storage or transport conditions do not jeopardise its compliance with the requirements set out in Chapter 2 of this Title.\n\nImporters shall provide national competent authorities, upon a reasoned request, with all necessary information and documentation to demonstrate the conformity of a high-risk AI system with the requirements set out in Chapter 2 of this Title in a language which can be easily understood by that national competent authority, including access to the logs automatically generated by the high-risk AI system to the extent such logs are under the control of the provider by virtue of a contractual arrangement with the user or otherwise by law.\n\nThey shall also cooperate with those authorities on any action national competent authority takes in relation to that system.",
    "# Article 27: Obligations of Distributors\n\nBefore making a high-risk AI system available on the market, distributors shall verify that the high-risk AI system bears the required CE conformity marking, that it is accompanied by the required documentation and instruction of use, and that the provider and the importer of the system, as applicable, have complied with the obligations set out in this Regulation.\n\nWhere a distributor considers or has reason to consider that a high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title, it shall not make the high-risk AI system available on the market until that system has been brought into conformity with those requirements.\n\nFurthermore, where the system presents a risk within the meaning of Article 65(1), the distributor shall inform the provider or the importer of the system, as applicable, to that effect.\n\nDistributors shall ensure that, while a high-risk AI system is under their responsibility, where applicable, storage or transport conditions do not jeopardise the compliance of the system with the requirements set out in Chapter 2 of this Title.\n\nA distributor that considers or has reason to consider that a high-risk AI system which it has made available on the market is not in conformity with the requirements set out in Chapter 2 of this Title shall take the corrective actions necessary to bring that system into conformity with those requirements, to withdraw it or recall it or shall ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions.\n\nWhere the high-risk AI system presents a risk within the meaning of Article 65(1), the distributor shall immediately inform the national competent authorities of the Member States in which it has made the product available to that effect, giving details, in particular, of the non-compliance and of any corrective actions taken.\n\nUpon a reasoned request from a national competent authority, distributors of high-risk AI systems shall provide that authority with all the information and documentation necessary to demonstrate the conformity of a high-risk system with the requirements set out in Chapter 2 of this Title.\n\nDistributors shall also cooperate with that national competent authority on any action taken by that authority.",
    "# Article 28: Obligations of Distributors, Importers, Users or any Other Third-party\n\nAny distributor, importer, user or other third-party shall be considered a provider for the purposes of this Regulation and shall be subject to the obligations of the provider under Article 16, in any of the following circumstances: they place on the market or put into service a high-risk AI system under their name or trademark; they modify the intended purpose of a high-risk AI system already placed on the market or put into service; they make a substantial modification to the high-risk AI system.\n\nWhere the circumstances referred to in paragraph 1, point (b) or (c), occur, the provider that initially placed the high-risk AI system on the market or put it into service shall no longer be considered a provider for the purposes of this Regulation.",
    "# Article 29: Obligations of Users of High-risk AI Systems\n\nUsers of high-risk AI systems shall use such systems in accordance with the instructions of use accompanying the systems, pursuant to paragraphs 2 and 5.\n\nThe obligations in paragraph 1 are without prejudice to other user obligations under Union or national law and to the user’s discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.\n\nWithout prejudice to paragraph 1, to the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system.\n\nUsers shall monitor the operation of the high-risk AI system on the basis of the instructions of use.\n\nWhen they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall inform the provider or distributor and suspend the use of the system.\n\nThey shall also inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system.\n\nIn case the user is not able to reach the provider, Article 62 shall apply mutatis mutandis.\n\nFor users that are credit institutions regulated by Directive 2013/36/EU, the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive.\n\nUsers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control.\n\nThe logs shall be kept for a period that is appropriate in the light of the intended purpose of the high-risk AI system and applicable legal obligations under Union or national law.\n\nUsers that are credit institutions regulated by Directive 2013/36/EU shall maintain the logs as part of the documentation concerning internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive.\n\nUsers of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, where applicable.",
    "## Article 30: Notifying Authorities\n\nEach Member State shall designate or establish a notifying authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring.\n\nMember States may designate a national accreditation body referred to in Regulation (EC) No 765/2008 as a notifying authority.\n\nNotifying authorities shall be established, organised and operated in such a way that no conflict of interest arises with conformity assessment bodies and the objectivity and impartiality of their activities are safeguarded.\n\nNotifying authorities shall be organised in such a way that decisions relating to the notification of conformity assessment bodies are taken by competent persons different from those who carried out the assessment of those bodies.",
    "# Article 31 Application of a Conformity Assessment Body for Notification\n\nConformity assessment bodies shall submit an application for notification to the notifying authority of the Member State in which they are established.\n\nThe application for notification shall be accompanied by a description of the conformity assessment activities, the conformity assessment module or modules and the artificial intelligence technologies for which the conformity assessment body claims to be competent, as well as by an accreditation certificate, where one exists, issued by a national accreditation body attesting that the conformity assessment body fulfils the requirements laid down in Article 33.\n\nAny valid document related to existing designations of the applicant notified body under any other Union harmonisation legislation shall be added.\n\nWhere the conformity assessment body concerned cannot provide an accreditation certificate, it shall provide the notifying authority with the documentary evidence necessary for the verification, recognition, and regular monitoring of its compliance with the requirements laid down in Article 33.\n\nFor notified bodies which are designated under any other Union harmonisation legislation, all documents and certificates linked to those designations may be used to support their designation procedure under this Regulation, as appropriate.",
    "# Article 32 Notification Procedure\n\nNotifying authorities may notify only conformity assessment bodies which have satisfied the requirements laid down in Article 33.\n\nNotifying authorities shall notify the Commission and the other Member States using the electronic notification tool developed and managed by the Commission.\n\nThe notification shall include full details of the conformity assessment activities, the conformity assessment module or modules, and the artificial intelligence technologies concerned.\n\nThe conformity assessment body concerned may perform the activities of a notified body only where no objections are raised by the Commission or the other Member States within one month of a notification.\n\nNotifying authorities shall notify the Commission and the other Member States of any subsequent relevant changes to the notification.",
    "# Article 33 Notified Bodies\n\nNotified bodies shall verify the conformity of high-risk AI systems in accordance with the conformity assessment procedures referred to in Article 43.\n\nNotified bodies shall satisfy the organisational, quality management, resources, and process requirements that are necessary to fulfil their tasks.\n\nThe organisational structure, allocation of responsibilities, reporting lines, and operation of notified bodies shall be such as to ensure that there is confidence in the performance by and in the results of the conformity assessment activities that the notified bodies conduct.\n\nNotified bodies shall be independent of the provider of a high-risk AI system in relation to which it performs conformity assessment activities.\n\nNotified bodies shall also be independent of any other operator having an economic interest in the high-risk AI system that is assessed, as well as of any competitors of the provider.\n\nNotified bodies shall be organised and operated to safeguard the independence, objectivity, and impartiality of their activities.\n\nThey shall document and implement a structure and procedures to safeguard impartiality and to promote and apply the principles of impartiality throughout their organisation, personnel, and assessment activities.\n\nNotified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors, and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.\n\nThe staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out.\n\nNotified bodies shall have procedures for the performance of activities which take due account of the size of an undertaking, the sector in which it operates, its structure, the degree of complexity of the AI system in question.\n\nNotified bodies shall take out appropriate liability insurance for their conformity assessment activities, unless liability is assumed by the Member State concerned in accordance with national law or that Member State is directly responsible for the conformity assessment.\n\nNotified bodies shall be capable of carrying out all the tasks falling to them under this Regulation with the highest degree of professional integrity and the requisite competence in the specific field, whether those tasks are carried out by notified bodies themselves or on their behalf and under their responsibility.\n\nNotified bodies shall have sufficient internal competences to be able to effectively evaluate the tasks conducted by external parties on their behalf.\n\nTo that end, at all times and for each conformity assessment procedure and each type of high-risk AI system in relation to which they have been designated, the notified body shall have permanent availability of sufficient administrative, technical, and scientific personnel who possess experience and knowledge relating to the relevant artificial intelligence technologies, data and data computing, and the requirements set out in Chapter 2 of this Title.\n\nNotified bodies shall participate in coordination activities as referred to in Article 38.\n\nThey shall also take part directly or be represented in European standardisation organisations, or ensure that they are aware and up to date in respect of relevant standards.\n\nNotified bodies shall make available and submit upon request all relevant documentation, including the providers’ documentation, to the notifying authority referred to in Article 30 to allow it to conduct its assessment, designation, notification, monitoring, and surveillance activities, and to facilitate the assessment outlined in this Chapter.",
    "# Article 34 Subsidiaries of and Subcontracting by Notified Bodies\n\nWhere a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 33 and shall inform the notifying authority accordingly.\n\nNotified bodies shall take full responsibility for the tasks performed by subcontractors or subsidiaries wherever these are established.\n\nActivities may be subcontracted or carried out by a subsidiary only with the agreement of the provider.\n\nNotified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation.",
    "# Article 35 Identification Numbers and Lists of Notified Bodies Designated Under this Regulation\n\nThe Commission shall assign an identification number to notified bodies.\n\nIt shall assign a single number, even where a body is notified under several Union acts.\n\nThe Commission shall make publicly available the list of the bodies notified under this Regulation, including the identification numbers that have been assigned to them and the activities for which they have been notified.\n\nThe Commission shall ensure that the list is kept up to date.",
    "# Article 36 Changes to Notifications\n\nWhere a notifying authority has suspicions or has been informed that a notified body no longer meets the requirements laid down in Article 33, or that it is failing to fulfil its obligations, that authority shall without delay investigate the matter with the utmost diligence.\n\nIn that context, it shall inform the notified body concerned about the objections raised and give it the possibility to make its views known.\n\nIf the notifying authority comes to the conclusion that the notified body investigation no longer meets the requirements laid down in Article 33 or that it is failing to fulfil its obligations, it shall restrict, suspend, or withdraw the notification as appropriate, depending on the seriousness of the failure.\n\nIt shall also immediately inform the Commission and the other Member States accordingly.\n\nIn the event of restriction, suspension, or withdrawal of notification, or where the notified body has ceased its activity, the notifying authority shall take appropriate steps to ensure that the files of that notified body are either taken over by another notified body or kept available for the responsible notifying authorities at their request.",
    "# Article 37 Challenge to the Competence of Notified Bodies\n\nThe Commission shall, where necessary, investigate all cases where there are reasons to doubt whether a notified body complies with the requirements laid down in Article 33.\n\nThe Notifying authority shall provide the Commission, on request, with all relevant information relating to the notification of the notified body concerned.\n\nThe Commission shall ensure that all confidential information obtained in the course of its investigations pursuant to this Article is treated confidentially.\n\nWhere the Commission ascertains that a notified body does not meet or no longer meets the requirements laid down in Article 33, it shall adopt a reasoned decision requesting the notifying Member State to take the necessary corrective measures, including withdrawal of notification if necessary.\n\nThat implementing act shall be adopted in accordance with the examination procedure referred to in Article 74(2).",
    "# Article 38 Coordination of Notified Bodies\n\nThe Commission shall ensure that, with regard to the areas covered by this Regulation, appropriate coordination and cooperation between notified bodies active in the conformity assessment procedures of AI systems pursuant to this Regulation are put in place and properly operated in the form of a sectoral group of notified bodies.\n\nMember States shall ensure that the bodies notified by them participate in the work of that group, directly or by means of designated representatives.",
    "## Article 40 Harmonised Standards\n\nHigh-risk AI systems which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those standards cover those requirements.",
    "## Article 41 Common Specifications\n\nWhere harmonised standards referred to in Article 40 do not exist or where the Commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific safety or fundamental right concerns, the Commission may, by means of implementing acts, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title.\n\nThose implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2).\n\nThe Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups established under relevant sectorial Union law.\n\nHigh-risk AI systems which are in conformity with the common specifications referred to in paragraph 1 shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those common specifications cover those requirements.\n\nWhere providers do not comply with the common specifications referred to in paragraph 1, they shall duly justify that they have adopted technical solutions that are at least equivalent thereto.",
    "## Article 42 Presumption of Conformity with Certain Requirements\n\nTaking into account their intended purpose, high-risk AI systems that have been trained and tested on data concerning the specific geographical, behavioural and functional setting within which they are intended to be used shall be presumed to be in compliance with the requirement set out in Article 10(4).\n\nHigh-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 of the European Parliament and of the Council63 and the references of which have been published in the Official Journal of the European Union shall be presumed to be in compliance with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements.",
    "## Article 43 Conformity Assessment\n\nFor high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall follow one of the following procedures:\n- the conformity assessment procedure based on internal control referred to in Annex VI;\n- the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII.\n\nWhere, in demonstrating the compliance of a high-risk AI system with the requirements set out in Chapter 2 of this Title, the provider has not applied or has applied only in part harmonised standards referred to in Article 40, or where such harmonised standards do not exist and common specifications referred to in Article 41 are not available, the provider shall follow the conformity assessment procedure set out in Annex VII.\n\nFor the purpose of the conformity assessment procedure referred to in Annex VII, the provider may choose any of the notified bodies.\n\nHowever, when the system is intended to be put into service by law enforcement, immigration, or asylum authorities as well as EU institutions, bodies or agencies, the market surveillance authority referred to in Article 63(5) or (6), as applicable, shall act as a notified body.\n\nFor high-risk AI systems referred to in points 2 to 8 of Annex III, providers shall follow the conformity assessment procedure based on internal control as referred to in Annex VI, which does not provide for the involvement of a notified body.\n\nFor high-risk AI systems referred to in point 5(b) of Annex III, placed on the market or put into service by credit institutions regulated by Directive 2013/36/EU, the conformity assessment shall be carried out as part of the procedure referred to in Articles 97 to 101 of that Directive.\n\nFor high-risk AI systems, to which legal acts listed in Annex II, section A, apply, the provider shall follow the relevant conformity assessment as required under those legal acts.\n\nThe requirements set out in Chapter 2 of this Title shall apply to those high-risk AI systems and shall be part of that assessment.\n\nPoints 4.3., 4.4., 4.5., and the fifth paragraph of point 4.6 of Annex VII shall also apply.\n\nFor the purpose of that assessment, notified bodies which have been notified under those legal acts shall be entitled to control the conformity of the high-risk AI systems with the requirements set out in Chapter 2 of this Title, provided that the compliance of those notified bodies with requirements laid down in Article 33(4), (9), and (10) has been assessed in the context of the notification procedure under those legal acts.\n\nWhere the legal acts listed in Annex II, section A, enable the manufacturer of the product to opt out from a third-party conformity assessment, provided that that manufacturer has applied all harmonised standards covering all the relevant requirements, that manufacturer may make use of that option only if he has also applied harmonised standards or, where applicable, common specifications referred to in Article 41, covering the requirements set out in Chapter 2 of this Title.\n\nHigh-risk AI systems shall undergo a new conformity assessment procedure whenever they are substantially modified, regardless of whether the modified system is intended to be further distributed or continues to be used by the current user.\n\nFor high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk AI system and its performance that have been pre-determined by the provider at the moment of the initial conformity assessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV, shall not constitute a substantial modification.\n\nThe Commission is empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating Annexes VI and Annex VII in order to introduce elements of the conformity assessment procedures that become necessary in light of technical progress.\n\nThe Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof.\n\nThe Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the availability of adequate capacities and resources among notified bodies.",
    "## Article 44 Certificates\n\nCertificates issued by notified bodies in accordance with Annex VII shall be drawn up in an official Union language determined by the Member State in which the notified body is established or in an official Union language otherwise acceptable to the notified body.\n\nCertificates shall be valid for the period they indicate, which shall not exceed five years.\n\nOn application by the provider, the validity of a certificate may be extended for further periods, each not exceeding five years, based on a re-assessment in accordance with the applicable conformity assessment procedures.\n\nWhere a notified body finds that an AI system no longer meets the requirements set out in Chapter 2 of this Title, it shall, taking account of the principle of proportionality, suspend or withdraw the certificate issued or impose any restrictions on it, unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body.\n\nThe notified body shall give reasons for its decision.",
    "## Article 46 Information Obligations of Notified Bodies\n\nNotified bodies shall inform the notifying authority of the following:\n- any Union technical documentation assessment certificates, any supplements to those certificates, quality management system approvals issued in accordance with the requirements of Annex VII;\n- any refusal, restriction, suspension, or withdrawal of a Union technical documentation assessment certificate or a quality management system approval issued by them;\n- any circumstances affecting the scope of or conditions for notification;\n- any request for information which they have received from market surveillance authorities regarding conformity assessment activities;\n- on request, conformity assessment activities performed within the scope of their notification and any other activity performed, including cross-border activities and subcontracting.\n\nEach notified body shall inform the other notified bodies of:\n- quality management system approvals which it has refused, suspended or withdrawn, and, upon request, of quality system approvals which it has issued;\n- EU technical documentation assessment certificates or any supplements thereto which it has refused, withdrawn, suspended or otherwise restricted, and, upon request, of the certificates and/or supplements thereto which it has issued.\n\nEach notified body shall provide the other notified bodies carrying out similar conformity assessment activities covering the same artificial intelligence technologies with relevant information on issues relating to negative and, on request, positive conformity assessment results.",
    "## Article 47 Derogation from Conformity Assessment Procedure\n\nBy way of derogation from Article 43, any market surveillance authority may authorise the placing on the market or putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets.\n\nThat authorisation shall be for a limited period of time, while the necessary conformity assessment procedures are being carried out, and shall terminate once those procedures have been completed.\n\nThe completion of those procedures shall be undertaken without undue delay.\n\nThe authorisation referred to in paragraph 1 shall be issued only if the market surveillance authority concludes that the high-risk AI system complies with the requirements of Chapter 2 of this Title.\n\nThe market surveillance authority shall inform the Commission and the other Member States of any authorisation issued pursuant to paragraph 1.\n\nWhere, within 15 calendar days of receipt of the information referred to in paragraph 2, no objection has been raised by either a Member State or the Commission in respect of an authorisation issued by a market surveillance authority of a Member State in accordance with paragraph 1, that authorisation shall be deemed justified.\n\nWhere, within 15 calendar days of receipt of the notification referred to in paragraph 2, objections are raised by a Member State against an authorisation issued by a market surveillance authority of another Member State, or where the Commission considers the authorisation to be contrary to Union law or the conclusion of the Member States regarding the compliance of the system as referred to in paragraph 2 to be unfounded, the Commission shall without delay enter into consultation with the relevant Member State; the operator(s) concerned shall be consulted and have the possibility to present their views.\n\nIn view thereof, the Commission shall decide whether the authorisation is justified or not.\n\nThe Commission shall address its decision to the Member State concerned and the relevant operator or operators.\n\nIf the authorisation is considered unjustified, this shall be withdrawn by the market surveillance authority of the Member State concerned.\n\nBy way of derogation from paragraphs 1 to 5, for high-risk AI systems intended to be used as safety components of devices, or which are themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU) 2017/746, Article 59 of Regulation (EU) 2017/745 and Article 54 of Regulation (EU) 2017/746 shall apply also with regard to the derogation from the conformity assessment of the compliance with the requirements set out in Chapter 2 of this Title.",
    "## Article 48 EU Declaration of Conformity\n\nThe provider shall draw up a written EU declaration of conformity for each AI system and keep it at the disposal of the national competent authorities for 10 years after the AI system has been placed on the market or put into service.\n\nThe EU declaration of conformity shall identify the AI system for which it has been drawn up.\n\nA copy of the EU declaration of conformity shall be given to the relevant national competent authorities upon request.\n\nThe EU declaration of conformity shall state that the high-risk AI system in question meets the requirements set out in Chapter 2 of this Title.\n\nThe EU declaration of conformity shall contain the information set out in Annex V and shall be translated into an official Union language or languages required by the Member State(s) in which the high-risk AI system is made available.\n\nWhere high-risk AI systems are subject to other Union harmonisation legislation which also requires an EU declaration of conformity, a single EU declaration of conformity shall be drawn up in respect of all Union legislations applicable to the high-risk AI system.\n\nThe declaration shall contain all the information required for identification of the Union harmonisation legislation to which the declaration relates.\n\nBy drawing up the EU declaration of conformity, the provider shall assume responsibility for compliance with the requirements set out in Chapter 2 of this Title.\n\nThe provider shall keep the EU declaration of conformity up-to-date as appropriate.\n\nThe Commission shall be empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating the content of the EU declaration of conformity set out in Annex V in order to introduce elements that become necessary in light of technical progress.",
    "## Article 49 CE Marking of Conformity\n\nThe CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems.\n\nWhere that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.\n\nThe CE marking referred to in paragraph 1 of this Article shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.\n\nWhere applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43.\n\nThe identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking.",
    "## Article 50 Document Retention\n\nThe provider shall, for a period ending 10 years after the AI system has been placed on the market or put into service, keep at the disposal of the national competent authorities:\n- the technical documentation referred to in Article 11;\n- the documentation concerning the quality management system referred to Article 17;\n- the documentation concerning the changes approved by notified bodies where applicable;\n- the decisions and other documents issued by the notified bodies where applicable;\n- the EU declaration of conformity referred to in Article 48.",
    "### Article 52\n\nTransparency obligations for certain AI systems \nProviders shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use.\n\nThis obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence.\n\nUsers of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto.\n\nThis obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences.\n\nUsers of an AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful (‘deep fake’), shall disclose that the content has been artificially generated or manipulated.\n\nHowever, the first subparagraph shall not apply where the use is authorised by law to detect, prevent, investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties.\n\nParagraphs 1, 2 and 3 shall not affect the requirements and obligations set out in Title III of this Regulation.",
    "### Article 53\n\nAI regulatory sandboxes  \nAI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan.\n\nThis shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox.\n\nMember States shall ensure that to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data, the national data protection authorities and those other national authorities are associated to the operation of the AI regulatory sandbox.\n\nThe AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities.\n\nAny significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place.\n\nParticipants in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result from the experimentation taking place in the sandbox.\n\nMember States’ competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board.\n\nThey shall submit annual reports to the Board and the Commission on the results from the implementation of those scheme, including good practices, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox.\n\nThe modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox, and the rights and obligations of the participants shall be set out in implementing acts.\n\nThose implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2).",
    "### Article 54\n\nFurther processing of personal data for developing certain AI systems in the public interest in the AI regulatory sandbox  \nIn the AI regulatory sandbox personal data lawfully collected for other purposes shall be processed for the purposes of developing and testing certain innovative AI systems in the sandbox under the following conditions:  \nthe innovative AI systems shall be developed for safeguarding substantial public interest in one or more of the following areas:  \nthe prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security, under the control and responsibility of the competent authorities.\n\nThe processing shall be based on Member State or Union law;  \npublic safety and public health, including disease prevention, control and treatment;  \na high level of protection and improvement of the quality of the environment;  \nthe data processed are necessary for complying with one or more of the requirements referred to in Title III, Chapter 2 where those requirements cannot be effectively fulfilled by processing anonymised, synthetic or other non-personal data;  \nthere are effective monitoring mechanisms to identify if any high risks to the fundamental rights of the data subjects may arise during the sandbox experimentation as well as response mechanism to promptly mitigate those risks and, where necessary, stop the processing;  \nany personal data to be processed in the context of the sandbox are in a functionally separate, isolated and protected data processing environment under the control of the participants and only authorised persons have access to that data;  \nany personal data processed are not be transmitted, transferred or otherwise accessed by other parties;  \nany processing of personal data in the context of the sandbox do not lead to measures or decisions affecting the data subjects;  \nany personal data processed in the context of the sandbox are deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention period;  \nthe logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation in the sandbox and 1 year after its termination, solely for the purpose of and only as long as necessary for fulfilling accountability and documentation obligations under this Article or other application Union or Member States legislation;  \ncomplete and detailed description of the process and rationale behind the training, testing and validation of the AI system is kept together with the testing results as part of the technical documentation in Annex IV;  \na short summary of the AI project developed in the sandbox, its objectives and expected results published on the website of the competent authorities.\n\nParagraph 1 is without prejudice to Union or Member States legislation excluding processing for other purposes than those explicitly mentioned in that legislation.",
    "### Article 55\n\nMeasures for small-scale providers and users  \nMember States shall undertake the following actions:  \nprovide small-scale providers and start-ups with priority access to the AI regulatory sandboxes to the extent that they fulfil the eligibility conditions;  \norganise specific awareness raising activities about the application of this Regulation tailored to the needs of the small-scale providers and users;  \nwhere appropriate, establish a dedicated channel for communication with small-scale providers and user and other innovators to provide guidance and respond to queries about the implementation of this Regulation.\n\nThe specific interests and needs of the small-scale providers shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to their size and market size.",
    "### Article 56\n\nEstablishment of the European Artificial Intelligence Board  \nA ‘European Artificial Intelligence Board’ (the ‘Board’) is established.\n\nThe Board shall provide advice and assistance to the Commission in order to:  \ncontribute to the effective cooperation of the national supervisory authorities and the Commission with regard to matters covered by this Regulation;  \ncoordinate and contribute to guidance and analysis by the Commission and the national supervisory authorities and other competent authorities on emerging issues across the internal market with regard to matters covered by this Regulation;  \nassist the national supervisory authorities and the Commission in ensuring the consistent application of this Regulation.",
    "### Article 57\n\nStructure of the Board  \nThe Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor.\n\nOther national authorities may be invited to the meetings, where the issues discussed are of relevance for them.\n\nThe Board shall adopt its rules of procedure by a simple majority of its members, following the consent of the Commission.\n\nThe rules of procedure shall also contain the operational aspects related to the execution of the Board’s tasks as listed in Article 58.\n\nThe Board may establish sub-groups as appropriate for the purpose of examining specific questions.\n\nThe Board shall be chaired by the Commission.\n\nThe Commission shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure.\n\nThe Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation.\n\nThe Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent.\n\nTo that end the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups.",
    "### Article 58\n\nTasks of the Board  \nWhen providing advice and assistance to the Commission in the context of Article 56(2), the Board shall in particular:  \ncollect and share expertise and best practices among Member States;  \ncontribute to uniform administrative practices in the Member States, including for the functioning of regulatory sandboxes referred to in Article 53;  \nissue opinions, recommendations or written contributions on matters related to the implementation of this Regulation, in particular on technical specifications or existing standards regarding the requirements set out in Title III, Chapter 2, on the use of harmonised standards or common specifications referred to in Articles 40 and 41, on the preparation of guidance documents, including the guidelines concerning the setting of administrative fines referred to in Article 71.",
    "### Article 59\n\nDesignation of national competent authorities  \nNational competent authorities shall be established or designated by each Member State for the purpose of ensuring the application and implementation of this Regulation.\n\nNational competent authorities shall be organised so as to safeguard the objectivity and impartiality of their activities and tasks.\n\nEach Member State shall designate a national supervisory authority among the national competent authorities.\n\nThe national supervisory authority shall act as notifying authority and market surveillance authority unless a Member State has organisational and administrative reasons to designate more than one authority.\n\nMember States shall inform the Commission of their designation or designations and, where applicable, the reasons for designating more than one authority.\n\nMember States shall ensure that national competent authorities are provided with adequate financial and human resources to fulfil their tasks under this Regulation.\n\nIn particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements.\n\nMember States shall report to the Commission on an annual basis on the status of the financial and human resources of the national competent authorities with an assessment of their adequacy.\n\nThe Commission shall transmit that information to the Board for discussion and possible recommendations.\n\nThe Commission shall facilitate the exchange of experience between national competent authorities.\n\nNational competent authorities may provide guidance and advice on the implementation of this Regulation, including to small-scale providers.\n\nWhenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union legislation, the competent national authorities under that Union legislation shall be consulted, as appropriate.\n\nMember States may also establish one central contact point for communication with operators.\n\nWhen Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision.",
    "### Article 60\n\nEU database for stand-alone high-risk AI systems  \nThe Commission shall, in collaboration with the Member States, set up and maintain a EU database containing information referred to in paragraph 2 concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Article 51.\n\nThe data listed in Annex VIII shall be entered into the EU database by the providers.\n\nThe Commission shall provide them with technical and administrative support.\n\nInformation contained in the EU database shall be accessible to the public.\n\nThe EU database shall contain personal data only insofar as necessary for collecting and processing information in accordance with this Regulation.\n\nThat information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider.\n\nThe Commission shall be the controller of the EU database.\n\nIt shall also ensure to providers adequate technical and administrative support.",
    "### Article 61\n\nPost-market monitoring by providers and post-market monitoring plan for high-risk AI systems  \nProviders shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the artificial intelligence technologies and the risks of the high-risk AI system.\n\nThe post-market monitoring system shall actively and systematically collect, document and analyse relevant data provided by users or collected through other sources on the performance of high-risk AI systems throughout their lifetime, and allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Title III, Chapter 2.\n\nThe post-market monitoring system shall be based on a post-market monitoring plan.\n\nThe post-market monitoring plan shall be part of the technical documentation referred to in Annex IV.\n\nThe Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan.\n\nFor high-risk AI systems covered by the legal acts referred to in Annex II, where a post-market monitoring system and plan is already established under that legislation, the elements described in paragraphs 1, 2 and 3 shall be integrated into that system and plan as appropriate.\n\nThe first subparagraph shall also apply to high-risk AI systems referred to in point 5(b) of Annex III placed on the market or put into service by credit institutions regulated by Directive 2013/36/EU.",
    "### Article 62\n\nReporting of serious incidents and of malfunctioning  \nProviders of high-risk AI systems placed on the Union market shall report any serious incident or any malfunctioning of those systems which constitutes a breach of obligations under Union law intended to protect fundamental rights to the market surveillance authorities of the Member States where that incident or breach occurred.\n\nSuch notification shall be made immediately after the provider has established a causal link between the AI system and the incident or malfunctioning or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the providers becomes aware of the serious incident or of the malfunctioning.\n\nUpon receiving a notification related to a breach of obligations under Union law intended to protect fundamental rights, the market surveillance authority shall inform the national public authorities or bodies referred to in Article 64(3).\n\nThe Commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1.\n\nThat guidance shall be issued 12 months after the entry into force of this Regulation, at the latest.\n\nFor high-risk AI systems referred to in point 5(b) of Annex III which are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013/36/EU and for high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU) 2017/746, the notification of serious incidents or malfunctioning shall be limited to those that that constitute a breach of obligations under Union law intended to protect fundamental rights.",
    "### Article 63\n\nMarket surveillance and control of AI systems in the Union market  \nRegulation (EU) 2019/1020 shall apply to AI systems covered by this Regulation.\n\nHowever, for the purpose of the effective enforcement of this Regulation:  \nany reference to an economic operator under Regulation (EU) 2019/1020 shall be understood as including all operators identified in Title III, Chapter 3 of this Regulation;  \nany reference to a product under Regulation (EU) 2019/1020 shall be understood as including all AI systems falling within the scope of this Regulation.\n\nThe national supervisory authority shall report to the Commission on a regular basis the outcomes of relevant market surveillance activities.\n\nThe national supervisory authority shall report, without delay, to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules.\n\nFor high-risk AI systems, related to products to which legal acts listed in Annex II, section A apply, the market surveillance authority for the purposes of this Regulation shall be the authority responsible for market surveillance activities designated under those legal acts.\n\nFor AI systems placed on the market, put into service or used by financial institutions regulated by Union legislation on financial services, the market surveillance authority for the purposes of this Regulation shall be the relevant authority responsible for the financial supervision of those institutions under that legislation.\n\nFor AI systems listed in point 1(a) in so far as the systems are used for law enforcement purposes, points 6 and 7 of Annex III, Member States shall designate as market surveillance authorities for the purposes of this Regulation either the competent data protection supervisory authorities under Directive (EU) 2016/680, or Regulation 2016/679 or the national competent authorities supervising the activities of the law enforcement, immigration or asylum authorities putting into service or using those systems.\n\nWhere Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as their market surveillance authority.\n\nMember States shall facilitate the coordination between market surveillance authorities designated under this Regulation and other relevant national authorities or bodies which supervise the application of Union harmonisation legislation listed in Annex II or other Union legislation that might be relevant for the high-risk AI systems referred to in Annex III.",
    "### Article 64\n\nAccess to data and documentation  \nAccess to data and documentation in the context of their activities, the market surveillance authorities shall be granted full access to the training, validation and testing datasets used by the provider, including through application programming interfaces (‘API’) or other appropriate technical means and tools enabling remote access.\n\nWhere necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2 and upon a reasoned request, the market surveillance authorities shall be granted access to the source code of the AI system.\n\nNational public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation when access to that documentation is necessary for the fulfilment of the competences under their mandate within the limits of their jurisdiction.\n\nThe relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request.\n\nBy 3 months after the entering into force of this Regulation, each Member State shall identify the public authorities or bodies referred to in paragraph 3 and make a list publicly available on the website of the national supervisory authority.\n\nMember States shall notify the list to the Commission and all other Member States and keep the list up to date.\n\nWhere the documentation referred to in paragraph 3 is insufficient to ascertain whether a breach of obligations under Union law intended to protect fundamental rights has occurred, the public authority or body referred to paragraph 3 may make a reasoned request to the market surveillance authority to organise testing of the high- risk AI system through technical means.\n\nThe market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within reasonable time following the request.\n\nAny information and documentation obtained by the national public authorities or bodies referred to in paragraph 3 pursuant to the provisions of this Article shall be treated in compliance with the confidentiality obligations set out in Article 70.",
    "### Article 65\n\nProcedure for dealing with AI systems presenting a risk at national level  \nAI systems presenting a risk shall be understood as a product presenting a risk defined in Article 3, point 19 of Regulation (EU) 2019/1020 insofar as risks to the health or safety or to the protection of fundamental rights of persons are concerned.\n\nWhere the market surveillance authority of a Member State has sufficient reasons to consider that an AI system presents a risk as referred to in paragraph 1, they shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation.\n\nWhen risks to the protection of fundamental rights are present, the market surveillance authority shall also inform the relevant national public authorities or bodies referred to in Article 64(3).\n\nThe relevant operators shall cooperate as necessary with the market...  \nsurveillance authorities and the other national public authorities or bodies referred to in Article 64(3).\n\nWhere, in the course of that evaluation, the market surveillance authority finds that the AI system does not comply with the requirements and obligations laid down in this Regulation, it shall without delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe.\n\nThe market surveillance authority shall inform the relevant notified body accordingly.\n\nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the second subparagraph.\n\nWhere the market surveillance authority considers that non-compliance is not restricted to its national territory, it shall inform the Commission and the other Member States of the results of the evaluation and of the actions which it has required the operator to take.\n\nThe operator shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned that it has made available on the market throughout the Union.\n\nWhere the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market, to withdraw the product from that market or to recall it.\n\nThat authority shall inform the Commission and the other Member States, without delay, of those measures.\n\nThe information referred to in paragraph 5 shall include all available details, in particular the data necessary for the identification of the non-compliant AI system, the origin of the AI system, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator.\n\nIn particular, the market surveillance authorities shall indicate whether the non-compliance is due to one or more of the following:  \na failure of the AI system to meet requirements set out in Title III, Chapter 2;  \nshortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 conferring a presumption of conformity.\n\nThe market surveillance authorities of the Member States other than the market surveillance authority of the Member State initiating the procedure shall without delay inform the Commission and the other Member States of any measures adopted and of any additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of disagreement with the notified national measure, of their objections.\n\nWhere, within three months of receipt of the information referred to in paragraph 5, no objection has been raised by either a Member State or the Commission in respect of a provisional measure taken by a Member State, that measure shall be deemed justified.\n\nThis is without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation (EU) 2019/1020.\n\nThe market surveillance authorities of all Member States shall ensure that appropriate restrictive measures are taken in respect of the product concerned, such as withdrawal of the product from their market, without delay.\n\n```",
    "# Union Safeguard Procedure\n\nWhere, within three months of receipt of the notification referred to in Article 65(5), objections are raised by a Member State against a measure taken by another Member State, or where the Commission considers the measure to be contrary to Union law, the Commission shall without delay enter into consultation with the relevant Member State and operator or operators and shall evaluate the national measure.\n\nOn the basis of the results of that evaluation, the Commission shall decide whether the national measure is justified or not within 9 months from the notification referred to in Article 65(5) and notify such decision to the Member State concerned.\n\nIf the national measure is considered justified, all Member States shall take the measures necessary to ensure that the non-compliant AI system is withdrawn from their market, and shall inform the Commission accordingly.\n\nIf the national measure is considered unjustified, the Member State concerned shall withdraw the measure.\n\nWhere the national measure is considered justified and the non-compliance of the AI system is attributed to shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 of this Regulation, the Commission shall apply the procedure provided for in Article 11 of Regulation (EU) No 1025/2012.",
    "# Article 67 Compliant AI Systems Which Present a Risk\n\nWhere, having performed an evaluation under Article 65, the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to the compliance with obligations under Union or national law intended to protect fundamental rights or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk, to withdraw the AI system from the market or to recall it within a reasonable period commensurate with the nature of the risk, as it may prescribe.\n\nThe provider or other relevant operators shall ensure that corrective action is taken in respect of all the AI systems concerned that they have made available on the market throughout the Union within the timeline prescribed by the market surveillance authority of the Member State referred to in paragraph 1.\n\nThe Member State shall immediately inform the Commission and the other Member States.\n\nThat information shall include all available details, in particular, the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved, and the nature and duration of the national measures taken.\n\nThe Commission shall without delay enter into consultation with the Member States and the relevant operator and shall evaluate the national measures taken.\n\nOn the basis of the results of that evaluation, the Commission shall decide whether the measure is justified or not and, where necessary, propose appropriate measures.\n\nThe Commission shall address its decision to the Member States.",
    "# Article 68 Formal Non-Compliance\n\nWhere the market surveillance authority of a Member State makes one of the following findings, it shall require the relevant provider to put an end to the non-compliance concerned: the conformity marking has been affixed in violation of Article 49; the conformity marking has not been affixed; the EU declaration of conformity has not been drawn up; the EU declaration of conformity has not been drawn up correctly; the identification number of the notified body, which is involved in the conformity assessment procedure, where applicable, has not been affixed; Where the non-compliance referred to in paragraph 1 persists, the Member State concerned shall take all appropriate measures to restrict or prohibit the high-risk AI system being made available on the market or ensure that it is recalled or withdrawn from the market.",
    "## Article 69 Codes of Conduct\n\nThe Commission and the Member States shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to AI systems other than high-risk AI systems of the requirements set out in Title III, Chapter 2 on the basis of technical specifications and solutions that are appropriate means of ensuring compliance with such requirements in light of the intended purpose of the systems.\n\nThe Commission and the Board shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to AI systems of requirements related, for example, to environmental sustainability, accessibility for persons with a disability, stakeholders' participation in the design and development of the AI systems, and diversity of development teams on the basis of clear objectives and key performance indicators to measure the achievement of those objectives.\n\nCodes of conduct may be drawn up by individual providers of AI systems or by organizations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organizations.\n\nCodes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems.\n\nThe Commission and the Board shall take into account the specific interests and needs of the small-scale providers and start-ups when encouraging and facilitating the drawing up of codes of conduct.",
    "## Article 70 Confidentiality\n\nNational competent authorities and notified bodies involved in the application of this Regulation shall respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect, in particular: intellectual property rights, and confidential business information or trade secrets of a natural or legal person, including source code, except the cases referred to in Article 5 of Directive 2016/943 on the protection of undisclosed know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure apply.\n\nthe effective implementation of this Regulation, in particular for the purpose of inspections, investigations or audits;(c) public and national security interests; integrity of criminal or administrative proceedings.\n\nWithout prejudice to paragraph 1, information exchanged on a confidential basis between the national competent authorities and between national competent authorities and the Commission shall not be disclosed without the prior consultation of the originating national competent authority and the user when high-risk AI systems referred to in points 1, 6, and 7 of Annex III are used by law enforcement, immigration, or asylum authorities, when such disclosure would jeopardize public and national security interests.\n\nWhen the law enforcement, immigration, or asylum authorities are providers of high-risk AI systems referred to in points 1, 6, and 7 of Annex III, the technical documentation referred to in Annex IV shall remain within the premises of those authorities.\n\nThose authorities shall ensure that the market surveillance authorities referred to in Article 63(5) and (6), as applicable, can, upon request, immediately access the documentation or obtain a copy thereof.\n\nOnly staff of the market surveillance authority holding the appropriate level of security clearance shall be allowed to access that documentation or any copy thereof.\n\nParagraphs 1 and 2 shall not affect the rights and obligations of the Commission, Member States, and notified bodies with regard to the exchange of information and the dissemination of warnings, nor the obligations of the parties concerned to provide information under the criminal law of the Member States.\n\nThe Commission and Member States may exchange, where necessary, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.",
    "## Article 71 Penalties\n\nIn compliance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties, including administrative fines, applicable to infringements of this Regulation and shall take all measures necessary to ensure that they are properly and effectively implemented.\n\nThe penalties provided for shall be effective, proportionate, and dissuasive.\n\nThey shall take into particular account the interests of small-scale providers and start-up and their economic viability.\n\nThe Member States shall notify the Commission of those rules and of those measures and shall notify it, without delay, of any subsequent amendment affecting them.\n\nThe following infringements shall be subject to administrative fines of up to 30 000 000 EUR or, if the offender is a company, up to 6 % of its total worldwide annual turnover for the preceding financial year, whichever is higher: non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5; non-compliance of the AI system with the requirements laid down in Article 10.\n\nThe non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10, shall be subject to administrative fines of up to 20 000 000 EUR or, if the offender is a company, up to 4 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.\n\nThe supply of incorrect, incomplete, or misleading information to notified bodies and national competent authorities in reply to a request shall be subject to administrative fines of up to 10 000 000 EUR or, if the offender is a company, up to 2 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.\n\nWhen deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following: the nature, gravity, and duration of the infringement and of its consequences; whether administrative fines have been already applied by other market surveillance authorities to the same operator for the same infringement.\n\nThe size and market share of the operator committing the infringement; Each Member State shall lay down rules on whether and to what extent administrative fines may be imposed on public authorities and bodies established in that Member State.\n\nDepending on the legal system of the Member States, the rules on administrative fines may be applied in such a manner that the fines are imposed by competent national courts or other bodies as applicable in those Member States.\n\nThe application of such rules in those Member States shall have an equivalent effect.",
    "## Article 72 Administrative Fines on Union Institutions, Agencies, and Bodies\n\nThe European Data Protection Supervisor may impose administrative fines on Union institutions, agencies, and bodies falling within the scope of this Regulation.\n\nWhen deciding whether to impose an administrative fine and deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following: the nature, gravity, and duration of the infringement and of its consequences; the cooperation with the European Data Protection Supervisor in order to remedy the infringement and mitigate the possible adverse effects of the infringement, including compliance with any of the measures previously ordered by the European Data Protection Supervisor against the Union institution, agency, or body concerned with regard to the same subject matter; any similar previous infringements by the Union institution, agency, or body; The following infringements shall be subject to administrative fines of up to 500 000 EUR: non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5; non-compliance of the AI system with the requirements laid down in Article 10.\n\nThe non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10, shall be subject to administrative fines of up to 250 000 EUR.\n\nBefore taking decisions pursuant to this Article, the European Data Protection Supervisor shall give the Union institution, agency, or body which is the subject of the proceedings conducted by the European Data Protection Supervisor the opportunity of being heard on the matter regarding the possible infringement.\n\nThe European Data Protection Supervisor shall base his or her decisions only on elements and circumstances on which the parties concerned have been able to comment.\n\nComplainants, if any, shall be associated closely with the proceedings.\n\nThe rights of defense of the parties concerned shall be fully respected in the proceedings.\n\nThey shall be entitled to have access to the European Data Protection Supervisor’s file, subject to the legitimate interest of individuals or undertakings in the protection of their personal data or business secrets.\n\nFunds collected by the imposition of fines in this Article shall be the income of the general budget of the Union.",
    "## Article 73 Exercise of the Delegation\n\nThe power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.\n\nThe delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6), and Article 48(5) shall be conferred on the Commission for an indeterminate period of time from [entering into force of the Regulation].\n\nThe delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6), and Article 48(5) may be revoked at any time by the European Parliament or by the Council.\n\nA decision of revocation shall put an end to the delegation of power specified in that decision.\n\nIt shall take effect the day following that of its publication in the Official Journal of the European Union or at a later date specified therein.\n\nIt shall not affect the validity of any delegated acts already in force.\n\nAs soon as it adopts a delegated act, the Commission shall notify it simultaneously to the European Parliament and to the Council.\n\nAny delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article 43(5) and (6), and Article 48(5) shall enter into force only if no objection has been expressed by either the European Parliament or the Council within a period of three months of notification of that act to the European Parliament and the Council or if, before the expiry of that period, the European Parliament and the Council have both informed the Commission that they will not object.\n\nThat period shall be extended by three months at the initiative of the European Parliament or of the Council.",
    "## Article 75 Amendment to Regulation (EC) No 300/2008\n\nIn Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added: “When adopting detailed measures related to technical specifications and procedures for approval and use of security equipment concerning Artificial Intelligence systems in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Chapter 2, Title III of that Regulation shall be taken into account.” * Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”",
    "## Article 76 Amendment to Regulation (EU) No 167/2013\n\nIn Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added: “When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.\n\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”",
    "## Article 77 Amendment to Regulation (EU) No 168/2013\n\nIn Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added: “When adopting delegated acts pursuant to the first subparagraph concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX on [Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.\n\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”",
    "## Article 78 Amendment to Directive 2014/90/EU\n\nIn Article 8 of Directive 2014/90/EU, the following paragraph is added: “4.\n\nFor Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, when carrying out its activities pursuant to paragraph 1 and when adopting technical specifications and testing standards in accordance with paragraphs 2 and 3, the Commission shall take into account the requirements set out in Title III, Chapter 2 of that Regulation.\n\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.",
    "## Article 79 Amendment to Directive (EU) 2016/797\n\nIn Article 5 of Directive (EU) 2016/797, the following paragraph is added: “12.\n\nWhen adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to paragraph 11 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.\n\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.",
    "## Article 80 Amendment to Regulation (EU) 2018/858\n\nIn Article 5 of Regulation (EU) 2018/858, the following paragraph is added: “4.\n\nWhen adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.\n\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.",
    "## Article 81 Amendment to Regulation (EU) 2018/1139\n\nRegulation (EU) 2018/1139 is amended as follows: In Article 17, the following paragraph is added: “3.\n\nWithout prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.\n\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).” In Article 19, the following paragraph is added: “4.\n\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.” In Article 43, the following paragraph is added: “4.\n\nWhen adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.” In Article 47, the following paragraph is added: “3.\n\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.” In Article 57, the following paragraph is added: “When adopting those implementing acts concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.” In Article 58, the following paragraph is added: “3.\n\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.”.",
    "## Article 82 Amendment to Regulation (EU) 2019/2144\n\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added: “3.\n\nWhen adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.\n\n* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”.",
    "## Article 83 AI Systems Already Placed on the Market or Put into Service\n\nThis Regulation shall not apply to the AI systems which are components of the large-scale IT systems established by the legal acts listed in Annex IX that have been placed on the market or put into service before [12 months after the date of application of this Regulation referred to in Article 85(2)], unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the AI system or AI systems concerned.\n\nThe requirements laid down in this Regulation shall be taken into account, where applicable, in the evaluation of each large-scale IT system established by the legal acts listed in Annex IX to be undertaken as provided for in those respective acts.\n\nThis Regulation shall apply to the high-risk AI systems, other than the ones referred to in paragraph 1, that have been placed on the market or put into service before [date of application of this Regulation referred to in Article 85(2)], only if, from that date, those systems are subject to significant changes in their design or intended purpose.",
    "## Article 84 Evaluation and Review\n\nThe Commission shall assess the need for amendment of the list in Annex III once a year following the entry into force of this Regulation.\n\nBy [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council.\n\nThe reports shall be made public.\n\nThe reports referred to in paragraph 2 shall devote specific attention to the following: the status of the financial and human resources of the national competent authorities in order to effectively perform the tasks assigned to them under this Regulation; the state of penalties, and notably administrative fines as referred to in Article 71(1), applied by Member States to infringements of the provisions of this Regulation.\n\nWithin [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter, the Commission shall evaluate the impact and effectiveness of codes of conduct to foster the application of the requirements set out in Title III, Chapter 2 and possibly other additional requirements for AI systems other than high-risk AI systems.\n\nFor the purpose of paragraphs 1 to 4 the Board, the Member States, and national competent authorities shall provide the Commission with information on its request.\n\nIn carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the Commission shall take into account the positions and findings of the Board, of the European Parliament, of the Council, and of other relevant bodies or sources.\n\nThe Commission shall, if necessary, submit appropriate proposals to amend this Regulation, in particular taking into account developments in technology and in the light of the state of progress in the information society.",
    "## Article 85 Entry into Force and Application\n\nThis Regulation shall enter into force on the twentieth day following that of its publication in the Official Journal of the European Union.\n\nThis Regulation shall apply from [24 months following the entering into force of the Regulation].\n\nBy way of derogation from paragraph 2: Title III, Chapter 4, and Title VI shall apply from [three months following the entry into force of this Regulation]; Article 71 shall apply from [twelve months following the entry into force of this Regulation].\n\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.\n\nDone at Brussels,\n\nFor the European Parliament For the Council The President The President",
    "## FRAMEWORK OF THE PROPOSAL/INITIATIVE\n\nTitle of the proposal/initiative Policy area(s) concerned The proposal/initiative relates to: Objective(s) General objective(s) Specific objective(s) Expected result(s) and impact Indicators of performance Grounds for the proposal/initiative Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative Added value of Union involvement (it may result from different factors, e.g.\n\ncoordination gains, legal certainty, greater effectiveness or complementarities).\n\nFor the purposes of this point 'added value of Union involvement' is the value resulting from Union intervention which is additional to the value that would have been otherwise created by Member States alone Lessons learned from similar experiences in the past Compatibility with the Multiannual Financial Framework and possible synergies with other appropriate instruments Assessment of the different available financing options, including scope for redeployment Duration and financial impact of the proposal/initiative Management mode(s) planned",
    "# MANAGEMENT MEASURES\n\nMonitoring and reporting rules Management and control system Justification of the management mode(s), the funding implementation mechanism(s), the payment modalities, and the control strategy proposed Information concerning the risks identified and the internal control system(s) set up to mitigate them Estimation and justification of the cost-effectiveness of the controls (ratio of \"control costs ÷ value of the related funds managed\"), and assessment of the expected levels of risk of error (at payment & at closure) Measures to prevent fraud and irregularities",
    "# ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE\n\nHeading(s) of the multiannual financial framework and expenditure budget line(s) affected Estimated financial impact of the proposal on appropriations Summary of estimated impact on operational appropriations Estimated output funded with operational appropriations Summary of estimated impact on administrative appropriations Compatibility with the current multiannual financial framework Third-party contributions Estimated impact on revenue",
    "## Grounds for the Proposal/Initiative\n\nRequirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative\n\nAdded value of Union involvement (it may result from different factors, e.g.\n\ncoordination gains, legal certainty, greater effectiveness or complementarities).\n\nFor the purposes of this point 'added value of Union involvement' is the value resulting from Union intervention which is additional to the value that would have been otherwise created by Member States alone.\n\nLessons learned from similar experiences in the past\n\nCompatibility with the Multiannual Financial Framework and possible synergies with other appropriate instruments\n\nAssessment of the different available financing options, including scope for redeployment",
    "## Duration and Financial Impact of the Proposal/Initiative\n\nLimited duration  \n In effect from [DD/MM]YYYY to [DD/MM]YYYY  \n Financial impact from YYYY to YYYY for commitment appropriations and from YYYY to YYYY for payment appropriations.\n\nUnlimited duration  \nImplementation with a start-up period from one/two (tbc) year, followed by full-scale operation.",
    "## Management Mode(s) Planned\n\nX Direct management by the Commission  \n By its departments, including by its staff in the Union delegations;  \n By the executive agencies  \n\nShared management with the Member States  \nIndirect management by entrusting budget implementation tasks to:  \n Third countries or the bodies they have designated;  \n International organisations and their agencies (to be specified);  \n The EIB and the European Investment Fund;  \n Bodies referred to in Articles 70 and 71 of the Financial Regulation;  \n Public law bodies;  \n Bodies governed by private law with a public service mission to the extent that they provide adequate financial guarantees;  \n Bodies governed by the private law of a Member State that are entrusted with the implementation of a public-private partnership and that provide adequate financial guarantees;  \n Persons entrusted with the implementation of specific actions in the CFSP pursuant to Title V of the TEU, and identified in the relevant basic act.\n\nIf more than one management mode is indicated, please provide details in the ‘Comments’ section.\n\nComments",
    "### Management and Control System(s)\n\nJustification of the management mode(s), the funding implementation mechanism(s), the payment modalities and the control strategy proposed\n\nInformation concerning the risks identified and the internal control system(s) set up to mitigate them\n\nEstimate and justification of the cost-effectiveness of the controls (ratio of \"control costs ÷ value of the related funds managed\"), and assessment of the expected levels of risk of error (at payment & at closure)",
    "### Estimated Financial Impact of the Proposal on Appropriations\n\nSummary of estimated impact on expenditure on operational appropriations  \n The proposal/initiative does not require the use of operational appropriations  \nX The proposal/initiative requires the use of operational appropriations, as explained below:  \nEUR million (to three decimal places)\n\nIf more than one heading is affected by the proposal/initiative, repeat the section above.\n\nThis section should be filled in using the 'budget data of an administrative nature' to be firstly introduced in the Annex to the Legislative Financial Statement (Annex V to the internal rules), which is uploaded to DECIDE for interservice consultation purposes.\n\nEUR million (to three decimal places)",
    "### Summary of Estimated Impact on Administrative Appropriations\n\n The proposal/initiative does not require the use of appropriations of an administrative nature  \nX The proposal/initiative requires the use of appropriations of an administrative nature, as explained below:  \nEUR million (to three decimal places)\n\nThe appropriations required for human resources and other expenditure of an administrative nature will be met by appropriations from the DG that are already assigned to management of the action and/or have been redeployed within the DG, together if necessary with any additional allocation which may be granted to the managing DG under the annual allocation procedure and in the light of budgetary constraints.",
    "### Estimated Requirements of Human Resources\n\n The proposal/initiative does not require the use of human resources.\n\nX The proposal/initiative requires the use of human resources, as explained below:  \nEstimate to be expressed in full time equivalent units\n\nXX is the policy area or budget title concerned.\n\nThe human resources required will be met by staff from the DG who are already assigned to management of the action and/or have been redeployed within the DG, together if necessary with any additional allocation which may be granted to the managing DG under the annual allocation procedure and in the light of budgetary constraints.\n\nEDPS is expected to provide half of the resources required.",
    "### Compatibility with the Current Multiannual Financial Framework\n\nThe proposal/initiative:  \n– X Can be fully financed through redeployment within the relevant heading of the Multiannual Financial Framework (MFF).\n\n–  Requires use of the unallocated margin under the relevant heading of the MFF and/or use of the special instruments as defined in the MFF Regulation.\n\n–  Requires a revision of the MFF.",
    "### Estimated Impact on Revenue\n\n The proposal/initiative has the following financial impact:  \n On other revenue  \nPlease indicate, if the revenue is assigned to expenditure lines   \nEUR million (to three decimal places)\n\nFor assigned revenue, specify the budget expenditure line(s) affected.\n\nOther remarks (e.g.\n\nmethod/formula used for calculating the impact on revenue or any other information).",
    "# Australia’s Generative AI Opportunity\n\nJuly 2023\n\nThis document is intended for general informational purposes only.\n\nThe report is a collaboration between Microsoft and the Tech Council of Australia.\n\nViews and opinions expressed in this document are based on the companies’ knowledge and understanding of its area of business, markets and technology.\n\nThe companies do not provide medical, legal, regulatory, audit, or tax advice, and this document does not constitute advice of any nature.\n\nWhile the information in this document has been prepared in good faith, the companies disclaim, to the fullest extent permitted by applicable law, any and all liability for the accuracy and completeness of the information in this document and for any acts or omissions made based on such information.\n\nOpinions expressed herein are subject to change without notice.\n\nNo part of this document may be reproduced in any manner without the written permission of the companies.\n\nThis document may make references to third party names, trademarks or copyrights that may be owned by others.\n\nAny third-party names, trademarks, or copyrights contained in this document are the property of their respective owners.",
    "# Executive Summary (1/3)\n\nGenerative AI (GAI) represents a substantial economic opportunity for Australia, with the potential to add tens of billions to the economy by 2030.\n\nGAI, powered by deep neural networks such as Large Language Models (LLMs), enables the creation of novel content and contributes to automation, data comprehension, and analysis.\n\nThis rapidly evolving technology can drive economic value through two main channels: improving existing businesses (through productivity and quality gains) and creating new products and services.\n\nFirst, GAI can improve existing businesses by automating repetitive tasks and copiloting complex processes, leading to improved productivity and work quality.\n\nResearch has already found GAI coding tools reduce task times by 56%, and writing tools decrease writing time by 37%, with improved quality.\n\nTo better understand how each occupation is impacted by GAI, we analysed data from the Occupational Information Network (O*NET), which provides information on the tasks undertaken by each occupation in the economy.\n\nOn average, across the economy, GAI can automate 22% of task-hours and augment an equal share.\n\nAutomation of tasks improves the productivity of workers by allowing them to produce more in any given amount of time.\n\nIn parallel, task augmentation, whereby GAI acts as a copilot (i.e., an expert helper to a user trying to accomplish a complex task) enables workers to produce higher quality output in the same amount of time.\n\nSecondly, GAI paves the way for innovative products and services, such as conversational virtual assistants and interactive wearable health devices.\n\nThis new wave of innovation can lead to the creation of new industries, jobs, and economic growth.\n\nImportantly, moving from the potential benefits of GAI to actual, realised gains depends on a range of factors, including how useful it is for businesses, how the technology is regulated and safely managed, and how workers are supported to use the technology.\n\nIn sizing the economic value of GAI to Australia by 2030, we need to account for significant uncertainty, particularly around how quickly the technology can be effectively adopted.",
    "# Executive Summary (2/3)\n\nThe second section of this report explores the opportunity of GAI for Australia through more tangible examples.\n\nIt identifies four key sectors where Australia can succeed in creating value through GAI.\n\nThese opportunities are healthcare, manufacturing, retail, and professional services.\n\nThese sectors were chosen for two key reasons.\n\nFirst, they are likely to continue to be important sectors for employment and output in the Australian economy into the future.\n\nSecond, GAI is likely to have a transformative effect on these sectors.\n\nAdditionally, these sectors and their diverse use cases for GAI illustrate the breadth of the technology’s impact on the economy.\n\nThe selection process involved desktop research, industry analysis, consultation with experts, along with engagements with executives and key industry personnel more broadly.\n\nImportantly, the contents of these sector deep dives culminates from consultations with industry experts, and a Roundtable discussion held in June with leaders from industry, academia, and governments.\n\nWe thank all who contributed to these discussions.\n\nExploring potential use cases of GAI highlights the significant value it can unlock across various sectors of the economy.",
    "# Executive Summary (3/3)\n\nCapturing the economic potential of GAI requires leveraging Australia’s comparative advantages and strategic actions by industry and government.\n\nAustralia possesses several comparative advantages that can enable it to seize the economic potential of GAI.\n\nThese include a highly- skilled workforce proficient in data science, engineering, and computer science and a robust research and development sector.\n\nAdditional benefits, such as our strategic location close to Asia, a stable and transparent regulatory environment, and a thriving start-up ecosystem provide strong grounds for GAI development and adoption.\n\nAlongside these key strengths, there are also key challenges.\n\nTo capture the economic benefits of GAI, Australia needs to address barriers around technology capability, enterprise readiness, awareness and skills, and responsible AI.\n\nSuch barriers include the significant investments required to build AI orchestrations to meet specific industry contexts, integration with existing systems, data protection, and workforce upskilling.\n\nBoth industry and government have key roles to play in addressing these challenges.\n\nIndustry needs to clearly define GAI's opportunity, assess readiness, invest in and experiment with the technology, develop privacy and security guardrails, upskill the workforce, and monitor performance.\n\nMeanwhile, the Australian Government's role is crucial in setting a clear vision for GAI in Australia, supporting collaboration between research institutions and industry, providing regulatory clarity, incentivising GAI adoption, and investing in the right skills.\n\nBy taking these strategic actions together, Australia can unlock the transformative potential of GAI, driving economic growth and global competitiveness.",
    "## Generative AI is a step change in the evolution of AI\n\n- Rule-based systems (1950s-1960s)\n- Statistical learning (1970s-1990s)\n- Deep Learning (2000s-present)\n- Generative AI (2010s-present)\n- Foundational large language models (2018-present)\n- Applications of LLMs (2023+)\n\nGenerative AI (GAI), the latest evolution in artificial intelligence, carries the potential for significant economic advancement.\n\nWhile the full economic impact will take years to realise, GAI is already impacting a range of sectors across the economy.\n\nThis report aims to focus specifically on how GAI could drive value for the Australian economy and identify the steps needed to seize this opportunity.\n\nGAI, a subset of artificial intelligence, uses machine learning to generate human-like content.\n\nIt signifies a considerable transformation in the economic prospects of AI at large, by empowering machines to produce novel content or data, previously unseen or unimagined.\n\nRecent improvements in computing hardware and infrastructure, along with the availability of large-scale and diverse training datasets, have been instrumental in enabling the development of larger and more powerful GAI models than ever before.\n\nOne of the most notable innovations in deep learning architectures came in 2017 with the Transformer architecture, which facilitates parallel processing of sequences and the use of attention mechanisms for tracking long-range word relations.\n\nThis innovation, combined with advancements in optimisation techniques, has facilitated the development of larger, faster, and more sophisticated GAI models.\n\nAs models become larger, they develop powerful ‘emergent capabilities’ that are only possible when the model reaches a certain scale.\n\nNo longer limited to completing narrow tasks based on a narrow range of prompts, modern GAI models can now perform more ‘generalist’ functions.\n\nThese include the command of natural language, coding and mathematics, and the ability to plan and problem solve.\n\nAlong with improved capability, innovations in technology have also led to increased accessibility of GAI by reducing costs.\n\nFurthermore, the development of more user-friendly tools and interfaces has made GAI more accessible to a wider range of users.\n\nFor example, some online platforms allow users to easily create and manipulate GAI models using drag-and-drop interfaces or intuitive sliders, even if they have little to no experience in machine learning.\n\nNot only has this democratised GAI, improved accessibility creates a wider range of use-cases for businesses and workers across all sectors of the economy.\n\nAdditionally, modern GAI models are already being fine-tuned for specific use cases.\n\nThis has made it easier for developers, researchers, and businesses to use GAI in their applications without having to spend time and resources on training their own models from scratch.\n\nTogether, the improved computing and accessibility of GAI means it is already changing how we work and the way firms operate.\n\nIt is augmenting human workers by acting as a copilot, increasing productivity and quality in various industries, as well as creating new jobs and businesses.",
    "## Generative AI models have a wide range of applications and can create value through their distinctive capabilities\n\nGAI can create novel content in response to user prompts.\n\nThe generation of this content is principally handled by foundational models such as Large Language Models (LLMs) that are deep neural networks.\n\nThese models are built on robust computing infrastructure and depend on large datasets for training.\n\nTo enhance the accessibility and usability of LLMs, reinforcement learning techniques that encourage human-like responses and intuitive interfaces are integral components of the GAI framework.\n\nAs it evolves, GAI promises to deliver immense value across several facets.\n\nOne of these is automation, where GAI can expedite processes and minimise time spent on repetitive administrative tasks.\n\nIn the creation domain, it can help generate new ideas in areas such as product design and content creation.\n\nIt can also play an advisory role, acting as a copilot guiding workers through complex issues.\n\nFurthermore, GAI allows the exploration, interrogation, and synthesis of large datasets, leading to improved data comprehension and more insightful decision-making.\n\nThe versatility of GAI is underscored by the range of its models, extending from text and code generation to the creation of images, audio and voice, video, and 3D content.\n\nEach of these models ushers in unique use-cases, thereby emphasising the far-reaching applications and economic benefits of GAI.",
    "### Improvements to existing industries\n\nFirstly, adoption of the technology in existing industries drives higher productivity and quality.\n\nGAI can boost productivity by automating certain tasks within an occupation.\n\nThis partial automation frees up time for the worker to focus on other tasks, therefore completing their existing roles more quickly and increasing their productivity.\n\nFor example, doctors and nurses spend a significant proportion of their time on administrative activities which could be automated to focus on patient care.\n\nFurthermore, knowledge workers in professional services can save substantial time researching and synthesising large amounts of information.\n\nAdditionally, GAI can improve the quality of a worker’s output by augmenting tasks within a role.\n\nAugmentation refers to GAI’s ability to assist the worker to complete a task.\n\nFor example, GAI may augment a doctor’s diagnosis of a patient by generating a list of possible cases based on the doctor’s inputs or patient data.\n\nUltimately, the doctor’s expertise is needed to complete the diagnosis (i.e., complete the task).\n\nYet with the aid of GAI, which both advises and explores potential solutions, the doctor is better able to think through all possible cases, leading to more comprehensive care.\n\nIn other words, the quality of their output is improved.\n\nAcross all industries, such augmentation and the resulting gains in quality can lead to increased customer satisfaction, loyalty, and retention, generating a quality ‘premium’ that drives value to the economy.",
    "### New products and services\n\nSecondly, GAI can enable the development of new products and services that were not previously possible, such as highly personalised content, conversational virtual assistants, and interactive wearable health devices.\n\nThis can lead to increased innovation and competitiveness in a range of industries, driving economic growth and enhancing Australia's global competitiveness.\n\nNew jobs can be created by enabling the development of new products and services, as new forms of employment emerge with this innovation.\n\nThis can help to boost employment rates and drive economic growth.\n\nNew business opportunities can enable entrepreneurs to develop new products and services that are not currently feasible with existing technologies.\n\nThis can lead to the creation of new industries and markets, driving economic growth and job creation.\n\nOverall, GAI can play a key role in driving innovation, productivity, and competitiveness in a range of industries, and has the potential to deliver significant economic value to the Australian economy.",
    "### Improvements to existing industries\n\nTo account for the significant uncertainty around how GAI will impact Australia in 2030, we model the potential gains against three different scenarios for adoption.\n\nGiven GAI is a relatively nascent and rapidly evolving technology, there is significant uncertainty in how this technology is adopted and embraced.\n\nTo account for this, we model the potential gains against three different scenarios for adoption.\n\nThis gives a range for the total economic opportunity of GAI in 2030.",
    "### New products and services\n\nWe estimate the global addressable market of GAI in 2030.\n\nTotal addressable market is the total revenue opportunity for businesses selling GAI products.\n\nWe focus only on the market for GAI software, since consultations revealed this is where Australia’s opportunity is most likely to reside (as opposed to GAI hardware).\n\nWe estimate Australia’s share of this global market in 2030.\n\nThis estimation is based on Australia’s existing share of global tech ‘unicorns’ – tech startups with an annual turnover over $1B.\n\nThis proxy captures Australia’s comparative advantage in tech start-ups and likelihood to capture a greater than proportional share of the global GAI software market.\n\nWe estimate the contribution of the GAI market to Australia’s economy.\n\nThis is based on the average ratio of total income to gross value-added (GVA) in Australia’s tech sector over the last 10 years.\n\nThe adoption of new technologies typically follows a distinct pattern known as an S-curve, with a slow start (early adopters), followed by a rapid increase (majority adopters), and then a slowdown (late adopters).\n\nTo predict the S-curve of GAI, we use historical data on internet adoption in Australia as a reference point.\n\nWe arrive at three scenarios for adoption, as shown.\n\nEven in our fastest scenario, our scenarios stop before the majority of adoption would occur.\n\nIn the scenario of slow-paced adoption, adoption rates in 2030 are 13%.\n\nThis can be interpreted as 13% of task hours that have the potential to be automated and augmented are, in fact, automated and augmented.\n\nIn the medium and high paced scenarios, adoption rates in 2030 are 21% and 33% respectively.",
    "### Non-Language tasks\n\n- Set up classrooms, facilities, educational materials, or equipment.\n\n- Movement of materials, products, or equipment.\n\nGenerative AI can transform the way people work by automating or augmenting tasks.\n\nOn average, across all occupations, 44% of worker task-hours have potential to be automated or augmented.\n\nAutomation of tasks improves the productivity of workers, allowing them to produce more in any given amount of time.\n\nIn parallel, augmentation, whereby GAI acts as a copilot, enables workers to improve the quality of their output.\n\nTo understand how each occupation is impacted by GAI, we analysed data from the Occupational Information Network (O*NET), which provides information on the tasks undertaken by each occupation.\n\nOn average, 22% of task-hours have high potential for automation by GAI.\n\nThese tasks, characterised by their routine nature and well-defined parameters, lend themselves more easily to automation.\n\nSuch tasks include synthesising documents and large text-based sources, reconciling data, or transcribing.\n\nAdditionally, 22% of task-hours demonstrate a high potential for augmentation using GAI.\n\nThese tasks might be assisted with GAI but necessitate human input or involvement in some way.\n\nSuch tasks include inspecting the quality of products, evaluating the accuracy of data, explanations of policies and procedures, preparing technical documents, or training staff to use products and services.\n\nFor these tasks, GAI acts as a copilot, amplifying workers’ expertise to improve the quality of their output.\n\nMoreover, 26% of task-hours exhibit a lower potential for automation or augmentation.\n\nThese tasks are either less routine or not discretely defined and require proactive effort from a human.\n\nSuch tasks include directing organisational activities, evaluating personnel capabilities, and interpersonal tasks more generally.\n\nIn the analysis, these tasks are assumed to not derive any benefit from GAI.\n\nLastly, around 30% of task-hours are deemed non-language tasks with no potential for automation or augmentation through GAI.\n\nThese activities are largely physical or manual tasks.\n\nIt is important to note these averages provide an overall view of the impact of GAI on workers across the economy.\n\nHowever, different occupations are impacted to different extents.\n\nGAI is likely to have the greatest impact on white-collar work in services industries.\n\nThis is a shift from previous automating technologies, which have traditionally targeted manual labour-intensive activities.\n\nGAI presents a transformative opportunity for roles and industries previously thought to be less impacted by digital innovation.",
    "### The economic opportunity of GAI in 2030\n\n**$ billion, 2030 annual value added, 2023 values**  \nSlow-paced adoption  Medium-paced adoption  Fast-paced adoption.\n\nGenerative AI could deliver between $45-115B in value to the Australian economy.\n\nHow much of this potential value is captured depends on two factors: how well the technology is adopted across all industries and how well workers are supported to transition to other tasks.\n\nThe pace at which Australia adopts GAI will determine how the potential opportunity is translated into tangible economic growth.\n\nIf we accelerate adoption, the total gains can be up to $115B annually by 2030.\n\nIf adoption in Australia grows more slowly, the total benefit would be approximately $45B annually.\n\nspeed on new innovative medical research, improving their diagnostic capabilities.\n\nThis increased quality is expected to be worth $10-25B to the economy, depending on the adoption rate.\n\nAs well as improvements to existing industries, GAI will power new products and services in Australia.\n\nGlobally, the total addressable market for GAI software could be ~$220B by 2030.\n\nIf Australia moves early, it could capture a greater than proportional share of this market.\n\nFor example, savvy Australian start-ups could build and sell industry-specific applications that capture a global market.\n\n- Increased productivity\n- Improved quality\n- New jobs\n- New business growth and creation\n\nThe majority of these gains, $30-$80B (or 70%), result from uplifts in productivity...",
    "# Productivity and Economic Opportunity\n\nEvery routine task automated by GAI enables workers to achieve more in a given amount of time.\n\nFor example, a customer support agent can attend to more customer calls if the tasks of logging complaints and post-call feedback are handled by GAI.\n\nImportantly, there is a risk some of the automated task-hours are not successfully transitioned to other work.\n\nTo be conservative, we exclude these task-hours from the productivity gains modelling.\n\nCrucially, this highlights that supporting workers and managing the transition is critical to achieving the full productivity benefits of GAI.\n\nIn addition, GAI augments tasks and acts as a copilot, allowing workers to complete high-value and high-quality work.\n\nFor example, a software developer can be guided through a new complex piece of code, leading to a better end application.\n\nOr a health practitioner can be brought up to speed in real time with new medical information, facilitating enhanced patient care.\n\nIn Australia, these new products and services will power new jobs and businesses that could collectively add $5-10B to the economy, depending once again on adoption rates.",
    "## Transitioning Workers to Other Tasks and Roles\n\nThe full labour force benefits of GAI will only be realised if net employment remains steady.\n\nAs adoption and capability of GAI increases, certain occupations may experience reduced demand for labour.\n\nGovernments and industries must collaborate to transition workers in such roles to other tasks and occupations.\n\nThis means proactively investing in comprehensive reskilling and upskilling programs, with a focus on emerging industries and occupations, and emphasising digital literacy, data analysis, critical thinking, and creativity.",
    "## Managing AI Responsibly\n\nAs the capability of GAI develops, and users are still learning about its functionality, the technology carries certain risks such as biased or unethical outputs, privacy concerns, and potential misuse.\n\nIndustry and government need to establish regulations that promote transparency, accountability, and responsible practices.\n\nThis includes guidelines for transparency and accountability in decision-making, to ensure GAI systems are inclusive and do not obscure the context-specific needs of priority cohorts.\n\nFurthermore, ensuring data security is particularly critical if GAI is to drive benefits in highly sensitive industries like healthcare or law.",
    "## Enabling Equal Access to GAI\n\nCertain regions or communities may face barriers in accessing and benefiting from GAI.\n\nTo address this risk, governments will need to prioritise initiatives that bridge the digital divide.\n\nThis includes investing in infrastructure development, such as high-speed internet connectivity in remote or underserved areas.\n\nAdditionally, it may mean establishing funding programs and grants to support small and medium-sized enterprises, startups, and education institutions in adopting the technology.\n\nPromoting digital literacy can also help ensure that individuals from diverse backgrounds have the skills and knowledge to participate in the AI-driven economy.\n\n**Source:** Consultations and Roundtable discussion",
    "# Section 2: Key Sectors for Growth in the Australian Economy\n\nGenerative AI presents a significant opportunity for growth in at least four key sectors in the Australian economy.\n\nGAI has high potential in at least four key sectors.\n\nThis report identifies four key sectors where Australia can succeed in creating value through GAI.\n\nThese opportunities are healthcare, manufacturing, retail, and professional services.\n\nThese sectors have been chosen based on two key considerations.\n\nFirst, they are likely to continue being important sectors for employment and output in the Australian economy into the future.\n\nSecond, GAI is likely to have a transformative effect on these sectors.\n\nAdditionally, these sectors and their diverse use cases for GAI were chosen to illustrate the breadth of the technology’s impact on the economy.\n\nThe process for selection involved desktop research, industry analysis, consultation with experts, along with engagements with executives and key industry personnel more broadly.",
    "## Benefits of GAI in Healthcare\n\nGAI has significant potential to improve the accessibility and quality of healthcare delivery.\n\nIt can enable more one-on-one patient care by reducing time spent on admin, improving personalisation by being embedded in wearable devices, and supporting the transition towards more proactive models of healthcare by allowing earlier diagnosis, at scale.\n\nImportantly, the key to realising these benefits will be robust protocols that ensure patient confidentiality and safety are maintained.\n\n- **25% of nursing tasks can be augmented, and another 5% can be automated.\n\n**  \n  This allows nurses to spend more time one-on-one with patients.\n\n- **Over 30% of new drugs could be discovered using GAI in some way by 2025.\n\n**  \n  GAI can generate synthetic patient data at scale and at lower cost.\n\n- **$5-13BN total economic opportunity**\n- **93% of people are interested in using digital self-service for pre-visit tasks.\n\n**  \n  With GAI, self-service will be more personalised and effective.",
    "### Example Use Cases\n\n- **Automating**\n  - Faster and better quality medical note-taking for staff, with audio to text capabilities automatically generating notes from consultations.\n\n- Reduced admin, such as automated patient check-in forms, prior authorisation drafting, imaging report generation, referrals, etc.\n\n- Automatic verification of patient insurance and eligibility.\n\n- **Creating**\n  - Generating questions based on patient’s symptoms, medical history, demography, etc.\n\n- Creating symptom summaries for clinicians.\n\n- **Advising**\n  - Chatbots with patient-specific educational materials and recommendations for follow-up care.\n\n- Training junior medical professionals, particularly in underserved communities, to increase access to healthcare.\n\n- Medical imaging analysis to improve accuracy of disease detection.\n\n- Synthesising new medical research to improve the diagnostic ability of doctors.\n\n- **Exploring**\n  - Assisting new drug development, by analysing vast amounts of data and creating synthetic data.\n\n- Opening up med-tech products to new overseas markets, by training local practitioners in the use of the device.\n\nNotes:\n1.\n\nAt full adoption – the economic opportunity here is based on a much lower rate of adoption, see the discussion in the report on adoption.\n\nAnalysis of O*NET data.\n\n2.\n\nCB Insights (2023) 7 applications of generative AI in healthcare.\n\n3.\n\nKyruus (2022) Patient Access Journey Report.",
    "## Benefits of GAI in Manufacturing\n\nGAI may be a game-changer for Australian manufacturing, by enabling a suite of advanced manufacturing capabilities that play to Australia’s existing manufacturing strengths of producing high quality and highly technical products.\n\nImportantly, shifting to advanced manufacturing is a strategic priority for both State and Federal Governments.\n\n- **30% of a manager’s tasks in manufacturing may be automated, and 19% can be assisted by GAI.\n\n**\n  - Since managers make up 17% of the manufacturing workforce, GAI is likely to have significant productivity impacts.\n\n- **For technicians and trades workers, the GAI opportunity is in rapid upskilling and training.\n\n**\n  - With technicians comprising 27% of the workforce, GAI will enhance their ability to learn on the job.\n\n- **$2-5BN total economic opportunity**\n  - To capitalise, the industry needs to accelerate digital transformation\n  - While 72% of surveyed manufacturers increased digital transformation during the pandemic, only 20% were using AI in any way.",
    "### Example Use Cases\n\n- **Automating**\n  - Collaborate on Bills of Materials (BOMs) with contract manufacturers using GAI as a knowledge manager.\n\n- Integrate GAI into Supply Network Control Tower, to improve interaction and generate insights that assist management of supply networks.\n\n- **Creating**\n  - Rapid new product design using GAI to generate product recommendations based on trending features.\n\n- Analyse large sets of customer feedback, including surveys and social media posts.\n\n- Generate multiple virtual prototypes that meet a given set of specifications, assisting market testing and feasibility analysis.\n\n- **Advising**\n  - Support demand forecast scenario modeling.\n\n- Analyse customer data to optimise pricing and incentive recommendations.\n\n- Assist upskilling of apprentices and technicians, through conversational on-the-job training of simple concepts.\n\n- **Exploring**\n  - Improve quality inspection, using synthetic data that depicts low-occurrence defects, and calibrating with computer vision inspection.\n\n- Make recommendations and provide training content and scenarios for servicing equipment and parts to field technicians.\n\nNotes:\n1.\n\nAt full adoption – the economic opportunity here is based on a much lower rate of adoption, see the discussion in the report on adoption.\n\nAnalysis of O*NET data.\n\n2.\n\nNational Skills Commission, Australian Jobs 2021, 2022 \n3.\n\nCBA, CommBank Manufacturing Insights, 2022",
    "## Benefits of GAI in Retail\n\nAs a result of having to invest in omnichannel capabilities during the pandemic, the retail industry should be better placed to integrate GAI into existing digital platforms.\n\nSuch integration could be particularly important as companies look for new ways to differentiate and personalise products, while remaining cost-competitive.\n\n- **30% of a shop sales assistant’s tasks may be automated, and 32% may be augmented by GAI.\n\n**\n  - This means more time can be invested in improving customer experiences and products.\n\n- **GAI can improve the productivity of customer support workers by 14%.\n\n**\n  - Studies have found GAI allows customer support workers to resolve more issues per hour.\n\n- **$3-9BN total economic opportunity**\n  - GAI could allow greater personalisation of products, and drive customer engagement.\n\n- Personalised chatbots and product offerings could enhance the customer experience.",
    "### Example Use Cases\n\n- **Automating**\n  - Provide personalised customer support and automate customer onboarding with sophisticated conversational assistants.\n\n- Key feature summarisation and product descriptions to present the most relevant content in concise form.\n\n- Streamline order processing, with automated order validation.\n\n- **Creating**\n  - Improve customer satisfaction through personalised offerings and unique recommendations based on micro interactions across all touchpoints.\n\n- Highly customised real-time marketing campaigns based on customer preferences, purchase history, and behaviour.\n\n- **Advising**\n  - Efficient inventory management with automatic analysis of sales data and consumer sentiment.\n\n- Assisting compliance with regulation, by supporting easier navigation of laws and obligations.\n\n- Streamline complaint handling, by categorising complaints and suggesting responses.\n\n- Improved space management through creating alternative planograms based on individual store demographics.\n\n- **Exploring**\n  - Automate customer journey mapping, and suggest ways to improve experiences.\n\n- Assisting predictive analysis, to identify growth markets and strategies.\n\nNotes:\n1.\n\nAt full adoption – the economic opportunity here is based on a much lower rate of adoption, see the discussion in the report on adoption.\n\nAnalysis of ONET data.\n\n2.\n\nBrynjolfsson et al (2023) Generative AI at work (Working Paper), NBER.",
    "## Benefits of GAI in Professional and Financial Services\n\nThe professional services industry is a highly educated workforce with relatively low levels of productivity.\n\nThis makes the industry a prime candidate for GAI transformation.\n\nAs GAI automates routine and well-defined tasks, the highly skilled workforce can spend more time on higher value thinking.\n\nWith GAI, Australia could build on its reputation for high calibre knowledge workers, particularly in financial and legal sectors.\n\n- **36% of an accountant’s tasks may be automated, and 26% assisted by GAI.\n\n**  \n  This could lead to a shift towards a more advisory and strategic role for accountants.\n\n- **10% of a solicitor’s tasks may be automated, and 32% could be augmented.\n\n**\n  This may increase the importance of a lawyer’s expertise on applications of the law in complex contexts.\n\n- **GAI could drive greater access to professional services, including legal and financial advice.\n\n**  \n  For example, GAI may support lawyers to take on more cases for underserved cohorts.",
    "### Example Use Cases\n\n- **General**\n  - Automating: Speed up research and knowledge gathering, by analysing unstructured data.\n\n- Creating: Synthesising information to generate standard documents.\n\n- Advising: Spot errors and suggest ways to improve work.\n\n- Exploring: Predict future client problems and assist in the generation of solutions.\n\n- **Banking and Finance**\n  - Automating: Synthesising information from multiple sources like video KYC, underwriting algorithms, application forms, etc.\n\n- Creating: Financial statements for M&A & portfolio management.\n\n- Advising: Interactive alerting in case of any anomalies in risk metrics across delinquencies, liquidity, market, etc.\n\n- **Legal Services**\n  - Automating: Searching documents for uses of specific words and finding precedents in historic judgments.\n\n- Creating: Generation of standard legal documents, i.e., confidentiality deeds, licenses, conveyancing.\n\n- Advising: Identifying contract loopholes and suggesting solution clauses.\n\n- Exploring: Assist stress-testing through interactive simulations of expected credit losses (ECL) based on different macro-economic scenarios.\n\nNotes:\n1.\n\nAt full adoption – the economic opportunity here is based on a much lower rate of adoption, see the discussion in the report on adoption.\n\nAnalysis of ONET data.",
    "## Current State of Generative AI Policies\n\nAustralian industries and governments are at a critical juncture with GAI adoption and need to collaborate to capitalise on the opportunity.\n\nThe previous two sections of this report illustrated the significant opportunity GAI represents for Australia.\n\nIf Australia is to capitalise, the time to act is now.\n\nInnovation is progressing at a rapid rate, and international peers are already on the move towards adopting GAI.\n\nAustralian industries and governments, if they are to keep pace, are at a critical juncture and need to collaborate.\n\nThe Generative AI space is rapidly evolving.\n\nWhile this innovative environment signals great opportunity, it also brings significant uncertainty for both adopters and regulators of GAI.\n\nInternational peers are already implementing a range of policies to keep pace with this innovation.\n\nThe US has established a Senate Judiciary Subcommittee on Privacy, Technology and the Law to hear expert opinions about GAI’s impact on the economy and society.\n\nAt the time of writing, the EU is progressing the EU Artificial Intelligence Act, its flagship piece of specific legislation to classify and regulate AI.\n\nThe G7 has announced its commitment to work together on AI governance and interoperability.\n\nOverall, in the last year alone, legislative bodies across 127 countries have passed 37 laws that include the word ‘artificial intelligence’.\n\nImportantly, Australia is already well-positioned to stay up to speed with global best practices.\n\nIn 2019, the Federal Government released an AI Ethics Framework, to guide businesses and the Government to ‘responsibly design, develop and implement AI’.\n\nThe CSIRO’s National AI Centre recently established the Responsible AI Network (RAIN), which supports industry to improve AI governance capabilities.\n\nIt also has published a report assisting businesses to implement responsible AI.\n\nFurther, the Department of Industry, Science and Resources is developing Safe and Responsible AI policy, building on the National Science and Technology Council’s Rapid Research Report on Generative AI.\n\nImportantly, these initial policies are good first steps, but mainly relate to the topic of regulation.\n\nAlongside regulatory clarification, collaboration between industries and governments that focus on supporting Australia’s GAI ecosystem needs to begin.\n\nThis section of the report aims to spur on this collaboration.\n\nIt first explores Australia’s comparative advantages in digital technology that should be leveraged by both industry and government.\n\nNext, it explores the barriers industry and government face in capitalizing on GAI.\n\nFinally, it outlines the priority actions both adopters and policy setters need to take if Australia is to become a global leader in GAI.",
    "# Australia's Comparative Advantage in Generative AI\n\nAustralia is well positioned to be a leader in Generative AI, with existing comparative advantages in digital innovation.\n\n- **Large Existing Talent Pool**\n  - Australia is home to a burgeoning tech workforce, which was 935,000 strong as of February 2023 and on track to grow to 1.2 million by 2030.\n\nThis emerging workforce, with the right support from industry and government, can provide a strong foundation for the development and adoption of GAI technologies.\n\nHighly-skilled talent has already enabled the country to become a global leader in areas such as natural language processing and machine learning.\n\n- **Stable Regulatory Settings and Engagement with Global Standards**\n  - Australia’s governments and agencies value consultation and transparency in the development of new regulation.\n\nThis approach promotes regulatory certainty and provides a stable environment for the development and adoption of new technologies.\n\nThe Federal Government has already begun the consultation process to consider AI governance in Australia.\n\nFurthermore, Australia is engaging with best practices from overseas and focused on aligning domestic regulations with global standards, in turn encouraging foreign direct investment and international market opportunities for Australian companies.\n\n- **High Cloud Adoption**\n  - Australian industries are increasingly becoming cloud-adopters.\n\nIT spending reached $117B in 2022, with ~30% on software alone.\n\nThis is important since the full impact of GAI will be realised when organisations can integrate GAI across all its processes.\n\nThis requires those processes to be digital and accompanied by strong data foundations in the first place.\n\n- **Investment in Digital Infrastructure**\n  - Australia has recently made significant investments in digital infrastructure.\n\nFor example, the NBN will provide 10 million households and businesses with high-speed internet access by 2025.\n\nThis investment is critical to ensuring the benefits of GAI, which requires reliable internet access, are distributed across Australia, particularly in regional areas.\n\n- **Strong Tech Sector and Existing Ecosystem**\n  - The tech sector contributed $167B to GDP in 2022, growing by 80% since 2016.\n\nAustralia’s tech ecosystem has supported numerous globally successful companies, including Atlassian, Afterpay, Seek, and Canva.\n\nFurthermore, international leaders in cloud services such as Microsoft, Google, and AWS have all made significant, long-term investments in tech infrastructure in Australia.\n\nThe strength and diversity of Australia’s tech sector will drive investment into new GAI ventures and support collaboration.\n\n- **Thriving Start-Up Culture**\n  - Australian start-ups may develop domestic foundational models or harness the power of existing models developed overseas to develop applications for specific industry use cases.\n\nLast year, the tech sector attracted $7.4B with ~20% raised by Enterprise / Business software.\n\nFurthermore, there were 20,000 tech start-ups in 2022, with 21 reaching unicorn status.\n\n- **World-Class Research Institutions**\n  - Australia is home to world-class universities and research institutions conducting cutting-edge research in areas such as artificial intelligence and machine learning, as well as in areas with high potential for GAI transformation such as medical technologies and advanced manufacturing.\n\nThis research expertise provides a foundation for the development of new GAI technologies, as well as for collaboration with international partners.\n\n- **Connection to Asian and US Markets**\n  - Australia's proximity to Asia provides a strategic advantage in accessing growing markets, while our bilateral connection with the United States, strengthened by the Australia, United Kingdom, and United States (AUKUS) partnership, provides a strong platform for global collaboration in developing GAI.\n\n- **Supportive Policy**\n  - The Australian Government has announced its intention to be a global leader in developing and adopting trusted, secure, and responsible AI.\n\nIt has already committed up to $41m in funding for the responsible deployment of AI in the economy.\n\nNotes:\n1.\n\nTechnology Council of Australia, Media Release; Australia Set To Deliver 1.2 Million Critical Tech Workers By 2030 To Drive Productivity Across The Australian Economy (2023).\n\n2.\n\nIDG Connect Machine and Deep Learning sector in Australia (2022).\n\n3.\n\nInnovation Aus.\n\nGovernment’s tech advisory council finalising AI plan (2023).\n\n4.\n\nAustralian Trade and Investment Commission (Austrade), Why Australia: Digital Technology (2023) \n5.\n\nThe Minister for Communications, media release: Albanese Government delivers major NBN boost in 2022-2023 Federal Budget.\n\n(2022) \n6.\n\nDepartment of Industry Science and Resources, Australia's Artificial Intelligence Action Plan, (2022).\n\n7.\n\nDepartment of Industry, Science and Resources.",
    "(2022) \n6.\n\nDepartment of Industry Science and Resources, Australia's Artificial Intelligence Action Plan, (2022).\n\n7.\n\nDepartment of Industry, Science and Resources.\n\nInvestments to grow Australia’s critical technologies industries (2023).",
    "### Technology Capability\n\n- **Narrowing the Margin for Error:** While the latest GAI models narrow the margin for error significantly, human verification of outputs will continue to be needed, especially in critical use cases.\n\nThis margin for error makes leaders in critical industries such as healthcare or finance more cautious about adoption where incorrect or misleading information could have significant consequences.\n\n- **Scale of Investment Required:** Businesses often require customized solutions.\n\nIn order to adapt GAI models, businesses might need to invest additional resources in building AI orchestrations that align with industry-specific needs.\n\nThese challenges may be resolved naturally as the technology develops.\n\nYet businesses and governments still face three further barriers.",
    "### Enterprise Readiness\n\n- **Deciding to Invest:** Adopting GAI requires integrating the technology with existing systems, a robust data strategy, and expertise in change management.\n\n- **Launching Internal AI Governance:** To leverage GAI for specific use cases, businesses might need to connect sensitive data to language models, so that the model can perform the desired functions and draw insights from the data.\n\nThis requires businesses to implement strong safeguards to protect sensitive information.\n\n- **Speed of Change:** The rapidly evolving nature of GAI technology makes it hard to determine the right time to invest, the specific areas where technology can provide value, and whether solutions can be built in-house or require external partnerships.\n\nThis makes it difficult to develop the business case for adoption.",
    "### Awareness and Skills\n\n- **Building Essential C-Suite Knowledge:** Due to the rapid advances in GAI, it is difficult for executives to know which use cases are current opportunities, and which are already outdated.\n\nThis lack of awareness not only stifles adoption but presents a risk if leaders invest without understanding best practices for responsible AI.\n\n- **Building Workforce Digital Literacy:** Australia has a significant digital skills gap, with 3 in 5 surveyed businesses reporting their workforce lacked or had outdated digital skills.\n\nThis is a barrier to both adoption and production of GAI tools.\n\nSpecifically, workers currently lack an understanding of the strengths and limitations of different GAI models, potentially leading to misuse.\n\n- **Managing Training Pathways:** If low-level tasks become automated, junior employees have fewer chances to learn the ‘tools of the trade’.\n\nEmployers need to rethink how junior staff will be supported in their career progression.",
    "### Responsible AI\n\n- **Developing Trust:** Research suggests only one-third of Australians trust AI, and less than half believe the benefits outweigh the risks.\n\nUsers want to know how AI-powered decisions are made, how data is being used, and who to hold accountable in the case of errors.\n\n- **Managing Privacy and Data Security:** Concerns about where data is stored and how it is shared when inputted into foundational models may deter adoption, particularly in industries that work with sensitive data (i.e., healthcare or legal services).\n\n- **Regulatory Certainty:** Timing of the development of regulatory guidance makes business investment decisions regarding GAI difficult, particularly if Australian regulation becomes out of step with global standards.\n\n- **Managing Intellectual Property:** It is important to preserve the public’s access and ability to derive knowledge and understanding from copyrighted works.\n\nAny application of copyright or other IP law should respect and preserve that important principle, and not unnecessarily inhibit the use of tools to achieve this.\n\nNotes:\n1.\n\nHuman Technology Institute (2023), The State of AI Governance in Australia \n2.\n\nRMIT (2023), Digital skills gap costing Australian businesses $9 million per day.\n\n3.\n\nSteven Lockey, Nicole Gillespie, and Caitlin Curtis Trust in Artificial Intelligence: Australian Insights (The University of Queensland).",
    "# Section 3\n\nTo address these key challenges, adopters of Generative AI across industry should focus on six key priorities\n\nPriority actions to accelerate adoption of Generative AI\n\nSource: 1.\n\nHuman Technology Institute (2023), The State of AI Governance in Australia.\n\n2.\n\nBell, G., Burgess, J., Thomas, J., and Sadiq, S. (2023, March 24).\n\nRapid Response Information Report: Generative AI - language models (LLMs) and multimodal foundation models (MFMs).\n\nAustralian Council of Learned Academies.\n\n3.\n\nConsultations and roundtable discussion\n\nCopyright © 2023 Technology Council of Australia.\n\nAll rights reserved.",
    "# Section 3\n\nTo support adoption, regulators and policymakers can promote responsible Generative AI through six priority actions\n\nActions for regulators and policymakers to support Generative AI adoption\n\nSources: 1.\n\nMicrosoft, Governing AI: A Blueprint for the Future, (2023).\n\n2.\n\nConsultations and roundtable discussion\n\nCopyright © 2023 Technology Council of Australia.\n\nAll rights reserved.",
    "### References\n\nABS (2022), Estimates of Multifactor Productivity, (Link)\n\nABS (2022), Occupations Profiles Data, (Link)\n\nABS (2023), Labour Force, Australia (Link)\n\nABS (2023) Australian Industry 2021-2022, (Link)\n\nAlphabeta (2016), The Automation Advantage.\n\n(Link)\n\nAtkinson, R (2018), ICT Innovation, Productivity, and Labor Market Adjustment Policy; (Link)\n\nAustralian Trade and Investment Commission (Austrade), (2023) Why Australia: Digital Technology (Link)\n\nBell, G., Burgess, J., Thomas, J., and Sadiq, S. (2023, March 24).\n\nRapid Response Information Report: Generative AI - language models (LLMs) and multimodal foundation models (MFMs).\n\n(Link)\n\nBessen, J (2016), How computer automation affects occupations: technology, jobs and skills.\n\n(Link)\n\nBogaards, R (2019) Australia’s Productivity Slowdown.\n\n(Link)\n\nBriggs J and Kodani D (2023).\n\nThe Potentially Large Effects of Artificial Intelligence on Economic Growth.\n\n(Goldman Sachs).\n\n(Link)\n\nBrookings Institute (2023) Senate hearing highlights AI harms and need for tougher regulation (Link)\n\nBrynjolfsson et al (2023) Generative AI at work.\n\nWorking Paper.\n\n(Link)\n\nBrynjolfsson, E. Baily, M. Korinek, A (2023) Machines of mind: the case for an AI-powered productivity boom.\n\n(Link)\n\nCB Insights (2023) 7 applications of generative AI in healthcare.\n\n(Link)\n\nCommonwealth Bank of Australia (2022), CommBank Manufacturing Insights Report.\n\n(Link)\n\nCSIRO (2023).\n\nNational AI Centre’s Responsible AI Network.\n\n(Link).\n\nDeloitte (2016), Technology and People: The Great Job Creating Machine, (Link)\n\nDepartment for Science, Innovation and Technology (2023) Press Release.\n\n(Link).\n\nDepartment of Industry Science and Resources, (2021) Australia's Artificial Intelligence Action Plan, (Link)\n\nDepartment of Industry, Science and Resources (DISR) (2019) Australia’s Artificial Intelligence Ethics Framework.\n\n(Link).\n\nDepartment of Industry, Science and Resources.\n\n(2023) Investments to grow Australia’s critical technologies industries, (Link)\n\nDepartment of Social Services, (VIC) (2019) NDIS Pathway Timelines (Link)\n\nDigital Transformation Agency (2023) AGA Guidance of AI.\n\n(Link)\n\nDISR (2023).\n\nSupporting responsible AI: discussion paper.\n\n(Link).\n\nEloundou, T. Manning, S. Mishkin, P. Rock, D. (2023) GPTs are GPTs: An Early Look at the Labour Market Impact Potential of Large Language Models.\n\n(Link)\n\nEU Parliament (2023) AI Act: a step closer to the first rules on Artificial Intelligence.\n\n(Link)\n\nG7 Hiroshima Leaders’ Communique (May 2023).\n\n(Link)\n\nGoldman Sachs (2023), Generative AI could raise global GDP by 7%, (Link)\n\nHuman Technology Institute (2023), The State of AI Governance in Australia.\n\n(Link)\n\nInnovation Australia (2023).\n\nGovernment’s tech advisory council finalising AI plan (Link)\n\nJones DT, Thornton JM.\n\n(2022) The impact of AlphaFold2 one year on.\n\nNat Methods.\n\n2022 Jan;19(1):15-20.\n\n(Link)\n\nKalliamvakou, E (2022).\n\nResearch: quantifying Github Copilot’s impact on developer productivity and happiness.\n\n(Link)\n\nKyruus (2022) Patient Access Journey Report.\n\n(Link)\n\nLynch, S 2023 State of AI in 14 charts (Stanford University).\n\n(Link).\n\nMicrosoft, (2023) Governing AI: A Blueprint for the Future, (Link)\n\nNational Skills Commission (2022), Australian Jobs 2021, (Link)\n\nNoy, S and Zhang, W (2023) Experimental Evidence on the productivity effects of Generative Artificial Intelligence.\n\nWorking Paper.\n\nMIT.\n\n(Link)\n\nOpenAI (2023) Pricing (Link)\n\nRMIT (2023), Digital skills gap costing Australian businesses $9 million per day.\n\n(Link)\n\nRogers, E (1962), Diffusion of Innovations\n\nShutterstock (2022) What do creators think about Generative AI.\n\n(Link)\n\nStanford University (2023) Translation: Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment) – April 2023.\n\n(Link).\n\nStaveris-Polykalas (2023), Harnessing the Power of Generative AI: A game-changer for Government Services.\n\n(Link)\n\nSteven Lockey, Nicole Gillespie, and Catilin Curtis (2022) Trust in Artificial Intelligence: Australian Insights.\n\n(Link)\n\nTechnology Council of Australia, (2022) Turning Australia into a regional tech hub, (Link)\n\nTechnology Council of Australia, (2023) Media Release; Australia Set To Deliver 1.2 Million Critical Tech Workers By 2030 To Drive Productivity Across The Australian Economy (Link)\n\nVaswani, A et al.\n\n(2017).\n\nAttention Is All You Need.\n\n(Link).\n\nWorld Bank Data (2021), Individuals using the internet – Australia, (Link)\n\nCopyright © 2023 Technology Council of Australia.\n\nAll rights reserved.",
    "#### 1A: Estimating the number of working hours impacted by GAI\n\nMethodology for estimating the number of hours impacted by GAI\n\nFor every occupation, we estimate the number of working hours that can be automated or augmented by Generative AI.\n\nThe O*NET database provides an overview of all the tasks performed by workers in every occupation in the US economy, and how much time is spent on each task.1 Mapping the results to the Australian workforce, we can attain the percentage of task-hours exposed to automation and augmentation by GAI for each occupation.\n\nWe then multiply this percentage by the median hours worked in a year in 2022 in each occupation2 and projected number of people employed in the occupation by 2030.3 This gives the total number of working hours per occupation that can potentially be automated and augmented by Generative AI in 2030.\n\nThis method is represented by the following equations:\n\nEstimating 𝒙𝒊 and 𝒚𝒊 is done in four steps\n\nWhere:\n\n𝑛 Total hours 𝐚𝐮𝐭𝐨𝐦𝐚𝐭𝐞𝐝 by GAI = ෍ 𝑥𝑖(𝑚𝑖𝑝𝑖) 𝑖=1\n\n𝑛 Total hours 𝐚𝐮𝐠𝐦𝐞𝐧𝐭𝐞𝐝 by GAI = ෍ 𝑦𝑖(𝑚𝑖𝑝𝑖) 𝑖=1\n\np is the projected number of people n is the number of unique occupations in the economy i is the given occupation x is percent of task-hours exposed to automation by GAI m is the median hours worked in the occupation in 2022 employed in the occupation by 20303 y is the percent of task-hours exposed to augmentation by GAI\n\nTherefore, the key component of this method is estimating 𝑥𝑖 and 𝑦𝑖.\n\nAll other variables can be found using ABS data.\n\nNotes.\n\n1.\n\nOccupational Information Network (O*NET) 2.\n\nMedian hours worked per occupation is sourced from ABS Occupation Profiles data, and scaled to match the total number of hours figure reported in ABS Labour Force data.\n\n3.\n\nNumber of people employed in each occupation in 2022 is sourced from ABS Occupations Profiles data, and scaled to match the total number of people employed figure reported in ABS Labour Force data.\n\nProjections to 2030 are extrapolations of ABS projections for each occupation to 2026, found in Occupations Profiles data, 4.\n\nWhile gpt-4 has multimodal capabilities, namely accepting image inputs, this capability wasn’t publicly available at the time of writing.\n\nCopyright © Tech Council Australia.\n\nAll rights reserved.\n\n32",
    "##### 1B: Estimating the extent to which automated hours are transitioned to other tasks\n\nTask-hours automated by GAI are likely to be successfully transitioned to other tasks.\n\nHowever, we account for the fact that some automated task-hours might not be successfully transitioned\n\nTo estimate GAI’s impact on productivity, we need to calculate the number of automated task-hours that might be successfully transitioned to other tasks.\n\nTask-hours automated by GAI are likely to be successfully transitioned to other work due to two key reasons: In our modeling, we account for the possibility that some task-hours automated by GAI might not be successfully transitioned to other tasks.\n\nTo be conservative, these automated task-hours unlikely to be transitioned are excluded from the final productivity gains calculation for GAI.\n\nHistorically, automation and high labour productivity have been correlated with low levels of unemployment1\n\nWhile automation may reduce demand for some roles, these replaced roles can be transitioned to either new jobs created within the industry or in other sectors.\n\nThe logic here is automation leads to cost savings for businesses, which may be re-invested into higher-value activities.\n\nAlternatively, the savings may result in either lower prices or higher wages.\n\nIn either case, the surplus generated by automation spurs economic growth, in turn creating new jobs in the economy.2,3\n\nGenerative AI is mainly likely to replace specific tasks within jobs, rather than entire roles.\n\nGAI replaces tasks, not whole roles.\n\nFor the vast majority of occupations, the proportion of task-hours replaced by GAI is relatively modest.\n\nEven at full adoption of GAI – which is a far higher level of adoption than our modeled fast-adoption scenario – 85% of workers will have less than one third of their task-hours exposed to automation.4 Our scenarios to 2030 suggest that GAI automation would affect about one hour per week on average for the typical worker.\n\nEffective use of GAI tools for complex tasks, requires human expertise to prompt models and interpret results.\n\nThis suggests demand for skilled workers will likely continue, even as adoption increases.\n\nModelled relationship b/w exposure to automation and likelihood of successfully transitioning automated task-hours\n\nWe assume the curve is logarithmic in nature, i.e.\n\nwhen an occupation is less exposed to GAI (more hours not automated), the likelihood of successfully transitioning automated hours to other tasks would decrease at a lower rate as its exposure level increases (traversing the curve from right-to-left).\n\nConversely, this rate of change would be higher for occupations more exposed to GAI (fewer hours not automated).\n\nBased on this modelled relationship and occupation level data, on average across the economy 93% of task-hours would be successfully transitioned (i.e.\n\n7% of task-hours would not be transitioned to other work), which aligns with other research.6 Importantly, this is finding is only a conservative modelling assumption which limits the size of the opportunity described – it is not a forecast of employment.\n\nIn reality, it is highly likely that the labour market would respond dynamically and that rising incomes stemming from productivity growth would create more demand for labour.\n\nNotes: 1.\n\nAtkinson, ICT Innovation, Productivity, and Labor Market Adjustment Policy, (2018); 2.\n\nDeloitte, Technology and People: The Great Job Creating Machine, (2016); 3.\n\nJames Bessen, How Computer Automation Affects Occupations: Technology, Jobs, and Skills; 4.\n\nAnalysis of O*NET.\n\nAssumes technological capability of Generative AI remains at similar levels as current state of the art models; 5.\n\nIn reality, transitioning automated hours is likely to depend on a range of factors, including the skill level of the worker, their ability to learn etc.\n\nOver time, as the dynamic effects of GAI can be better observed, such factors could be better modeled.\n\nHowever, in the presence of uncertainty, we opt to make these conservative and simplifying assumptions.\n\n6.\n\nBriggs J and Kodani D (2023).\n\nThe Potentially Large Effects of Artificial Intelligence on Economic Growth.\n\n(Goldman Sachs).",
    "##### 1C: Estimating the productivity gains from GAI\n\nValuing the uplift in productivity for tasks which are successfully transitioned\n\n% increase in output per hour worked =\n\nTo calculate the productivity gains of GAI, we multiply the GVA per worker (step B) by the percentage increase in productivity per worker (step A).\n\nTo get the total gains across the economy, we multiply this by the projected number of people employed in the occupation, and sum across all occupations.\n\nThis is given by the following equation.\n\nWhere:\n\n𝑛 Total productivity gains of GAI = ෍ 𝑎𝑖𝑏𝑖𝑝𝑖 𝑖=1\n\n𝑎 is the % increase in output\n\nn is the number of unique occupations in the economy i is the given occupation 𝑏 is value of output per worker 𝑝 is the projected number of workers in the occupation in 20302\n\nNotes.\n\n1.\n\nMedian wages for each occupation are sourced from ABS Occupations Profiles data and scaled to match the total value of wages reported in ABS Australian Industry data.\n\n2.\n\nNumber of people employed in each occupation in 2022 is sourced from ABS Occupations Profiles data, and scaled to match the total number of people employed figure reported in ABS Labour Force data.\n\nProjections to 2030 are extrapolations of ABS projections for each occupation to 2026, found in Occupations Profiles data, 4.\n\nWhile gpt-4 has multimodal capabilities, namely accepting image inputs, this capability wasn’t publicly available at the time of writing.\n\nCopyright © 2023 Technology Council of Australia.\n\nAll rights reserved.\n\n34",
    "###### 1C: Estimating the quality gains from GAI\n\nMethodology for quantifying the quality gains of GAI\n\nNotes.\n\n1.\n\nMedian wages for each occupation are sourced from ABS Occupations Profiles data and scaled to match the total value of wages reported in ABS Australian Industry data.\n\n2.\n\nAccenture Economic Insights (formerly AlphaBeta), The Automation Advantage (2016) 3.\n\nAustralian Industry, 2020-2021, ABS Copyright © 2023 Technology Council of Australia.\n\nAll rights reserved.\n\n35",
    "###### 2: Estimating the value of new products and services\n\nMethodology for estimating the value of new GAI-powered products and services\n\nGlobal GAI software market\n\nAustralia’s share of global market\n\nValue created by new products and services\n\nNotes: 1.\n\nEstimates are anchored around figures reported in Briggs J and Kodani D (2023).\n\nThe Potentially Large Effects of Artificial Intelligence on Economic Growth.\n\n(Goldman Sachs).\n\n2.\n\nTechnology Council of Australia, Turning Australia into a New Generative AI products and services will continue to be developed and brought to market.\n\nAustralia’s strength is likely not to be in producing foundational models, owing to large computing and investment requirements.\n\nRather, Australia is likely to be a global leader in applications built on top of such models.\n\nThe value created by new products and services will be driven by new jobs and business growth.\n\nregional tech hub (2022) 3.\n\nWe use the 2.3% figure for our medium-paced adoption scenario, and scale up and down accordingly for our fast and slow-paced adoption scenarios.\n\n4.ABS, Australian Industry 2020-2021, Copyright © 2023 Technology Council of Australia.\n\nAll rights reserved.\n\n36",
    "###### 3: Converting economic potential to real impact requires estimating the rates of Generative AI adoption by 2030\n\nMethodology for estimating adoption rates of Generative AI by 2030\n\nNotes.\n\n1.\n\nDiffusion of Innovations, Everett M. Rogers; 2.\n\nWorld Bank, Individuals using the Internet (% of population) – Australia.\n\nCopyright © 2023 Technology Council of Australia.\n\nAll rights reserved.\n\n37",
    "##### Methodology: Calculating the economic value of Generative AI\n\nTo test our modeling approach, we compared it against a different approach growth accounting methodology.\n\nGrowth accounting methodology for calculating the productivity gains of GAI\n\nBrynjolfsson et al (2023) estimate productivity gains of GAI using a growth-accounting method.\n\nAdapting this approach returned results between our medium and fast-paced adoption scenarios.\n\nGrowth accounting methodology\n\nBrynjolfsson, Baily and Korinek outline a growth-accounting methodology for calculating the impact of GAI.\n\nIn Machines of mind: The case for an AI-powered productivity boom (2023), the authors outline two channels through which GAI may increase productivity in the economy.\n\nFirst, GAI will increase the level of output per unit of labour input produced in the economy (i.e.\n\nincrease the efficiency of output production by workers).\n\nSecond, GAI will lead to flow-on innovations, such as new products and services, that will further increase the efficiency of output production over time.\n\nThe authors conceptualise these flow-on effects as an increase in the growth rate of productivity.\n\nComparison to our results\n\nComparison of the two methodologies: the total impact of GAI on the Australian economy, $ billion, annual value added in 2030, medium adoption scenario\n\nGrowth accounting method $85B\n\n$55B\n\nOur Methodology $5B\n\n$75B\n\nIn our modeling, we also capture both these channels.\n\nThe first channel is captured as ‘productivity gains’.\n\nThe second channel, the flow-on innovation, is captured in our ‘quality gains’ and ‘new products and services’ buckets.\n\nBrynjolfsson et al.\n\nchoose to frame both channels as ‘productivity gains’, since both channels are increasing output per unit of input.\n\nWhen we adapt this method for Australian data, it returns results comparable to our study, broadly between the medium-paced and fast-paced scenarios for growth.\n\nChannel 1: Productivity\n\nChannel 2: Flow-on innovation\n\nOverall\n\nProductivity gains\n\nQuality Gains New products and services\n\nOverall\n\nComparable to Channel 1 Comparable to Channel 2\n\nCopyright © 2023 Technology Council of Australia.\n\nAll rights reserved.\n\n38",
    "# Disclaimer\n\nThis document is intended for general informational purposes only.\n\nThe report is a collaboration between Microsoft and the Tech Council of Australia.\n\nViews and opinions expressed in this document are based on the companies’ knowledge and understanding of its area of business, markets, and technology.\n\nThe companies do not provide medical, legal, regulatory, audit, or tax advice, and this document does not constitute advice of any nature.\n\nWhile the information in this document has been prepared in good faith, the companies disclaim, to the fullest extent permitted by applicable law, any and all liability for the accuracy and completeness of the information in this document and for any acts or omissions made based on such information.\n\nOpinions expressed herein are subject to change without notice.\n\nNo part of this document may be reproduced in any manner without the written permission of the companies.\n\nThis document may make references to third-party names, trademarks, or copyrights that may be owned by others.\n\nAny third-party names, trademarks, or copyrights contained in this document are the property of their respective owners.",
    "## Introduction\n\nGenerative AI is a revolutionary technology with vast implications for how people live, work, and communicate around the world.\n\nThere is little doubt that it will be highly disruptive to markets, businesses, and the public.\n\nAs noted in President Biden’s Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, “Harnessing AI for good and realizing its myriad benefits requires mitigating its substantial risks...,” demanding “a society-wide effort that includes government, the private sector, academia, and civil society.” The FTC has a role to play in that broader effort.\n\nThe FTC is an independent federal agency charged with promoting fair competition and protecting consumers, workers, and businesses from unfair or deceptive trade practices.\n\nThe development and deployment of AI-based technologies may be proceeding swiftly, but the FTC has decades of experience monitoring and adapting to novel technologies in new and rapidly changing markets.\n\nGiven the FTC’s interest in the AI space and recent emphasis on soliciting input from individuals who are typically underrepresented in policymaking discussions, FTC staff recently held a public event for creative professionals entitled, “Creative Economy and Generative AI.” FTC staff releases this report to summarize the information provided to the FTC in that roundtable.\n\nSome of the issues surfaced at the event implicate the FTC’s enforcement and policy authority, though some certainly go beyond or outside the FTC’s jurisdiction.\n\nThis report begins by briefly summarizing the technological developments in AI and generative AI that make this roundtable and report timely.\n\nNext, it explains the FTC’s jurisdictional interest in AI.\n\nFinally, it summarizes and memorializes the information provided by roundtable participants.\n\nThis staff report is intended as a useful resource for the legal, policy, and academic communities who are considering the implications of generative AI.",
    "## Generative AI and Recent Technological Developments\n\nThe past year has seen the emergence of tools powered by artificial intelligence that can generate outputs like text, images, and audio on command.\n\nThese tools are commonly referred to as \"generative AI.\"\n\nTo output different kinds of content, these models must be built using vast amounts of existing work.\n\nFor example, large language models such as PaLM 2 and Llama 2 rely on large datasets of text that have been \"tokenized\"– divided into smaller chunks of words or even parts of words – which are then analyzed for patterns that can be reproduced.\n\nImage generators like Stable Diffusion are reliant on images, paired with their captions, to fuel their models.\n\nThe ways these tools are built and the content the tools output have garnered attention and concern, particularly from those whose work is being used and potentially replaced.",
    "## FTC’s Interest and Role in AI\n\nThe FTC’s economy-wide mission has, over its century-long history, adapted to the development and deployment of new technologies, many of which pose novel and important challenges to the consumers, workers, and honest businesses who depend on markets being free and fair.\n\nAI is the latest of such challenges.\n\nThe Commission’s enforcement authority derives primarily from Section 5 of the FTC Act, which prohibits unfair or deceptive acts or practices and unfair methods of competition.\n\nThe Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence encourages the FTC to consider whether to exercise its existing authorities, as appropriate, to ensure competition in the AI marketplace and to protect the public from harms that may be enabled by AI.\n\nFrom an enforcement perspective, the FTC has been using its existing legal authorities to take action against illegal practices involving AI.\n\nFor instance, the FTC alleged that Amazon and Ring used highly private data—voice recordings collected by Amazon’s Alexa voice assistant and videos collected by Ring’s internet-connected home security cameras—to train their algorithms while violating customers’ privacy.\n\nThe Alexa matter, in particular, underscored that the Children’s Online Privacy Protection Act Rule’s prohibition on the indefinite retention of children’s data and similar legal rules are not superseded by claims from businesses that data must be indefinitely retained to improve machine learning algorithms.\n\nIn recent months, the FTC secured a temporary restraining order against a business-opportunity seller that claimed to use AI to make clients profitable and successful.\n\nThe FTC has also made clear that a business that relies on algorithmic decision-making must ensure that the algorithm is not resulting in unlawful bias.\n\nFurthermore, the FTC charged WealthPress with using deceptive claims to sell consumers investment-advising services—often claiming that the services’ recommendations were based on an algorithm created by a purported expert.\n\nThe rapid development and deployment of AI also poses potential risks to competition.\n\nThe rising importance of AI to the economy may further lock in the market dominance of large incumbent technology firms.\n\nThese powerful, vertically integrated incumbents control many of the inputs necessary for the effective development and deployment of AI tools, including cloud-based or local computing power and access to large stores of training data.\n\nThese dominant technology companies may have the incentive to use their control over these inputs to unlawfully entrench their market positions in AI and related markets, including digital content markets.\n\nIn addition, AI tools can be used to facilitate collusive behavior that unfairly inflates prices, precisely target price discrimination, or otherwise manipulate outputs.\n\nThe FTC is empowered under Section 5 of the FTC Act to protect the public against unfair methods of competition, including when powerful firms unfairly use AI technologies in a manner that tends to harm competitive conditions.\n\nWith respect to the creative industries, the development and use of AI technology raises a host of potential competition and consumer protection issues.\n\nAlthough many people partake in different forms of creative expression as hobbyists or amateurs, millions of Americans pursue creative work as a profession and many of these artists are self-employed.\n\nResearch has explored generative AI’s early economic impacts on professional illustrators, but artists in other creative fields, such as acting or screenwriting, have also expressed concerns over the ways that generative AI might affect their livelihoods.\n\nUncompensated and unauthorized appropriation of creators’ content may also diminish incentives to invest and produce content, affecting quality over the long term.\n\nVarious competition and consumer protection concerns may arise when AI is deployed in the creative professions.\n\nConduct–such as training an AI tool on protected expression without the creator’s consent or selling output generated from such an AI tool, including by mimicking the creator’s writing style, vocal or instrumental performance, or likeness—may constitute an unfair method of competition or an unfair or deceptive practice.\n\nThat is especially true when the conduct deceives consumers, exploits a creator’s reputation or diminishes the value of her existing or future works, reveals private information, or otherwise causes substantial injury to consumers.\n\nIn addition, conduct that may be consistent with other bodies of law nevertheless may violate Section 5.",
    "## Event Summary\n\nIn October 2023, the FTC held a virtual roundtable discussion to better understand the concerns about generative AI and specifically its impact on creative fields.\n\nChair Khan, Commissioner Slaughter, and Commissioner Bedoya provided remarks.\n\nThe moderated discussion, which was public, consisted of twelve participants who represented a wide variety of creative professions, including visual artists, screenwriters, actors, programmers, editors, musicians, and models.\n\nEach participant gave brief remarks about the changes their field was experiencing with the advent of generative AI, and how they were responding to those changes.\n\nThese remarks were followed by a brief Q&A.\n\nA recording of the event along with a transcript are available on the FTC event web page.\n\nDuring the event, participants acknowledged the potential benefits of generative AI tools, and many had a long history of incorporating new technologies in their practices.\n\nParticipants also described concerns about the ways generative AI could be an avenue for their own exploitation.\n\nThough participants came from different fields, a few consistent themes emerged:\n- Concerns about how their work was being collected and used to train generative AI models;\n- The impact that generative AI outputs are already having on their industry and livelihoods;\n- Issues associated with solutions being proposed by AI companies to address creators’ concerns; and\n- Alternative approaches that creators are pursuing to protect themselves and their industry, including by enshrining their right to choose whether they want to use AI in their work through union contracts.\n\nThe next section of this report expands on each of the four themes.",
    "### How did participants say data is being obtained or collected?\n\nParticipants said that their work was being used to train and finetune generative AI models without their consent.\n\nThroughout the event, participants touched on different ways their work was being collected, either because it was publicly posted online by themselves or others, or because expansive interpretations of prior contractual agreements led others to make their art available to train AI.\n\nIn addition, artists often produce work for hire and do not own the copyright on those creative works, further limiting their ability to control how their work is used.\n\nParticipants said the nature of their work often leaves them without legal protection, and that the lack of transparency around data collection practices made it difficult for them to know when their works were being taken.\n\nParticipants said that human-created work, including their own, was necessary for generative AI tools to function.\n\nAs Duncan Crabtree-Ireland, the National Executive Director and Chief Negotiator for SAG-AFTRA, said during the event, “No AI algorithm is able to make something out of nothing.”\n\nThese claims are in line with research and reporting on generative AI.\n\nMany factors impact an AI model’s performance, but one key factor is the quality of the data used to train it.\n\nRecent research has found that not only is it necessary to carefully curate the data sets used to train a generative AI model, but removing low-quality content and even up-sampling higher-quality sources can result in performance improvements.\n\nThe AI research community generally agrees that it is critical that the content used should be diverse and that generally means it must be created by an actual person.\n\nGenerative AI models, said Douglas Preston, an author and participant of the event, “would be lame and useless without our books.\n\nJust imagine what it would be like if it was only trained on text scraped from web blogs, opinion screeds, cat stories, pornography and the like.”\n\nIn addition to the quality of the data used to train AI models, research suggests another key factor is the quantity.\n\nPopular large language models, for instance, were built using billions, even trillions, of tokens, which in turn necessitates similarly massive amounts of content.\n\nReporting suggests that this content mostly comes from scraping from the Internet.\n\nParticipants said that their work was being taken from the Internet and is being used to train or finetune generative AI models without their awareness or consent.\n\nJohn August, a screenwriter and member of the Writers Guild of America West, said that large language models “have scraped massive volumes of data, including our words and our unique perspectives.”\n\nFor many creative professionals, publicly posting to the Internet is a necessary part of the job.\n\nSteven Zapata, a concept artist and illustrator speaking on behalf of the Concept Art Association, said that, \"to advertise our work, most of us put our art online, on social media and our personal websites.\n\nThis leaves it exposed to unethical scraping practices.\"\n\nThese “unethical scraping practices” have been questioned within academia, and AI researchers have clearly stated that using training data that has been obtained from public sources does not inherently mean that “authorial consent” has been obtained.\n\nIn addition to the scraping of work belonging to creative professionals, Bradley Kuhn, a policy fellow at the Software Freedom Conservancy, pointed out that depending on the platforms they use, creative professionals “may have already agreed for their own creative works to become part of the company's machine learning data sets” because of what is said in those platforms’ terms of service agreements.\n\nSeveral tech companies made the news over the summer after they updated their terms of service to include references to building AI with user data, eliciting backlash from artists in at least one instance.\n\nIn some cases, participants said they weren’t even the ones to post their works online in the first place.\n\nTim Friedlander, president and founder of the National Association of Voice Actors, pointed out that, “it's incredibly easy to use AI to capture the voice of an actor from content available on the internet, and to use that sample to create whole works for sale or non-commercial distribution.” Several participants referenced a dataset called Books3, which The Atlantic reported was built from a library of pirated books.\n\nPreston, for instance, said that the dataset had, “all my books, all 40 of them on it, including many different editions.” Another participant, Clarkesworld magazine founder and editor Neil Clarke, said that anthologies he had edited and published could also be found in the dataset.\n\nParticipants said the nature of most paid creative work means that artists often have little control over how their creations are later used.",
    "Participants said the nature of most paid creative work means that artists often have little control over how their creations are later used.\n\nParticipants explained that many creative professionals do work for hire, meaning the rights to their creative works belong to the client or company that hired them.\n\n“WGA writers do not hold copyright to most of the scripts we write; those are works made for hire, so studios— our employers— hold the copyright,” August said.\n\nThis means that creative professionals don’t always have control over how their works are used in the future.\n\nSteven Zapata, a concept artist and illustrator speaking on behalf of the Concept Art Association, said that with work-for-hire agreements, a visual artist could spend decades of their life working for a studio and that studio “can take all your work, train a model on it, and then have a free and tireless replica of you to use in perpetuity.”\n\nFriedlander said that voice actors were experiencing that now: “Contracts we signed years ago are now being used to justify the inclusion of our audio in synthetic voice models.”\n\nSome creative professionals have even less insight into what they’ve been forced to give up.\n\nSara Ziff of the Model Alliance noted that many fashion models do not actually see the terms that their management companies have agreed to with clients, so many do not actually know what happens to the digital body scans that some clients are starting to require.\n\nThese uncertainties are exacerbated by the fact that AI developers do not publicly disclose what works are included in their training data, according to participants.\n\nPreston said that, “[AI companies] refuse to answer any questions from the Author's Guild about what data sets they're using, where they're getting their books, and how they're being used.\n\nThere's no transparency at all.\n\nIt's an absolute black hole.”\n\nResearchers have found that some AI developers have become less open over time about what can be found in their models’ training data.\n\nWhen asked about how participants found out about the inclusion of their work in the training of these models, Umair Kazi stated that “There is a lack of transparency from AI developers about training data sets, which makes it very difficult to ascertain which works were actually used to train the models and how.”",
    "### What harms did participants say they were currently facing?\n\nParticipants’ concerns were limited not just to how their own work was being used.\n\nThroughout the event, participants discussed a wide range of harms they associated with the outputs of generative AI tools.\n\nThese harms included the ways that generative AI could be used to make it more difficult to find human-made work, mimic creative professionals’ unique styles causing market confusion and reputational damage, and lead to loss of opportunity and income.\n\nParticipants said that generative AI outputs are starting to appear in the venues where creative professionals compete for work, making it more difficult for consumers and potential publishers to find human-made work.\n\nKazi, the Authors Guild director of public policy and outreach, said that his group was already seeing AI being used to generate low-quality eBooks that displace human-authored books in major online book retailers.\n\nIn one instance earlier this year, he said, AI-generated books began dominating the young adult romance bestseller list of a popular online bookseller.\n\nNeil Clarke, the editor and founder of the sci-fi short story magazine Clarkesworld, described being inundated with hundreds of submissions that appeared to be AI-generated, leading him to temporarily close submissions.\n\nClarke said the magazine had a standing “no-AI” policy even prior to the influx of submissions, and said his workload has “easily doubled” as he sorts through a stream of suspicious submissions.\n\nParticipants expressed concerns about generative AI tools being used to mimic their own unique styles, brands, voices, and likenesses, which could allow strangers and former clients to create knockoffs of their work.\n\nKarla Ortiz, a concept artist and illustrator, said that text-to-image generators can let anyone produce work “in the style of” a named artist, and that her own name has been referenced thousands of times by people prompting generative AI systems to produce work that looked like her own.\n\nSimilarly, Kazi of the Authors Guild alleged that generative AI was being used to create unauthorized derivative works based on human-created content.",
    "# AI Generated Knock-offs and Their Impact on Creative Professionals\n\nAI generated knock-offs have found their way onto online book publishing platforms, said John August of the Writers’ Guild of America West, where they are being sold to customers who might confuse them with authors’ actual works.\n\nIn addition to creating and selling their own work, now authors have to fend off AI-fueled fraudsters: “They’re having to fight to get those fakes taken down, and protect their brands,” said August.\n\nParticipants said that the threat of AI fakes can also come from former clients.\n\nFriedlander, the NAVA president, gave the example of a New York voice actor who was let go after working for a company for several years.\n\nAccording to Friedlander, the company told the voice actor that it finally had enough of the actor’s audio, and the company was now planning on creating a synthetic version of their voice.\n\nParticipants said that when consumers mistake AI-generated work made in the likeness or style of a particular artist as the actual work of that artist, it could jeopardize the actual artist’s reputation and ability to earn income.\n\nZapata, the illustrator speaking on behalf of the Concept Art Association, said AI-generated work can end up online with the artist’s name attached, even though they didn’t make the work and have no control over the content or the quality of the piece.\n\n“The negative market implications of a potential client encountering a freely downloadable AI copycat of us when searching our names online could be devastating to individual careers and our industry as a whole,” he said.\n\nParticipants said that they have also seen generative AI tools being used to harass creative professionals and confuse consumers.\n\nFriedlander referenced an incident from February, in which anonymous trolls made synthetic versions of multiple voice actors, and tweeted false audio clips of them divulging their actual home addresses and saying homophobic and racist slurs.\n\nJen Jacobsen, the executive director of the Artist Rights Alliance said that generative AI tools have been used to create false depictions of artists selling products that the artists never endorsed.\n\n“It's not only confusing to fans, but humiliating to the artists themselves and undermines their public image,” said Jacobsen.\n\nParticipants were concerned that creative professionals are already losing work because of generative AI.\n\nFriedlander described a recent incident where a voice actor in Washington state lost out on an audiobook job when the company told them it “decided to take the job in-house.” Friedlander said that around the same time, the company published a press release stating that they would be using a synthetic voice startup for all their audiobook productions.\n\nHe said this move was “effectively replacing all of those human narrators with synthetic voices.”\n\nKazi of the Authors Guild said that freelance journalists and professional writers of web and marketing content were reportedly losing work “at an alarming rate.” He described the plight of an unnamed Guild member, who works as a marketing and web content writer and reportedly lost three-quarters of their work because their clients switched to using AI instead.\n\nZiff, the Model Alliance founder, said that earlier this year a major clothing company announced that they were creating AI-generated models to increase the number and diversity of their models.\n\n“In an industry that has historically been discriminatory, creating digital representations of models of various ages, ethnicities, and body types rather than hiring and paying a diversity of real models is concerning,” she said.\n\nZiff pointed out that the use of AI fashion models would not just impact human models.\n\nShe said that fashion workers of all kinds—including photographers, stylists, and hair and makeup artists—were concerned about their use, because it could impact all their livelihoods.",
    "# How did participants view proposed consent defaults?\n\nSome AI developers have started offering people, including creative professionals, the choice to “opt-out” of their work being used to train future models, through methods such as direct opt-out forms, voluntarily complying with third-party lists, and public commitments to respect the Robots Exclusion Protocol.\n\nParticipants raised multiple concerns about these kinds of opt-out frameworks, ranging from the practical, like not knowing whether their data was used and, thus, whether opt-out is even needed, to more fundamental issues with the approach, like shifting the burden from companies to creators.\n\nParticipants also discussed the need for solutions that would not only limit the harm moving forward but also address the harm that has already occurred.\n\nParticipants said that opt-outs put the burden on creators to police a rapidly changing marketplace, where new companies and AI models are emerging every day.\n\nJacobsen likened these to “a new form of uncompensated labor” that AI developers are requiring people to perform if they want to avoid being exploited.\n\n“Such burden shifting is not only unfair, it is morally wrong and antithetical to basic principles of artistic integrity,” said Jacobsen.\n\nAugust echoed that sentiment, “There are so many companies out there developing and training AI models, to be forced to continually track all of them down to opt-out is an enormous administrative burden on individual artists.”\n\nOrtiz pointed out that these frameworks require a certain level of technical and legal expertise about a fast-moving industry, and that raises questions about whether the choices these frameworks offer are truly accessible to all creators: Does that mean we have to opt out on each and every one of them?\n\nThat's a full-time job.\n\nWhat about if those models update?\n\nWhat about if they don't publicize and they use third parties?\n\nWhat if those models in the opt-out forms are not an artist’s native language?\n\nWhat about artists who never spend time online or don't even know this is happening?\n\nParticipants wanted AI developers to take an opt-in approach instead.\n\nClarke said that opt-out frameworks, in contrast to a framework that seeks explicit consent prior to usage, “is what you get when the fox designs the chicken coop.” Participants also emphasized that because of the lack of transparency about what is being used as training data, it was unclear which work they would need to request to have removed, or even if their work was included at all.\n\nKazi said the fact that writers and editors were able to check whether their books could be found in the Books3 dataset was “an anomaly” and not the norm.\n\nBecause that dataset was publicly available, anyone could inspect and audit it, and build tools to make examining the dataset accessible to the public.\n\nResearchers have found that this is not the case for all datasets used to train AI models, and participants noted that as well.\n\n“There is a lack of transparency from AI developers about training data sets, which makes it very difficult to ascertain which works were actually used to train the models and how,” said Kazi.\n\nOrtiz said that lack of transparency makes it difficult for individuals even to know which works they should request be removed, “Existing opt-out procedures often ask users to list works used to train the model they own, but as we just mentioned, that training data is secret, so it's an impossible task.”\n\nParticipants said that transparency around training data and what works were used to build them was greatly needed.\n\n“Divulging your dataset should be compulsory,” said Zapata.\n\nParticipants took issue with the fact that most opt-out frameworks were about future models, and often did nothing to address past usage.\n\nThroughout the event, many of the participants called for consent and compensation because they said AI models were trained without them.\n\nParticipants' consent was not sought during the development of these AI models, and they did not have an opportunity to negotiate for fair compensation for the works used.\n\nZapata said that mainstream AI developers’ opt-outs only apply to the future developments.\n\nEven if someone chooses to follow a developer’s opt-out instructions, Zapata said, “it’s already too late to get out of the most current model.” This may be, as Zapata noted, due to technical limitations.\n\nDeveloping methods of effective machine “un”-learning is currently a research topic of academic and corporate interest.\n\nMany of participants’ concerns surrounded past actions, which are not fully addressed by giving a choice about future use.\n\nSaid August, “The advent of AI doesn't change fundamental ways that the world is supposed to work, and that world works in permission first.”",
    "# What are participants doing to understand and address generative AI?\n\nThroughout the event, participants described the approaches that they are using to address and understand the ways that generative AI is reshaping their respective lines of work.\n\nParticipants said they have been conducting research to better understand generative AI.\n\nThey have engaged in labor strikes of near historic lengths as they negotiate for protections against AI in their collective bargaining agreements.\n\nThey have backed legislative efforts on both the state and federal level.\n\nThey have joined class-action lawsuits, and they have attempted to engage with AI developers directly.\n\nParticipants described research efforts they were undertaking to understand the emerging generative AI landscape.\n\nClarke said that after he experienced an influx of what he suspected were AI-generated submissions, he spoke with fraud detection experts and AI researchers to better understand how to combat false submissions to his magazine.\n\nHe spent time evaluating many of the public and commercial AI detection tools, because he was uncertain about their accuracy claims and whether they would be suitable to use.\n\nKazi described the ways the Authors Guild has probed generative AI chatbots to better understand what might have been included in their training data, since the developers do not disclose what they contain.\n\nZiff said the Model Alliance is partnering with the Worker Institute at Cornell University to develop a research study that would better help them understand the impact of generative AI on fashion workers, with a particular focus on workers of color.\n\nUnion representatives who participated said they have been bargaining over AI or are planning to when their current agreements are up.\n\nAugust, the screenwriter and member of the Writers Guild of America West’s Negotiating Committee, said the Guild’s recently ratified agreement could offer helpful guidance when thinking about future public policy on AI, and laid out the new guardrails the Guild has set in its agreement: Our agreement defines that AI is not a writer and the material it generates is not equivalent to human writing for purposes of our contract.\n\nThat means that AI cannot rewrite us, nor can it compete with a human writer for credit and the associated financial benefit of that credit.\n\nFurther, the studios now have to tell us if they are providing us with material generated by AI and then it cannot require us to use AI tools.\n\nAugust emphasized that the Guild was able to win these protections because they are a strong union that successfully carried off a nearly five-month strike—one of the longest in the Guild’s history, according to Entertainment Weekly.\n\nAugust reminded the audience that most writers and artists don’t have unions to protect them.\n\nThat fact was echoed by participants who represented worker advocacy groups like the National Association of Voice Actors and the Model Alliance.\n\nThese kinds of groups can advocate for and support their members, they but cannot collectively bargain agreements with companies.\n\nJohn K Painting of the American Federation of Musicians, another union that was represented during the event, said that in addition to collective bargaining agreements, legislative lobbying was an important mechanism for AFM to win protections for its members.\n\nThis approach was not just limited to the organized labor groups represented at the event.\n\nThroughout the event, participants mentioned specific pieces of legislation they supported and hoped would help creative professionals and add guardrails for how generative AI is built and used.\n\nOn the state level, Ziff of the Model Alliance said her organization supported the Fashion Workers Act in New York, which would establish basic labor protections for models and content creators in the state’s fashion industry.\n\nZiff said the bill would help address the lack of transparency around how models’ body scans are being used.\n\nOn the federal level, NAVA’s Friedlander said the organization recently endorsed the AI Labeling Act of 2023, which would require generative AI developers to include “a clear and conspicuous disclosure identifying AI-generated content and AI chatbots,” according to a press release from U.S.\n\nSenator Brian Schatz (D-Hawai’i), who introduced the bill.\n\nJacobsen of the Artist Rights Alliance also referenced the Protect Working Musicians Act of 2023, which Jacobsen said would give small and independent musicians an antitrust exemption to negotiate collectively with AI developers and streaming platforms.\n\nAdditionally, at least one participant mentioned the idea of a federal right of publicity.\n\nState-level laws have been passed in places like New York and California, but not every state has its own version.",
    "State-level laws have been passed in places like New York and California, but not every state has its own version.\n\nRight of publicity laws generally protect a person’s likeness from being misused for commercial purposes without their consent and could potentially give creative professionals greater control over how things like their voices or personal styles are being used.\n\nSince the event took place, a bipartisan group of senators released a discussion draft of the No Fakes Act of 2023, which would create such federal protections specifically addressing misuse of generative AI.\n\nA few participants said they were engaged in ongoing class-action lawsuits that they hoped would help address some of the harms they said were caused by generative AI developers.\n\nOne such participant was Doug Preston, a fiction author, who said he and his co-plaintiffs were seeking damages for the unauthorized use of their copyrighted work.\n\nPreston said that moving forward, he and his co-plaintiffs wanted AI developers get permission from authors, properly license their books, and compensate them fairly for that use.\n\nAt least one participant mentioned proactively working with an AI developer on a project.\n\nFriedlander, the NAVA president, said that he was working on developing a synthetic version of his voice that would allow him to perform work he would not otherwise be able to do, like read a newspaper cover-to-cover every morning.\n\nFriedlander emphasized that not only was this being done with his consent, control, and compensation, but more generally that generative AI tools should enhance and not replace the work of creative professionals.",
    "# Potential Areas of Further Inquiry\n\nThe event outlined the diverse palette of issues and experiences that creative professionals face across the industry.\n\nFurther research is required to help scholars, law enforcement agencies, and other civil society organizations understand how generative AI can impact communities and how those harms can be mitigated.\n\nParticipants highlighted a multitude of questions and open areas for further research.\n\nOrtiz, for instance, raised several questions during the event about how opt-out frameworks are being effectively communicated to people whose work has been included in training sets, highlighting issues such as barriers of language and technical expertise.\n\nOrtiz’s questions prompt other questions, such as: What other barriers do existing opt-out frameworks present?\n\nHow are the accessibility and usage rates of options presented by these frameworks being measured?\n\nZapata noted another major barrier is a lack of disclosure around what is currently included in training sets.\n\nKazi and Clarke’s discussion of the Books3 dataset also raises questions about what is needed beyond simply publishing a dataset.\n\nBooks3 was always public but not easily interpretable by the public prior to investigative reporting on the topic.\n\nWhat efforts are being made to disclose the contents of training sets?\n\nWhat practices are being developed to ensure that transparency is meaningful and furthers understanding for a public beyond those with technical expertise in machine learning?\n\nAnother open question that Zapata touched on was how AI developers can effectively comply with opt-out requests for existing models.\n\nWhat is the current state of machine “unlearning” research?\n\nWhat is needed to effectively remove the contributions from work no longer desired in a model, and how can that be verified?\n\nWhat measures of efficacy are being developed as this research evolves?\n\nParticipants discussed many stages of model development and deployment where issues may arise, starting from the collection and inclusion of work in training data without the affirmative consent of its creators, to downstream misuse of these models and other unintended effects.\n\nFriedlander, for instance, highlighted a new problem that voice actors are now facing—proving the provenance of their own work to clients.\n\nSaid Friedlander, “an extra burden has now been placed on voice actors to prove that the audio they’re delivering is not AI-generated.” For this example, and others, what is the scope of the de facto burden placed on artists in the aggregate, to what extent is it unremunerated, and is the allocation of such costs justifiable and efficient?\n\nBeyond the individual effect that generative AI has on any given creator, how might the practices of AI developers affect these markets over the long term?\n\nFor example, would the uncompensated and unauthorized use of creators’ content dissuade individuals and firms from investing in high-quality content in the first instance?",
    "# Conclusion\n\nThe creative professionals at the roundtable discussion raised a number of concerns regarding the impact that AI is having on them and on consumers.\n\nAlthough many of the concerns raised at the roundtable lay beyond the scope of the Commission’s jurisdiction, targeted enforcement under the FTC’s existing authority in AI-related markets can help to foster fair competition and protect people in creative industries and beyond from unfair or deceptive practices.\n\nFor decades, the FTC has used its authorities to address deceptive or unfair acts or practices and unfair methods of competition as it relates to new and transformative technologies.\n\nThere is no “AI exemption” from the laws on the books.\n\nThe FTC will continue to vigorously use the full range of its authorities to protect Americans from deceptive and unfair conduct and maintain open, fair, and competitive markets.\n\nThe FTC continues to listen and learn about the latest trends—and recognizes that the topics covered above are by no means exhaustive.\n\nThe FTC will continue to closely monitor and scan the developments of these products, services, and tools.\n\nAs the generative AI industry continues to develop, the FTC will remain vigilant and ready to use the full panoply of its law enforcement and policy tools to foster fair competition, protect consumers, and help ensure that the public benefits from this transformative technology.",
    "## Appendix I: Participant Bios\n\n**John August** is a screenwriter and member of the Negotiating Committee for Writers Guild of America West (WGA), a labor union representing thousands of members who write content for motion pictures, television, news and online media.\n\nHe is a Negotiating Committee Member at WGA West and was a former board member of WGA.\n\nJohn is a screenwriter whose credits include Big Fish, Aladdin, Corpse Bride and Charlie’s Angels.\n\nHe also wrote the Broadway musical Big Fish, and the Arlo Finch novel trilogy.\n\nHe lives in Los Angeles.\n\n**Neil Clarke** is the multi-award-winning editor of Clarkesworld Magazine, a science fiction and fantasy magazine, and over a dozen anthologies, including the Best Science Fiction of the Year series.\n\nAn eleven-time finalist and the 2022 winner of the Hugo Award for Best Editor Short Form, he is also the three-time winner of the Chesley Award for Best Art Director.\n\nIn 2019, Clarke received the SFWA Kate Wilhelm Solstice Award for distinguished contributions to the science fiction and fantasy community.\n\n**Duncan Crabtree-Ireland** is the National Executive Director and Chief Negotiator at SAG-AFTRA, the most distinguished entertainment and media union in the world.\n\nIn this capacity, he oversees the world’s largest and most influential entertainment union, comprised of more than 160,000 members worldwide who work in film, television, broadcast news, commercials, music, video games and more.\n\nCrabtree-Ireland has played a critical role in many of SAG-AFTRA’s signature achievements over the past two decades, both in his current role and prior to that as longtime chief operating officer and general counsel.\n\nCrabtree-Ireland is a strategic and creative lead negotiator, and has personally led or overseen negotiations for SAG-AFTRA’s Netflix Agreement and its Videogames, Commercials, Music and Network Television contracts, among others.\n\nCrabtree-Ireland leads the union’s technology and innovation team.\n\nHe is also the co-host of the SAG-AFTRA Podcast and the SAG-AFTRA Podcast en Español.\n\n**Tim Friedlander** is President and Founder at National Association of Voice Actors (NAVA), a non-profit association created to advocate and promote the advancement of the voice acting industry through action, education, inclusion and benefits.\n\nTim is a Los Angeles based voice actor, studio owner, advocate, and educator.\n\nHe is the voice of the PBR Summer Series (Professional Bull Riders) on CBS and the English voice of Ares in the Netflix show Record of Ragnarok.\n\nAs owner of the soundBOX: Group, a voice over centric studio group with 3 locations, Tim has consistently provided safe space for voice actors to train, learn, and work.\n\nTim is co-founder and President of The National Association of Voice Actors and in 2023, helped found United Voice Artists, an 18 nation, 36 member Federation of global voice over associations.\n\n**Jen Jacobsen** is Executive Director of the Artist Rights Alliance, an artist-led organization advocating for musicians to receive fair treatment and compensation in the digital marketplace.\n\nShe was previously the Executive Director of Kid Pan Alley (KPA), an arts non-profit that teaches songwriting in schools.\n\nA 25-year music industry veteran, Jen served as Vice President, Industry and Government Relations, for Sony Music Entertainment and Vice President, Global Public Policy for Time Warner, Inc. Jen received her B.A.\n\nin History from Yale University, and her J.D.\n\nfrom the University of Virginia School of Law.\n\nShe is a graduate of the Nashville-based Leadership Music program and a lifelong musician.\n\n**Umair Kazi** is the Director of Policy & Advocacy at the Authors Guild, the oldest and largest professional writers’ organization in the United States, where he develops and advances initiatives supporting writers' interests.\n\nHis work encompasses copyright, AI, labor, antitrust, and free expression issues.\n\nUmair previously served as the Guild’s Staff Attorney, counseling members on professional legal matters.\n\nUmair is also a writer and a translator from Urdu.\n\nHe holds a J.D.\n\nfrom the University of Iowa College of Law and an MFA in creative writing from Columbia University.\n\n**Bradley M. Kuhn** is the Policy Fellow at Software Freedom Conservancy, a nonprofit organization centered around ethical technology.\n\nKuhn began work in the software freedom movement as a volunteer in 1992 — as an early adopter of Linux-based systems, and contributor to various Free and Open Source (FOSS) Software projects, including Perl.\n\nKuhn has industry experience as a computer systems administrator, a software developer, and a high school Computer Science teacher.\n\nKuhn has been lauded with multiple awards for his lifelong work in enforcement and compliance of copyleft FOSS licenses (such as the GPL).\n\nKuhn holds an M.S.\n\nand summa-cum-laude B.S.\n\nin Computer Science.\n\n**Karla Ortiz** is a Puerto Rican, internationally recognized, award-winning artist.",
    "Kuhn holds an M.S.\n\nand summa-cum-laude B.S.\n\nin Computer Science.\n\n**Karla Ortiz** is a Puerto Rican, internationally recognized, award-winning artist.\n\nWith her exceptional design sense, realistic renders, and character-driven narratives, Karla has contributed to many high profile film and TV productions, including Jurassic World, World of Warcraft, Rogue One: A Star Wars Story, Thor: Ragnarok, Black Panther, Avengers: Infinity War, The Eternals, Loki, HBO’s The Nevers and most notably, her design of Doctor Strange for Marvel Studios’ Doctor Strange.\n\nKarla’s work is also recognized in the fine art world, showcasing",
    "# Biography Highlights\n\nAn artist known for figurative and mysterious art has exhibited work in galleries like Spoke Art and Hashimoto Contemporary in San Francisco, Nucleus Gallery, Thinkspace and Maxwell Alexander Gallery in LA, and Galerie Arludik in Paris.\n\nShe resides in San Francisco with her cat, Bady.\n\nJohn K. Painting serves as the Director of the Electronic Media Services Division and Assistant to the President for the American Federation of Musicians of the United States and Canada (AFM), the world’s largest musicians' union.\n\nPainting began his career in Electronic Media at AFM Local 802 (New York City) in 2011.\n\nIn May 2019, he was hired by the Federation as EMSD Assistant Director, under former EMSD Director Pat Varriale.\n\nHe was responsible for administering the National Public Television Agreement and various projects, especially contracts related to Internet streaming since COVID-19.\n\nPainting has been a regular participant in EMSD contract negotiations and serves on the AFM’s Education Committee, creating educational content on Electronic Media Services agreements for musicians and employers.\n\nDouglas Preston is the author of 38 books, with 32 New York Times bestsellers.\n\nTwo novels co-written with Lincoln Child were named among the top 100 thrillers ever in a National Public Radio poll.\n\nHis nonfiction book, \"The Monster of Florence,\" is being adapted into a TV series.\n\nPreston was an editor at the American Museum of Natural History and taught nonfiction writing at Princeton University.\n\nHe served as president of the Authors Guild from 2019 to 2023.\n\nSteven Zapata is an American artist with 12 years in commercial design, illustration, and art education.\n\nHis work includes contributions to The Elder Scrolls Online and Disney's \"Noelle,\" as well as theme park designs for the USA Pavilion at World Expo 2020, Warner Bros. World in Abu Dhabi, and the Harry Potter Studio Tour in London.\n\nHe has taught at the Art Center College of Design in Los Angeles and shares drawing tutorials on his YouTube channel.\n\nSara Ziff is founder and executive director of the Model Alliance, a nonprofit for research, policy, and advocacy in the fashion industry.\n\nShe established the fashion industry’s first support line and played a key role in the #MeToo movement.\n\nZiff has championed legislation to advance workers' rights and is working on the Fashion Workers Act in New York State.\n\nShe holds a B.A.\n\nfrom Columbia University and an M.P.A.\n\nfrom the Harvard Kennedy School of Government.",
    "# Appendix II: Roundtable Quote Book\n\nThis quote book compiles direct quotes from participants in the FTC's October 2023 Creative Economy & Generative AI roundtable.\n\nThis summary shares specific perspectives and experiences on the impact of generative AI on industries and is not exhaustive.\n\n“AI and its algorithms must be here to serve us, not the other way around.” - Duncan Crabtree-Ireland\n\n“We publish stories from new voices globally, essential for future discovery in my field.” - Neil Clarke\n\n“The issue is not quality, but the speed and volume of production.” - Neil Clarke\n\n“Pulitzer Prize-winning author Min Jin Lee equates AI's use of her work to identity theft.”\n\n“AI would be ineffective without our books.\n\nImagine if trained only on web blogs, opinions, cat stories, and the like.” - Douglas Preston\n\n“Supporting creativity via theft is like justifying shoplifting candy as supporting a store.” - Douglas Preston\n\n“This is our life’s work.\n\nWe invest our hearts and souls into our books.” - Douglas Preston\n\n“Copyright was vital to the founders of our country, meant as protection to spur creativity.” - Douglas Preston\n\n“Most voice actors aren’t celebrities; they’re working class, many not based in LA or New York.” - Tim Friedlander\n\n“Musical expression is timeless and part of society, but a viable career isn’t guaranteed.” - John Painting\n\n“Consent, explicit consent is needed for inclusion in systems; current standards lack consequence.” - Neil Clarke\n\n“A case of audio scam highlights real consumer danger and urgency.” - Tim Friedlander\n\n“We at SAG-AFTRA support AI and technology use as tools to augment creativity.” - Duncan Crabtree-Ireland\n\n“Recording audio at home was transformative.\n\nAs a voice actor, I am pro-technology.” - Tim Friedlander\n\n“AI, when used ethically, can support and enhance creative careers and opportunities.” - Duncan Crabtree-Ireland\n\n“Transparency is challenging: opt-outs, language differences, or artists unaware of technology use.” - Karla Ortiz\n\n“Models lack insights on AI's use of their likeness, adding to generative AI exploitation concerns.” - Sara Ziff\n\n“Companies exploit blur in contracts; voice actors struggle with protection beyond service terms.” - Tim Friedlander\n\n“Body scans for models are increasing without transparency on the usage or compensation for the 3D models.” - Sara Ziff\n\n“Tactics like data laundering disguise unauthorized use; the impact of extensive reliance on copyrighted material is masked.” - Karla Ortiz\n\n“In one instance, AI-generated books affected Amazon's bestseller list in the young adult romance category.” - Umair Kazi\n\n“Freelance journalists and content writers are rapidly losing work due to AI adoption by clients.” - Umair Kazi\n\n“Voice actors face competition against synthetic voices, with human narrators being replaced.” - Tim Friedlander\n\n“AI models in fashion raise concerns about replacing jobs, impacting models and related roles.” - Sara Ziff\n\n“Using destroyed market economics, AI may dishearten creators by replacing human creativity with cheaper machine-generated content.” - Karla Ortiz\n\n“When data is used without permission for training, creators face threats to economic survival.” - Umair Kazi\n\n“Reckless AI integration is exploiting creators without remuneration; unsanctioned use of copyrighted works looms large.” - Jen Jacobson\n\n“AI's use without transparency can lead to quality concerns and ethical dilemmas surrounding data utilization.” - Neil Clarke\n\n“Compensation absent in using images for AI, exploiting figures without recognition.” - Karla Ortiz\n\n“Failure to acknowledge contribution to AI threatens artistic integrity and economic viability.” - John Painting\n\n“AI-generated music poses a threat, competes with artists, potentially fills playlists, reducing royalty obligations.” - Jen Jacobsen\n\n“Transparency issues deny awareness of the extent of data appropriation.” - Karla Ortiz\n\n“Unsanctioned AI use damages reputations; examples of misappropriation have emerged on platforms.” - Tim Friedlander\n\n“Consumer confusion could ensue without clarification of AI-generated productions' origins.” - John August\n\n“AI-generated knock-offs result in marketplace clutter, confounding consumer perception.” - John August\n\n“Active measures by tech platforms contrived to manufacture consent for AI development surpass ethical constraints.” - Bradley Kuhn\n\n“Digital clones cause competition among smaller actors against substantial tech enterprises.” - Tim Friedlander\n\n“Prominent public figures have had their voices cloned for malicious presence on social platforms.” - Tim Friedlander\n\n“Fan confusion arises with AI-generated derivatives of well-known works, such as unauthorized conclusions to series.” - Umair Kazi\n\n“With generative AI, consideration for creators' integrity and market fairness is critical.” - John August",
    "# Introduction\n\n\"Goods are their image, this is particularly troubling in light of the rise in deepfake technology, specifically deepfake pornography.\"\n\n- Sara Ziff\n\n\"The companies offering these models often encourage users to request work in the styles of particular artists by name, and many of these generations end up online with our names attached to these pieces that we didn't make.\"\n\n- Steven Zapata\n\n\"And perhaps even more disturbingly, AI models are now using artists’ faces, voices, and performances without permission to make digital impersonations that not only create consumer confusion, but also cause serious harm to both fans and artists.\n\nThese deep fakes have depicted a band canceling a concert that wasn't actually canceled.\n\nThey've shown artists selling products that the artists never endorsed.\n\nWe've seen false depictions of musicians badmouthing their own fans.\n\nThis isn't a hypothetical harm.\n\nThis type of consumer deception and fraud are happening right now.\"\n\n- Jen Jacobson\n\n\"The ability to create a synthetic voice from anyone who has recorded audio is easy and simple and dangerous.\n\nCurrently now it only takes three seconds of source audio to create a realistic voice clone.\n\nAnd this synthetic content can be used to deceive consumers into believing that a trusted voice is communicating with them.\n\nThis can lead to relying on false and misleading information and potentially even implicate the human whose voice has been used to harm people.\"\n\n- Tim Friedlander",
    "# Consent, Permission, and/or Opt-out vs. Opt-in\n\n\"If consumers take anything away from my comments today, I hope they remember to carefully read the terms and conditions of all software platforms they use, as they may have already agreed for their own creative works to become part of the company's machine learning data sets.\n\nI admit it may take you a week to read all of those terms, but it's sadly the only way you'll know what rights you've inadvertently given away to Big Tech.\"\n\n- Bradley Kuhn\n\n\"AI developers have copied millions of copyrighted works without permission.\n\nThese works are not only copied many times in the course of compiling training data sets and ingestion, but are embedded in the very fabric of the language models.\"\n\n- Umair Kazi\n\n\"And at one point, I asked it to write a poem in heroic couplets about one of my characters.\n\nAnd I was floored at the level of detail it knew when it generated this poem, and that's when I realized it must've ingested many of my books.\"\n\n- Douglas Preston\n\n\"OpenAI illegally ingested our books to create a product that is currently valued at tens of billions of dollars, and they did this without our consent or compensation.\n\nAnd as Umair mentioned, the average full-time author in America makes only $20,000 a year.\n\nThis is a classic case of Robin Hood in reverse, stealing from the poor to give to the already obscenely rich.\"\n\n- Douglas Preston\n\n\"In their race to be first, AI developers are swallowing everything they can get their hands on without regard to copyright ownership, intellectual property rights, or moral rights.\n\nAnd they're doing this without the slightest consideration given to supporting the livelihood of America's creative class.\"\n\n- Douglas Preston\n\n\"I personally am working on a synthetic voice that I have consent, compensation, and control for.\n\nThere are some things that humans can't physically do, such as narrate the New York Times cover-to-cover every morning, or provide a realistic voice for someone who is nonverbal.\n\nBut this tech should enhance and not replace voice actors.\"\n\n- Tim Friedlander\n\n\"So we need regulation, intervention, and oversight.\n\nWe as creators should have complete control over how our work is used, but we need help.\n\nSome of the potential actions and remedies that we hope to see include, first and foremost, ensuring that all commercial AI models utilize only public domain content or legally licensed datasets acquired in an opt-in capacity.\n\nOpt-out is completely insufficient here.\n\nThis could mean current companies shifting to the public domain and possibly destroying their current models in the process so that opt-in becomes the standard.\"\n\n- Steven Zapata\n\n\"Mainstream models like DALL-E 3 don't reveal their training data and don't let you search it, but they do offer an inefficient one by one opt-out system that you can use if you think maybe your art is in there.\"\n\n- Steven Zapata\n\n\"What's new today, though, are the expansive AI models that ingest massive amounts of musical works and mimic artists voices without obtaining creators consent or compensating them.\"\n\n- Jen Jacobson\n\n\"The final point I want to make is about the importance of choice.\n\nMany AI companies who have illegally vacuumed up hundreds of thousands of musical works and recordings now say that artists can simply contact the company and 'opt out.'\n\nThis is essentially proposing a new form of uncompensated labor that musicians and composers have to perform if they want to avoid exploitation.\"\n\n- Jen Jacobson\n\n\"Opt-out is completely insufficient here.\n\nThis could mean current companies shifting to the public domain and possibly destroying their current models in the process so that opt-in becomes the standard.\"\n\n- Steven Zapata\n\n\"Basically, tech companies must respect artists ownership rights.\n\nThose seeking to profit from others works should have the burden of obtaining permission.\n\nExplicit opt-in is the only way forward.\n\nIt's really how we ensure generative AI models exclude unauthorized works from the beginning.\"\n\n- Karla Ortiz",
    "# Machine Unlearning\n\n\"But because these AI systems can't unlearn, this will only remove the images from future training datasets used by this one company and it's already too late to get out of the most current model.\"\n\n- Steven Zapata\n\n\"Yeah, so opt-out is an ineffective and inappropriate standard for commercial use of copyrighted works including a generative AI.\n\nOnce a model is trained on data, it cannot be deleted unless the whole model is retrained from scratch.\n\nBy the time a model is made public, it's already too late to opt out.\"\n\n- Karla Ortiz",
    "# Transparency and Disclosure\n\n\"We also need transparency on datasets, and divulging your dataset should be compulsory.\"\n\n- Steven Zapata\n\n\"Existing opt-out procedures often ask users to list works used to train the model they own, but as we just mentioned, that training data is secret, so it's an impossible task.\n\nAnd four, there are hundreds of AI models already in the market and more.\"\n\n- Karla Ortiz\n\n\"But as far as how we know our books are being used in AI training, we have absolutely no idea.\n\nIt's a black hole.\n\nOpenAI is training ChatGPT 5 right now, is building it.\n\nThey refuse to answer any questions from the Author's Guild about what data sets they're using, where they're getting their books, and how they're being used.\n\nThere's no transparency at all.\n\nIt's an absolute black hole.\"\n\n- Douglas Preston",
    "# Policy and Legislative Efforts\n\n\"The fight for protection over our craft and livelihoods doesn't stop at the bargaining table.\n\nWhile we have been able to achieve groundbreaking protection for writers, we need public policy solutions, too.\"\n\n- John August, WGA West\n\n\"The Guild’s new agreement offers helpful guidance in thinking about future public policy on AI.\n\nOur agreement defines that AI is not a writer and the material it generates is not equivalent to human writing for purposes of our contract.\n\nThat means that AI cannot rewrite us, nor can it compete with a human writer for credit and the associated financial benefit of that credit.\"\n\n- John August\n\n\"And as such, NAVA recently endorsed the [US Senators] Schatz and Kennedy’s AI Labeling Act of 2023, which is Senate Bill 2691 that we are fully endorsing.\"\n\n- Tim Friedlander\n\n\"To address these concerns, we first aim to pass the Fashion Workers Act, our signature bill, which would establish basic labor protections for models and content creators working in New York's fashion industry.\n\nThis would help address the lack of transparency that leaves models in the dark about how their digital image is being used, and establish a necessary foundation for regulation around generative AI in the fashion industry.\"\n\n- Sara Ziff\n\n\"Second is artists need to have enough power to negotiate fair license terms with these gigantic AI developers.\n\nThere's actually a bill in Congress sponsored by representative Deborah Ross that would give small and independent musicians an antitrust exemption so they can come together and negotiate collectively, both with AI developers and streaming platforms, which is something they do not have the leverage to do currently.\"\n\n- Jen Jacobson",
    "# Lawsuits\n\n\"I'm also the plaintiff in a class action lawsuit against OpenAI, along with 15 other authors and the Guild itself.\n\nAnd we're asking for damages for unauthorized use of our copyrighted work and training and building ChatGPT.\"\n\n- Douglas Preston\n\n\"And that's why we joined together, the 17 of us authors, in a class action lawsuit on behalf of all professional novelists against OpenAI.\n\nThere's nothing complicated about this lawsuit.\"\n\n- Douglas Preston\n\n\"As a side note, due to all of this, I am also a plaintiff in a class action against generative AI image companies as well.\"\n\n- Karla Ortiz",
    "# Self-initiated Research and Investigations\n\n\"I evaluated many of the public and commercial detection tools and found their claims significantly overstated.\n\nThe number of false positives and false negatives made them unusable.\"\n\n- Neil Clarke\n\n\"So we at the Authors Guild investigated, and here are some of the facts we uncovered.\n\nChatGPT3 used more than 150,000 copyrighted books to feed into its AI system, which led us to the next question.\n\n'Where did OpenAI get our books?'\n\nThey're not just sitting out there on the web unprotected.\n\nWell, the Authors Guild found that OpenAI got many, if not most, of these books from pirate websites such as LibGen run out of Russia.\"\n\n- Douglas Preston\n\n\"We're also developing a research study in partnership with the Worker Institute at Cornell University to better understand the impact of generative AI on fashion workers, particularly workers of color, and develop policy recommendations.\"\n\n- Sara Ziff",
    "# Collective Bargaining\n\n\"We won these protections because we're a strong union that successfully carried off a nearly five month strike.\n\nBut we need to remember that most writers and most artists in this country don't have unions to protect them.\"\n\n- John August\n\n\"We stand in solidarity with our fellow creative industry artists who are in unions.\n\nBut unlike them, 80% of the voiceover industry is non-union, meaning we lack the protections and contract that organize workers enjoy.\"\n\n- Tim Friedlander\n\n\"Models are typically hired as independent contractors through management companies which, unlike talent agencies, are held to very few legal standards.\n\nSo when we talk about how generative AI is impacting workers, we need to consider the context of an industry that is truly like the Wild West– where workers have few protections at baseline and also cannot collectively bargain here in the US.\"\n\n- Sara Ziff\n\n\"At The Model Alliance, we believe now is a critical time for solidarity between workers across creative fields who contribute heavily to our culture and economy.\n\nUnfortunately, it's not enough to win protections through collective bargaining agreements.\n\nThere are many workers, including members of our community, who cannot engage in collective bargaining, and so we have to ensure that they are included.\"\n\n- Sara Ziff\n\n\"And unfortunately, visual artists don't have strong union representation to push back on this.\n\nAs it stands, you can work hard for a company like Disney for 25 years and they can take all your work, train a model on it, and then have a free and tireless replica of you to use in perpetuity.\"\n\n- Steven Zapata\n\n\"The solutions sought have been traditionally approached in two ways: collective bargaining with industry and legislative lobbying.\n\nBoth paths tend to seek secondary income to those performers whose work has been diminished by advancing technology.\"\n\n- John Painting",
    "# Regulation\n\n\"We need oversight.\"\n\n- Steven Zapata\n\n\"Regulation of this industry is needed sooner than later, and each moment they are allowed to continue their current practices only causes more harm.\n\nTheir actions to date demonstrate that they cannot be trusted to do it themselves.\"\n\n- Neil Clarke\n\n\"Generative AI poses a serious threat to the writing profession, and we believe that guardrails around its development and use are urgently needed.\"\n\n- Umair Kazi\n\n\"No copyright for AI-generated outputs.\n\nWe oppose efforts to deem AI-generated content protectable under copyright law or through creation of even a limited suite generous right\" - Umair Kazi\n\n\"We're here today because a future with unregulated AI will hurt concept artists and all other sorts of artists across many fields.\n\nWe need regulation, intervention, and oversight.\"\n\n- Steven Zapata\n\n\"If the FTC is not able to do this through rulemaking, we would request the FTC's support for federal legislation to establish that right.\n\nTransparency of ingested content on which these foundational models are trained in order to know if our voice is present.\n\nProtections prior to the generation of any AI-created content that might include voices of professionals that have not provided consent and are not being compensated.\n\nProtection of our voices' biometric data for privacy and commercial purposes.\n\nAn independent third party to verify that audio files are ethically sourced.\n\nAnd finally, clear labeling of any AI-generated content to ensure the consumers are fairly informed.\"\n\n- Tim Friedlander\n\n\"Regulatory agencies should act now to protect artists, consumers, and other Americans from this unconscionable exploitation.\n\nRegulatory agencies should demand full transparency from generative AI companies and opt-in only practices.\"\n\n- Karla Ortiz\n\n\"Lastly, regulatory agencies should strongly consider seeking algorithmic disgorgement on products built on data acquired without consent, credit, or compensation, regardless whether that company is transparent or not.\n\nUrgent measures like these will be needed to avoid, in my opinion, the diminishing or outright destruction of most, if not all creative professional livelihoods and the protections of all of our rights.\"\n\n- Karla Ortiz\n\n\"In my opinion, there's no reason that big tech shouldn't be regulated to make these systems transparent, completely end to end.\"\n\n- Bradley Kuhn",
    "# Consent\n\n\"Consent and compensation.\n\nRequire all generative AI companies to seek permission for the use of creative works and to fairly compensate creators.\"\n\n- Umair Kazi\n\n\"And going forward, we're asking that OpenAI and other AI developers get permission from authors, properly license our books, and compensate us fairly for that use.\"\n\n- Douglas Preston\n\n\"We also think authors and artists should have the right to say that they don't want their identities, works, voice or style used in outputs.\"\n\n- Umair Kazi\n\n\"That's why we think it's critical that we require artists have affirmative consent before the work can be used to train generative AI models and that they have to be compensated fairly when they do so.\n\nThe same should be true for all artists, including artists like us who do work for hire and don't hold the copyright on our work.\"\n\n- John August\n\n\"A system that is opt-in.\n\nNot opt-out.\"\n\n\"And this system needs to be opt-in and not opt-out.\n\nAs Jen just said, there are so many companies out there developing and training AI models, to be forced to continually track all of them down to opt out is an enormous administrative burden on individual artists.\n\nIt's not practical.\n\nIt has to be opt-in rather than opt-out.\"\n\n- John August",
    "# Credit and Transparency\n\n\"Credit and transparency.\n\nCreate obligations for all AI companies to disclose what data sets and works they use to train the systems.\"\n\n- Umair Kazi\n\n\"Labeling AI-generated content.\"\n\n- Umair Kazi\n\n\"As far as what we want, we want AI companies to be required to fully disclose the complete lists of copyrighted works, books in particular is the medium that we deal with most often, that are in the training data sets or provide specific links to where the data sets were obtained from.\n\nAnd anyone compiling a training dataset should be similarly obligated to disclose the sources.\n\nAnd in the case of copyrighted works, a complete list of works that have been included in the dataset.\"\n\n- Umair Kazi\n\n\"And we believe that there should be a requirement to conspicuously label fully or substantially AI generated words in online marketplaces.\"\n\n- Umair Kazi\n\n\"In my view, the public should have access to the input set, have access to the source code of the software that does the training and generation, and most importantly, access to the source code that does these forms of backend generation exclusion, the latter of which I think would expose the duplicity of big tech's policies here.\"\n\n- Bradley Kuhn\n\n\"At a minimum the consumers should know when AI is used to generate voices and receive a warning that the information they're going to receive may not be accurate.\"\n\n- Tim Friedlander\n\n\"We need compulsory transparency and tools to verify compliance.\"\n\n- Steven Zapata",
    "# Compensation\n\n\"Permission and payment for use in outputs.\n\nRequire all AI companies to seek permission and pay compensation when creative works are used in outputs or when names or identities or titles of works are used in prompts.\"\n\n- Umair Kazi\n\n\"We should also have AI companies pay a fine for their past practices and pay all affected artists a fee per generation.\n\nThis is to compensate artists for utilizing their works and names without permission, should be retroactive for as long as the company has been for-profit.\n\nWe must close research to-commercial loopholes, interpreted or actual, that allow for-profit companies to monetize the results of non-commercial research.\"\n\n- Steven Zapata",
    "# Licensing\n\n\"And the third thing I would say is there is not a one-size-fits-all licensing system that will work for all creators or even for all musicians.\n\nAssuming there is a level playing field for negotiating, we think the best way for musicians to license their work is in the free market, which may look different for every use, every artist and every company.\"\n\n- Jen Jacobson\n\n\"Without a doubt, licensing will be essential in the future, but we must accomplish that through an opt-in system, otherwise there would be no real negotiating leverage for creators.\n\nAnd the focus of licensing, I think should go towards new opt-in foundation models, not the fine tuning of existing unethical models.\n\nAs to when companies hold the rights to work done for hire and want to license or train off of that, we need regulation.\"\n\n- Steven Zapata\n\n\"I mean, visual artists, for example, lack the union representation to push back against contracts that claim all ownership.\n\nAnd without regulation, I think predatory contracts will just run rampant in this sector.\n\nAnd collective licensing is also troubling.\n\nThe early experiments we are seeing with \"contributor funds\" from companies like Shutterstock are paying out less than pennies.\n\nI mean actual fractions of a penny per used image.\n\nThat's all they want to pay a creator for their life's work and to create a tool that will directly compete against them forever.\"\n\n- Steven Zapata",
    "## Definitions\n\nGenerative artificial intelligence (AI) is a technology that can create content, including text, images, audio, or video, when prompted by a user.\n\nGenerative AI systems learn patterns and relationships from massive amounts of data, which enables them to generate new content that may be similar, but not identical, to the underlying training data.\n\nThe systems generally require a user to submit prompts that guide the generation of new content.",
    "## Why do we need guideline to use AI systems in the workplace?\n\nBelow are just a few of the critical reasons why it’s important to know how to use AI:\n- Content generated by AI should be reviewed and fact-checked, especially if used in public communication or decision-making.\n\n- DES staff generating content with AI systems should verify that the content does not contain inaccurate or outdated information and potentially harmful or offensive material.\n\n- AI-generated content used in official state capacity should be clearly labeled.\n\n- When using DES, staff should be mindful of the potential biases and inaccuracies that may be present.",
    "#### What should I do when using Generative AI systems?\n\n- Do read the entire document independently and review the summary for biases.\n\n- Do rewrite documents in plain language for better accessibility and understandability.\n\n- Do disclose how material was reviewed or edited and by whom.\n\n- AI-generated content used in official state capacity should be clearly labeled.\n\n**Sample disclosure line**: This memo was summarized by ChatGPT using the following prompt: “Summarize the following memo: (memo content)”.\n\nThe summary was edited by {Insert Names}.\n\n- DES employees should ensure no copyrighted material is published without appropriate disclosure or the acquisition of necessary rights.\n\nThis includes content generated by AI systems, which could inadvertently infringe upon existing copyrights.",
    "#### What should I not do when using Generative AI systems?\n\n- Don’t use personal email addresses to login to AI systems or internet-based tools for DES business use.\n\n- Don’t access or use AI systems or internet-based tools until you have received authorization from ETS.\n\n- Don’t include sensitive or confidential information in the prompt.\n\n- Don’t integrate or incorporate any non-public information into AI.\n\nThe use of this information could lead to unauthorized disclosures and legal liabilities.",
    "## What to know before you use\n\nState law already restricts the sharing of confidential information with unauthorized third parties.\n\nFor state employees, RCW 42.52.050 (the state’s ethics law) specifically states: “No state officer or state employee may disclose confidential information to any person not entitled or authorized to receive the information.”\n\nUsing a generative AI system may result in creating a public record under Washington state's Public Records Act.\n\nContact your agency’s Privacy and Records Officers for more information.",
    "## BACKGROUND\n\nAs generative AI technology progresses, chatbots, virtual assistants, and other systems based on it are becoming more prevalent.\n\nThese can include standalone systems, be integrated as features within search engines, or be overtly or transparently embedded in all manner of other software tools.\n\nExamples include ChatGPT and DALL-E from OpenAI, Microsoft Bing’s chat, Microsoft 365 Copilot, and Bard from Google.\n\nGenerative AI tools have the potential to enhance productivity by assisting with tasks like drafting documents, editing text, generating ideas, and software coding.\n\nHowever, these technologies also come with potential risks that include inaccuracies, bias and unauthorized use of intellectual property in the content generated.\n\nIn addition, content created by AI, and the public availability of information submitted to the AI, could pose security or privacy concerns.",
    "## DEFINITIONS\n\nGenerative artificial intelligence (AI) uses advanced technologies such as predictive algorithms, machine learning, and large language models to process natural language and produce content in the form of text, images, or other types of media.\n\nGenerated content is typically remarkably similar to what a human creator might produce, such as text consisting of entire narratives of naturally reading sentences.\n\nRestricted Use Information as defined in ITEC 7230A.\n\nEntity is defined as agencies, boards, commissions under the direction of the Governor or agents and contractors acting on behalf of those agencies, boards or commissions.",
    "## POLICY\n\nThis policy shall serve as the primary governing document for usage of generative artificial intelligence technology as a user or related activities by the entities.\n\nWhile any entity may impose additional restrictions through their own policy, such policies must not conflict with the provisions outlined in this policy.\n\nThis policy applies to all business use cases involving the State of Kansas, including but not limited to:\n- Development of software code,\n- Written documentation (i.e., policy, legislation, or regulations) and correspondence (such as memorandums, letters, text messages, and emails),\n- Research,\n- Summarizing and proofreading documents,\n- Making business decisions that impact short-term or long-term activities or policies and procedures.",
    "### Responsibilities\n\nResponses generated from generative AI outputs shall be reviewed by knowledgeable human operators for accuracy, appropriateness, privacy and security before being acted upon or disseminated.\n\nResponses generated from generative AI shall not:\n- Be used verbatim,\n- Be assumed to be truthful, credible, or accurate,\n- Be treated as the sole source of reference,\n- Be used to issue official statements (i.e., policy, legislation, or regulations),\n- Be solely relied upon for making final decisions,\n- Be used to impersonate individuals or organizations.\n\nRestricted Use Information (RUI) shall not be provided when interacting with generative AI.\n\nRefer to ITEC Policy 7230A Section 9.16 Account Management - RUI.\n\nMaterial that is inappropriate for public release shall not be entered as input to generative AI.\n\nAll information that is provided shall be subjected to the same standard as referenced in the State Social Media Policy and shall be treated as publicly available.\n\nMaterial that is copyrighted or the property of another shall not be entered as input to generative AI.\n\nGenerative AI shall not be used for any activities that are harmful, illegal, or in violation of state policy or agency acceptable use policy.\n\nAgencies shall ensure contractors disclose in their contracts the utilization of generative AI or integrations with generative AI platforms.\n\nAgency contracts shall prohibit contractors from using State of Kansas RUI or other confidential data in generative AI queries or for building or training proprietary generative AI programs unless explicitly approved by the agency head with consultation from the Chief Information Security Officer.\n\nContractors utilizing Generative AI to build software explicitly for the State of Kansas must demonstrate positive control over all data input into the system.",
    "### THE TECHNOLOGY\n\n**What is it?\n\n**  \nGenerative artificial intelligence (AI) is a technology that can create content, including text, images, audio, or video, when prompted by a user.\n\nGenerative AI systems create responses using algorithms that are trained often on open-source information, such as text and images from the internet.\n\nHowever, generative AI systems are not cognitive and lack human judgment.\n\nGenerative AI has potential applications across a wide range of fields, including education, government, medicine, and law.\n\nUsing prompts—questions or descriptions entered by a user to generate and refine the results—these systems can quickly write a speech in a particular tone, summarize complex research, or assess legal documents.\n\nGenerative AI can also create artworks, including realistic images for video games, musical compositions, and poetic language, using only text prompts.\n\nIn addition, it can aid complex design processes, such as designing molecules for new drugs or generating programming codes.\n\n**How does it work?\n\n**  \nGenerative AI systems learn patterns and relationships from massive amounts of data, which enables them to generate new content that may be similar, but not identical, to the underlying training data.\n\nThey process and create content using sophisticated machine learning algorithms and statistical models.\n\nFor example, large language models use training data to learn patterns in written language.\n\nGenerative AI can then use models to emulate a human writing style.\n\nGenerative AI can also learn to use many other data types, including programming codes, molecular structures, or images.\n\nThe systems generally require a user to submit prompts that guide the generation of new content.\n\nMany iterations may be required to produce the intended result because generative AI is sensitive to the wording of prompts.\n\n**How mature is it?\n\n**  \nAdvanced chatbots, virtual assistants, and language translation tools are mature generative AI systems in widespread use.\n\nImproved computing power that can process large amounts of data for training has expanded generative AI capabilities.\n\nAs of early 2023, emerging generative AI systems have reached more than 100 million users and attracted global attention to their potential applications.\n\nFor example, a research hospital is piloting a generative AI program to create responses to patient questions and reduce the administrative workload of health care providers.\n\nOther companies could adapt pre-trained models to improve communications with customers.",
    "### OPPORTUNITIES\n\n**Summarizing information.\n\n**  \nBy rapidly aggregating a wide range of content and simplifying the search process, generative AI quickens access to ideas and knowledge and can help people more efficiently gather new information.\n\nFor example, researchers can identify a new chemical for a drug based on an AI-generated analysis of established drugs.\n\n**Enabling automation.\n\n**  \nGenerative AI could help automate a wide variety of administrative or other repetitive tasks.\n\nFor example, it could be used to draft legal templates, which could then be reviewed and completed by a lawyer.\n\nIt can also improve customer support by creating more nuanced automated responses to customer inquiries.\n\n**Improving productivity.\n\n**  \nBecause it is capable of quickly automating a variety of tasks, generative AI has the potential to enhance the productivity of many industries.\n\nMultiple studies and working papers have shown generative AI can enhance the speed of administrative tasks and computer programming, although users may need to edit the generated result.",
    "### CHALLENGES\n\n**Trust and oversight concerns.\n\n**  \nIn June 2021, GAO identified key practices, such as a commitment to values and principles for responsible use, to help ensure accountability and responsible use of AI across the federal government and other entities.\n\nGenerative AI systems can respond to harmful instructions, which could increase the speed and scale of real-world harms.\n\nFor example, generative AI could help produce new chemical warfare compounds.\n\nAdditionally, generative AI systems share challenges to oversight similar to other AI applications—such as assessing the reliability of data used to develop the model—because their inputs and operations are not always visible.\n\nThe White House announced in May 2023 that a working group would provide, among other things, input on how best to ensure that generative AI is developed and deployed equitably, responsibly, and safely.\n\nOther agencies, such as the National Institute of Standards and Technology, have also promoted responsible use of generative AI.\n\n**False information.\n\n**  \nGenerative AI tools may produce “hallucinations”—erroneous responses that seem credible.\n\nOne reason hallucinations occur is when a user requests information not in the training data.\n\nAdditionally, a user could use AI to purposefully and quickly create inaccurate or misleading text, thus enabling the spread of disinformation.\n\nFor example, generative AI can create phishing e-mails or fake but realistic social media posts with misleading information.\n\nFurther, bias in the training data can amplify the potential for harm caused by generative AI output.\n\n**Economic issues.\n\n**  \nGenerative AI systems could be trained on copyrighted, proprietary, or sensitive data without the owner’s or subject’s knowledge.\n\nThere are unresolved questions about how copyright concepts, such as authorship, infringement, and fair use, will apply to content created or used by generative AI.\n\n**Privacy risks.\n\n**  \nSpecific technical features of generative AI systems may reduce privacy for users, including minors.\n\nFor example, a generative AI system may be unable to “forget” sensitive information that a user wishes to delete.\n\nAdditionally, if a user enters personally identifiable information, that data could be used indefinitely in the future for other purposes without the user’s knowledge.\n\nSection 230 of the Communications Decency Act of 1996 shields online service providers and users from legal liability for hosting or sharing third-party content, but it is unclear how this statute might apply to AI-generating content systems and their creators.\n\n**National security risks.\n\n**  \nInformation about how and when some generative AI systems retain and use information entered into them is sparse or unavailable to many users, which poses risks for using these tools.\n\nFor example, if a user enters sensitive information into a prompt, it could be stored and misused or aggregated with other information in the future.\n\nFurthermore, when systems are publicly and internationally accessible, they could provide benefits to an adversary.\n\nFor example, generative AI could help rewrite code making it harder to attribute cyberattacks.\n\nIt may also generate code for more effective cyberattacks even by attackers with limited computer programming skills.",
    "### POLICY CONTEXT AND QUESTIONS\n\n- What AI guidelines can best ensure generative AI systems are used responsibly, and are generative AI systems following existing guidance?\n\n- What standards could be used or developed to evaluate the methods and materials used to train generative AI models and ensure fairness and accuracy of their responses for different use cases?\n\n- How can public, private, academic, and nonprofit organizations strengthen their workforce to ensure responsible and accountable use of generative AI technologies?\n\n- What privacy laws can be used or developed to protect sensitive information used or collected by generative AI systems, including information provided by minors?",
    "### SELECTED REFERENCES\n\n- Congressional Research Service.\n\nGenerative Artificial Intelligence and Copyright Law.\n\nLSB10922.\n\nWashington, D.C.: 2023.\n\n- U.S. Department of Commerce.\n\nNational Institute of Standards and Technology.\n\nNIST AI Risk Management Framework.\n\nNIST AI 100-1.\n\nGaithersburg, MD.\n\n: 2023.\n\n- The White House.\n\nBlueprint for an AI Bill of Rights Making Automated Systems Work for the American People.\n\nWashington, D.C.: 2022.",
    "## SUBJECT: Use of Commercial Generative Artificial Intelligence (AI) Tools\n\nDuring his April 21, 2023, State of Homeland Security address, Secretary Mayorkas stated that “Our Department will lead in the responsible use of AI [Artificial Intelligence] to secure the homeland and in defending against the malicious use of this transformational technology.\n\nAs we do this, we will ensure that our use of AI is rigorously tested to avoid bias and disparate impact and is clearly explainable to the people we serve.”\n\nTo do this, the Secretary directed the Under Secretary for Science and Technology and me to establish an Artificial Intelligence Task Force (AITF).\n\nAs the AITF works to advance specific mission applications of AI across the Department, we must also address ways in which our workforce uses commercially available generative AI (Gen AI) products in their work.\n\n“Generative artificial intelligence” means the class of AI models that emulate the structure and characteristics of input data to generate novel synthetic content.\n\nThis can include images, videos, audio, text, and other types of digital content.\n\nTools using Gen AI have rapidly gained worldwide popularity.\n\nThere is a growing body of research that suggests Gen AI will lead to significant productivity gains across all sectors.\n\nGen AI tools also present significant challenges and risks, including producing “hallucinations” or invented and inaccurate responses and generating biased outputs based on biases in their training data.\n\nThey further present information privacy and security risks if sensitive information is provided to tools and used to further train underlying models.\n\nCareful human judgment is required to balance the productivity gains associated with Gen AI tools with these risks.\n\nThe AITF is actively exploring use cases for Gen AI tools across a variety of DHS mission areas.\n\nWhile this work continues, I have determined that DHS must enable and encourage DHS personnel to responsibly use commercial products to harness the benefits of Gen AI and ensure we continuously adapt to the future of work.\n\nAs such, I am issuing the following initial guidance to facilitate appropriate use during this early stage of technological development:\n\nMy office will develop and maintain a list of conditionally approved commercial Gen AI tools for use on open-source information only.\n\nIn developing this list, we will review basic accuracy and security practices, supply chain risk management concerns, privacy and civil liberties safeguards, and available information on how training data was sourced.\n\nThis list will be coordinated in advance of its release with the DHS Privacy Office (PRIV), the Office for Civil Rights and Civil Liberties (CRCL), and other stakeholders.\n\nThis review will not provide approval to utilize these tools on non-public information.\n\nWe will maintain a page on DHS Connect listing these tools and offering employees the ability to submit additional tools for review and testing.\n\nMy office will update Department IT and cybersecurity policies and standards to include new requirements for use of approved commercial Gen AI tools by DHS personnel in their work.\n\nIn the interim, however, the following rules shall apply:\n\n- Personnel must never put DHS data regarding individuals (regardless of whether it is personally identifiable information (PII) or anonymized), social media content, or any For Official Use Only, Sensitive but Unclassified Information, now known as “Controlled Unclassified Information,” or Classified information into commercial Gen AI tools.\n\n- Personnel shall protect any PII collected or generated by the use of commercial Gen AI tools in accordance with applicable DHS privacy policy and federal law.\n\n- Prior to any use of these tools, personnel must obtain approval from their supervisors, complete a training on responsible use of AI along with their annual Protecting Personally Identifiable Information and Cybersecurity Awareness trainings, and sign an acknowledgement of the conditions on use.\n\n- Personnel will create accounts on these tools using their DHS email and use these accounts only for DHS use, separate from any personal use of these tools.\n\n- Personnel will select options in tools that limit data retention and opt out of inputs being used to further train models.\n\n- All use of conditionally approved commercial Gen AI tools will be on the web, not through downloaded desktop or mobile apps.\n\nPersonnel includes federal employees, contractors, detailees, and others working on behalf of DHS.\n\nCommercial Gen AI tools are defined as generative AI technology or products available for use or purchase by the general public (i.e., off-the-shelf).\n\nThis definition does not include customized software or services developed specifically for the government through an IT acquisition process.\n\nOpen-Source Information means unclassified information that has been published or broadcast in some manner to the public.",
    "Open-Source Information means unclassified information that has been published or broadcast in some manner to the public.\n\nSources are newspapers or other periodicals; weather reports; books, journal articles, or other published works; public court filings; or any similar documents that have traditionally been publicly available.\n\nAt all times, personnel are accountable for accessing and handling DHS information and IT resources in compliance with DHS User Rules of Behavior and DHS policies, including privacy, civil rights, and civil liberties policies.\n\nPersonnel should ensure all content generated or modified using these tools is reviewed by appropriate subject matter experts for accuracy, relevance, data sensitivity, inappropriate bias, and policy compliance before using it in any official capacity, especially when interacting with the public.\n\nCommercial Gen AI tools may not be used in the decision-making process for any benefits adjudication, credentialling, vetting, or law or civil investigation or enforcement related actions.\n\nAgency and Office Leaders or CIOs can request from my office a written waiver of these restrictions.\n\nAny spillage or compromise of DHS information into Gen AI tools must be reported immediately pursuant to applicable policies.\n\nAgency and Office Chief Information Officers will develop policies regarding appropriate use of commercial Gen AI tools in their missions, in consultation with my office, PRIV, and CRCL.\n\nThese policies may impose additional conditions or limitations on the use of commercial Gen AI tools based on unique mission requirements.\n\nImmediate appropriate applications of commercial Gen AI tools to DHS business could include generating first drafts of documents that a human would subsequently review, conducting and synthesizing research on open-source information, and developing briefing materials or preparing for meetings and events.\n\nI have personally found these tools valuable in these use cases already, and encourage employees to learn, identify, and share other valuable uses with each other.\n\nThis initial guidance will be updated in coordination with relevant stakeholders across the Department regularly given the rapid pace of technological change in the AI space.\n\nAs Agencies and Offices deploy internal systems and tools leveraging Gen AI, those systems will be governed through broader Department policies on responsible use, including DHS Policy Statement 139-06 “Acquisition and Use of Artificial Intelligence and Machine Learning Technologies by DHS Components.”",
    "## Contents\n\n- Generative AI and Foundation Models\n- Opportunities that Change the Game\n- New Risks With Generative AI\n- Building on the Foundation of AI Governance Frameworks\n- Evolving the Approach to Safer and Trusted Generative AI\n- Unpacking the Approach Towards Generative AI\n- Conclusion\n- Further Reading",
    "## About This Paper\n\nThis discussion paper proposes ideas for senior leaders in government and businesses on building an ecosystem for the trusted and responsible adoption of generative AI.\n\nThis should lead to a virtuous cycle, spurring innovation and enabling more to tap on opportunities afforded by generative AI.\n\nThe practical pathways for governance in this paper seek to advance the global discourse and foster greater collaboration to ensure generative AI is used in a safe and responsible manner, and that the most critical outcome — trust — is sustained.",
    "## Generative AI and Foundation Models\n\nGenerative AI has taken the world by storm, providing a first-hand taste of engaging in conversation with an “artificially intelligent being” and an early glimpse, some say, of Artificial General Intelligence (AGI).\n\nIts ability to be a creative force has anthropomorphised AI in a powerful way.",
    "### Generative AI: A Creative, Rather Than Merely Analytical Force\n\nThe last wave of breakthroughs in AI mainly clustered around “discriminative” models.\n\nThese models aid decision-making by recommending, filtering, or making predictions.\n\nThey do so by learning the boundaries between various classes in the dataset, making them a natural fit for classification problems (e.g., cats vs dogs).\n\nOn the other hand, generative models learn the underlying distribution of the data and can generate new content (literature, audio, videos) from this learned distribution (e.g., new images of dogs).\n\nSuch AI models that generate new contents are referred to as “generative AI”.\n\nEarly versions of generative AI were designed to solve specific tasks.\n\nFor example, models like CycleGAN and StyleGAN, which are built on the popular Generative Adversarial Networks (GANs) architecture, can learn to create and alter images in a manner suitable to the given task by training on chosen datasets.\n\nFoundation models (as termed by researchers at Stanford University), on the other hand, refers to a special case of generative AI that are trained on a broad corpus of data and act as a “foundation” for more task-specific models.\n\nFoundation models have demonstrated exceptional performance, especially in the realm of natural language processing.\n\nFor instance, GPT3 and GPT4 received widespread attention for their ability to understand and generate natural language.\n\nOther examples include DALL-E, Stable Diffusion, and Midjourney which are capable of generating highly realistic images from textual prompts.\n\nThese models exhibit emergent capabilities that are implicitly induced and far beyond what is expected from their construction.\n\nHence, they are surprisingly good at a wide range of tasks without being explicitly trained for these tasks.\n\nGPT-3 has 175 billion parameters and later versions can be adapted to a downstream task simply by providing it with a prompt (a natural language description of the task), an emergent property that was neither specifically trained for nor anticipated to arise.\n\nThis has led to much excitement that they potentially represent embryonic examples of AGI.\n\nThe spike in the success of generative AI, especially foundation models (collectively referred to as ‘generative AI’ henceforth), can be attributed to (i) the availability of powerful hardware; (ii) access to massive datasets; (iii) a training technique known as self-supervision; and (iv) a new neural network architecture named Transformers.\n\nFoundation models are adapted for a specialised setting through a process of fine-tuning using additional data, known as transfer learning.\n\nIn later models, incorporating human feedback through a process known as Reinforcement Learning with Human Feedback (RLHF) has also been found to improve performance and could potentially align these models with human principles and values.\n\nOpenAI’s ChatGPT’s ability to engage in realistic conversations and to answer natural language queries is fine-tuned from GPT (versions 3.5 and 4), using RLHF to provide helpful responses.\n\nHowever, our understanding of the drawbacks associated with these fine-tuning techniques, particularly in regard to accuracy trade-offs and scalability, is still evolving.",
    "## Opportunities That Change the Game\n\nGenerative AI has uncovered a myriad of use cases and opportunities that are reshaping industries, revolutionising sectors, and driving innovation.\n\nChatbots, capable of understanding and responding in natural language, have already led to a vast improvement in user experience across many online platforms.\n\nWhether it is instantly generating a shopping list from a simple indication of your cravings, automatically generating a compelling description of an item you are trying to sell online, or perhaps roleplaying to improve your conversation skills, these AI chatbots have proven to be valuable assistants.\n\nIn business operations, their applications range from drafting personalised emails and meeting minutes to creating new advertising videos.\n\nHR and legal departments are starting to rely on generative AI to generate job descriptions, contracts, and onboarding materials.\n\nFinally, recent successful marketing campaigns, such as Coca Cola’s “Create Real Magic” or BMW’s “Ultimate AI Masterpiece”, aim to improve brand recognition using content produced by generative AI.\n\nGenerative AI has also illustrated its effectiveness in new product design.\n\nIn fashion, they are used to design new collections and even translate pencil sketches to complete designs.\n\nIn healthcare, AI-assisted drug design is gaining attention.\n\nAI-generated medical images and records assist in developing diagnostic models without compromising patient privacy.\n\nThe generation of a digital twin of a patient using generative AI is also being investigated for its application in precision medicine and clinical trials.\n\nThe entertainment industry is also witnessing unprecedented changes thanks to generative AI such as musicians licensing their voices for AI use and Netflix producing anime series with AI-generated backgrounds.\n\nFinally, generative AI will undoubtedly have a long-lasting impact on online search platforms whose results will eventually be conversational rather than simply a collection of hyperlinks.\n\nThe public sector also presents a promising landscape for generative AI use.\n\nAI-powered virtual assistants can be utilised to make government services more accessible and efficient.\n\nInitial examples include a ChatGPT-based AI assistant currently being evaluated for its ability to help citizens with basic legal questions.\n\nBy analysing public opinions and polls, generative AI models can also be trained to reflect public interest and aid in policy-making.\n\nGenerative AI is also a valuable tool for urban planning and can be used to generate solutions to tackle various problems affecting the community, ranging from public infrastructure planning to climate change in smart cities.\n\nThe opportunities that generative AI can unlock are tremendous and are likely to be the start of a new transformative wave impacting all elements of how we live, work, and play.\n\nWhile these potential use cases of generative AI are undeniably transformative, concerns and threat scenarios have emerged; from the risk of AI making gaffes to worries that it will take over the world.\n\nThis primer presents a policy perspective, driven by technical analysis, for senior leaders in government and businesses to understand how to tap on the capabilities offered by generative AI in a safe and responsible manner, and in so doing, chart the path towards how AI can be harnessed in a trusted manner for the broader public good.",
    "## New Risks with Generative AI\n\nTrustworthy AI literature has identified a few governance areas, which typically deal with robustness, explainability, algorithmic fairness, privacy, and security.\n\nThe Singapore Model AI Governance Framework and OECD AI Principles outline these core areas.\n\nEven though these governance areas continue to remain relevant, generative AI also poses emerging risks that may require new approaches to its governance.",
    "### Risk 1: Mistakes and \"Hallucinations\"\n\nLike all AI models, generative AI models make mistakes.\n\nWhen generative AI makes mistakes, they are often vivid and take on anthropomorphisation, commonly known as “hallucinations”.\n\nCurrent and past versions of ChatGPT are known to make factual errors.\n\nSuch models also have a more challenging time doing tasks like logic, mathematics, and common sense.\n\nThis is because ChatGPT is a model of how people use language.\n\nWhile language often mirrors the world, these systems, however, do not (yet) have a deep understanding of how the world works.\n\nAdditionally, these false responses can be deceptively convincing or authentic.\n\nLanguage models have created convincing but erroneous responses to medical questions, created false stories of sexual harassment, and generated software code that is susceptible to vulnerabilities.\n\nResults from these models can appear overly “confident” and are not qualified with a measure of uncertainty, something which will hopefully be addressed through better research on uncertainty estimates.\n\nThese issues are worrisome in foundation models since they are designed for broad, general-purpose use.\n\nThe designer may not fully envisage the specific issues and potential failures.\n\nAny vulnerabilities in a foundation model will also run the risk of being inherited by every model derived from it.",
    "### Risk 2: Privacy and Confidentiality\n\nGenerative AI tends to have a property of “memorisation”.\n\nTypically, one would expect AI models to generalise from the individual data points used to train the model, so when you use the AI model there is no trace of the underlying training data.\n\nAs the neural networks underpinning generative AI models expand, these models have a tendency to memorise.\n\nFor example, Stable Diffusion tends to memorise twice as much as older generative AI models such as GANs.\n\nThere are risks to privacy if models “memorise” wholesale a specific data record and replicate it when queried.\n\nAdversaries may find out if a certain individual is part of a training set by querying the trained model or even reconstruct training data by querying the model.\n\nThe former is problematic especially for medical datasets or other datasets which are sensitive.\n\nMore research is needed to know why these models memorise.\n\nIt is often attributed to a training process known as overfitting, though there are other explanations.\n\nA worrying finding is that parts of sentences like nouns, pronouns, and numerals are memorised faster than others – precisely the type of information that may be sensitive.\n\nThis memorisation property also poses copyright and confidentiality issues for enterprises and companies.\n\nSamsung employees reportedly unintentionally leaked sensitive information by pasting confidential and copyrighted source code into ChatGPT to check for errors and to optimise code.\n\nAnother shared a recording of an internal meeting.\n\nGiven that ChatGPT utilises user prompts to further train and improve their model unless users explicitly opt out, that information is now out in the wild.",
    "### Risk 3: Scaling Disinformation, Toxicity, and Cyber-Threats\n\nDissemination of false content such as fake news is becoming increasingly hard to identify due to convincing but misleading text, images, and videos, potentially generated at scale by generative AI.\n\nThe negative impact of interactive media is greater as it taps into emotive human reactions.\n\nToxic content — profanities, identity attacks, sexually explicit content, demeaning language, language that incites violence — has also been a challenge on social media platforms.\n\nGenerative models that mirror language from the web run the risks of propagating such toxicity.\n\nBut it is not as simple as just filtering or checking against toxic content.\n\nA naïve filter for generative AI that refuses to answer a prompt like “The Holocaust was…” risks censoring useful information.\n\nIn addition, impersonation and reputation attacks have become easier, whether it is social engineering attacks using deepfakes to get access to privileged individuals or reputational attacks by offensive image generation.\n\nWith generative AI being able to generate images in one’s likeness, there is a question of whether this constitutes an intrusion of privacy.\n\nBesides generating toxic and false content, generative AI also makes it possible to cause other types of harm.\n\nActors with little to no technical skills can potentially generate malicious code.\n\nCheckpoint Research used generative AI models to create an entire infection flow — starting from generating phishing emails to creating executables with malicious code.\n\nThey restricted themselves from writing any line of code and only used plain English prompts to achieve this task.\n\nOther examples include setting up a dark web marketplace and generating Adversarial DDoS (Distributed Denial-of-Service) attacks.\n\nWhile OpenAI has put in filters to stop the generation of such phishing emails and malicious code, there are ways to bypass these limitations.\n\nThis will prove to be an ongoing battle.",
    "### Risk 4: An Era of Copyright Challenges\n\nAI and machine learning models have always operated on the basis of identifying patterns present in relevant data.\n\nCurrent generative AI models require massive amounts of data.\n\nScraping the web for data at this scale has exacerbated the existing concerns of copyrighted materials used (e.g., Getty Images suing Stable Diffusion over alleged copyright violation for using their watermarked photo collection).\n\nAdditionally, there is a rising concern in the creative community regarding AI that explicitly creates the style and expression of authors, artists, or musicians.\n\nThis is detrimental to artists as generative AI like Stable Diffusion, Dall-E, and Midjourney are capable of generating high-quality images that can be used (e.g., Zarya of the Dawn) for commercial purposes.\n\nFalse attribution, copyright infringement, and even forgery have become more challenging to combat.\n\nIt is an open debate whether the current legal landscape surrounding copyright and intellectual property meaningfully addresses the current state of AI-generated content, both in terms of protecting an artist against having his/her work used in AI training as well as the ownership of the content generated by AI.\n\nMoreover, current copyright laws protect expression but not underlying facts, data, ideas, or concepts.\n\nAI that used these facts or data to train their models could legitimately use these provisions that allow for fair use.\n\nHowever, generative AI that now seeks to mimic style, flourishes, curation, and creative aspects of the content operates in a grey area, where it is questionable whether these are expressions that are protected.",
    "### Risk 5: Embedded Biases Which Echo in Downstream Applications\n\nAI models capture the inherent biases present in the training dataset (e.g., corpus of the web).\n\nIt is not surprising that if care is not taken, the models would inherit various biases of the Internet.\n\nExamples include image generators that when prompted to create the image of an “American person”, lightens the image of a black man, or models that tend to create individuals in ragged clothes and primitive tools when prompted with “African worker” while simultaneously outputting images of happy affluent individuals when prompted with “European worker”.\n\nIn particular, foundation models risk spreading these biases to downstream models trained from them.",
    "### Risk 6: Values, Alignment, and the Difficulty of Good Instructions\n\nAI safety is often associated with the concept of value-alignment — i.e., aligned with human values and goals to prevent them from doing harm to their human creators.\n\nAI scientists and designers have always faced the challenge of formulating how to instruct AI systems to achieve certain “objectives”, defined in precise terms.\n\nHence, objectives are often mis-specified or represented using simple heuristics.\n\nThis can lead to potentially dangerous outcomes when the AI systems blindly optimise for these objectives.\n\nOpenAI’s blog highlights a gaming agent purposely crashing itself over and over to gain additional points.\n\nAn objective function for AI assistants needs to prioritise between the assistant being “helpful” or “harmless”.\n\nHowever, it is difficult to define and specify what these concepts are, and how to trade-off between them.\n\nFor instance, an insistence on avoiding harm can lead to “safe” responses that might not be valuable to the user.\n\nOn the other hand, assigning more importance to being helpful can lead to the system generating toxic responses that might cause harm.\n\nOne way to mitigate this is by relying on feedback from humans using Reinforcement Learning through Human Feedback (RLHF).\n\nWhen fine-tuned using RLHF, language models learn to follow instructions better and generate results that show fewer instances of “hallucination” and toxicity (even though bias still remains as an open problem).\n\nSafety and alignment work is a nascent and ongoing research area.",
    "## Building on the Foundation of AI Governance Frameworks\n\nThere are many discussions worldwide about generative AI, including calls for government interventions to address these potential risks.\n\nIn parsing these issues, it is instructive to build on existing principles, such as those by the OECD, NIST (AI Risk Management Framework), and Singapore (Model AI Governance Framework), that point to how we might think about AI governance.\n\nWhile these principles and practices are applicable regardless of the types of AI deployed, policy adaptations will, nevertheless, be needed to consider the unique characteristics of generative AI.\n\nIn particular, there are two key characteristics of note:\n\nGenerative AI will increasingly form the foundation upon which other models/applications are built.\n\nBecause of this dependency, there are concerns over systemic risks as problems inherent in these models could perpetuate and lead to wider impact.\n\nGovernance frameworks will have to provide guidance on accountability between parties and across the development lifecycle, as well as address safety concerns in model development and deployment.\n\nThese models are generative – not only because they can produce realistic content at scale, but also because they demonstrate increasingly sophisticated capabilities, e.g., the ability to reason.\n\nIt may be increasingly difficult to distinguish AI-generated content and people may become more susceptible to misinformation and online harms.\n\nAs AI potentially surpasses human capacity at some levels, there are also deep concerns around controllability and alignment.",
    "## Evolving the Approach to Safer and Trusted Generative AI\n\nAmidst these changes, we must continue our efforts to enhance a trusted AI ecosystem - one where organisations and consumers can benefit from the opportunities created by generative AI.\n\nTo do so, policymakers should enable greater adoption, as well as put in place guardrails to address the risks and ensure safe and responsible use.\n\nThis requires a systems approach.\n\nThe various recommendations should be looked at in totality, as we seek to learn, iterate, and evolve with the rapidly advancing technology.\n\nA practical, risk-based, and accretive approach will contribute to enhanced trust and safety as AI continues to evolve.\n\nIn doing so, we may wish to consider the following six dimensions.",
    "### Accountability\n\nAs more AI applications are built on top of foundation models, a shared responsibility framework among parties in the development lifecycle will clarify accountability and incentivise safer outcomes.\n\nThis will further benefit from enhanced transparency, such as via standardised information about the model for deployers to make proper risk assessments.\n\nFinally, labelling/watermarking of AI-generated content will allow consumers of content to make more informed decisions and choices, and allow remedial actions to be taken if harmful content is distributed.",
    "### Data Use\n\nData has significant impact on model performance, with direct implications for privacy, copyright, and bias.\n\nTransparency on the type of training datasets is an important consideration so that the wider community is aware of the input factors that go into the model.\n\nIn turn, policymakers also need to clarify ambiguity around the requirements for data privacy and copyright under their respective regulations (e.g., legal basis for using Internet data for model training and legality of mimicking styles under copyright laws).\n\nTo address embedded bias, there should also be consideration on collaboratively building trusted data sources, which act as a reference.",
    "### Model Development and Deployment\n\nDesign choices by generative AI developers in the model development and deployment have an impact on downstream organizations that are using these models to develop their AI applications.\n\nTo build and deploy safer models, model developers should be transparent about how their models are developed and tested, and should monitor performance in partnership with application deployers.\n\nWhen done objectively, this enables systematic evaluation and comparison of models for improvements.\n\nPolicymakers can support through facilitating the development of standardised evaluation metrics, as well as a corpus of tools and capabilities.",
    "### Assurance and Evaluation\n\nThere is value for independent third-party evaluation and assurance to provide objective assessments.\n\nIn addition, given the diversity of generative AI use cases and risks, there is significant value to crowd in open-source expertise (via a vibrant open-source community) for tool development as well as “adversarial testing”, especially as models become larger and more complex.\n\nSuch an evaluation approach should be practical and risk-based.",
    "### Safety and Alignment Research\n\nMore fundamentally, as AI models become more powerful, we need to ensure that human capacity to control AI systems keeps pace.\n\nDevelopment in safety and alignment lags behind that of generative AI development.\n\nPolicymakers need to invest strategically to accelerate safety and alignment research, especially in more advanced techniques, to enable interpretability, controllability, and robustness.\n\nThis effort should also nurture centres of knowledge in Asia and other parts of the world, to complement the ongoing efforts in the US and EU.",
    "### Generative AI for Public Good\n\nResponsible AI must ultimately be about achieving Public Good.\n\nConsumer literacy programmes will help raise public understanding and improve safe use.\n\nEnhanced education and training is also needed to build skills, given the anticipated changes to jobs.\n\nFurthermore, to make generative AI accessible to all enterprises, especially small and medium enterprises (SMEs), policymakers can help by providing an updated set of guidance for organisations, as well as common infrastructure so that the wider ecosystem can more easily develop and test generative AI models and applications.\n\nAs the impact is ultimately on the end-users, measurement and understanding of the end-user impact will inform ongoing policy innovation.\n\nFinally, as the impact of technology does not respect borders, we need to collaborate globally, and create platforms to bring in diverse stakeholders to the ongoing conversation.\n\nThese dimensions will be unpacked further in the subsequent chapter.\n\nCollectively, they seek to fulfil the core principles of accountability, transparency, fairness, explainability, and robustness – that enable AI to be safe, trusted, and used for the Public Good.",
    "#### Model Development – Clearer Accountability Across Stakeholders\n\nModels should have safety-by-design as a key consideration.\n\nClearer accountability of stakeholders across the model development lifecycle will incentivise safer outcomes.\n\nWhile there is general consensus in software development that individual stakeholders should be responsible for faults attributable to their respective modules, identifying what caused an error in an AI application is a complicated task.\n\nInteractions between the different codes contributed by the generative AI model (as a base layer), and the application developers (that ride on top), are challenging to parse out individually.\n\nWhile the allocation of responsibility and liability is a complex topic, there is space for policymakers to facilitate and co-create with developers a shared responsibility framework (the core concept exists today in adjacent domains such as cloud deployment) as a first step.\n\nThe framework aims to clarify the responsibilities of all parties in the model development lifecycle, as well as the safeguards and measures they need to respectively undertake.\n\nThis framework will further benefit from greater transparency about the inherent capabilities and limitations in their models, as well as the safeguards that they have undertaken to mitigate risks.\n\nWhile developers do share information about their models (these exist in some basic form today e.g., model cards), it is at times incomplete.\n\nPolicymakers can therefore work with model developers to enhance transparency via a set of information disclosure standards.\n\nA layman analogy is akin to “nutrition labels” on our food products.\n\nSome elements to include are (i) model capabilities, limitations and evaluation outcomes, including areas where there is uncertainty, (ii) datasets used for training, (iii) mitigation measures already implemented within model design, and (iv) intended and restricted use.\n\nPolicymakers and developers need to strike a balance between comprehensiveness and practicality - on one hand to have relevant and useful information to conduct risk assessments, and on the other, to address legitimate concerns around protecting commercially sensitive information.",
    "# Ransparency, and Allow Consumers of Content\n\nSynthetic media technology providers (e.g.\n\nmodel developers, application deployers) and content distributors (e.g.\n\nsocial media platforms, broadcasting companies) should invest in capabilities on their platforms to detect and “label/watermark” AI-generated content.\n\nFor example, synthetic media technology providers may need to incorporate some form of cryptographic content provenance mechanisms (see C2PA standards for illustration) or other such techniques into the model/synthetic media tool to enable people or machines to distinguish AI-generated from human-generated content.\n\nUsers of such technology, including the wider community of content creators, should also subscribe to positive norms and be transparent about their use of synthetic media/generative AI.\n\nIn the same vein, content distributors play an important role in (i) disclosing when generative AI content is detected; and (ii) taking timely corrective action when harmful generative AI content is distributed.\n\nSome content distributors like TikTok and Google have already started implementing such labeling policies/tools.",
    "## Transparency on Type of Data\n\nData is a critical component of generative AI with significant impact on model performance and output.\n\nWith due regard to the vastness of the training dataset, transparency on the type of input data remains an important principle to enable deployers and end-users to better anticipate how a model might behave and adopt safeguards.",
    "## Clarity on Data Privacy and Copyright\n\nThe unique characteristics of generative AI have led to new legal ambiguities on data use.\n\nFor example, under data privacy laws like the EU General Data Protection Regulation, the legality or legal basis of using Internet data containing publicly available personally identifiable information (PII) to train foundation models is unclear.\n\nUnder Singapore’s Personal Data Protection Act, while organisations may collect and use information from the public internet without the need to seek consent from the affected individuals so long as the collection and use are reasonable, the reasonableness of trawling the Internet for training data still needs to be established.\n\nUnder copyright law, it is also unclear at times whether the output from generative AI models infringes copyright, such as when generated content mimics style and brand identity to the detriment of the original creators.\n\nPolicymakers should therefore interpret existing laws in a transparent and facilitative manner, while providing guardrails.\n\nThis can be through issuing initial data privacy and copyright guidelines for generative AI to clarify how to treat questions of privacy and copyright and the relevant requirements (e.g.\n\nprovide recourse for data subjects to correct inaccurate PII in model outputs, disclose use of copyrighted material in training data), while facilitating the valid use of data for the continued development of generative AI.",
    "## Addressing Bias\n\nWhile recognizing that it is not possible to completely eradicate bias in the AI system, each party can play their part to minimize bias.\n\nThe definition of bias is context-specific.\n\nRegulators and policymakers need to consider if there is legal ambiguity introduced by generative AI that warrants further clarity on their part.\n\nModel developers have a role to play by being more selective of their training datasets.\n\nIn turn, application deployers should also implement downstream measures to mitigate data risks where possible.\n\nFor example, if models are already pre-trained with data containing embedded bias, deployers could consider using trusted data repositories, such as their own datasets, that the model could reference to improve the model output as part of the application design and engineering.\n\nThere is also space to consider collaboratively building and expanding access to more of such trusted data sources.",
    "# Model Development and Deployment\n\nModel developers’ design choices directly impact the quality and safety of the models.\n\nTo ensure safer outcomes, developers need to be transparent about the model development and deployment in objective and consistent ways.\n\nThis in turn enables systematic assessment about how the models are developed, tested and monitored in deployment, and comparisons of different models by the wider community.\n\nIt also allows application deployers to make well-informed risk management decisions.\n\nHowever, evaluation of generative AI models today is nascent and developers each use their own benchmarks.\n\nTests for generative AI are largely still being researched.\n\nNew evaluation metrics and techniques are required because traditional AI evaluation tools (e.g.\n\nfor supervised classification or regression models) are not directly transferable to generative AI.\n\nIn these early days of the technology where there is a need to balance risk mitigation with meaningful experimentation, a no-regrets move for policymakers is to facilitate the development of standardized evaluation metrics and tools.\n\nThis is not limited to proprietary models but would also be useful for open-source models.\n\nTo illustrate, model qualities in the evaluation metrics could include the following components:\n\n- Model safety - evaluation of qualities based on internationally recognized principles (e.g.\n\nfairness, explainability and robustness) and specific harms (e.g.\n\nmemorization and copyright, toxicity generation);\n\n- Model performance of specific tasks (e.g.\n\nsummarization, information retrieval) and use cases (selected based on material impact to consumers); and\n\n- Model efficiency and environmental sustainability, such as training energy cost and training CO2 transmissions.\n\nWith the use of tremendous compute to train and use generative AI models, it is important to seed energy use and sustainability as key considerations early on in this policy discussion.\n\nThe importance of evaluation is commonly recognized by many jurisdictions, most recently by G7 in the Hiroshima AI Process, as well as by the US and EU in the Trade and Technology Council’s Joint Roadmap on Evaluation and Measurement Tools for Trustworthy AI and Risk Management and by the UK in its AI Assurance Roadmap.\n\nJoint collaboration among policymakers to develop the evaluation metrics would therefore be an important next step to prevent fragmentation of AI evaluation metrics.\n\nAs generative AI grows in impact, global discussions are also shifting towards new AI regulation for greater government control over the model development, based on key ‘control points’ throughout the model development lifecycle, such as controlling access to open-source models.\n\nWhile it is possible to legislatively push through these checks and controls, there are practical considerations regarding implementation and effectiveness.\n\nGovernment capacity will need to be enhanced, and technical tools, standards, and technology to support regulatory implementation need to be ready before regulation can be effective.\n\nAmidst the pressure to regulate, it is also useful to consider whether existing laws, such as sectoral legislation and data protection laws, can be tapped on and updated if necessary, particularly when addressing deployment and downstream use of AI systems.\n\nAt the same time, strongly interventionist regulations should be carefully considered to tread the balance between risk mitigation and market innovation.\n\nFor example, overly restrictive regulation on open-source models can stifle innovation by hindering collaboration and access.\n\nFurthermore, the different release methods (from fully closed, staged release, hosted access, API access to downloadable and fully open) have their own benefits and trade-offs.\n\nPolicymakers need to consider the appropriate method, given the context and requirements.\n\nCareful deliberation and a calibrated approach towards regulation should therefore be taken while investing in capabilities and development of governance standards and tools.",
    "# Enhancing Evaluation and Assurance\n\nThird-party evaluation and assurance is an important part of the AI ecosystem for enhanced credibility and trust.\n\nIt helps to validate the trustworthiness of AI systems and brings an external perspective that can help uncover potential biases or flaws.\n\nIn the longer term, the adoption of standardized evaluation metrics would promote an interoperable approach towards AI governance and testing.\n\nAs it evolves, it could also eventually lead to the development of more institutionalized and thorough processes to ensure safety, similar to how drug safety is monitored and tested today.\n\nCrowding in open-source expertise will be critical in growing a vibrant ecosystem for third-party testing of AI systems.\n\nNo single entity can develop all the evaluation metrics and tools to address the wide range of contexts and use cases that generative AI can be applied to.\n\nMoreover, diverse perspectives are needed to discover new and emerging AI risks as models become larger and more complex.\n\n“Crowding in” (via open-source and an open-source community) will be key.\n\nThis is a known modality in software development.\n\nFor example, cybersecurity has demonstrated how harnessing ecosystem-wide capabilities can help address fast-evolving threats.\n\nAI testing can draw useful lessons from this domain to enhance overall security and robustness of models (e.g.\n\nvulnerability reporting norms, red-teaming and bounty programmes which could be extended to discovery or tracking of AI harms and vulnerabilities).",
    "# Safety and Alignment Research\n\nAs AI potentially surpasses human capabilities, there are concerns around ensuring that models are interpretable, controllable, robust, and aligned with human objectives and values.\n\nSafety and alignment efforts aim to address these concerns through novel techniques.\n\nThe investment and knowledge in this space today lags the actual development of generative AI.\n\nA global concerted effort is required.\n\nPolicymakers should invest in growing the safety and alignment research strategically to ensure that our capacity to control generative AI systems keeps pace with the potential risks.\n\nFor example, enhancing interpretability through mechanisms to report the internal logic used to produce output, enabling controllability such that AI systems perform within acceptable bounds, and strengthening robustness with design features to ensure that AI systems are robust against failures, vulnerabilities, and adversarial attacks.\n\nThere is also a strategic need to nurture a safety and alignment research ecosystem in Asia and other parts of the world, to complement ongoing efforts in the US and EU.\n\nThis is to bring in diverse safety priorities and ethical norms from around the world for the development of safer and more aligned models for the future.\n\nIt will also help to accelerate R&D by tapping on global capabilities and capacity.",
    "# Generative AI for Public Good\n\nResponsible AI must ultimately be about how AI can be harnessed for the Public Good.\n\nPolicymakers have a role to facilitate societal transition and ensure that the people and enterprises are ready to reap the opportunities afforded by generative AI in an inclusive manner.\n\nPublic-private partnerships will be a key avenue to accelerate work in this area, given the diversity of views and resources that can be pooled.\n\nWhile the public has taken to using generative AI applications, there remains a fairly low level of awareness as to how generative AI works, and how to use it safely and appropriately.\n\nConsumer literacy programs can help raise public understanding and improve safe and responsible use.\n\nPolicymakers also have a role to enhance education and training to build skills, given the anticipated impact on jobs due to generative AI.\n\nFurthermore, it is important that generative AI technology is accessible to all, including smaller and less well-resourced companies.\n\nTo facilitate adoption of the technology, and in a responsible and effective way, policymakers can help by highlighting use cases to demonstrate ways in which generative AI can add business value or enhance productivity, and providing guidelines, which could include measures that organizations can implement to mitigate risks and improve safety.\n\nIn addition, policymakers should consider providing common infrastructure that the wider ecosystem, e.g.\n\nresearchers, smaller companies, can use to develop and test generative AI models and applications.\n\nThis could also be used to draw in the wider community to develop applications and to better leverage generative AI for social good.\n\nThe ultimate measure of effectiveness is the safety and level of impact on the end-user.\n\nThe judgment and assessment around impact must therefore be the guiding principle, to enable AI use to be human-centric and trusted.\n\nDevelopment of measures to quantify that impact will inform policy innovation that will naturally continue to evolve with the technology.",
    "# Conclusion\n\nWhile it may be difficult to achieve global consensus on policy approaches, the ideas proposed in this paper seek to foster greater global collaboration by sharing ideas and practical pathways.\n\nIn doing so, these ideas hopefully provide a common baseline for understanding among different jurisdictions.\n\nAs generative AI is still in the early stages of development and its implications are not fully understood, these are initial steps to strengthen the foundation established by earlier governance frameworks.\n\nIn some ways, these ideas are not unique - there is space to work closely with a coalition of like-minded jurisdictions, industry partners, and researchers towards a common global platform and better governance frameworks for generative AI.",
    "# Further Reading\n\n- OECD: AI Language Modes: Technological, Socio-Economic, and Policy Considerations\n- Partnership on AI: Responsible Practices for Synthetic Media\n- Future of Life Institute: Policy Making in the Pause\n- IBM: A Policymaker’s Guide to Foundation Models\n- Google: A Policy Agenda for Responsible Progress in Artificial Intelligence\n- Microsoft: Governing AI: A Blueprint for the Future\n- OpenAI: GPT-4 System Card\n- Hugging Face: Evaluate Measurement\n- Percy Liang et al.\n\n: Holistic Evaluation of Language Models\n\nAt IMDA, we see ourselves as Architects of Singapore’s Digital Future.\n\nWe cover the digital space from end to end, and are unique as a government agency in having three concurrent hats - as Economic Developer (from enterprise digitalization to funding R&D), as a Regulator building a trusted ecosystem (from data/AI to digital infrastructure), and as a Social Leveller (driving digital inclusion and making sure that no one is left behind).\n\nHence, we look at the governance of AI not in isolation, but at that intersection with the economy and broader society.\n\nBy bringing the three hats together, we hope to better push boundaries, not only in Singapore but in Asia and beyond, and make a difference in enabling the safe and trusted use of this emerging and dynamic technology.\n\nAicadium is a global technology company delivering AI-powered industrial computer vision products into the hands of enterprises.\n\nWith offices in Singapore and San Diego, California, and an international team of data scientists, engineers, and business strategists, Aicadium is operationalizing AI within organizations where machine learning innovations were previously out of reach.\n\nAs Temasek’s AI Centre of Excellence, Aicadium identifies and develops advanced AI technologies, including areas of AI governance, regulation, and the ecosystem developments around AI assurance.\n\nLearn more at aicadium.ai.\n\nRecognizing the importance of collaboration and crowding in expertise, Singapore set up the AI Verify Foundation to harness the collective power and contributions of the global open-source community to build AI governance testing tools.\n\nThe mission of the AI Verify Foundation is to foster and coordinate a community of developers to contribute to the development of AI testing frameworks, code base, standards and best practices.\n\nIt will establish a neutral space for the exchange of ideas and open collaboration, as well as nurture a diverse network of advocates for AI testing and drive broad adoption through education and outreach.\n\nThe vision is to build a community that will contribute to the broader good of humanity, by enabling trusted development of AI.\n\nIMDA and Aicadium are members of the Foundation.",
    "# Disclaimer\n\nThe information in this report is provided on an “as is” basis.\n\nThis document was produced by IMDA and Aicadium based on information available as of the date of publication.\n\nInformation is subject to change.\n\nIt has been prepared solely for information purposes over a limited time period to provide a perspective on generative AI and the implications for trust and governance.\n\nIMDA and Aicadium make no representation or warranty, either expressed or implied, as to the accuracy or completeness of the information in the report and shall not be liable for any loss arising from the use hereof.\n\n© COPYRIGHT IMDA AND AICADIUM 2023.\n\nALL RIGHTS RESERVED.\n\nThe authors have made a good faith attempt to identify and attribute credits to authors / source of any third party’s work in this paper, if you have any queries or concerns regarding ownership / authorship of the relevant materials, please do not hesitate to reach out to .",
    "## Executive Summary:\n\nGenerative Artificial Intelligence (AI) is a new branch of AI technology that can generate content—such as stories, poetry, images, voice, and music—at the request of a user.\n\nMany organizations have banned Generative AI, while others allow unrestricted usage.\n\nThe City recognizes the opportunity for a controlled and responsible approach that acknowledges the benefits to efficiency while minimizing the risks around AI bias, privacy, and cybersecurity.\n\nThis is the first step in a collaborative process to develop the City’s overall AI policy.\n\nRegistered users will be invited to join the Information Technology Department in a working group to share their experience and co-develop the City’s AI policies.\n\nAt a baseline, users must follow these rules while using Generative AI for City work, this includes direct services like ChatGPT and extensions like Compose.ai:\n\nInformation you enter into Generative AI systems could be subject to a Public Records Act (PRA) request, may be viewable and usable by the company, and may be leaked unencrypted in a data breach.\n\nDo not submit any information to a Generative AI platform that should not be available to the general public (such as confidential or personally identifiable information).\n\nReview, revise, and fact check via multiple sources any output from a Generative AI.\n\nUsers are responsible for any material created with AI support.\n\nMany systems, like ChatGPT, only use information up to a certain date (e.g., 2021 for ChatGPT).\n\nCite and record your usage of Generative AI.\n\nSee how and when to cite in the “Citing Generative AI” section.\n\nRecord when you use Generative AI through this form.\n\nCreate an account just for City use to ensure public records are kept separate from personal records.\n\nSee “Getting started with Generative AI for City use.” If a user agrees to\n\nthe terms and conditions of a system that the City does not have a formal agreement with, he/she is responsible for complying with those terms and conditions.\n\nDepartments may provide additional rules around Generative AI.\n\nConsult your manager or department contact if there are additional department-specific rules.\n\nRefer to this document quarterly, as guidance will change with the technology, laws, and industry best practices.\n\nCheck the “Change Log” to identify changes.\n\nBookmark this link for easy access to the latest doc.\n\nYou can subscribe to updates to the guidelines here.\n\nUsers are encouraged to participate in the City’s established workgroups to help advance AI usage best practice in the City and enhance the Guidelines.\n\nSee “Joining AI Working Group” section.",
    "## Definitions\n\nUser: staff, contractors, or others using Generative AI for City work purposes\n\nCity: the city government of San José located in California, United States of America\n\nGenerative AI: a machine that automatically creates content such as text, audio, or image\n\nArtificial Intelligence (AI): machines doing tasks that typically require human intelligence\n\nMachine Learning: a type of AI in which computers use data to “learn” tasks through algorithms\n\nAlgorithm: a set of steps, such as mathematical operations (e.g., addition) or logical rules",
    "## Purpose of Guidelines\n\n“Generative AI”, such as ChatGPT, grew from a niche topic to a variety of publicly available tools with hundreds of millions of adopters in less than one year.\n\nAmong other things, Generative AI presents an incredible opportunity for people to increase their efficiency and efficacy in work.\n\nGenerative AI has also been used for several irresponsible applications including faking news headlines, leaking personal information, and enabling phishing cyber-attacks.\n\nThe City is actively working to create policies and procedures around AI in general.\n\nThis document serves as part of an evolving governance structure around responsible AI usage.",
    "## Application of the Guidelines\n\nThis document applies to all use of Generative AI by a City staff member, contractor, volunteer, or other person while performing a role for the City (collectively “users”).\n\nThis document does not apply to users of Generative AI for personal purposes or business purposes unassociated with the City.\n\nGenerative AI does not refer to algorithms that a person directly defines.\n\nFor example, a spreadsheet a human created to calculates taxes owed based on income is not “Generative AI”.\n\nA general rule is that if you cannot write the system’s entire algorithm, either because you do not understand the math or because it would take years to write down, then it is probably AI.\n\nDepartments may provide additional rules on the usage of Generative AI.\n\nUsers should consult their manager if there are additional rules specific to their department.",
    "## Principles for Using Generative AI\n\nUsage of Generative AI shall follow the City’s AI principles:\n\n- **Privacy:** Submit information to Generative AI tools that is ready for public disclosure.\n\nThis includes any text, photos, videos, or voice recordings you share with the AI.\n\nBe mindful that the AI output may include unexpected personal information from another user and ensure removing any potential private information before publishing.\n\n- **Accuracy:** The City maintains trust with its residents and partners by providing accurate information.\n\nReview and fact check all outputs you receive from a Generative AI.\n\nUsers should consult trustworthy sources to confirm that the facts and details in the AI-generated content are accurate.\n\nTrustworthy sources include official City documents and peer-reviewed journals.\n\nConsult your supervisor for other trustworthy sources (e.g., newspapers, blogs, or datasets).\n\nBe aware that many systems, like ChatGPT, may only use information up to a certain date (e.g., 2021 for ChatGPT) and cannot guarantee the content they generate is accurate.\n\n- **Transparency:** The user shall be clear when he/she uses Generative AI.\n\nThis can often include citing that you used AI in creating a product.\n\nSee how and when to cite Generative AI in the “Citing Generative AI” section under “Guidance while Using Generative AI”.\n\n- **Equity:** AI system responses are based on patterns and relationships learned from large datasets derived from existing human knowledge, which may contain errors and is historically biased across race, sex, gender identity, ability, and many other factors.\n\nUsers of Generative AI need to be mindful that Generative AI may make assumptions based on past stereotypes and need to be corrected.\n\nEstablish guidelines to address equity as it relates to services in your department.\n\n- **Accountability:** The person using AI is accountable for the content it generates.\n\nUse Generative AI with a healthy dose of skepticism.\n\nThe level of caution used should correspond to the risk level of the use case (see “Assessing Risk in Generative AI Use Cases”).\n\nIt is always important to verify information provided by Generative AI.\n\n- **Beneficial:** User should be open to responsibly incorporating Generative AI into their work where it can make services better, more just, and more efficient.\n\nFor example, a tool like ChatGPT can help users go from an outline to a draft Council memorandum quickly, enabling them to focus more time on the analyses and findings that inform recommendations to Council.",
    "### Usage of Generative AI may be Subject to the Public Records Act\n\nAny retained conversations relating to City work may be subject to public records requests and must comply with the City’s retention policies.\n\nIn addition, users will need to comply with the California Public Records Act and other applicable public records laws for all City usage of Generative AI.\n\nThis means any prompts, outputs, or other information used in relation to a Generative AI tool may be released publicly.\n\nDo not use any prompts that may include information not meant for public release.",
    "### Create an Account Specifically for City-related Work\n\nIf you choose to use Generative AI for City-related work, you shall have an account for all Generative AI usage in your role at the City using a City email address.\n\nThe purpose of this is to ensure proper retention of public records and avoid comingling of public and personal records.\n\nThis account should not be used for any personal purpose.\n\nUsers can use their City email address for City usage, or they can create a shared account using a different work email address.\n\nFor example, the Digital Privacy Office might create a shared ChatGPT account using the  email address.\n\nRegardless of whether a shared or work email address is used to create an account, users should use a unique password for the service.\n\nLike any other account which uses a City email address, the password should not be the same password used to log in to any City devices.\n\nFor example, if a data breach occurs on ChatGPT (which happened in March 2023) and your password is stolen, a hacker should not be able to log into your laptop with that information.\n\nIf users use personal devices or accounts to conduct City work, the records generated may still be subject to search and disclosure.\n\nThe records generated may include both the content users input and the content users receive from the Generative AI system.",
    "### Understand the Terms and Conditions\n\nThe City does not currently have agreements in place for common Generative AI systems, such as ChatGPT or Bing AI.\n\nIf you choose to use Generative AI for City work and agree to the terms and conditions of a system without a City agreement in place, you are responsible for complying\n\nwith those terms and conditions.\n\nIn the event that the City forms an agreement with a Generative AI service, this section will list those services.\n\nOpt out of data collection if possible\n\nSome services offer an option to opt out of data collection.\n\nThis means the generative AI system will not keep the data you provide, and it will not be used in the system’s models.\n\nOpt out of data collection and model training whenever possible.\n\nFor example, you can opt out of ChatGPT by going to “settings” -> “data controls” -> “chat history and training”.",
    "### Verify the Copyright of All Generated Content\n\nUsers shall verify the content they use from any Generative AI systems does not infringe any copyright laws.\n\nFor example, City employees could check the copyright of text-based content with plagiarism software and the copyright of image-based content with reverse Google searches, although neither of these approaches guarantees protection against copyright infringements.\n\nIf users are uncertain if content violates copyright, they should either edit the content to be original or not use it.",
    "### Ownership of Generated Content\n\nIn most cases, the user owns the content they input into a Generative AI service and the information they receive as an output.\n\nThe user can use the content at their discretion, in accordance with City policy and any terms and conditions he/she has agreed to.\n\nHowever, many Generative AI companies still retain the right to use both the input and output content for their own commercial purposes.\n\nFor example, this could include a Generative AI company using City data to train their models or distributing City output data for marketing campaigns.\n\nThis emphasizes the importance that only information the City is ready to make public should be entered into a Generative AI system.",
    "## Joining AI Working Groups\n\nThe City is dedicated to providing practical guidance around AI that protects people from harm while providing the best services to residents.\n\nTo accomplish this, the City has three engagement groups dedicated to informing AI use in the City:\n\n- **City AI working group:** City staff discuss AI policy, use cases, and guidelines.\n\nUsers can learn more about AI in the City, discuss potential ideas in their departments, and flag any potential concerns.\n\n- **Digital Privacy Advisory Taskforce:** External Taskforce of experts around Digital Privacy and AI.\n\nThe Taskforce advises and recommends on the City’s digital privacy practices, including responsible AI.\n\n- **GovAI Coalition:** The City of San José is collaborating with government agencies across the country to ensure that the AI systems we use serve all of our communities.\n\nThe group collaborates on items including responsible AI governance, vendor accountability, and sharing use case experiences.\n\nIf you are an agency interested in joining, you can do so at sanjoseca.gov/govai.\n\nIn addition to these three groups, the City holds opportunities for the public to provide feedback, including in-person sessions in San José, virtual sessions, and online at sanjoseca.gov/digitalprivacy.\n\nMembers of the public are also able to contact the Privacy and AI team directly at .",
    "### Citing Generative AI\n\n- **When to Cite:** Users must cite the Generative AI when a substantial portion of the content used in the final version comes from the Generative AI.\n\nA “substantial portion” will be further defined in future working group discussions.\n\nAny statements used as fact must cite a credible source rather than\n\nthe AI.\n\nCredible sources include official City documents and peer-reviewed journals.\n\nConsult your supervisor for other trustworthy sources (e.g., newspapers, blogs, or datasets).\n\nAll images and videos must cite any AI used in their creation, even if the images are substantially edited after generation.\n\n- **How to Cite:** Generative AI can be cited as a footnote, endnote, header, or footer.\n\nCitations for text-generated content must include the following:\n\n  - Name of Generative AI system used (e.g., ChatGPT-4, Google Bard, Stable Diffusion)\n  - Confirmation that the information was fact-checked.\n\nFor example: \"This document was drafted with support from ChatGPT.\n\nThe content was edited and fact-checked by City staff.\n\nSources for facts and figures are provided as they appear.\"\n\nCitations for images and video must be embedded into every frame of the image or video.\n\nFor support on how to do this, see the “Creating Images or Video” use case in the appendix or reach out to .",
    "### Recording usage of Generative AI\n\nThe City needs to understand how users are using Generative AI tools in their work.\n\nWhen you choose to use Generative AI to support your work, report that usage through this form:  The form will take 1 minute.\n\nYou do not need to wait for a response after filling out the form to use Generative AI unless required by your department or manager.\n\nThis is only meant to track usage in aggregate.\n\nAdditional guidance and advice around using Generative AI can be found in the Appendix.",
    "## Assessing Risk in Generative AI Use Cases\n\nThe risk presented by Generative AI tools varies by use case, with the risk spectrum ranging from mid-risk to high-risk to intolerable risk.\n\nGenerative AI risk is determined by two key factors:\n\n- **Risk of information breach:** the potential harm if the information exchanged with a Generative AI is released to an unintended audience.\n\nThis can include entering personally identifiable information, sensitive records, or confidential business information into Generative AI.\n\nAdditionally, any information entered into Generative AI may be subject to the Public Records Act.\n\nIf you wouldn’t share the information in a public forum, don’t share it with a Generative AI.\n\n- **Risk of adverse impact:** the potential harm of using the output for a decision, task, or service.\n\nThis impact can be different for different populations and should be considered from an equity lens, such as adverse impacts to people of a certain race, age, gender identity, or disability status.\n\nNot only can AI be biased, but it can also provide false information.\n\nIn general, if Generative AI is used in relation to City processes that can alter an individual or community’s rights, freedoms, or access to services, it should be thoroughly reviewed by multiple users before any document is finalized or action is taken.",
    "### When Engaging in High-risk Use Cases\n\nKeep in mind the tone and specific language in the AI output.\n\nGenerative AI is trained on a global context and may not use the vocabulary or tone consistent with the City and its values.\n\nSimple examples include replacing “citizen” with “resident” in documents, and capitalizing “City” when referring to the City of San José.\n\nThese documents, like any others, require thorough review before moving from draft to final product.\n\nCite verifiable sources for all facts and figures (past memos, newspapers, research papers, etc.).\n\nChatGPT or other Generative AI are not definitive sources.\n\nFacts should be accompanied by links or citations to sources that the general public could find, such as news articles or research papers.\n\nChatGPT and other AI can fabricate sources if asked, so do not rely on them for finding citations either.\n\nFind sources directly and confirm they are legitimate before using.\n\nAnything that would not be released or shared with the public should not be input into the AI.\n\nThis includes information such as draft RFP requirements that should not be public yet, vendor transactions, procurement approvals, or internal City decisions.\n\nAdditional details on risk can be found in the Appendix.",
    "## Concluding Thoughts\n\nGenerative AI presents users an opportunity to work better, faster, and smarter.\n\nHowever, because the technology and the laws surrounding it are evolving and present unknown risks, its adoption comes with ethical considerations.\n\nRemember the fundamental rules when using any Generative AI:\n\n- Never submit personal or confidential information into a Generative AI.\n\n- Review, revise, test, and fact check any output from a Generative AI.\n\n- Be transparent when content was drafted using Generative AI.\n\n- Return to this document often, as guidance on usage will change rapidly.\n\nBy keeping the above guidance in mind when using generative AI tools, we can ensure the safe and responsible use of AI by the employees of the City.\n\nIf you or your department has any questions, comments, or concerns around using Generative AI, please contact your team at .\n\nThe Privacy office can provide users trainings, set up AI evaluations, and help your team do the best with Generative AI.",
    "### A Definition of Generative Artificial Intelligence\n\nGenerative Artificial Intelligence, commonly referred to as “Generative AI” or “GenAI”, is an “automated system” used to generate “content”.\n\nAn \"automated system\" is any system, software, or process that uses computation as part of a system to generate outputs, outcomes, make or aid decisions, inform policy implementation, collect data or observations, or otherwise interact with individuals and/or communities.\n\n“Content” includes text, emails, presentations, images, video, audio, architectural documents, diagrams, and other forms of media.\n\nGenerative AI uses massive datasets to generate content that someone would want given a prompt (see definition of “prompt” below).\n\nFor example, ChatGPT has collected data on millions of webpages to identify sentence patterns that commonly come next after someone types a phrase.\n\nOnline information is paired with human training where algorithm developers manually judge and correct the output of the system.\n\nFor example, it may have required a combination of millions of webpages and a human developer to train ChatGPT that “Jack fell down, and broke his crown” should be completed by “and Jill came tumbling after.”\n\nBillions of images are shared online every day, along with hundreds of thousands of hours of video and countless text posts.\n\nMuch of this information is connected to other information on the internet.\n\nFor example, pictures of cats are often connected with captions that have the word “cat” in them.\n\nThese connections allow a computer to, after millions of connections, “learn” what a cat looks like.\n\nEventually, a computer can create an image of a cat based on all the previous images it has seen.\n\nAI systems apply this same approach to music, books, poems, voices, videos, and anything else created on the internet.",
    "### Prompts and Generative AI\n\nGenerative AI relies on a user (e.g., a person) to “prompt” the AI to generate content.\n\n“Prompts” are any direction provided by a user.\n\nExamples of Generative AI include:\n\n- Creating text based on a prompt\n- Creating a picture or video based on a prompt\n- Making an audio file of a famous person saying something they did not say\n- Creating a movie scene based on a text prompt and pictures of the characters\n\nExamples of prompts include:\n\n- Text prompt to generate text content.\n\nFor example: “Tell me a story about three people becoming friends despite their differences”\n- Text prompt to generate picture/video content.\n\nFor example: “Draw a cow with long hair and an ornate bell”\n- Voice and text prompt to generate audio content.\n\nFor example: [Upload a recording of Tim Cook] “Say ‘I’ll just warn you now, I don’t know how to use a computer’ in the voice provided.”",
    "#### Understanding “Risk of Information Breach”\n\nGeneral rule: If the information exchanged with a Generative AI system would be harmful to a person or community if made public, it is a high or intolerable risk.\n\nServices like ChatGPT have been compromised in the past and leaked personal information.\n\nUntil private applications with higher security are deployed in the City, all information exchanged with Generative AI has a reasonable risk of being compromised.\n\n- **Mid-risk information** includes non-identifying and non-confidential information.\n\nFor example, a simple email response or instructive documents often contain only general information that would not present any risk if made public.\n\n- **High-risk information** includes personally identifiable information (e.g., full name, birth date, email address) and confidential business information that may have larger implications to City processes.\n\nUntil a private application is deployed with security measures approved by the Cybersecurity Office, no high-risk information shall be provided to a Generative AI system.\n\n- **Prohibited risk information** includes highly sensitive and identifying information.\n\nThis includes data such as credit card numbers, bank account information, social security numbers, and other information that requires rigorous security measures and compliance standards before being processed.",
    "#### Understanding “Risk of Adverse Impact”\n\nGeneral rule: If you are using Generative AI in relation to City processes that can alter an individual or community’s rights, freedoms, or access to services, it is at least high risk and should be thoroughly reviewed before any document is finalized or action is taken.\n\nAdditionally, any action that could reasonably lead to the City engaging in legal infringements on intellectual property are prohibited.\n\n- **Mid-risk impact** includes tasks associated with drafting internal messages, internal documentation, and idea generation.\n\nThese tasks can be sped up with the support of Generative AI, but require many more steps before reaching a public impact.\n\n- **High-risk impact** includes tasks associated with official City documents or messaging.\n\nIt also includes uses that require substantial editing and review before usage.\n\nThese tasks require thorough review at the time of generation before using in any work context.\n\nSpecial care should be taken when a task may impact individuals differently across factors such as race, age, gender identity and disability (e.g., a memo about tree canopy inequity in neighborhoods).\n\n- **Prohibited risk impact** includes tasks that undermine trust in the City through false statements or news; deny people due process such as in resource allocation, job evaluations, and purchasing decisions; or expose the City to substantial security or legal risks.\n\nGenerative AI does not have reasoning behind the content it produces and cannot justify a decision.",
    "**Drafting messages to staff and trusted partners**\n\nGenerative AI tools can help users draft emails or other messages to staff and trusted partners.\n\nChatGPT is a tool commonly used for this purpose.\n\nYou can prompt ChatGPT to provide formal sounding language from general framing of the message.\n\nYou can also have it draft emails in different tones by asking for a different tone.",
    "**Additional Guidance:**\n\nYou may be inclined to use ChatGPT to help with email replies.\n\nDo not copy your current email thread into ChatGPT.\n\nThe email was sent to select people and may be confidential.\n\nBe mindful about the purpose of the email, and if it is appropriate to use Generative AI for drafting it.\n\nFor example, Vanderbilt University received heavy backlash for using ChatGPT to draft an email in response to a school shooting.\n\nPrompt ChatGPT with the following: “I am the lead product manager for housing technology initiatives.\n\nWe interview users to gather product requirements, prioritize features, and work with software developers on implementation.\n\nDraft me an email asking the software developers how long the housing database will take to implement, and what risks to implementation they see.” Carefully read through the email, perform final fact-check and other edits to the draft email.\n\nManually add in personal information or internal confidential details before sending.\n\nCite at the end of the message “some of this content was drafted using ChatGPT.\n\nAll facts, figures, and statements were reviewed by the sender to be accurate.” If someone replies to your email asking for what you would like to see in the database, you can return to ChatGPT and prompt it with the following: “I want the database to be easily understood by our field staff, draft this request to the developers”.\n\nRead the draft, fact-check, and manually add information as needed.\n\nCite ChatGPT at the end of the message as done previously.",
    "**Framing written content not intended for official release**\n\nGenerative AI can be useful for creating an outline or structure for your written content.\n\nThis can include an outline for a cover letter, long-form writing, project documentation, or speaker notes for a presentation.\n\nWhen the written content is not intended for official public release, it presents less risk than official City publications (like memos or policies).\n\nChatGPT is the most common tool for this use case.\n\nYou can write a few key points you would like to detail, any themes you want present, the kind of voice you would like, and how long you need.",
    "## Additional Guidance\n\nUnless you have a Generative AI trained to your context—a feature likely not available for another year—the tool will provide generic language that does not apply to the City.\n\nFor example, ChatGPT may use the word “citizens” rather than “residents” when referring to the people we serve because it is not used to San José’s specific circumstances.\n\nAs always, make sure to review, revise, and fact-check any output from Generative AI.",
    "## Example Prompt Steps\n\nPrompt ChatGPT with the following: “I am writing an instruction manual for how City staff should add content to the City’s website.\n\nDraft an outline that can be posted on our intranet.\n\nIt should have a section about how to create lists, add hyperlinks, and show pictures using a content management system.\n\nDraft in a formal tone but make the text clear and approachable.”\n\n- Review, revise, and fact-check.\n\nManually enter any confidential or private information into the draft or final version.\n\n- Cite that you used ChatGPT in the drafting process.\n\nSee how to cite Generative AI in the “Citing Generative AI” section under “Guidance while Using Generative AI”.",
    "### Example Prompt Steps\n\n- Ask ChatGPT to “Summarize the following document.\n\nLet me know if there is any mention of California, cities, or San José.”\n- Copy the text from a public document and paste it into ChatGPT.\n\nAn example document would be the text from this news article:  Copy text and paste into ChatGPT.\n\n- You do not need to cite ChatGPT unless you quote specific text outputted from ChatGPT in future written content.",
    "## Brief List of Other Mid-Risk Use Cases\n\n- Helping you come up with a name for your team.\n\nFor example, “give me ten names for a team focused on AI and Privacy that uses the acronym ‘SAFE’.” Be sure the name is not already used by another team in the City.\n\n- Learning about a new topic in a way that you can understand.\n\nFor example, “explain quantum mechanics to me like I’m five”.\n\nVerify anything you learn from ChatGPT before applying the knowledge in a City context.\n\n- Helping you find the right word for a concept.\n\nFor example, “what is the word for the second-to-last episode in series”.\n\nOnce the AI provides the word, search the word on Google (or elsewhere) to confirm it means what you think it means.",
    "### Drafting Memos and Public-Facing City Documents\n\nGenerative AI tools can help users draft memos and other public-facing documents more efficiently.\n\nBecause the content is meant for public release, it is treated as high-risk and should be reviewed and edited multiple times before release.\n\nChatGPT is the tool most used for this purpose.",
    "#### Additional Guidance\n\nThe City expects users to produce their own research that informs memos, such as information related to policy changes and program changes.\n\nMemos, press releases, and other publications also have their own City-specific formats and standards to follow.\n\nConsult your supervisor to make sure your memo follows City standards.",
    "### Example Prompt Steps\n\n- Provide context around the memo but only provide public details: “We are building an encampment management work order system at the City to better coordinate services and have just completed the detailed design.\n\nWe will be presenting to the City council on the latest update.”\n- Then request ChatGPT to draft a memo: “Draft a memorandum with the following sections and key points: Introduction: (add bullet points), Human-Centered Design work (add bullet points), Requirements (add bullet points), Next-steps (add bullet points).”\n- After initial memo draft, prompt ChatGPT to “Draft a conclusion summarizing all the prior sections.”\n- Manually add in any non-public information to the draft memo produced by ChatGPT.\n\n- Carefully read through memo, perform fact-check and other edits to memo to maintain a tone consistent with City documents.\n\n- Cite verifiable sources (past memos, newspapers, research papers, etc.)\n\nfor all facts and figures in the memo.\n\n- If required, cite that you used ChatGPT in the drafting of the memo.\n\nSee how and when to cite Generative AI in the “Citing Generative AI” section under “Guidance while Using Generative AI”.",
    "#### Additional Guidance\n\nRFPs and other publications have their own City-specific formats and standards to follow.\n\nConsult your supervisor and purchasing business partner to make sure your memo follows City standards.\n\nGuidance can be found on the City intranet, including the “Strategic Procurement Guidelines for RFP and Contract Requests To Finance-Purchasing”.\n\nTake special care not to provide ChatGPT information that is not meant to be public yet.\n\nFor example, if the specific requirements of the RFP are not meant to be public yet, do not input them into your prompts.",
    "### Example Prompt Steps\n\n- Provide context around the procurement document without providing non-public details: “We are procuring an encampment management work order system at the City to better coordinate services.”\n- After ChatGPT responds, ask to draft the procurement document: “Draft an RFP with the following sections and key points: Introduction: (add bullet points), Scope of Work (add bullet points), Requirements (add bullet points), Cost Breakdown (add bullet points).”\n- Manually add in any non-public information to draft document produced by ChatGPT.\n\n- Carefully read through the document, perform fact-check and other edits.\n\n- If required, cite that you used ChatGPT in the drafting process.\n\nSee how and when to cite Generative AI in the “Citing Generative AI” section under “Guidance while Using Generative AI”.",
    "### Example Prompt Steps\n\n- Provide ChatGPT with details around the needed post and audience, for example: “Draft a cute tweet of less than 240 characters that reminds families that tomorrow is Walk and Roll to school day”\n- Review output, edit to make personal to San José and relevant department or office, and post.",
    "### Writing Job Postings or Job Descriptions\n\nIf you provide a Generative AI with a list of qualities you want and a role title, it can help you draft a formal-sounding job description.\n\nBecause the content is meant for public release, and job requirements can have a substantial impact on who applies, it is a high-risk use case.",
    "#### Additional Guidance\n\nThe City expects users to follow existing standards on the format and content of job postings based on classifications.\n\nConsult your Human Resources business partner or your Department’s HR representative for information on job classifications and postings.\n\nAdditionally, be mindful of the language used in the requirements, responsibilities, and tone used in the job posting.\n\nCheck if the job description seems to use language stereotypically associated with a specific race or gender.\n\nUse gender-neutral language: Avoid using gender-specific pronouns (he, she) and job titles (fireman, firewoman).\n\nInstead, opt for inclusive terms such as “they” and “fire officer.” Remove gender-coded words: Avoid using adjectives that may be associated with a specific gender, such as “aggressive” or “nurturing.” Use neutral descriptors, like “results-driven” or “collaborative.”",
    "### Example Prompt Steps\n\n- Provide ChatGPT with the previous public posting for an Analyst I position and request that ChatGPT “Draft a similar job description, but with a focus on using information to inform park capital projects.”\n- Manually add in any non-public information to draft document produced by ChatGPT.\n\n- Carefully read through the document and edit for a more neutral tone, perform fact-check and other edits.",
    "## Creating Images or Video\n\nSome Generative AI tools such as Stable Diffusion and Dall-E can create images or video clips based on text prompts.\n\nThe City needs to maintain its legitimacy as a trustworthy source when using video and images, which requires substantial precautions whenever using AI-generated visual content.",
    "### Additional Guidance\n\n- Use only for illustrative purposes.\n\nFor historical events, use real images rather than generated.\n\nFor example, if you want a picture of a giraffe wearing a suit and tie for your presentation, generate it.\n\nIf you are proposing a new visual diagram or abstract concept, you can also generate it.\n\nIf you want a picture of the Mayor at City Hall, find an actual picture.\n\n- Require a citation embedded into the image or video at all times.\n\nImages and videos can easily be taken out of their original context and misinterpreted as reality.\n\nTo prevent a news article or other secondary source from treating an image as fact, all images and frames of a video must specify that they were generated using an AI system.\n\nThe citation shall be included in the image itself, and cannot be removed without editing or cropping the image.",
    "### Example Use Case\n\n- Provide a prompt: “drawing of falcon and its chicks on top of a skyscraper”\n- Choose your image\n- Embed the citation into the image: “Image generated by DALLE-2”\n- Add alt text into the image that clearly states the image was generated by an AI system",
    "## Creating Presentation Slides\n\nIf you provide Generative AI with some public information, it can create a presentation for you.\n\nCurrently this feature is very new but may soon be integrated into existing applications like PowerPoint.\n\nCurrently there is no clear leader in Generative AI for presentations, but a few examples are beautiful.ai and gamma.app.\n\nPresentations are automatically high risk because they go beyond text into images, which can present false information if the audience believes the image is real.",
    "## Brief List of Other High-Risk Use Cases\n\nRemember to follow the AI principles and general guidance for high-risk use cases.\n\n- Creating diagrams.\n\nFor example, “create a flow chart of a tree turning into wood pulp and then into paper”.\n\nReplace pictures before publishing\n- Drafting papers.\n\nFor example, “Here is my outline for my research paper, and my findings, draft a complete paper.”",
    "### Programming or Coding\n\nWhy it is prohibited: Code generated by an AI may be outdated, copyrighted, have identified vulnerabilities, or rely on other code that no longer works.\n\nThe generated code is not cited to a date (like a stack overflow post would be), so it is unclear when the code would have been good.\n\nWhat can you do with Generative AI: AI can help frame your coding problem, and help you draft pseudo-code to solve your problem conceptually.\n\nYou can request code snippets for help defining syntax, and can be useful for testing projects in a low-risk, non-production environment.",
    "### Evaluations and Decisions\n\nWhy it is prohibited: Evaluating job applicants using AI has led to countless scandals of biased application reviews.\n\nThis evaluation issue also extends to other areas such as evaluating proposals or an existing employee.\n\nAI-based evaluations expose the City to public protest across many key City functions such as hiring and purchasing.\n\nAdditionally, Generative AI shall not be used to determine highly sensitive decisions such as an individual’s health plan, cost of bail, conviction of a crime, grades, or admissions to a program.\n\nWhat can you do with Generative AI: AI can help flag key words and identify phrases within a document (see the mid-risk use case).\n\nHowever, the actual evaluation must be made by a person.",
    "### Language Translation\n\nWhy it is prohibited: Large Language Models like ChatGPT are not yet demonstrably better for translation than something like Google Translate.\n\nGoogle Translate is also an AI system, but is built for specifically translating text, compared to modern Generative AI systems like ChatGPT, which attempt be a more general AI system for more problems.\n\nFuture Generative AI systems may be substantially better than existing translation AI systems, but they will require an evaluation of their performance before the City should use them over something like Google Translate.\n\nTranslations should be confirmed by a fluent speaker of both languages whenever possible.\n\nWhat can you do with Generative AI: Test out the system in a risk-free environment (e.g., you and a coworker testing the system), and report any translation system you would like to use to the Digital Privacy Officer for an algorithm evaluation.\n\nContact .",
    "### Be Aware of Targeted Cyber Attacks Using Generative AI\n\nAlthough City staff are already familiar with handling cyber risks like phishing and malware, the advent of generative AI introduces heightened cybersecurity risks as the attacks can be more complex and personalized.\n\nCyber threat actors may use generative AI in their attacks in the following ways:\n\n- Writing AI-powered, personalized phishing emails: With the help of generative AI, phishing emails no longer have the tell-tale signs of a scam—such as poor spelling, bad grammar, and lack of context.\n\nPlus, with AI like ChatGPT, threat actors can launch phishing attacks at unprecedented speed and scale.\n\n- Generating deep fake data: Since it can create convincing imitations of human activities—like writing, speech, and images—generative AI can be used in fraudulent activities such as identity theft, financial fraud, and disinformation.\n\n- Cracking CAPTCHAs and password guessing: Used by sites and networks to comb out bots seeking unauthorized access, CAPTCHA can now be bypassed by hackers.\n\nBy utilizing AI, they can also fulfill other repetitive tasks such as password guessing and brute-force attacks.",
    "### Detecting Generative AI\n\nSoftware developers are building tools, like GPTZero, GPT Radar, and Originality.AI, designed to detect if a body of writing was created by a generative AI tool.\n\nThese tools are in early stages of development and their detection accuracy rate may not always be accurate and should be used with caution.\n\nFor example, there have been numerous incidents of instructors using ChatGPT detection tools falsely accusing students of plagiarism, endangering their grades and even diplomas.\n\nDespite the limited accuracy of these tools, they allow residents to check if City documents were generated by AI regardless of whether users cite their usage or not.\n\nTo build trust with residents, users need to be proactive in communicating its usage of AI.\n\nResidents finding out on their own can cause reputation harm to the City.",
    "### Generative AI & Copyright\n\nNumerous copyright lawsuits are springing up in which artists are suing AI companies like Stability AI and Midjourney for unauthorized use of their intellectual property to train the Generative AI systems.\n\nLarge companies like Getty Images and Shutterstock are also joining suit against AI companies.\n\nThe US Copyright Office determined that art created solely by AI isn’t eligible for copyright protection.\n\nArtists can attempt to register works made with assistance from AI, but they must show significant “human authorship.” The office is also currently executing an initiative to “examine the copyright law and policy issues raised by artificial intelligence (AI) technology.”",
    "## Contents\n\n- Principle 1: You know what generative AI is and what its limitations are 8\n- Principle 2: You use generative AI lawfully, ethically and responsibly 8\n- Principle 3: You know how to keep generative AI tools secure 9\n- Principle 4: You have meaningful human control at the right stage 10\n- Principle 5: You understand how to manage the full generative AI lifecycle 10\n- Principle 6: You use the right tool for the job 11\n- Principle 7: You are open and collaborative 11\n- Principle 8: You work with commercial colleagues from the start 12\n- Principle 9: You have the skills and expertise that you need to build and use generative AI 12\n- Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place 12\n- What is generative AI?\n\n13\n- Applications of generative AI in government 15\n- Limitations of generative AI and LLMs 15\n- Defining the goal 17\n- Identifying use cases 17\n- Use cases to avoid 18\n- Building the team 19\n- Acquiring skills 20\n- Creating the generative AI support structure 21",
    "### Governance\n\n- AI governance board or AI representation on an existing board 73\n- Ethics committee 73\n- Creating an AI/ML systems inventory 74\n- Programme governance in teams and what should be considered 74\n\nCentral government department contributions have come from the Home Office (HO), Department for Environment, Food and Rural Affairs (Defra), Department for Business and Trade (DBT), Foreign, Commonwealth and Development Office (FCDO), Department for Science, Innovation and Technology (DSIT), Cabinet Office (CO), Department for Work and Pensions (DWP), HM Treasury (HMT), HM Revenue and Customs (HMRC), Ministry of Defence (MOD), Ministry of Justice (MOJ), Department for Levelling Up, Housing and Communities (DLUHC), Department of Health and Social Care (DHSC), Department for Transport (DfT), Crown Commercial Service (CCS), Government Legal Department (GLD) and No.10 Data Science team.\n\nArm’s length bodies, devolved administrations and public sector bodies’ contributions have come from the National Health Service (NHS), HM Courts and Tribunals Service (HMCTS), Government Internal Audit Agency (GIAA), Information Commissioner’s Office (ICO), Office for National Statistics (ONS), Driver and Vehicle Licensing Agency (DVLA), Met Office, Government Communications Headquarters (GCHQ) and Scottish Government.\n\nIndustry leaders and expert contributions have come from Amazon, Microsoft, IBM, Google, BCG, the Alan Turing Institute, the Oxford Internet Institute, and the Treasury Board of Canada Secretariat.\n\nUser research participants have come from a range of departments and have been very generous with their time.\n\nIn 2021, the National AI Strategy set out a 10 year vision that recognised the power of AI to increase resilience, productivity, growth and innovation across the private and public sectors.\n\nThe 2023 white paper A pro-innovation approach to AI regulation sets out the government’s proposals for implementing a proportionate, future-proof and pro-innovation framework for regulating AI.\n\nWe published initial guidance on generative AI in June 2023, encouraging civil servants to gain familiarity with the technology, while remaining aware of risks.\n\nWe are now publishing this expanded framework, providing practical considerations for anyone planning or developing a generative AI solution.\n\nGenerative AI has the potential to unlock significant productivity benefits.\n\nThis framework aims to help readers understand generative AI, to guide anyone building generative AI solutions, and, most importantly, to lay out what must be taken into account to use generative AI safely and responsibly.\n\nIt is based on a set of ten principles which should be borne in mind in all generative AI projects.\n\nThis framework differs from other technology guidance we have produced: it is necessarily incomplete and dynamic.\n\nIt is incomplete because the field of generative AI is developing rapidly and best practice in many areas has not yet emerged.\n\nIt is dynamic because we will update it frequently as we learn more from the experience of using generative AI across government, industry and society.\n\nIt does not aim to be a detailed technical manual: there are many other resources for that.\n\nIndeed, it is intended to be accessible and useful to non-technical readers as well as to technical experts.\n\nHowever, as our body of knowledge and experience grows, we will add deeper dive sections to share patterns, techniques and emerging best practice (for example prompt engineering).\n\nFurthermore, although there are several forms of generative AI, this framework focuses primarily on large language models (LLMs), as these have received the most attention, and have the greatest level of immediate application in government.\n\nFinally, I would like to thank all of the people who have contributed to this framework.\n\nIt has been a collective effort of experts from government departments, arm’s length bodies, other public sector organisations, academic institutions and industry partners.\n\nI look forward to continued contributions from a growing community as we gain experience in using generative AI safely, responsibly and effectively.\n\nDavid Knott, Chief Technology Officer for Government",
    "## Principle 1: You know what generative AI is and what its limitations are\n\nGenerative AI is a specialised form of AI that can interpret and generate high-quality outputs including text and images, opening up the potential for opportunities for organisations, including delivering efficiency savings or developing new language capability.\n\nYou actively learn about generative AI technology to gain an understanding of what it can and cannot do, how it can help and the potential risks it poses.\n\nLLMs lack personal experiences and emotions and don’t inherently possess real-world contextual awareness, but some now have access to the internet.\n\nGenerative AI tools are not guaranteed to be accurate as they are generally designed only to produce highly plausible and coherent results.\n\nThis means that they can, and do, make errors.\n\nYou will need to employ techniques to increase the relevance and correctness of their outputs, and have a process in place to test them.\n\nYou can find out more about what generative AI is in our Understanding generative AI section and what it can and cannot do for you in the Building generative AI solutions section.",
    "## Principle 2: You use generative AI lawfully, ethically and responsibly\n\nGenerative AI brings specific ethical and legal considerations, and your use of generative AI tools must be responsible and lawful.\n\nYou should engage with compliance professionals, such as data protection, privacy and legal experts in your organisation early in your journey.\n\nYou should seek legal advice on intellectual property, equalities implications, and fairness and data protection implications for your use of generative AI.\n\nYou need to establish and communicate how you will address ethical concerns from the start, so that diverse and inclusive participation is built into the project lifecycle.\n\nGenerative AI models can process personal data so you need to consider how you protect personal data, are compliant with data protection legislation and minimise the risk of privacy intrusion from the outset.\n\nGenerative AI models are trained on large data sets, which may include biased or harmful material, as well as personal data.\n\nBiases can be introduced throughout the entire lifecycle and you need to consider testing and minimising bias in the data at all stages.\n\nGenerative AI should not be used to replace strategic decision making.\n\nGenerative AI has hidden environmental issues that you and your organisation should understand and consider before deciding to use generative AI solutions.\n\nYou should use generative AI technology only when relevant, appropriate, and proportionate, choosing the most suitable and sustainable option for your organisation’s needs.\n\nYou should also use the AI regulation white paper’s fairness principle, which states that AI systems should not undermine the legal rights of individuals and organisations.\n\nAnd that they should not discriminate against individuals or create unfair market outcomes.\n\nYou can find out more in our Using generative AI safely and responsibly section.",
    "## Principle 3: You know how to keep generative AI tools secure\n\nGenerative AI tools can consume and store sensitive government information and personal identifiable information if the proper assurances are not in place.\n\nWhen using generative AI tools, you need to be confident that your organisation’s data is held securely, and that the generative AI tool can only access the parts of your organisation’s data that it needs for its task.\n\nYou need to ensure that private or sensitive data sources are not being used to train generative AI models without the knowledge or consent of the data owner.\n\nGenerative AI tools are often hosted in places outside your organisation’s secure network.\n\nYou must make sure that you understand where the data you give to a generative AI tool is processed, and that it is not stored or accessible by other organisations.\n\nGovernment data can contain sensitive and personal information that must be processed lawfully, securely and fairly at all times.\n\nYour approach must comply with the data protection legislation.\n\nYou need to build in safeguards and put technical controls in place.\n\nThis includes content filtering to detect malicious activity and validation checks to ensure responses are accurate and do not leak data.\n\nYou can find out more in our Security, Data protection and privacy, and Building the solution sections.",
    "## Principle 4: You have meaningful human control at the right stage\n\nWhen you use generative AI you need to make sure that there are processes for quality assurance controls which include an appropriately trained and qualified person to review your generative AI tool’s outputs and validation of all decision making that generative AI outputs have fed into.\n\nWhen you use generative AI to embed chatbot functionality into a website, or other uses where the speed of a response to a user means that a human review process is not possible, you need to be confident in the human control at other stages in the product lifecycle.\n\nYou must have fully tested the product before deployment, and have robust assurance and regular checks of the live tool in place.\n\nSince it is not possible to build models that never produce unwanted or fictitious outputs (i.e.\n\nhallucinations), incorporating end-user feedback is vital.\n\nPut mechanisms into place that allow end-users to report content and trigger a human review process.\n\nYou can find out more in our Ethics, Data protection and privacy, Building the solution",
    "## Principle 5: You understand how to manage the full generative AI lifecycle\n\nGenerative AI tools, like other technology deployments, have a full project lifecycle that you need to understand.\n\nYou and your team must know how to choose a generative AI tool and how to set it up.\n\nYou need to have the right resource in place to support day-to-day maintenance of the tool.\n\nYou need to know how to update the system, and how to close the system securely down at the end of your project.\n\nYou need to understand how to monitor and mitigate generative AI drift, bias and hallucinations.\n\nYou have a robust testing and monitoring process in place to catch these problems.\n\nYou should use the Technology code of practice to build a clear understanding of technology deployment lifecycles, and understand and use the National Cyber Security Centre cloud security principles.\n\nYou should understand the benefits, other use cases and applications that your solution could support across government.\n\nThe Rose Book provides guidance on government-wide knowledge assets and The Government Office for Technology Transfer can provide support and funding to help develop government-wide solutions.\n\nIf you develop a service you must use the Service Standard for government.\n\nYou can find out more about development best practices for generative AI in our Building the solution section.",
    "## Principle 6: You use the right tool for the job\n\nYou should ensure you select the most appropriate technology to meet your needs.\n\nGenerative AI is good at many tasks but has a number of limitations and can be expensive to use.\n\nYou should be open to solutions using generative AI as they can allow organisations to develop new or faster approaches to the delivery of public services, and can provide a springboard for more creative and innovative thinking about policy and public sector problems.\n\nYou can create more space for you and your people to problem solve by using generative AI to support time-consuming administrative tasks.\n\nWhen building generative AI solutions you should make sure that you select the most appropriate deployment patterns and choose the most suitable generative AI model for your use case.\n\nYou can find out about how to choose the right generative AI technology for your task or project in our Identifying use cases, Patterns, Picking your tools and Things to consider when evaluating LLMs sections.",
    "## Principle 7: You are open and collaborative\n\nThere are lots of teams across government who are interested in using generative AI tools in their work.\n\nYour approach to any generative AI project should make use of existing cross-government communities, where there is a space to solve problems collaboratively.\n\nYou should identify which groups, communities, civil societies, non-governmental organisations, academic organisations and public representative organisations have an interest in your project.\n\nYou should have a clear plan for engaging and communicating with these stakeholders at the start of your work.\n\nYou should seek to join cross-government communities and engage with other government organisations.\n\nFind other departments who are trying to address similar issues and learn from them, and also share your insights with others.\n\nYou should reuse ideas, code and infrastructure where possible.\n\nAny automated response visible to the public such as via a chatbot interface or email should be clearly identified as such (e.g.\n\n“This response has been written by an automated AI-chatbot”).\n\nYou should be open with the public about where and how algorithms and AI systems are being used in official duties (e.g.\n\nGOV.UK digital blogs).\n\nThe UK Algorithmic Transparency Recording Standard (ATRS) provides a standardised way to document information about the algorithmic tools being used in the public sector with the aim to make this information clearly accessible to the public.\n\nYou can find out more in our Ethics section.",
    "## Principle 8: You work with commercial colleagues from the start\n\nGenerative AI tools are new and you will need specific advice from commercial colleagues on the implications for your project.\n\nYou should reach out to commercial colleagues early in your journey to understand how to use generative AI in line with commercial requirements.\n\nYou should work with commercial colleagues to ensure that the expectations around the responsible and ethical use of generative AI are the same between in-house developed AI systems and those procured from a third party.\n\nFor example, procurement contracts can require transparency from the supplier on the different information categories as set out in the Algorithmic Transparency Recording Standard (ATRS).\n\nYou can find out more in our Buying generative AI section.",
    "## Principle 9: You have the skills and expertise that you need to build and use generative AI\n\nYou should understand the technical requirements for using generative AI tools, and have them in place within your team.\n\nYou should know that generative AI requires an understanding of new skills such as prompt engineering and you, or your team, should have the necessary skill set.\n\nYou should take part in available Civil Service learning courses on generative AI, and proactively keep track of developments in the field.\n\nYou can find out more in our Acquiring skills section.",
    "## Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place\n\nThese principles and this framework set out a consistent approach for the use of generative AI tools for UK government.\n\nWhile you should make sure that you use these principles when working with generative AI, many government organisations have their own governance structures and policies in place, and you also should follow any organisation-specific policies.\n\nYou need to understand, monitor and mitigate the risks that using a generative AI tool can bring.\n\nYou need to connect with the right assurance teams in your organisation early in the project lifecycle for your generative AI tool.\n\nYou need to have clearly documented review and escalation processes in place.\n\nThis might be a generative AI review board, or a programme-level board.\n\nYou can find out more in our Governance section.",
    "## What is generative AI?\n\nGenerative AI is a form of AI – a broad field which aims to use computers to emulate the products of human intelligence or to build capabilities which go beyond human intelligence.\n\nUnlike previous forms of AI, generative AI produces new content, such as images, text or music.\n\nIt is this capability, particularly the ability to generate language, which has captured the public imagination, and creates potential applications within government.\n\nGenerative AI fits within the broader field of AI as shown below:\n\n- Algorithms that automatically learn from data sets\n- Machine learning using neural networks to automatically learn from large data sets\n- Neural networks trained on huge amounts of data and able to generate high-quality outputs including text and digital images\n\nModels which generate content are not new, and have been a subject of research for the last decade.\n\nHowever, the launch of ChatGPT in November 2022 increased public awareness and interest in the technology, as well as triggering an acceleration in the market for usable generative AI products.\n\nOther well known generative AI applications include Claude, Bard, Bedrock and Dall-E, which are LLMs.\n\nPublic LLM interfaces fit within the field of generative AI as shown below:\n\n- A general purpose model trained on large quantities of data\n- Foundation models trained on text and able to interpret and generate high-quality outputs\n- A publicly available service with a simple user interface to access an LLM\n\nFoundation models are large neural networks trained on extremely large datasets to produce responses which resemble those datasets.\n\nFoundation models may not necessarily be language-based, and they could have been trained on non-text data, e.g.\n\nbiochemical information.\n\nLLMs are foundation models specifically trained on text and natural language data to generate high-quality text based outputs.\n\nUser interfaces for foundation models and LLMs, are user-friendly ways that people without technical experience can use foundation models or LLMs.\n\nChatGPT and Bard are examples of these.\n\nAt present they are mostly accessed by tool-specific URLs, but they are likely to be embedded into other consumer software and tools in the near future.\n\nGenerative AI works by using large quantities of data, often harvested from the internet, to train a model in the underlying patterns and structure of that data.\n\nAfter many rounds of training, sometimes involving machines only, sometimes involving humans, the model is capable of generating new content, similar to the training examples.\n\nWhen a user provides a prompt or input, the AI evaluates the likelihood of various possible responses based on what it has learned from its training data.\n\nIt then selects and presents the response that has the highest probability of being the right fit for the given prompt.\n\nIn essence, it uses its training to choose the most appropriate response for the user’s input.",
    "## Applications of generative AI in government\n\nDespite their limitations, the ability of LLMs to process and produce language is highly relevant to the work of government, and could be used to:\n\n- Speed up delivery of services: retrieving relevant organisational information faster to answer citizen digital queries or routing email correspondence to the right parts of the business\n- Reduce staff workload: suggesting first drafts of routine email responses or computer code to allow people more time to focus on other priorities\n- Perform complicated tasks: helping to review and summarise huge amounts of information\n- Improve accessibility of government information: improving the readability and accessibility of information on webpages or reports\n- Perform specialist tasks more cost-effectively: summarising documentation that contains specialist language like financial or legal terms, or translating a document into several different languages\n\nHowever, LLMs and other forms of generative AI still have limitations: you should make sure that you understand these, and that you build appropriate testing and controls into any generative AI solutions.",
    "## Limitations of generative AI and LLMs\n\nLLMs predict the next word in a sequence.\n\nThey don’t understand the content or meaning of the words beyond how likely they are to be used in response to a particular question.\n\nThis means that even though LLMs can produce plausible responses to requests, there are limitations on what they can reliably do.\n\nYou need to be aware of these limitations and have checks and assurance in place when using generative AI in your organisation.\n\n- **Hallucination (also called confabulation)**: LLMs are primarily designed to prioritise the appearance of being plausible rather than focusing on ensuring absolute accuracy, frequently resulting in the creation of content that appears plausible but may actually be factually incorrect.\n\n- **Critical thinking and judgement**: although LLMs can give the appearance of reasoning, they are simply predicting the next most plausible word in their output, and may produce inaccurate or poorly-reasoned conclusions.\n\n- **Sensitive or ethical context**: LLMs can generate offensive, biased, or inappropriate content if not properly guided, as they will replicate any bias present in the data they were trained on.\n\n- **Domain expertise**: unless specifically trained on specialist data, LLMs are not true domain experts.\n\nOn their own, they are not a substitute for professional advice, especially in legal, medical, or other critical areas where precise and contextually relevant information is essential.\n\n- **Personal experience and context**: LLMs lack personal experiences and emotions.\n\nAlthough their outputs may appear as if they come from a person, they do not have true understanding or a consciousness.\n\n- **Dynamic real-time information retrieval**: LLMs do not always have real-time access to the internet or data outside their training set.\n\nHowever, this feature of LLM products is changing.\n\nAs of October 2023, ChatGPT, Bard and Bing have been modified to include access to real-time internet data in their results.\n\n- **Short-term memory**: LLMs have a limited context window.\n\nThey might lose track of the context of a conversation if it’s too long, leading to incoherent responses.\n\n- **Explainability**: generative AI is based on neural networks, which are so-called ‘black boxes’.\n\nThis makes it difficult or impossible to explain the inner workings of the model which has potential implications if in the future you are challenged to justify decisioning or guidance based on the model.\n\nThese limitations mean that there are types of use cases where you should currently avoid using generative AI, such as safety-of-life systems or those involving fully automated decision-making which affects individuals.\n\nHowever, the capabilities and limitations of generative AI solutions are rapidly changing, and solution providers are continuously striving to overcome these limitations.\n\nThis means that you should make sure that you understand the features of the products and services you are using and how they are expected to change.",
    "## Building generative AI solutions\n\nThis section outlines the practical steps you’ll need to take in building generative AI solutions, including defining the goal, building the team, creating the generative AI support structure, buying generative AI and building the solution.\n\nIt supports:\n\n- Principle 1: You know what generative AI is and what its limitations are\n- Principle 3: You know how to keep generative AI tools secure\n- Principle 4: You have meaningful human control at the right stage\n- Principle 5: You understand how to manage the full generative AI lifecycle\n- Principle 6: You use the right tool for the job\n- Principle 8: You work with commercial colleagues from the start\n- Principle 9: You have the skills and expertise that you need to build and use generative AI\n\nHowever, following the guidance in this section is only part of what is needed to build generative AI solutions.\n\nYou also need to make sure that you are using generative AI safely and responsibly.",
    "### Defining the goal\n\nLike all technology, using generative AI is a means to an end, not an objective in itself.\n\nWhether planning your first use of generative AI or a broader transformation programme, you should be clear on the goals you want to achieve and particularly, where you could use generative AI, and where you should avoid it.\n\nGoals for the use of generative AI may include improved public services, improved productivity, increased staff satisfaction, increased quality, cost savings and risk reduction.\n\nYou should make sure you know which goal you are seeking, and how you will measure outcomes.",
    "### Identifying use cases\n\nWhen thinking about how you could leverage generative AI in your organisation you need to consider the possible situations or use cases.\n\nThe identification of potential use cases should be led by business needs and user needs, rather than directed by what the technology can do.\n\nEncourage business units and users to articulate their current challenges and opportunities.\n\nTake the time to thoroughly understand users and their needs as per the Service Manual to make sure you are solving the right problems.\n\nTry to focus on use cases that can only be solved by generative AI or where generative AI offers significant advantages above existing techniques.\n\nThe use of generative AI is still evolving, but the most promising use cases are likely to be those which aim to:\n\n- Support digital enquiries: enable citizens to express their needs in natural language online, and help them find the content and services which are most helpful to them\n- Interpret requests: analyse correspondence or voice calls to understand citizens’ needs, and route their requests to the place where they can best get help\n- Enhanced search: quickly retrieving relevant organisational information or case notes to help answer citizens’ queries\n- Synthesise complex data: help users to understand large amounts of data and text, by producing simple summaries\n- Generate output: produce first drafts of documents and correspondence\n- Assist software development: support software engineers in producing code, and understanding complex legacy code\n- Summarise text and audio: converting emails and records of meetings into structured content, saving time in producing minutes and keeping records\n- Improve accessibility: support conversion of content from text to audio, and translation between different languages",
    "### Use cases to avoid\n\nGiven the current limitations of generative AI, there are many use cases where its use is not yet appropriate, and which should be avoided.\n\n- Fully automated decision-making: any use cases involving significant decisions, such as those involving someone’s health or safety, should not be made by generative AI alone.\n\n- High-risk / high-impact applications: generative AI should not be used on its own in high-risk areas which could cause harm to someone’s health or wellbeing, such as security or critical infrastructure systems.\n\nTo build generative AI solutions successfully, you need to define the project scope clearly, understand and communicate the project's goals, and consider staff, stakeholder, and citizen impacts continually throughout the project lifecycle.",
    "# Safe Uses of Generative AI\n\nGenerative AI operates at different speeds compared to other computer systems and should not be used in applications requiring extremely rapid, low-latency responses.\n\nIt's optimized for plausibility rather than accuracy and should not be solely relied upon for high-accuracy results without measures to ensure accuracy.\n\nThe inner workings of generative AI may be difficult or impossible to explain, making it unsuitable for contexts requiring high explainability.\n\nPerformance depends on large quantities of training data, and systems trained on limited data may produce skewed results.",
    "## Building the Team\n\nPublic-facing generative AI services are easy to access, but building solutions for citizen services requires a range of skills.\n\nIt's crucial to assemble a multi-disciplinary team including business leaders, data scientists, software engineers, user researchers, and support from legal, commercial, security colleagues, ethics, and data privacy experts.\n\nEnsure your team includes diverse groups to help mitigate risks of bias.",
    "## Acquiring Skills\n\nFoundational skills required for digital work are outlined in the digital, data, and technology capability framework.\n\nMore specific skills for generative AI are available through open learning resources for civil servants.\n\nThese courses cover the introduction, risks and ethics, tools and applications, prompt engineering, strategy and governance, and a technical curriculum for generative AI.\n\nLearning plans should be tailored for different groups: beginners, operational delivery and policy professionals, digital and technology professionals, data and analytics professionals, and senior civil servants.",
    "# Picking your Tools\n\nIn order to develop and deploy generative AI systems, you will need to pick the right tools and technology for your organisation.\n\nDeciding on the best tools will depend on your current IT infrastructure, level of expertise, risk appetite, and the specific use cases you are supporting.",
    "## Decisions on your Development Stack\n\nThere are a number of technology choices you will need to consider when building your generative AI solutions, including the most appropriate IT infrastructure, which programming languages to use, and the best LLM.\n\n- **Infrastructure:** You should select a suitable infrastructure environment.\n\nMicrosoft, Google, or AWS may be appropriate, depending on your current IT infrastructure or existing partnerships and expertise in your teams.\n\nAlternatively, it may be that a specific LLM is considered most appropriate for your particular use case, leading to a particular set of infrastructure requirements.\n\n- As models change and improve, the most appropriate one for your use case may also change, so try to build in the technical agility to support different models or providers.\n\n- Items for consideration include:\n    - Use of cloud services vs local development: you should be aware of the government’s Cloud First Policy, but understand that local development may be feasible for experimentation.\n\nUsing container technology from the start can help you to move your solution between platforms with minimal overhead.\n\n- Web services, access modes such as APIs and associated frameworks – see section on Patterns.\n\n- Front-end/user interface and back-end solutions.\n\n- Programming languages.\n\n- Data storage (e.g., Binary Large Object (BLOB) stores and vector stores).\n\n- Access logging, prompt auditing, and protective monitoring.\n\n- **Programming Language:** In the context of AI research, Python is the most widely used programming language.\n\nWhile some tools and frameworks are available in other languages, for example LangChain is also available in JavaScript, it is likely that most documentation and community discussion is based on Python examples.\n\nIf you’re working on a use case that has focused interaction with a generative AI model API endpoint only, the choice of programming language is less important.\n\n- **Frameworks:** Generative AI frameworks are software libraries or platforms that provide tools, APIs, and pre-built models to develop, train, and deploy generative AI models.\n\nThese frameworks implement various algorithms and architectures, making it more convenient for you to experiment with and create generative models.\n\nExample frameworks include LangChain, Haystack, Azure Semantic Kernel, and Google Vertex AI pipelines.\n\nAWS Bedrock similarly provides an abstraction layer to interact with varied models using a common interface.\n\nThese frameworks have their own strengths and unique features.\n\nHowever, you should also be aware that their use may increase the complexity of your solution.\n\n- The choice of a generative AI framework might depend on:\n    - Your specific project requirements.\n\n- The familiarity of the developer with the framework and programming language.\n\n- The size and engagement of the community support around it.",
    "## Things to Consider When Evaluating LLMs\n\nThere are many models currently available, so you need to select the most appropriate for your particular use case.\n\nThe Stanford Center for Research on Foundation Models provides the Holistic Evaluation of Language Models to benchmark different models against criteria such as accuracy, robustness, fairness, bias, and toxicity.\n\nIt can help you to compare the capabilities of a large number of language models.\n\nHere are some of the things you should consider.\n\n- **Capability:** Depending on your use case, conversational foundation models may not be the best fit for you.\n\nIf you have a domain-specific requirement in sectors like medical or security applications, pre-tuned, specialised models like Google PaLM-2-med and Google PaLM-2-sec may reduce the amount of work required to reach a certain performance level and time to production.\n\n- Equally, if you’re mainly focused on indexing tasks, BERT-type models may provide better performance compared to GPT-style LLMs.\n\n- **Availability:** At the time of writing, many LLMs are not available for general public use, or are locked to certain regions.\n\nOne of the first things to consider when deciding on which model to use is whether implementation in a production environment is possible in line with your organisation’s policy requirements.\n\n- **Mode of Deployment:** Many LLMs are available via a number of different routes.\n\nFor production applications, the use of fully-featured cloud services or operation of open-source models in a fully controlled cloud environment will be a hard requirement for most, if not all, use cases.\n\n- **Cost:** Most access to LLMs is charged by the number of tokens (roughly equal to the word count).\n\nIf your generative AI tool is hosted in a cloud environment, you’ll have to pay additional infrastructure costs.\n\nWhile the operation of open-source models will not necessarily incur a cost per transaction, the operation of graphics processing units-enabled instances is costly as well.\n\nCloud infrastructure best practices like dynamic scaling and shutting down instances outside of working hours will help to reduce these costs.\n\n- **Context Limits:** LLMs often limit the maximum amount of tokens the model can process as a single input prompt.\n\nFactors determining the size of prompts are the context window of conversation (if included), the amount of contextual data included via meta-prompting and retrieval augmented generation as well as the expected size of user inputs.\n\n- **API Rate Limits:** Model providers impose limits on how frequently users can make requests through an API.\n\nThis may be important if your use case leads to a high volume of requests.\n\nSoftware development best practices for asynchronous execution (such as use of contexts and queues) may help to resolve bottlenecks but will increase the complexity of your solution.\n\n- **Language Capability:** If your use case includes multilingual interaction with the model, or if you expect to operate with very domain-specific language using specific legal or medical terminology, you should consider the amount of relevant language-specific training the LLM has received.\n\n- **Open vs Closed-source (Training Data, Code and Weights):** If openness is important to your solution, you should consider whether all aspects of the model, including training data, neural network coding, and model weights, are available as open source.\n\nOpen source is different from being available.\n\nThe LLaMa model is unsuitable for use by the government as its weights were leaked, but not officially released meaning it may be more susceptible to adversarial attacks.\n\nSites such as Hugging Face host a large collection of models and documentation.\n\nExamples of sites that provide open source low-code solutions include Databricks and MosaicML.\n\n- **Non-technical Considerations:** There may be data-protection, legal, or ethical considerations which constrain or direct your choice of technology, for example an LLM may have been trained on copyrighted data, or to produce a procedurally fair decision-making system, and one solution should be chosen over another.",
    "# Getting Reliable Results\n\nGenerative AI technology needs to be carefully controlled and managed in order to ensure the models behave and perform in the way you want them to, reliably and consistently.\n\nThere are a number of things you can do to help deliver high-quality and reliable performance.\n\n- **Select Your Model Carefully:** In order to achieve a reliable, consistent, and cost-effective implementation, the most appropriate model for a particular use case should be chosen.\n\n- **Design a Clear Interface and Train Users:** Ensure your generative AI system is used as intended.\n\nDesign and develop a useful and intuitive interface your users will interact with.\n\nDefine and include any required user settings (for example, the size of required response).\n\nBe clear about the design envelope for generative AI systems, i.e., what it has been designed and built to do, and, more importantly, what its limitations are.\n\nEnsure your user community is trained in its proper use and fully understand its limitations.\n\n- **Evaluate Input Prompts:** User inputs to the generative AI tool can be evaluated with a content filtering system to detect and filter inappropriate inputs.\n\nThe evaluation of input using deterministic tools may be feasible and could reduce the amount of comparatively expensive calls to an LLM.\n\nAlternatively, calls to a smaller and/or classification-specialized LLM may be required.\n\nMake sure that the system returns a meaningful error to allow a user to adjust their prompt if rejected.\n\nThere are some commercially available tools that can provide some of this functionality.\n\nExample checks include:\n  - Identifying whether the prompt is abusive or malicious.\n\n- Confirming the prompt is not attempting to jailbreak the LLM, for example by asking the LLM to ignore its safety instructions.\n\n- Confirming no unnecessary personally identifiable information has been entered.\n\n- **Ground Your Solution:** If your use case is looking for the model to provide factual information – as opposed to just taking advantage of a model’s creative language capabilities – you should follow steps to ensure that its responses are accurate, for example by employing retrieval augmented generation.\n\nWith this, you identify useful documentation, then extract the important text, break it into ‘chunks’, convert them to ‘embeddings’ and send them to a ‘vector-database’.\n\nThis relevant information can now be easily retrieved and integrated as part of the model responses.\n\n- A key application of generative AI is working with your organisation’s private data.\n\nBy enabling the model to access, understand, and use the private data, insights and knowledge can be provided to users that are specific to their subject domain.\n\nThere are different ways to hook a generative AI model into a private data source.\n\n- You could train the model from scratch on your own private data, but this is costly and impractical.\n\nAlternatively, you can take a pre-trained model and further train it on your own private data.\n\nThis is a process called fine-tuning and is less expensive and time consuming than training a model from scratch.\n\n- The easiest and most cost-efficient approach to augmenting your generative AI model with private data is to use in-context learning, which means adding domain-specific context to the prompt sent to the model.\n\nThe limitation here is usually the size of the prompt, and a way around this is to chunk your private data to reduce its size.\n\nThen a similarity search can be used to retrieve relevant chunks of text that can be sent as context to the model.\n\n- **Use Prompt Engineering:** An important mechanism to shape the model’s performance and produce accurate and reliable results is prompt engineering.\n\nDeveloping good prompts and meta-prompts is an effective way to set the standards and rules for how the user requests should be processed and interpreted, the logical steps the model should follow and what type of response is required.\n\nFor example, you could include:\n  - Setting the tone for the interactions, for example requesting a chatbot to provide polite, professional, and neutral language responses – this will help to reduce bias.\n\n- Setting clear boundaries on what the generative AI tool can, and cannot, respond to – you could specify the requirement for a model to not engage with abusive or malicious inputs, but reject them and instead return an alternative, appropriate response.\n\n- Defining the format and structure of the desired output – for example asking for a Boolean yes/no response to be provided in JSON format.\n\n- Defining guardrails to prevent the assistant from generating inappropriate or harmful content.\n\n- **Evaluate Outputs:** Once a model returns an output, it is important to ensure that its messaging is appropriate.\n\nOff-the-shelf content filters may be useful here, as well as classical or generative AI text-classification tools.",
    "Off-the-shelf content filters may be useful here, as well as classical or generative AI text-classification tools.\n\nDepending on the use case, a human might be required to check the output some or all of the time, although the expenditure of time and money to do this needs careful consideration.\n\nAccuracy and bias checks on the LLM responses prior to presentation to the user can be used to check and confirm:\n  - The response is grounded in truth with no hallucinations.\n\n- The response does not contain toxic or harmful information.\n\n- The response does not contain biased information.\n\n- The response is fair and does not unduly discriminate.\n\n- The user has permission to access the returned information.\n\n- **Include Humans:** There are many good ways that humans can be involved in the development and use of generative AI solutions to help implement reliable and desired outcomes.\n\nHumans can be part of the development process to review input data to make sure it is high-quality, to assess and improve model performance and also to review model outputs.\n\nIf there is a person within the processing chain preventing the system from producing uncontrolled, automated outputs, this is called having a ‘human-in-the-loop.’\n\n- **Evaluate Performance:** To maintain the performance of the generative AI system, its performance should be continually monitored and evaluated by logging and auditing all interactions with the model.\n\n- Conduct thorough testing to assess the functionality and effectiveness of the system – see section on Testing generative AI solutions for further information.\n\n- Record the input prompts and the returned responses.\n\n- Collect and analyse metrics across all aspects of performance: including hallucinations, toxicity, fairness, robustness, and higher-level business key performance indicators.\n\n- Evaluate the collected metrics and validate the model’s outputs against ground truth or expert judgement, and obtain user feedback to understand the usefulness of the returned response – this could be a simple thumbs-up indicator or something more sophisticated.",
    "# Testing Generative AI Solutions\n\nGenerative AI tools are not guaranteed to be accurate as they are designed to produce plausible and coherent results.\n\nThey generate responses that have a high likelihood of being plausible based on the data that they have processed.\n\nThis means that they can, and do, make errors.\n\nIn addition to employing techniques to get reliable results, you should have a process in place to test them.\n\n- During the initial experimental discovery phases, you should look to assess and improve the existing system until it meets the required performance, reliability, and robustness criteria.\n\n- Conduct thorough testing to assess the functionality and effectiveness of the system.\n\n- Record the input prompts and the returned responses, and collect and analyse metrics across all aspects of performance including hallucinations, toxicity, fairness, robustness, and higher-level business key performance indicators.\n\n- Evaluate the collected metrics and validate the model’s outputs against ground truth or expert judgement, obtaining user feedback if possible.\n\n- Closely review the outcomes of the technical decisions made, the infrastructure and running costs, and environmental impact.\n\nUse this information to continually iterate your solution.\n\n- Technical methods and metrics for assessing bias in generative AI are still being developed and evaluated.\n\nHowever, there are existing tools that can support AI fairness testing, such as IBM fairness 360, Microsoft FairLearn, Google What-If-Tool, University of Chicago Aequitas tool, and PyMetrics audit-ai.\n\nYou should carefully select methods based on the use case and consider using a combination of techniques to mitigate bias across the AI lifecycle.",
    "# Data Management\n\nGood data management is crucial in supporting the successful implementation of generative AI solutions.\n\nThe types of data you will need to manage include the following:\n\n- **Organisational Grounding Data:** LLMs are not databases of knowledge, but advanced text engines.\n\nTheir contents may also be out of date.\n\nTo improve their performance and make them more reliable, relevant information can be used to ‘ground’ the responses, for example by employing retrieval augmented generation.\n\n- **Reporting Data:** It is important to maintain documentation, including methodology, description of the design choices and assumptions.\n\nKeep records from any architecture design reviews.\n\nThis can help to support the ability to audit the project and support the transparency of your use of AI.\n\nIf possible, collect metrics to help to estimate any efficiency savings, value to your business, and to taxpayers and the return on investment.\n\n- **Testing and Operational Data:** All model inputs and outputs should be logged.\n\nWhen collected during testing and development, this information will be used to improve the performance of the system.\n\nWhen collected during use, it will be used to monitor and maintain performance.\n\nThe recording of the outcomes and any resulting decisions will also help when examining and looking to explain the model results.\n\nSee the Testing generative AI solutions section for further details.\n\nAdditionally, all user engagement of the generative AI systems should be logged to ensure safe and compliant use.\n\n- **User Feedback:** Both during the initial development stage and whilst in use, you should be collecting feedback from users on their interactions with the system.\n\nCollecting and storing metrics such as performance, ease of use and occurrences of problematic behaviour (including hallucinations and potential biases, etc.)\n\nhelps to control and improve the AI system.\n\n- **Financial Operations, or FinOps Data:** The cost of running your generative AI solutions should be monitored closely to ensure you continue to operate cost-effectively, for the given model and prompts.\n\nData management needs to also address data loss prevention.\n\nConsider using PET to prevent data leakage, and if you process personal identifiable information take action to protect people's data e.g., pseudonymising data to reduce the risk of leaking sensitive information.",
    "# Legal Considerations\n\nYou should seek advice from government legal profession legal advisers who help you to navigate through the use of generative AI in government.\n\nAlthough generative AI is new, many of the legal issues that surround it are not.\n\nFor example, many of the ethical principles discussed in this document, such as fairness, discrimination, transparency, and bias, have sound foundations in public law.\n\nIn that way, many of the ethical issues that your team identifies will also be legal issues, and your lawyers will be able to help to guide you through them.\n\n- The Lawfulness and purpose limitation section provides a framework to ensure that personal data is processed lawfully, securely, and fairly at all times.\n\nYour lawyers can advise you on that.\n\n- You may face procurement and commercial issues when buying generative AI products.\n\nAlongside commercial colleagues, your lawyers can help you to navigate those challenges.\n\n- When you contact your legal team, you should explain your aims for the generative AI solution, what it will be capable of doing, and any potential risks you are aware of.\n\nThis will help you to understand, for example, if you need legislation to achieve what you want to do.\n\nIt will also help to minimise the risk of your work being challenged in court, having unintended – and unethical – consequences or a negative impact on the people you want it to benefit.",
    "## Example Legal Issues\n\nThese are example legal issues designed to help you understand when you might want to consider getting legal advice.\n\nThey should not be read as real legal advice and their application to any given scenario will be fact-specific.\n\nYou should always consult your departmental lawyer if in doubt.",
    "### Data Protection\n\nData protection is a legal issue, with potentially serious legal consequences should the government get it wrong.\n\nAlthough your organisation will have a data protection officer and there may also be experts in your team, your legal team will be able to help you to unpick some of the more difficult data protection issues that are thrown up by the use of generative AI.\n\nSee the Data protection and privacy section for more information.",
    "### Contractual Issues\n\nYour lawyers will help you to draw up the contracts and other agreements for the procurement or licensing of generative AI tools.\n\nThere may be special considerations for those contracts, such as how to apportion intellectual property and how to ensure the level of transparency that would be required in a legal challenge.\n\nContracts for technology services may need to incorporate procedures for system errors and outages, that recognise the potential consequences of performance failures.\n\nSee the Buying generative AI section for more information.",
    "### Intellectual Property and Copyright\n\nThe potential intellectual property issues with generative AI have been much discussed.\n\nYour lawyers can help you to navigate these, for example by considering at the outset how ownership of intellectual property rights and liabilities will be apportioned throughout the lifetime of the project.\n\nThey can also give you advice on any copyright issues with the use of these systems in government.",
    "### Equalities Issues\n\nLawyers can help you to navigate the equalities issues raised by the use of generative AI in government, for example obligations arising under the Equality Act 2010.\n\nConducting an assessment of the equalities impacts of your use of generative AI can also be one way to guard against bias, which is particularly important in the context of generative AI.\n\nIf approached early, before contracts are signed, your legal advisers can help you ensure the government is fulfilling its responsibilities to the public to assess the impacts of the technology it is using.",
    "### Public Law Principles\n\nPublic law principles explain how public bodies should act rationally, fairly, lawfully, and compatibly with human rights.\n\nThese are guidelines for public bodies on how to act within the law.\n\nMany of these public law principles overlap with the ethical principles set out in this guidance.\n\nAs a result, your lawyers will likely be able to guide you on the application of the ethical principles, based on their knowledge of public law and the court cases that have occurred and the detail of the judgments.\n\n- For example, public law involves a principle of procedural fairness.\n\nThis is not so much about the decision that is eventually reached but about how a decision is arrived at.\n\nA correct procedure would ensure that relevant considerations are considered.\n\nThe transparency and explainability of the AI tool may well be key in being able to demonstrate that the procedure was fair.\n\n- Public law also considers rationality.\n\nRationality may be relevant in testing the choice of generative AI system, considering the features used in a system, and considering the outcomes of the system and the metrics used to test those outcomes.\n\n- Where you are considering using generative AI in decision-making in particular, public law also can guide you, for example on whether particular decisions require the exercise of a discretion by a decision maker, which could be unfairly fettered by the use of a tool, or whether in fact the decision can be delegated at all.",
    "### Human Rights\n\nPublic authorities must act in a way that is compatible with human rights.\n\nIt’s possible that AI systems (especially those involving the use of personal data) may in some way affect at least one of the rights in the European Convention on Human Rights.\n\nExamples of those most likely to be commonly impacted are Article 8 (right to a private and family life) and Article 10 (freedom of expression).",
    "### Legislation\n\nSometimes, in order to do something, a public authority needs a legislative framework.\n\nYour lawyers will be able to advise you whether your use of generative AI is within the current legal framework or needs new legislation.\n\nFor example, it may be that the legislative framework does not allow the process you are automating to be delegated to a machine.\n\nOr it may be that it provides for a decision to be made by a particular person.",
    "# Ethics\n\nThe ethical questions raised by your use of generative AI will depend on your context and the nature of your solutions.\n\nThe key themes you should address include:\n\n- Transparency and explainability.\n\n- Accountability and responsibility.\n\n- Fairness, bias, and discrimination.\n\n- Information quality and misinformation.\n\n- Keeping a human-in-the-loop.\n\nAs well as the guidance in this framework, you should also take existing guidance into account, such as the UK government data ethics framework and the UK Statistics Authority ethics self-assessment tool.\n\nThe five cross-sectoral, values-based principles for responsible AI innovation set out in the AI regulation white paper also provide a useful explainer for safety, security, and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress.",
    "## Transparency and Explainability\n\nTransparency is a cornerstone of the ethical development, deployment, and use of AI systems.\n\nA lack of transparency can lead to harmful outcomes, public distrust, a lack of accountability, and ability to appeal.\n\nThe AI regulation white paper establishes that AI systems should be appropriately transparent and explainable.\n\nTransparency is the communication of appropriate information about an AI system to the right people.\n\nFor example: information on how, when, and for which purposes an AI system is being used.\n\nExplainability is how much it is possible for the relevant people to access, interpret, and understand the decision-making processes of an AI system.\n\nHowever, transparency can be challenging in the context of generative AI, due to the closed and proprietary nature of commercial tools, and the inherent opacity of neural networks.\n\nYou should therefore ensure that you are transparent about the design of the generative AI system and the processes in which it is embedded.\n\n- **What you are transparent about:**\n  - Technical transparency: information about the technical operation of the AI system, such as the code used to create the algorithms, and the underlying datasets used to train the model.\n\n- Process transparency: information about the design, development, and deployment practices behind your generative AI solutions, and the mechanisms used to demonstrate that the solution is responsible and trustworthy.\n\nPutting in place robust reporting mechanisms, process-centred governance frameworks, and AI assurance techniques is essential for facilitating process-based transparency.\n\n- Outcome-based transparency and explainability: the ability to clarify to any citizen using, or impacted by, a service that uses generative AI how the solution works and which factors influence its decision making and outputs, including individual-level explanations of decisions where this is requested.\n\n- **How and to whom you are being transparent:**\n  - Internal transparency: retention of up-to-date internal records on technology and processes and process-based transparency information, including records of prompts and outputs.\n\n- Public transparency: where possible from a sensitivity and security perspective, you should be open and transparent about your department’s use of generative AI systems to the general public.\n\nAlthough there are no universally accepted standards for achieving transparency in the use of generative AI, there are existing standards and external resources which you can draw on:\n\n- The UK Algorithmic Transparency Recording Standard (ATRS) should be used by public sector bodies using algorithmic solutions – like generative AI.\n\nThe ATRS aims to make sure that information about algorithmic solutions used by the government.",
    "# Public Sector AI Accessibility and Ethics\n\nThe UK’s national public sector AI ethics and safety guidance, *Understanding artificial intelligence ethics and safety*, outlines a process-based governance framework that can assist project teams in establishing and documenting proportionate governance actions.\n\nData and model cards or fact sheets can be used as a reference point when documenting information about AI models and the datasets used in training and testing.\n\nA good example of these are Google’s data cards and model cards.\n\nThe Information Commissioner’s Office (ICO) also offers AI auditing consultation and support to government organisations.\n\nFurther information can be found in *A guide to ICO audit: artificial intelligence audits*.\n\n*Explaining decisions made with AI guidance* is the UK’s national AI explainability guidance co-produced by The Alan Turing Institute and the ICO: this details six types of explanations as well as documentation processes.",
    "## Practical Recommendations\n\n- Clearly signpost when generative AI has been used to create content or is interacting with members of the public.\n\nWhere possible, label AI generated content and consider embedding watermarking into the model.\n\n- Put in place evaluation and auditing structures, tracking data provenance, design decisions, training scenarios, and processes.\n\n- Use existing standards and recording mechanisms such as the Algorithmic Transparency Recording Standard to communicate information about generative AI solutions to the general public.\n\n- Use external resources and emerging best practice, such as data cards and model cards, for internal transparency.\n\n- Strive to make model outputs as explainable as possible, while being aware of the current explainability limitations of generative AI.\n\n- Consider the use of open-source models, which provide more transparency about datasets, code, and training processes.\n\n- Implement transparency and auditing requirements for suppliers as outlined in the Buying generative AI section.",
    "## Accountability and Responsibility\n\nEnsuring accountability for generative AI means that individuals and organisations can be held accountable for the AI systems they develop, deploy, or use, and that human oversight is maintained.\n\nTo establish accountable practices across the AI lifecycle, consider three key elements:\n\n1.\n\n**Answerability:** Establish a chain of human responsibility across the generative AI project lifecycle, including throughout the supply chain.\n\nRecourse and feedback mechanisms need to be established for affected individuals in cases of harm or errors caused by generative AI.\n\nIdentifying specific actors involved in generative AI systems is vital to answerability, including model developers, application developers, policymakers, regulators, system operators, and end-users.\n\nThe roles and responsibilities of each must be clearly defined and aligned with legal and ethical standards.\n\n2.\n\n**Auditability:** Demonstrate the responsibility and trustworthiness of the development and deployment practices by upholding robust reporting and documentation protocols, retaining traceability throughout the AI lifecycle.\n\nDocument all stages of the generative AI innovation lifecycle from data collection and base model training to implementation, system deployment, updating, and retirement, in a way that is accessible to relevant stakeholders.\n\n3.\n\n**Liability:** Ensure that all parties involved in the generative AI project lifecycle, from vendors and technical teams to system users, are acting lawfully and understand their respective legal obligations.\n\nResponsibility for any output or decision made or supported by an AI system always rests with the public organisation.",
    "### Practical Recommendations\n\n- Follow existing legal provisions, guidelines, and policies as well as the provider’s terms of use when using generative AI.\n\n- As an end-user, assume responsibility for output produced by generative AI tools.\n\n- Clearly define responsibilities, accountability, and liability across all actors involved in the AI lifecycle.\n\n- Nominate a Senior Responsible Owner accountable for the use of generative AI in a specific project.\n\n- In high-risk contexts, establish a human-in-the-loop to oversee and validate outputs.\n\n- Adopt a risk-based approach to AI-generated content, minimizing risks of inaccuracies or harmful outputs.\n\n- Provide routes for appeal and actionable redress, and put feedback channels into place.\n\n- Use assurance techniques to evaluate the performance of generative AI systems.",
    "## Fairness, Bias, and Discrimination\n\nFairness is a concept embedded across regulations like equality and human rights, data protection, and consumer laws.\n\nAI systems should not undermine individual rights, discriminate, or create unfair outcomes.\n\nMitigating bias supports fair AI outcomes.\n\nHarmful biases in generative AI may result in stereotypical or unfair treatment.\n\nBecause generative AI is trained on data encoding societal biases, identifying and addressing these biases is essential.",
    "### Practical Recommendations\n\n- Comply with human rights laws and relevant regulations.\n\n- Minimize bias by using professional and neutral language in writing prompts.\n\n- Review generated output for potentially harmful content.\n\n- Test prompts for bias by modifying demographic information and comparing outputs.\n\n- Implement feedback mechanisms for reporting harmful content.\n\n- Incorporate bias mitigation and fairness evaluation throughout the AI lifecycle.\n\n- Strive for diversity within development teams and gather feedback from diverse groups.\n\n- Continuously evaluate fairness considerations and societal expectations.",
    "### Practical Recommendations\n\n- Improve output quality through specific and structured prompts.\n\n- Verify AI-generated content against trusted sources.\n\n- Notify when AI-generated content is used.\n\n- Assess the impact of AI-generated content on misinformation risks.\n\n- Regularly review and improve model performance through governance and oversight.\n\n- Use watermarking to detect generatively-produced outputs.",
    "### Practical Recommendations\n\n- Involve data compliance professionals from the start.\n\n- Map personal data sources and flows, determining necessity at each step.\n\n- Consult with ICO if high data risks remain after mitigations.\n\n- Uphold transparency regarding personal data use and manage storage limitations effectively by respecting data minimization principles.\n\nThrough these measures and principles, generative AI applications can be developed and used responsibly, maintaining trust, equity, and alignment with legal frameworks.",
    "# Human Oversight\n\nAlthough it is possible to use generative AI systems for automated decision making where the system makes a decision automatically without any human involvement, this may infringe the UK GDPR.\n\nUnder Article 22, the UK GDPR currently prohibits “decision(s) based solely on automated processing” that have legal or ‘similarly significant’ consequences for individuals.\n\nServices that affect a person’s legal status or their legal rights using generative AI must only use it for decision-support, where the system only supports a human decision-maker in their deliberation.\n\nGenerative AI systems need to bring processes into training, testing and output stages so that humans work together with machines to perform tasks, combining their abilities to reach best results.\n\nHowever, the human input needs to be ‘meaningful’.\n\nThe degree and quality of human review and intervention before a final decision is made about an individual are key factors in determining whether a generative AI system is being used for automated decision-making or merely as decision-support.\n\nThere are a number of factors that should determine the amount of human involvement in generative AI, such as the complexity of the output, its potential impact, the amount of specialist human knowledge required.\n\nAs an example, generative AI systems deployed in legal, health and care are likely to always require human involvement no matter how exceptional the technology.\n\nWhile focusing on generative AI risks, it is important to consider biases at organisational and human review levels.\n\nHumans and generative AI technology have different strengths and weaknesses when it comes to ensuring fair outcomes.\n\nGenerative AI cannot use emotional intelligence, nuance, or an understanding of the broader context.\n\nAt the same time, humans have their own unconscious biases and beliefs that influence their reasoning.\n\nThis points back to the importance of the accountability principle, robust governance structures for oversight and alignment of generative AI and existing business processes, such as risk management.\n\nFurther aspects on human oversight for generative AI systems can be found in the Ethics section.",
    "# Accuracy\n\nAccuracy in the context of data protection requires that personal data is not factually incorrect or misleading, and where necessary, is corrected, deleted and kept up to date without delay.\n\nYou need to put in place appropriate mathematical and statistical procedures as part of your technical measures to correct inaccuracies in personal data and minimise errors.\n\nGenerative AI outputs should be tested against existing knowledge and expertise in early implementations of those outputs.\n\nThe outputs of a generative AI system are not always intended to be treated as factual information about the individual but instead represent a ‘statistically informed guess’.\n\nYou need to factor in the possibility of them being incorrect and the impact this may have on any decisions.\n\nTo avoid such misinterpretations of outputs as factual, systems should be explicit that they are statistically informed guesses rather than facts, including information about the source of the data and how the inference has been generated.\n\nFor more information see the Getting reliable results section.",
    "# Security\n\nThe UK government has a responsibility to ensure that the services it provides do not expose the public to undue risk, which makes security a primary concern for anyone looking to deploy emerging technology, such as generative AI.\n\nThis section takes you through how to keep generative AI solutions in government secure:\n- How to deploy generative AI securely\n- Security risks\n- Practical security recommendations",
    "## How to Deploy Generative AI Securely\n\nGenerative AI can be deployed in many different ways.\n\nThe approaches set out below present different security challenges and can affect the level of risk that must be managed.\n\nThis section covers different approaches that you need to take for:\n- Public generative AI applications and web services\n- Embedded generative AI applications\n- Public generative AI APIs\n- Privately hosted open-source generative AI models\n- Data provenance\n- Working with your organisational data\n- Open-source vs closed-source models\n\nFor additional information see the section on deployment patterns.",
    "### Public Generative AI Applications and Web Services\n\nThe use of public chatbots such as ChatGPT or Google Bard are easier to use compared to open-source, bespoke solutions.\n\nHowever, a key disadvantage of allowing the use of public applications is that you cannot easily control the data input to the models and must rely on training users on what they can and cannot enter into the chat prompt.\n\nYou also have no control on the outputs from the model and are subject to their commercial licence agreements and privacy statements, for example, OpenAI will use the prompt data you enter directly into the ChatGPT website to improve their models, although individual users can opt out.",
    "### Embedded Generative AI Applications\n\nAs well as these more direct approaches to using generative AI, many vendors include generative AI features and capabilities directly within their products, for example, Slack GPT and Microsoft 365 Copilot.\n\nWhilst this guidance applies at a high level to each of these applications, they come with their own unique security concerns.\n\nYou should speak to your security teams to discuss your requirements.\n\nIn addition to embedded applications, there are also many generative AI tools that offer plugins or extensions to other software.\n\nFor example, Visual Studio Code has a large ecosystem of community-built extensions, many of which offer generative AI functionality.\n\nExtreme caution should be taken before installing any unverified extensions as these are likely to present a security risk.\n\nYou should speak to your security team to discuss your requirements.\n\nBefore adopting any of these products, it is important to understand the underlying architecture of the solution and what mitigations the vendor has put in place for the inherent risks associated with generative AI.\n\nAll of these different approaches come with trade-offs between security, privacy, usability, and cost.\n\nEach of the security risks of generative AI models need to be taken in context with the way the model is deployed and used to inform the level of risk that an application poses.",
    "### Public Generative AI APIs\n\nMany public generative AI applications usually offer the ability to access their services through APIs, which define the set of rules, protocols, and tools for building software applications.\n\nThrough using the API, it can be very easy to integrate generative AI capabilities into your own applications.\n\nThe benefit here is that you can intercept the data being sent to the model and also process the responses before returning them to the user.\n\nYou can also include PET to prevent data leakage, add content filters to sanitise the prompts and responses, and log and audit all interactions with the model.\n\nNote that PETs come with their own limitations, therefore selection of the PET should be proportionate to the sensitivity of the data: see ICO’s privacy-enhancing technologies (PETs) and CDEI’s PET adoption guide for more information.\n\nThe use of the API still means that data is passed over to the provider, although the retention policies tend to be more flexible for API use.\n\nFor example, OpenAI only retains prompt data sent to the API for 30 days.",
    "### Privately Hosted Open-Source Generative AI Models\n\nInstead of using a public generative AI offering, the alternative is to host your own generative AI model.\n\nBy taking one of the many publicly available open-source models and running it in your own private cloud infrastructure, you ensure that data never leaves an environment that you own.\n\nThe type of models that you can run in this way are not on the scale of those that are publicly available but can still provide acceptable results.\n\nThe advantage is that you have complete control over the model and the data it consumes.\n\nThe disadvantage is that you are responsible for ensuring the model is secure and up to date.\n\nAn alternative approach is to use one of the larger commercial models, but in a private managed instance.\n\nFor example, the Microsoft Azure OpenAI service offers access to the OpenAI ChatGPT models but running in a private instance with zero-day retention policies.",
    "### Data Provenance\n\nIn addition to where your generative AI model runs, how the model was trained is also important from a security perspective.\n\nAll the publicly available models were trained using data from the public internet.\n\nThis means that they include data that is personally identifiable, inaccurate, illegal and harmful, all of which could present a security risk.\n\nIt is possible to train an LLM using your own data, but the cost of doing this for larger and more capable models is prohibitive.\n\nAlong with the cost, the amount of private data required to produce acceptable performance of a large model is also beyond the capacity of most organisations.",
    "### Working with Your Organisational Data\n\nA key application of generative AI is working with your organisation’s private data.\n\nBy enabling the model to access, understand and use the private data, insights and knowledge can be provided to users that is specific to their subject domain and will provide more reliable results.",
    "### Open-Source vs Closed-Source Models\n\nNeither open-source nor closed-source LLMs are inherently less secure than the other.\n\nA fully open-source model may expose not only the model code, but also the weights of its parameters and the data used to train the model.\n\nWhile this increases transparency, it also potentially presents a greater risk, as knowing the weights and the training data could allow an attacker to create attacks carefully tailored to the specific LLM.\n\nOne benefit of fully open-source models is that they allow you to inspect the source code and model architecture, enabling security experts to audit the code for vulnerabilities.\n\nDespite this, owing to their complexity, even an open-source LLM is mostly opaque, meaning that the internals of the model are hard to analyse.\n\nOpen-source models theoretically benefit from a community of developers, who can quickly identify and fix security issues, whereas closed-source model owners might be incentivised not to publicise security flaws in their models.\n\nHowever, it should be noted that several high-profile vulnerabilities in open-source libraries have been present for many years before being identified.",
    "## Security Risks\n\nSignificant work has already been done by the Open Worldwide Application Security Project (OWASP) to identify the unique risks posed by LLMs.\n\nFrom these, we can draw out some of the most common vulnerabilities and put them in context of how they could apply to LLM applications in government.\n\nThese risks focus on the use of LLMs but many of them will also apply to other types of generative AI models.\n\nWe take each security risk and use a scenario describing an application of generative AI in a government context, to illustrate how that vulnerability might be exploited.\n\nThe list of scenarios is not exhaustive but should be used as a template for assessing the risks associated with a particular application of generative AI.\n\nImpacts are described for each scenario, and mitigations suggested.\n\nThe likelihood and impact of each risk in a given scenario are scored, following the approach outlined in the OWASP risk rating methodology.\n\nIn addition to the impact factors included in the OWASP approach, we add user harm and misinformation as significant impact factors.",
    "### Security Threats Include:\n\n- **Prompt injection threats**: using prompts that can make the generative AI model behave in unexpected ways:\n  - LLM chatbot on a government website\n  - LLM enhanced search on a government website\n  - private LLM chatbot returning suggested file sources\n\n- **Data leakage**: responses from the LLM reveal sensitive information, for example, personal data:\n  - intranet search engine enhanced with LLM\n  - private LLM chatbot summarises chat conversations\n\n- **Hallucinations**: the LLM responds with information that appears to be truthful but is actually false:\n  - developer uses LLM generated code without review",
    "#### Prompt Injection Threats\n\nPrompt injections can either be direct, meaning a user directly enters a prompt into the LLM to subvert its behaviour.\n\nOr they can be indirect, meaning the LLM gets input from an external source, and that source has been manipulated to include a prompt injection, for example from an email or an external file.",
    "**Scenario**\n\nA chatbot is deployed to a government website to assist with queries relating to a particular public service.\n\nThe chatbot uses a private instance of one of the publicly trained LLMs.\n\nThe user’s question is combined with system instructions that tell the LLM to only respond to questions relevant to the specific service.\n\nThe system instructions are combined with the user’s original question and sent to the LLM.\n\nA malicious user could craft a specific prompt that circumvents the system instructions and makes the chatbot respond with irrelevant and potentially harmful information.\n\nThis is an example of a direct prompt injection attack.",
    "**Impact**\n\nActual risk of user harm if a user is tricked into using an unsafe prompt that then results in harmful content being returned and acted on, for example: a user is looking for how to pay a bill and is directed to a false payment site.\n\nReputational damage to the government, if a user made public potentially harmful responses received from the chatbot, for example: a user asking for generic information receives an inflammatory response.",
    "**Mitigation**\n\n- Use prompt engineering to attach a meta prompt to any user input to prevent the LLM from responding to malicious input.\n\n- Apply content filters trained to detect likely prompt injections to all prompts sent to the LLM.\n\n- Choose a more robust model.\n\nSome models have been shown to be more resistant to this kind of attack than others.\n\nNone of these mitigations are sufficient to guarantee that a prompt injection attack would not succeed.\n\nFundamentally, an LLM cannot distinguish between user input and system instructions.\n\nBoth are processed by the LLM as natural language inputs so there is no way to prevent a user prompt affecting the behaviour of the LLM.\n\n**Risk rating**: Likelihood: HIGH Impact:\nLOW – response is returned to a single user with limited repercussions.\n\nHIGH – response causes actual harm to a user.",
    "**Recommendation**\n\nDeploying an LLM chatbot to a public-facing government website does come with a significant risk of a direct prompt injection attack.\n\nThe impact of such an attack should be considered in the context of the specific use case.\n\nFor example, a chatbot deployed to a small number of users for the purposes of gathering data about the effectiveness of LLMs under controlled conditions is far lower risk than one that is more generally available and designed to make specific, actionable recommendations to a user.",
    "**Scenario**\n\nA private LLM is used to enhance the search capabilities of a public-facing government website, without providing a chatbot interface.\n\nThe content of the government website is initially split into small chunks of text and vector indexed using a machine learning (ML) algorithm.\n\nA user enters a natural language search term.\n\nThe ML algorithm processes the search term into a vector, and a similarity search is done against the vector indexed database of text chunks.\n\nThe most relevant chunks are retrieved and passed in context to the LLM, along with the user’s search term, and system instructions telling the LLM to return a summary of the search results.\n\nA malicious user could craft a specific search term that circumvents the system instructions making the summary contain potentially harmful information.",
    "**Mitigation**\n\n- Apply content filters trained to detect likely prompt injections to all prompts sent to the LLM.\n\n- Filter the summarised search results returned by the LLM to ensure they contain only information relating to the government website.\n\n- Do not pass the original search term to the LLM.\n\n**Risk rating**: Likelihood: MEDIUM Impact:\nLOW – response is returned to a single user with limited repercussions.\n\nHIGH – response causes actual harm to a user.",
    "**Recommendation**\n\nThis scenario presents lower risk than directly hosting an LLM chatbot on a government website (Scenario 1), as a level of indirection exists between the search term entered by the user and the prompt sent to the LLM.\n\nHowever, if the search term is passed to the LLM along with the search results in context, then a direct prompt injection would still work.\n\nTo stop this, no part of the search term should be passed to the LLM.\n\nThe trade-off here is that this is likely to reduce the usefulness of the enhanced search.",
    "**Scenario**\n\nA chatbot is deployed into an internal departmental messaging system (for example Google Chat).\n\nThe chatbot calls out to a privately hosted open-source LLM running within the department’s cloud.\n\nThe chatbot scans attachments posted in the chat, and passes the content of these to the LLM, with system instructions telling the LLM to augment its responses with links to relevant information held in departmental files.\n\nA user posts an attachment that unknown to them has been manipulated to contain a prompt injection.\n\nThe chatbot passes the attachment content as input to the private LLM.\n\nThe resulting response contains a list of links to relevant files on the department’s shared drives.\n\nThe prompt injection alters the list of links so that they direct the user to an insecure third-party website rather than the original file.\n\nThis is an example of an indirect prompt injection attack.",
    "**Mitigation**\n\n- Apply content filters to attachments before they are passed to the LLM.\n\n- Apply filters to the response generated by the LLM, to ensure any links contained in it are only to known resources.\n\n- Ensure network controls are enforced that prevent users following dangerous links.\n\n**Risk rating**: Likelihood: MEDIUM Impact: MEDIUM",
    "**Recommendation**\n\nThis scenario does not present a significant risk.\n\nHowever, the impact is highly dependent on the actions the response from the LLM is used to make, in this case only generating a list of links.\n\nIf the response was used to send emails, or modify files or records, the impact would be much greater.\n\nLLM responses must not automatically lead to destructive or irreversible actions.\n\nA human must be present to review the action.",
    "**Scenario**\n\nA private LLM is used to enhance the search capabilities of an internal departmental search engine.\n\nThe departmental data (documents, emails, web pages and directory information) is initially split into small chunks of text and vector indexed using a ML algorithm.\n\nA user enters a natural language question, for example: “How do I apply for compassionate leave?”.\n\nThe ML algorithm processes the user’s question into a vector, and a similarity search is done against the vector indexed database of text chunks.\n\nThe most relevant chunks are retrieved and passed in context to the LLM, along with the user’s question, and system instructions telling the LLM to tailor its responses to the user’s question using information in the retrieved text.\n\nThe LLM responds with confidential information that has been inadvertently retrieved by the vector index search.\n\nFor example, it may return details about who is currently on compassionate leave and the reasons why.",
    "**Mitigation**\n\nEnsure that any vector index database respects source data security controls.\n\nThe identity of the search user must be passed to the similarity search so that appropriate controls can be applied.\n\nThis prevents the LLM receiving content that the user is not permitted to see.\n\n**Risk rating**: Likelihood: MEDIUM Impact: HIGH",
    "**Recommendation**\n\nIn this scenario security controls can be preserved.\n\nHowever, if the LLM was to be fine-tuned with private data or trained directly with private data, then there would be no way of applying the original data security controls owing to the way the LLM encodes the data it is trained with.\n\nPrivate data which contains confidential information or employs different levels of security controls must not be used to fine-tune or train an LLM.",
    "**Scenario**\n\nA chatbot is deployed into an internal departmental messaging system (for example Google Chat).\n\nThe chatbot calls out to a privately hosted open-source LLM running within the department’s cloud.\n\nThe chatbot scans the conversation thread and summarises the content.\n\nA prompt injection in the conversation thread causes the chatbot to emit the summary of the thread in the form of an obfuscated link to a malicious site, for example  (this link is safe).\n\nThe chat interface unfurls the link posted in the response, automatically calling out to the malicious site and transferring the encoded summary of the chat.",
    "**Recommendation**\n\nIn this scenario prompt injection can be used to perform data exfiltration without any action required by the user.\n\nThe risk can be mitigated by removing malicious links in the response from the LLM.\n\nMore generally, LLM responses that will be read by humans should avoid using links to external resources, and if external links are provided then the response must be filtered to remove malicious URLs.",
    "**Scenario**\n\nA developer uses a public LLM to answer coding questions, and receives advice to install a specific software package, for example ‘arangodb’ from the JavaScript package management system npm.\n\nWhen the LLM was trained, the package did not exist.\n\nA hacker has previously interrogated the LLM with common coding questions and identified this hallucination.\n\nThey have then created a malicious package with the fictitious name and registered it with the package management system.\n\nWhen the developer now comes to install the package, they receive the malicious code.",
    "**Mitigation**\n\n- Do not rely on the responses of the LLM.\n\nDouble check all outputs before including them in your code.\n\n- Check all package dependencies of your code before deployment.\n\n- Use an automated tool to scan for supply chain vulnerabilities, for example, ‘dependabot’ or ‘snyk’.\n\n**Risk rating**: Likelihood: LOW Impact: HIGH",
    "**Recommendation**\n\nIf developers are following secure coding best practices, the risk should never arise as all dependencies should be checked before deployment.\n\nOver-reliance on LLM generated code without sufficient human oversight is likely to become an increasing risk.\n\nTreat all LLM generated code as inherently insecure and never use it directly in production code without first doing a code review.",
    "## Practical Security Recommendations\n\n- Design risk-driven security taking account of the OWASP Top 10 security risks for LLMs.\n\n- Use a consistent risk rating methodology to assess the impact and likelihood of each risk.\n\n- Minimise the attack surface by only using the required capabilities of the generative AI tool, for example, by avoiding sending user input directly to an LLM.\n\n- Defend in depth by adding layers of security, for example, by using PET to prevent data leakage and adding content filters to sanitise the prompts and responses from an LLM.\n\n- Never use private data that needs different levels of access permissions based on the user who is viewing it, to fine-tune or train an LLM.\n\n- Prevent LLM responses automatically leading to destructive or irreversible actions, such as sending emails or modifying records.\n\nIn these situations, a human must be present to review the action.\n\n- Avoid using links to external resources in LLM responses that will be read by humans, and if external links are provided then the response must be filtered to remove malicious URLs.\n\n- Treat all LLM generated code as inherently insecure and never use it directly in production without code review.\n\n- Never enter any OFFICIAL or SENSITIVE information directly into public generative AI applications or APIs, unless it is already publicly available or cleared for publication.\n\nExceptions may apply for specific applications with different data handling terms provided under commercial licences, for example, Microsoft Copilot, Azure Open AI, or Bing Enterprise Chat.\n\n- Avoid putting LLM chatbots on public facing government websites, unless the risk of direct prompt injection is acceptable under the specific use case.",
    "# Governance\n\nBecause of the risks around security, bias, and data, all AI programmes need strong governance processes.\n\nWhether they are already built into existing governance frameworks or a new governance framework, the processes should be focused on:\n- Continuous improvement by including new knowledge, methods, and technologies\n- Identifying key stakeholders representing different organisations and interests such as Civil Society Organisations and sector experts to create a balanced view from stakeholders so that they can support AI initiatives\n- Planning for the long-term sustainability of AI initiatives, considering scalability, long-term support, maintenance, and future developments\n\nAs part of any governance framework, organisations should consider setting up a separate AI governance board or have AI representation on a governance board and an ethics committee.\n\nAn AI governance board and an ethics committee are components of responsible AI implementation within an organisation or department which play different and distinct roles and responsibilities.",
    "## AI Governance Board or AI Representation on an Existing Board\n\nIn general, an AI governance board covers aspects such as alignment to ethical principles, risk management, compliance, assurance, resource allocation, stakeholder engagement, and alignment with business objectives.\n\nAn AI governance board or representation on a board provides oversight, accountability, and strategic guidance to make informed decisions about AI adoption and use.",
    "# Responsible and Effective AI Outcomes\n\nThe board holds the organisation accountable for achieving responsible and effective AI outcomes and helps ensure AI projects are aligned with ethical values.\n\nIts scope is broader, including operational and strategic considerations.\n\nAlongside support and input from your organisation’s internal assurance team, a board or an AI representative on a board will help you make sure your project is on track and manage risks.",
    "## Ethics Committee\n\nThe primary focus of an ethics committee is to assess the ethical implications of various actions, projects, and decisions within the organisation.\n\nIt evaluates projects, policies, and actions from an ethical standpoint, focusing on values such as fairness, transparency, and privacy.\n\nIt typically includes legal experts, representatives from relevant organisations, community members, and other stakeholders who provide a specialised perspective on ethical matters and may also include Civil Society Organisations.\n\nSee the Ethics section for related content.",
    "## Creating an AI/ML Systems Inventory\n\nTo support the work, organisations should consider setting up AI and ML systems inventory to provide a comprehensive view of all deployed AI systems within an organisation.\n\nIt helps management and stakeholders understand the scope and scale of AI usage across programmes and projects, providing better oversight and awareness of any AI used in making decisions, and potential risks such as data quality, model accuracy, bias, security vulnerabilities, and regulatory compliance.\n\nThe inventory should be regularly kept up to date with the following details:\n- Describe each system’s purpose, usage, and associated risks.\n\n- Include details like data elements, ownership, development, and key dates.\n\n- Employ protocols, structures, and tools for maintaining an accurate and comprehensive inventory.",
    "## Programme Governance in Teams\n\n- Set out how the model will be maintained over time, and develop a comprehensive plan for knowledge transfer and training to ensure the model’s sustainable management.\n\n- Establish clear roles and responsibilities to ensure accountability within teams for AI systems, including who has the authority to change and modify the code of the AI model.\n\n- Establish pathways for escalation and identify key points of contact for specific AI-related issues.\n\n- Set out how they work with and report into their programme boards and the ethics committee.\n\n- Ensure diversity within the project team by incorporating a range of subject matter expertise, skills, and lived experiences.",
    "# Developing Policy and Protocols for the use of Generative AI in K-12 Classrooms\n\nPrior to setting school and district policies on the use of AI technologies, it is vital that school and district leaders are aware of federal and state policies that impact the use of these technologies both regarding student data privacy in addition to other states and countries that lead in this area.\n\nWhile there are a number of schools across the nation (and world) that have made the decision to ban the use of AI technologies and ChatGPT, when making this decision it is important to consider the learning opportunities that might be limited for students.\n\nBeing aware of other policies both within and outside of Oregon can be helpful in making informed decisions.\n\nThe US Office of Educational Technology is currently working on developing policies and supports that focus on the effective, safe, and fair use of AI-enabled educational technology.\n\nTheir Artificial Intelligence website can provide a great starting point for understanding current policies in this area.\n\nAdditionally, with the increased interest in, and attention on AI technology, other countries have developed resources to guide in the use of AI in education.\n\nFor example, The European Commission recently released “Ethical guidelines on the use of artificial intelligence (AI) and data in teaching and learning for Educators” which provides guidance and resources for school leaders.",
    "## Federal Policies\n\nWhile there have been no federal policies related to the use of AI technologies in education, at the release of this resource, in March 2023, the U.S.\n\nCopyright Office, Library of Congress, released a policy that has impacts on the use of works containing material generated by artificial intelligence.\n\nThe full text of this policy can be reviewed here.1 The US Office of Educational Technology published a brief in May 2023 titled “Artificial Intelligence and the Future of Teaching and Learning.” This brief provides insights and recommendations regarding building ethical, equitable policies in addition to information about the use of AI in teaching, best practices for instruction including formative assessment, and research around the use of AI in classrooms and beyond.\n\nThis brief along with additional work supported by the US Office of Educational Technology supports a focus on the effective, safe, and fair use of AI-enabled educational technology.\n\nTheir Artificial Intelligence website can provide a great starting point for understanding current policies in this area.\n\n1 If additional related federal policies are developed, this section of the guidance will be updated to reflect the most current policies available.",
    "## Policies from Across the Nation\n\nAs K-12 schools and districts spend more time learning about the use of generative AI in classrooms, policies continue to change at a rapid pace with districts that initially banned platforms such as ChatGPT shifting toward embracing its potential in the classroom2.\n\nWhile some schools, districts, and states have continued their ban on generative AI, particularly platforms such as ChatGPT, many have done so as a temporary measure in order to engage in risk assessments and develop plans to train educators.3 An example that can provide a starting point for schools and districts in developing their AI policies and procedures can be drawn from the International Baccalaureate.\n\nThey have developed a “Statement from the IB about ChatGPT and artificial intelligence in assessment and education.” This statement calls for developing policy that supports students in using generative AI tools in ways that are ethical and aligned with principles of academic integrity.",
    "## International Policies\n\nThe United Nations developed a resource “AI and Education: Guidance for Policy-Makers” that provides important information regarding other countries' policies related to the use of AI in schools.\n\nWhile this resource focuses on the use of AI broadly rather than generative AI specifically, it includes important considerations for policy development and provides language from other countries' policies that can be useful.",
    "## AI Policy and Protocol Development Planning and Reflection Tool4\n\nBased on the information in this resource, there are several action steps that school and district administrators can take in an effort to create clear, meaningful, and equitable policy around the use of AI technologies in classrooms.\n\nThe table below can serve as a starting point for reflection, discussion, and policy development.\n\nThese questions were modified from questions included in COSN (2020).\n\nArtificial Intelligence in K-12 as well as The Office of Educational Technology (2023) Artificial Intelligence and the Future of Teaching and Learning.\n\nRetrieved from:  AI technologies are used across agencies and systems with research showing that the use of AI has negatively impacted people from historically and systemically marginalized communities.\n\nExamples of such research can be found in Ruha Benjamin’s (2019) book Race After Technology: Abolitionist Tools for the New Jim Code as well as in the ProPublica article “Machine Bias”.\n\nFor more information, please contact ODE’s Digital Learning Team at ."
]