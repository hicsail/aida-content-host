[
    "## Group Claims Existing Mechanisms for Avoiding AI Harms Will Likely Not Suffice\n\nNew York, NY, July 11, 2023 – In response to major advances in Generative AI technologies—as well as the significant questions these technologies pose in areas including intellectual property, the future of work, and even human safety—the Association for Computing Machinery’s global Technology Policy Council (ACM TPC) has issued “Principles for the Development, Deployment, and Use of Generative AI Technologies.”\n\nDrawing on the deep technical expertise of computer scientists in the United States and Europe, the ACM TPC statement outlines eight principles intended to foster fair, accurate, and beneficial decision-making concerning generative and all other AI technologies.\n\nFour of the principles are specific to Generative AI, and an additional four principles are adapted from the TPC’s 2022 “Statement on Principles for Responsible Algorithmic Systems.”\n\nThe Introduction to the new Principles advances the core argument that “the increasing power of Generative AI systems, the speed of their evolution, broad application, and potential to cause significant or even catastrophic harm, means that great care must be taken in researching, designing, developing, deploying, and using them.\n\nExisting mechanisms and modes for avoiding such harm likely will not suffice.”\n\nThe document then sets out these eight instrumental principles, outlined here in abbreviated form:",
    "### Generative AI-Specific Principles\n\n**Limits and guidance on deployment and use:** In consultation with all stakeholders, law and regulation should be reviewed and applied as written or revised to limit the deployment and use of Generative AI technologies when required to minimize harm.\n\nNo high-risk AI system should be allowed to operate without clear and adequate safeguards, including a “human in the loop” and clear consensus among relevant stakeholders that the system's benefits will substantially outweigh its potential negative impacts.\n\nOne approach is to define a hierarchy of risk levels, with unacceptable risk at the highest level and minimal risk at the lowest level.\n\n**Ownership:** Inherent aspects of how Generative AI systems are structured and function are not yet adequately accounted for in intellectual property (IP) law and regulation.\n\n**Personal data control:** Generative AI systems should allow a person to opt out of their data being used to train a system or facilitate its generation of information.\n\n**Correctability:** Providers of Generative AI systems should create and maintain public repositories where errors made by the system can be noted and, optionally, corrections made.",
    "### Adapted Prior Principles\n\n**Transparency:** Any application or system that utilizes Generative AI should conspicuously disclose that it does so to the appropriate stakeholders.\n\n**Auditability and contestability:** Providers of Generative AI systems should ensure that system models, algorithms, data, and outputs can be recorded where possible (with due consideration to privacy), so that they may be audited and/or contested in appropriate cases.\n\n**Limiting environmental impact:** Given the large environmental impact of Generative AI models, we recommend that consensus on methodologies be developed to measure, attribute, and actively reduce such impact.\n\n**Heightened security and privacy:** Generative AI systems are susceptible to a broad range of new security and privacy risks, including new attack vectors and malicious data leaks, among others.\n\n“Our field needs to tread carefully with the development of Generative AI because this is a new paradigm that goes significantly beyond previous AI technology and applications,” explained Ravi Jain, Chair of the ACM Technology Policy Council’s Working Group on Generative AI and lead author of the Principles.\n\n\"Whether you celebrate Generative AI as a wonderful scientific advancement or fear it, everyone agrees that we need to develop this technology responsibly.\n\nIn outlining these eight instrumental principles, we’ve tried to consider a wide range of areas where Generative AI might have an impact.\n\nThese include aspects that have not been covered as much in the media, including environmental considerations and the idea of creating public repositories where errors in a system can be noted and corrected.”\n\n“These are guidelines, but we must also build a community of scientists, policymakers, and industry leaders who will work together in the public interest to understand the limits and risks of Generative AI as well as its benefits.\n\nACM’s position as the world’s largest association for computing professionals makes us well-suited to foster that consensus and look forward to working with policy makers to craft the regulations by which Generative AI should be developed, deployed, but also controlled,” added James Hendler, Professor at Rensselaer Polytechnic Institute and Chair of ACM’s Technology Policy Council.\n\n“Principles for the Development, Deployment, and Use of Generative AI Technologies” was jointly produced and adopted by ACM’s US Technology Policy Committee (USTPC) and Europe Technology Policy Committee (Europe TPC).\n\nLead authors of this document for USTPC were Ravi Jain, Jeanna Matthews, and Alejandro Saucedo.\n\nImportant contributions were made by Harish Arunachalam, Brian Dean, Advait Deshpande, Simson Garfinkel, Andrew Grosso, Jim Hendler, Lorraine Kisselburgh, Srivatsa Kundurthy, Marc Rotenberg, Stuart Shapiro, and Ben Shneiderman.\n\nAssistance also was provided by Ricardo Baeza-Yates, Michel Beaudouin-Lafon, Vint Cerf, Charalampos Chelmis, Paul DeMarinis, Nicholas Diakopoulos, Janet Haven, Ravi Iyer, Carlos E. Jimenez-Gomez, Mark Pastin, Neeti Pokhriyal, Jason Schmitt, and Darryl Scriven.",
    "## About the ACM Technology Policy Council\n\nACM’s global Technology Policy Council sets the agenda for global initiatives to address evolving technology policy issues and coordinates the activities of ACM's regional technology policy committees in the US and Europe.\n\nIt serves as the central convening point for ACM's interactions with government organizations, the computing community, and the public in all matters of public policy related to computing and information technology.\n\nThe Council’s members are drawn from ACM's global membership.",
    "## About ACM\n\nACM, the Association for Computing Machinery, is the world’s largest educational and scientific computing society, uniting computing educators, researchers, and professionals to inspire dialogue, share resources, and address the field’s challenges.\n\nACM strengthens the computing profession’s collective voice through strong leadership, promotion of the highest standards, and recognition of technical excellence.\n\nACM supports the professional growth of its members by providing opportunities for life-long learning, career development, and professional networking.",
    "# Introduction\n\nAt Pagefield, we are excited about the transformative potential of AI in our industry.\n\nWe are committed to staying at the forefront of technological advancements, continuously evaluating and integrating cutting-edge AI solutions to accelerate our work and elevate the quality of our deliverables.\n\nWe are dedicated to exploring AI-driven innovations in communication and engagement strategies, whilst also understanding the potential risks which we outline below.\n\nBy using advanced natural language processing and sentiment analysis tools, we can gain deeper insights into public opinion, monitor media trends, and respond swiftly and effectively to emerging issues.\n\nThis empowers us to craft compelling narratives, influence conversations, and protect and enhance the reputations of our clients.\n\nWe are also focused on how we can harness AI technologies to streamline administrative tasks, enabling us to dedicate more time and resources to deliver innovative and strategic communications solutions for our valued clients.\n\nPagefield also prides itself on the personal relationships and human expertise we bring to our client engagements and remains fully invested in the unique insights and creative thinking that only human professionals can provide.\n\nHere are some examples of how you could use generative AI at work: Suggest headlines or text for social media posts – but the final choices would need to be approved by the client lead for accuracy.\n\nCreate stories by speeding up brainstorm-based idea generation e.g.\n\n“suggest stories about the impact of genetic testing on privacy” – but all suggestions would need to be evaluated by the client team and the sources used should be disclosed to the client where appropriate.\n\nSummarise a news story – but make sure you’ve actually read the article yourself and you’re confident the summary is accurate.\n\nExperiment with using AI as a research or analytical tool in a similar way to using Google Search or Wikipedia – but links back to original sources would need to be followed and double checked for any facts or data points used in client work.\n\nBut you mustn’t use AI to: Share confidential information – because not only will you be in breach of your employment contract with Pagefield, you don’t know where the data you’re sharing could end up.\n\nName a specific client – when giving instructions to a generative AI programme, don’t mention the client in question.\n\nFor example, don’t say “Pagefield,” say “a medium sized integrated communications and campaigns agency based in London.” Rely exclusively on AI-generated output – always fact check everything generated by AI as you don’t know the sources it used to create the content; it could be biased or plagiarised.",
    "# Our Policy\n\nIf you choose to use generative AI for work purposes, we ask that you do so transparently, with accountability, and fully in line with the safeguards set out in this policy.\n\nYou are the author and held accountable for the work you produce.\n\nThis policy applies to all employees who use generative AI programmes on company-owned devices or networks or on their personal devices when conducting business on behalf of Pagefield and associated clients.\n\nEmployees must use AI programmes responsibly and for legitimate business purposes only, and only for the purposes for which they were designed and intended.\n\nEmployees must be transparent about their use of generative AI for work purposes and acknowledge the generative AI programme as a source when used with the relevant client lead and colleagues as relevant.\n\nEmployees must always verify data produced via generative AI.\n\nIt should never be used as a single source of the truth and only to supplement additional research or writing methods.\n\nEmployees must safeguard the confidentiality, integrity, and availability of company information at all times.\n\nCompany information relating to Pagefield, its clients or associated organisations and individuals, or information that could reasonably be used to identify Pagefield or its client(s), or a commercially sensitive course of action should never be entered into generative AI technology without the express permission of the client and client lead.\n\nEmployees must not disclose company information or trade secrets to any third party, including AI programmes like ChatGPT, Bard and Midjourney.\n\nAny suspected data leaks or breaches must be escalated immediately to Jacqui Beaumont, our nominated Data Protection Officer, in line with our data protection and privacy policies.\n\nEmployees must not use AI programmes to engage in activities that could damage the reputation of the company or violate the rights of others.\n\nEmployees must not use AI programmes to create or disseminate malicious software or engage in hacking or other unauthorised activities.\n\nEmployees must report any suspicious or unauthorised use of AI programmes to management immediately.\n\nEmployees must remain vigilant for the malicious use of generative AI by scammers.\n\nDo not share any sensitive information in your conversations.\n\nThe company reserves the right to investigate any suspected breaches of this policy, which may subsequently result in disciplinary action, up to and including dismissal.\n\nIt is essential that employees use these tools responsibly and in accordance with Company policies and legal requirements.\n\nThis policy is intended to promote the safe and ethical use of AI programmes and protect the interests of the Company and its stakeholders.\n\nTechnology and the law change regularly, and this policy will be updated to account for changes as and when necessary.\n\nEmployees will be informed when the policy has changed, but it is their responsibility to read the latest version of this document.",
    "### Introduction\n\nIronclad, Inc. (“Ironclad” or the “Company”) recognizes that large language model-based, generative AI applications (collectively, “AI” or “generative AI”), such as OpenAI’s ChatGPT, have the potential to be incredibly useful and time-saving on a variety of tasks.\n\nWe anticipate a growing interest in the use of AI in Ironclad’s business operations.\n\nAt the same time, Ironclad also recognizes that AI technologies are still being refined, are known to produce inaccurate or distorted information, and that the use of AI can create significant risks for the Company.\n\nWe believe it is essential to establish clear guidelines for the responsible use of AI.\n\nThis Policy provides guidelines for using AI in a way that protects Ironclad’s proprietary information and complies with applicable laws, regulations, ethical standards, and Ironclad’s company values.\n\nAs AI is a rapidly evolving technology, Ironclad will review and update this Policy to reflect technological advancements, legal developments, and industry best practices.",
    "### Scope\n\nThis Policy applies to all Ironclad employees, executives, consultants, agents, vendors, and other third parties who have access to Company Data.\n\nFor purposes of this policy, these individuals will be referred to as “Ironclad Staff.” This Policy applies to the direct use of generative AI tools by Ironclad Staff members, separate from any generative integrations that Ironclad has embedded in its products.\n\nAlthough Ironclad may partner with companies offering AI (e.g., Ironclad’s integration with OpenAI), Ironclad Staff’s direct use of AI tools offered by these same companies is not covered by the partnership or services agreements with those companies, but is instead subject to the companies’ terms of use for their AI tools.",
    "### Definitions\n\nThe term “Company Data” should be interpreted broadly for purposes of this Policy, and includes, but is not limited to, at least the following: All Company business information and all personal data (whether of employees, executives, contractors, consultants, Customers, consumers, users, or other persons) that is accessed, collected, used, processed, stored, shared, distributed, transferred, disclosed, destroyed, or disposed of by any of the Company systems; all proprietary information and intellectual property (including, but not limited to, source code, designs, schematics, product roadmaps, product plans, product specifications, market analyses, white papers, strategy documents, financial information, internal communications, Customer lists, Customer files, Customer contact information, Customer contracts, Customer’s proprietary data, and any non-public Company information.\n\nCompany Data includes information in written, electronic, audio, video, or any other form or medium.\n\nCompany Data can include any level of information covered by Ironclad’s Data Classification Matrix.\n\nThe terms “Ironclad Customer” or “Customer” refer to any unique contracting entity listed within an active order form with Ironclad, including all individuals acting on the entity’s behalf.\n\nThe term “Customer Data” refers to any and all data that the third parties who contract as Customers with Ironclad provide to Ironclad to use, store, transmit, or process.",
    "#### Human Backstop\n\nIronclad Staff must carefully review AI-generated material for inaccurate or incomplete information and potential infringement of third-party rights.\n\nYou are ultimately responsible for all content produced with the assistance of AI, as if you were the original creator.\n\nThe source of AI-generated material should be disclosed when appropriate.",
    "#### Required Actions\n\nBefore using any generative AI tool for any Company business, you must opt out of letting generative AI tools use any data you feed the tool to train their AI models (Opt out for OpenAI via this link).\n\nBefore using any generative AI tool for any Company business, consult Ironclad’s Data Classification Matrix to determine the classification of the data you intend to feed into the tool, to determine if it is too sensitive to share.\n\nCarefully review AI-generated material for accuracy, completeness, and protection of both third-party rights and Ironclad’s proprietary information.",
    "#### How You May Use Generative AI\n\nIf you use AI for authorized, Company-related activities, you must use accounts created with Ironclad email addresses/credentials.\n\nYour usage of AI must comply with this Policy, Ironclad’s Code of Business Conduct and Ethics, and the confidentiality obligations in employment documentation signed by Ironclad Staff at the time of hire.\n\nYou may only use data with generative AI tools that is legally obtained and used with the necessary permissions.\n\nYou may only use data with generative AI tools that is not confidential, highly confidential, or restricted, as defined by Ironclad’s Data Classification Matrix.\n\nYou may only use vendor integrations or products featuring generative AI that have been approved by the Legal and Security teams.\n\nYou must report any security incidents or suspected breaches immediately to  and .",
    "### Prohibited Use of AI\n\nDo not use personal accounts with AI tools for Company-related purposes.\n\nDo not use Customer Data with generative AI tools.\n\nDo not use any Company Data classified as confidential, highly confidential, or restricted information (as defined in our Data Classification Matrix).\n\nDo not use personally identifiable information (e.g., people’s names, addresses, emails) with generative AI tools.\n\nDo not use generative AI tools for Company-related purposes if you have not opted out of letting generative AI tools use any data you feed to the tool to train their AI models.",
    "### Enforcement\n\nThe Chief Information Security Officer (“CISO”) and Security Team will verify compliance to this Policy through various methods, including but not limited to, business tool reports, and internal and external audits.\n\nAny exception to the Policy must be approved by the CISO and General Counsel, or designees, in advance.\n\nAny Ironclad Staff member found to have violated this policy may be subject to disciplinary action, up to and including termination of employment or engagement, or legal action where appropriate.",
    "# July 27, 2023\n\nDear Suppliers,\n\nGenerative AI is an emerging technology with great promise of streamlining processes and increasing efficiency.\n\nBut, as with any technology, if not properly used, configured, managed, or secured, there’s a potential to introduce security, data privacy, and confidential information risks (such as data leaks or misuse of data).\n\nTherefore, it is essential to adhere to Intel’s Supplier Policies and Expectations when using Generative AI as it relates to your business processes conducted with Intel.",
    "## Before using any Generative AI tool for work as it relates to Intel information, please review the below guidance:\n\n- Do NOT put any Intel Confidential or Intel Top Secret data, including software code or extracts from contracts or sensitive documents into the application, including in the questions or prompts you enter.\n\n- Do NOT put any Intel personal data into the application.\n\n- Do NOT put any third-party confidential information provided by Intel into the application.\n\nPlease check back often as these details are updated frequently as more is learned in this developing space.\n\nIf any sensitive Intel-related information (as described above) is entered into a Generative AI tool, we require that you (the supplier) contact your Intel Commodity or Account Managers immediately with a copy to Intel’s IP Program Office at , and provide the relevant details.\n\nIntel Supply Chain Risk Management\n\n©2023 Intel Corporation  \nIntel Corporation, 2200 Mission College Blvd., M/S RNB4-145, Santa Clara, CA 95054 USA",
    "# How WIRED Will Use Generative AI Tools\n\nSome publications are already using text and image generators.\n\nHere’s how WIRED will—and won’t—use the technology.\n\nLIKE PRETTY MUCH everyone else in the past few months, journalists have been trying out generative AI tools like ChatGPT to see whether they can help us do our jobs better.\n\nAI software can’t call sources and wheedle information out of them, but it can produce half-decent transcripts of those calls, and new generative AI tools can condense hundreds of pages of those transcripts into a summary.\n\nWriting stories is another matter, though.\n\nA few publications have tried—sometimes with disastrous results.\n\nIt turns out current AI tools are very good at churning out convincing (if formulaic) copy riddled with falsehoods.\n\nThis is WIRED, so we want to be on the front lines of new technology, but also to be ethical and appropriately circumspect.\n\nHere, then, are some ground rules on how we are using the current set of generative AI tools.\n\nWe recognize that AI will develop and so may modify our perspective over time, and we’ll acknowledge any changes in this post.\n\nWe welcome feedback at .",
    "## Text Generators (e.g.\n\nLaMDA, ChatGPT)\n\nWe do not publish stories with text generated by AI, except when the fact that it’s AI-generated is the whole point of the story.\n\n(In such cases we’ll disclose the use and flag any errors.)\n\nThis applies not just to whole stories but also to snippets—for example, ordering up a few sentences of boilerplate on how Crispr works or what quantum computing is.\n\nIt also applies to editorial text on other platforms, such as email newsletters.\n\n(If we use it for non-editorial purposes like marketing emails, which are already automated, we will disclose that.)\n\nThis is for obvious reasons: The current AI tools are prone to both errors and bias, and often produce dull, unoriginal writing.\n\nIn addition, we think someone who writes for a living needs to constantly be thinking about the best way to express complex ideas in their own words.\n\nFinally, an AI tool may inadvertently plagiarize someone else’s words.\n\nIf a writer uses it to create text for publication without a disclosure, we’ll treat that as tantamount to plagiarism.\n\nWe do not publish text edited by AI either.\n\nWhile using AI to, say, shrink an existing 1,200-word story to 900 words might seem less problematic than writing a story from scratch, we think it still has pitfalls.\n\nAside from the risk that the AI tool will introduce factual errors or changes in meaning, editing is also a matter of judgment about what is most relevant, original, or entertaining about the piece.\n\nThis judgment depends on understanding both the subject and the readership, neither of which AI can do.\n\nWe may try using AI to suggest headlines or text for short social media posts.\n\nWe currently generate lots of suggestions manually, and an editor has to approve the final choices for accuracy.\n\nUsing an AI tool to speed up idea generation won’t change this process substantively.\n\nWe may try using AI to generate story ideas.\n\nAn AI might help the process of brainstorming with a prompt like “Suggest stories about the impact of genetic testing on privacy,” or “Provide a list of cities where predictive policing has been controversial.” This may save some time and we will keep exploring how this can be useful.\n\nBut some limited testing we’ve done has shown that it can also produce false leads or boring ideas.\n\nIn any case, the real work, which only humans can do, is in evaluating which ones are worth pursuing.\n\nWhere possible, for any AI tool we use, we will acknowledge the sources it used to generate information.\n\nWe may experiment with using AI as a research or analytical tool.\n\nThe current generation of AI chatbots that Google and Microsoft are adding to their search engines answer questions by extracting information from large amounts of text and summarizing it.\n\nA reporter might use these tools just like a regular search engine, or to summarize or trawl through documents or their own interview notes.\n\nBut they will still have to go back to the original notes, documents, or recordings to check quotes and references.\n\nIn this sense, using an AI bot is like using Google Search or Wikipedia: It might give you initial pointers, but you must follow the links back to the original sources.\n\nIn practice, though, AI will make mistakes and miss things that a human would find relevant—perhaps so much so that it doesn’t save any time.\n\nEven if these tools do prove useful, we won’t want our reporters to rely on them any more than we’d let them rely on the limited information on Wikipedia.\n\nWe’ll continue to insist on the same standards of research and original reporting as always.\n\nWe also know that there are many professionally published research databases out there that come with lawful and highly accurate text- and data-mining tools, so we will constantly evaluate whether those meet our needs.",
    "## Image Generators (e.g.\n\nDall-E, Midjourney, Stable Diffusion)\n\nWe may publish AI-generated images or video, but only under certain conditions.\n\nSome working artists are now incorporating generative AI into their creative process in much the same way that they use other digital tools.\n\nWe will commission work from these artists as long as it involves significant creative input by the artist and does not blatantly imitate existing work or infringe copyright.\n\nIn such cases we will disclose the fact that generative AI was used.\n\nWe specifically do not use AI-generated images instead of stock photography.\n\nSelling images to stock archives is how many working photographers make ends meet.\n\nAt least until generative AI companies develop a way to compensate the creators their tools rely on, we won’t use their images this way.\n\nWe or the artists we commission may use AI tools to spark ideas.\n\nThis is the visual equivalent of brainstorming—type in a prompt and see what comes up.\n\nBut if an artist uses this technique to come up with concepts, we will still require them to create original images using their normal process, and not merely reproduce what the AI suggested.\n\nThis policy was updated on May 22, 2023 to allow the use of AI-generated images and video under certain conditions.\n\nCOOKIES SETTINGS",
    "# NOTE\n\nThis policy is designed to address your employees’ use of third-party generative AI tools like ChatGPT, Google’s Bard, Microsoft Bing, and DALL-E 2 to perform their duties – with or without your knowledge – where the tools being used are not made available by the Company.\n\nThis policy is not intended to establish guidelines for other, approved AI or GenAI tools made available by the Company for employee use.",
    "## Purpose\n\nPublicly available applications driven by generative artificial intelligence (GenAI), such as chatbots (ChatGPT, Google’s Bard, Microsoft Bing) or image generators (DALL-E 2, Midjourney) are impressive and widely popular.\n\nBut while these content-generating tools may offer attractive opportunities to streamline work functions and increase our efficiency, they come with serious security, accuracy, and intellectual property risks.\n\nThis policy highlights the unique issues raised by GenAI, helps employees understand the guidelines for its acceptable use, and protects the Company’s confidential or sensitive information, trade secrets, intellectual property, workplace culture, commitment to diversity, and brand.",
    "## Scope\n\nThis policy applies to the use of any third-party or publicly available GenAI tools, including ChatGPT, Google Bard, DALL-E, Midjourney, and other similar applications that mimic human intelligence to generate answers, work product, or perform certain tasks.\n\n(This policy does not cover other GenAI or AI tools formally approved or installed for your use by the Company.)\n\nOptional: list any GenAI tools that you have approved or installed.",
    "### DO:\n\n- Understand that GenAI tools may be useful but are not a substitute for human judgment and creativity.\n\n- Understand that many GenAI tools are prone to “hallucinations,” false answers or information, or information that is stale, and therefore responses must always be carefully verified by a human.\n\n- Treat every bit of information you provide to a GenAI tool as if it will go viral on the Internet, attributed to you or the Company, regardless of the settings you have selected within the tool (or the assurances made by its creators).\n\n- Inform your supervisor when you have used a GenAI tool to help perform a task.\n\n- Verify that any response from a GenAI tool that you intend to rely on or use is accurate, appropriate, not biased, not a violation of any other individual or entity’s intellectual property or privacy, and consistent with Company policies and applicable laws.",
    "### DO NOT:\n\n- Do not use GenAI tools to make or help you make employment decisions about applicants or employees, including recruitment, hiring, retention, promotions, transfers, performance monitoring, discipline, demotion, or terminations.\n\n- Do not upload or input any confidential, proprietary, or sensitive Company information into any GenAI tool.\n\nExamples include passwords and other credentials, protected health information, personnel material, information from documents marked Confidential, Sensitive, or Proprietary, or any other non-public Company information that might be of use to competitors or harmful to the Company if disclosed.\n\nThis may breach your or the Company’s obligations to keep certain information confidential and secure, risks widespread disclosure, and may cause the Company’s rights to that information to be challenged.\n\n- Do not upload or input any personal information (names, addresses, likenesses, etc.)\n\nabout any person into any GenAI tool.\n\n- Do not represent work generated by a GenAI tool as being your own original work.\n\n- Do not integrate any GenAI tool with internal Company software without first receiving specific written permission from your supervisor and the IT Department.\n\n- [If applicable] Do not use GenAI tools other than those on the approved list from the IT Department.\n\nMalicious chatbots can be designed to steal or convince you to divulge information.",
    "## Disclaimer\n\nNothing in this policy is designed or intended to interfere with, restrain, or prevent employee communications regarding wages, hours, or other terms and conditions of employment or any other rights protected by the National Labor Relations Act.\n\nNOTE: Before implementing this policy, coordinate with your Fisher Phillips attorney to determine if you need to integrate this policy with your specific circumstances and any possible related policies, such as:\n\n- Confidentiality and Trade Secrets\n- Data Security\n- Acceptable Use of Computers and Electronic Media\n- Equal Employment Opportunity\n- Discrimination and Harassment\n- Workplace Code of Ethics\n\nIf your company is regulated by HIPAA/HITECH, GLBA, or FCRA, or you are a federal contractor subject to affirmative action laws, contact your Fisher Phillips attorney to determine the extent to which you need to integrate this policy into your existing policies.\n\nAdditional customizations may be also warranted for certain industries and/or workplaces.",
    "### Researchers compare AI policies and guidelines at 52 news organizations around the world\n\nArtificial intelligence is informing and assisting journalists in their work, but how are newsrooms managing its use?\n\nResearch on AI guidelines and policies from 52 media organizations from around the world offers some answers.\n\nby Clark Merrefield | December 12, 2023 |\n\n(Mojahid Mottakin / Unsplash)\n\nIn July 2022, just a few newsrooms around the world had guidelines or policies for how their journalists and editors could use digital tools that run on artificial intelligence.\n\nOne year later, dozens of influential, global newsrooms had formal documents related to the use of AI.\n\nIn between, artificial intelligence research firm OpenAI launched ChatGPT, a chatbot that can produce all sorts of written material when prompted: lines of code, plays, essays, jokes and news-style stories.\n\nElon Musk and Sam Altman founded OpenAI in 2015, with multibillion dollar investments over the years from Microsoft.\n\nNewsrooms including USA Today, The Atlantic, National Public Radio, the Canadian Broadcasting Corporation and the Financial Times have since developed AI guidelines or policies — a wave of recognition that AI chatbots could fundamentally change the way journalists do their work and how the public thinks about journalism.\n\nResearch posted during September 2023 on preprint server SocArXiv is among the first to examine how newsrooms are handling the proliferating capabilities of AI-based platforms.\n\nPreprints have not undergone formal peer review and have not been published in an academic journal, though the current paper is under review at a prominent international journal according to one of the authors, Kim Björn Becker, a lecturer at Trier University in Germany and a staff writer for the newspaper Frankfurter Allgemeine Zeitung.\n\nThe analysis provides a snapshot of the current state of AI policies and documents for 52 news organizations, including newsrooms in Brazil, India, North America, Scandinavia and Western Europe.\n\nNotably, the authors write that AI policies and documents from commercial news organizations, compared with those that receive public funding, “seem to be more fine-grained and contain significantly more information on permitted and prohibited applications.”\n\nCommercial news organizations were also more apt to emphasize source protection, urging journalists to take caution when, for example, using AI tools for help making sense of large amounts of confidential or background information, “perhaps owing to the risk legal liability poses to their business model,” they write.\n\nKeep reading to learn what else the researchers found, including a strong focus on journalistic ethics across the documents, as well as real world examples of AI being used in newsrooms — plus, how the findings compare with other recent research.",
    "### AI guidance and rules focus on preserving journalistic values\n\nAI chatbots are a type of generative AI, meaning they create content when prompted.\n\nThey are based on large language models, which themselves are trained on huge amounts of existing text.\n\n(OpenAI rivals Google and Meta in the past year have announced their own large language models).\n\nSo, when you ask an AI chatbot to write a three-act play, in the style of 19th century Norwegian playwright Henrik Ibsen, about the struggle for human self-determination in a future dominated by robots, it is able to do this because it has processed Ibsen’s work along with the corpus of science fiction about robots overtaking humanity.\n\nSome news organizations for years have used generative AI for published stories, notably the Associated Press for simple coverage of earnings reports and college basketball game previews.\n\nOthers that have dabbled in AI-generated content have come under scrutiny for publishing confusing or misleading information.\n\nThe authors of the recent preprint paper analyzed the AI policies and guidelines, most of them related to generative AI, to understand how publishers “address both expectations and concerns when it comes to using AI in the news,” they write.\n\nThe most recent AI document in the dataset is from NPR, dated July 2023.\n\nThe oldest is from the Council for Mass Media, a self-regulatory body of news organizations in Finland, dated January 2020.\n\n“One thing that was remarkable to me is that the way in which organizations dealt with AI at this stage did exhibit a very strong sense of conserving journalistic values,” says Becker.\n\n“Many organizations were really concerned about not losing their credibility, not losing their audience, not trying to give away what makes journalism stand out — especially in a world where misinformation is around in a much larger scale than ever before.”\n\nOther early adopters include the BBC and German broadcaster Bayerischer Rundfunk, “which have gained widespread attention through industry publications and conferences,” and “have served as influential benchmarks for others,” the authors write.\n\nMany of the documents were guidelines — frameworks, or best practices for thinking about how journalists interact with and use AI, says Christopher Crum, a doctoral candidate at Oxford University and another co-author.\n\nBut a few were prescriptive policies, Crum says.\n\nAmong the findings:\n\n- Just over 71% of the documents mention one or more journalistic values, such as public service, objectivity, autonomy, immediacy — meaning publishing or broadcasting news quickly — and ethics.\n\n- Nearly 70% of the AI documents were designed for editorial staff, while most of the rest applied to an entire organization.\n\nThis would include the business side, which might use AI for advertising or hiring purposes.\n\nOne policy only applied to the business side.\n\n- And 69% mentioned AI pitfalls, such as “hallucinations,” the authors write, in which an AI system makes up facts.\n\n- About 63% specified the guidelines would be updated at some point in the future — 6% of those “specified a particular interval for updates,” the authors write — while 37% did not indicate if or when the policies would be updated.\n\n- Around 54% of the documents cautioned journalists to be careful to protect sources when using AI, with several addressing the potential risk of revealing confidential sources when feeding information into an AI chatbot.\n\n- Some 44% allow journalists to use AI to gather information and develop story ideas, angles and outlines.\n\nAnother 4% disallow this use, while half do not specify.\n\n- Meanwhile, 42% allow journalists to use AI to alter editorial content, such as editing and updating stories, while 6% disallow this use and half do not specify.\n\n- Only 8% state how the AI policies would be enforced, while the rest did not mention accountability mechanisms.",
    "### How the research was conducted\n\nThe authors found about two-thirds of the AI policy documents online and obtained the remainder through professional and personal contacts.\n\nAbout two-fifths were written in English.\n\nThe authors translated the rest into English using DeepL, a translation service based on neural learning, a backbone of AI.\n\nThey then used statistical software to break the documents into five-word blocks, to assess their similarity.\n\nIt’s a standard way to linguistically compare texts, Crum says.\n\nHe explains that the phrase “I see the dog run fast” would have two five-word blocks: “I see the dog run,” and “see the dog run fast.”\n\nIf one document said, “I see the dog run fast” while another said, “I see the dog run quickly,” the first block of five words would be the same, the second block different — and the overall similarity between the documents would be lower than if the sentences were identical.\n\nAs a benchmark for comparison, the authors performed the same analysis on the news organizations’ editorial guidelines.\n\nThe editorial guidelines were a bit more similar than the AI guidelines, the authors find.\n\n“Because of the additional uncertainty in the [AI] space, the finding is that the AI guidelines are coalescing at a slightly lower degree than existing editorial guidelines,” Crum says.\n\n“The potential explanation might be, and this is speculative and not in the paper, something along the lines of, editorial guidelines have had more time to coalesce, whereas AI guidelines at this stage, while often influenced by existing AI guidelines, are still in the nascent stages of development.”\n\nThe authors also manually identified overarching characteristics of the documents relating to journalistic ethics, transparency and human supervision of AI.\n\nAbout nine-tenths of the documents specified that if AI were used in a story or investigation, that had to be disclosed.\n\n“My impression is not that organizations are afraid of AI,” Becker says.\n\n“They encourage employees to experiment with this new technology and try to make some good things out of it — for example, being faster in their reporting, being more accurate, if possible, finding new angles, stuff like that.\n\nBut at the same time, indicating that, under no circumstances, shall they pose a risk on journalistic credibility.”",
    "### AI in the newsroom is evolving\n\nThe future of AI in the newsroom is taking shape, whether that means journalists primarily using AI as a tool in their work, or whether newsrooms become broadly comfortable with using AI to produce publicly facing content.\n\nThe Journalist’s Resource has used DALL.E 2, an OpenAI product, to create images to accompany human-reported and written research roundups and articles.\n\nJournalists, editors and newsroom leaders should, “engage with these new tools, explore them and their potential, and learn how to pragmatically apply them in creating and delivering value to audiences,” researcher and consultant David Caswell writes in a September 2023 report for the Reuters Institute for the Study of Journalism at Oxford.\n\n“There are no best practices, textbooks or shortcuts for this yet, only engaging, doing and learning until a viable way forward appears.\n\nCaution is advisable, but waiting for complete clarity is not.”\n\nThe Associated Press in 2015 began using AI to generate stories on publicly traded firms’ quarterly earnings reports.\n\nBut recently, the organization’s AI guidelines released during August 2023 specify that AI “cannot be used to create publishable content and images for the news service.”\n\nThe AP had partnered with AI-content generation firm Automated Insights to produce the earnings stories, The Verge reported in January 2015.\n\nThe AP also used Automated Insights to generate more than 5,000 previews for NCAA Division I men’s basketball games during the 2018 season.\n\nEarly this year, Futurism staff writer Frank Landymore wrote that tech news outlet CNET had been publishing AI-generated articles.\n\nOver the summer, Axios’ Tyler Buchanan reported USA Today was pausing its use of AI to create high school sports stories after several such articles in The Columbus Dispatch went viral for peculiar phrasing, such as “a close encounter of the athletic kind.”\n\nAnd on Nov. 27, Futurism published an article by Maggie Harrison citing anonymous sources alleging that Sports Illustrated has recently been using AI-generated content and authors, with AI-generated headshots, for articles on product reviews.\n\nSenior media writer Tom Jones of the Poynter Institute wrote the next day that the “story has again unsettled journalists concerned about AI-created content, especially when you see a name such as Sports Illustrated involved.”\n\nThe Arena Group, which publishes Sports Illustrated, posted a statement on X the same day the Futurism article was published, denying that Sports Illustrated had published AI-generated articles.\n\nAccording to the statement, the product review articles produced by a third-party company, AdVon Commerce, were “written and edited by humans,” but “AdVon had writers use a pen or pseudo name in certain articles to protect author privacy — actions we strongly condemn — and we are removing the content while our internal investigation continues and have since ended the partnership.”\n\nOn Dec. 11, the Arena Group fired its CEO.\n\nArena’s board of directors “met and took actions to improve the operational efficiency and revenue of the company,” the company said in a brief statement, which did not mention the AI allegations.\n\nSeveral other high level Arena Group executives were also fired last week, including the COO, according to the statement.\n\nMany of the 52 policies reviewed for the preprint paper take a measured approach.\n\nAbout half caution journalists against feeding unpublished work into AI chatbots.\n\nMany of those that did were from commercial organizations.\n\nFor example, reporters may obtain voluminous government documents, or have hundreds of pages of interview notes or transcripts and may want to use AI to help make sense of it all.\n\nAt least one policy advised reporters to treat anything that goes into an AI chatbot as published — and publicly accessible, Becker says.\n\nCrum adds that the research team was “agnostic” in its approach — not for or against newsrooms using AI — with the goal of conveying the current landscape of newsroom AI guidelines and policies.",
    "### Themes on human oversight in other recent research\n\nBecker, Crum and their coauthor on the preprint, Felix Simon, a communication researcher and doctoral student at Oxford, are among a growing body of scholars and journalists interested in informing how newsrooms use AI.\n\nIn July, University of Amsterdam postdoctoral researcher Hannes Cools and Northwestern University communications professor Nick Diakopoulos published an article for the Generative AI in the Newsroom project, which Diakopoulos edits, examining publicly available AI guidelines from 21 newsrooms.\n\nCools and Diakopoulos read the documents and identified themes.\n\nThe guidelines generally stress the need for human oversight.\n\nCools and Diakopoulos examined AI documents from many of the same newsrooms as the preprint authors, including the CBC, Insider, Reuters, Nucleo, Wired and Mediahuis, among others.\n\n“At least for the externally facing policies, I don’t see them as enforceable policies,” says Diakopoulos.\n\n“It’s more like principal statements: ‘Here’s our goals as an organization.’\n\nAs for feeding confidential material into AI chatbots, Diakopoulos says that the underlying issue is about potentially sharing that information with a third party — OpenAI, for example — not in using the chatbot itself.\n\nThere are “versions of generative AI that run locally on your own computer or on your own server,” and those should be unproblematic to use as a journalistic tool, he says.\n\n“There was also what I call hybridity,” Diakopoulos says.\n\n“Kind of the need to have humans and algorithms working together, hybridized into human-computer systems, in order to keep the quality of journalism high while also leveraging the capabilities of AI and automation and algorithms for making things more efficient or trying to improve the comprehensiveness of investigations.”\n\nFor local and regional newsrooms interested in developing their own guidelines, there may be little need to reinvent the wheel.\n\nThe Paris Charter, developed among 16 organizations and initiated by Reporters Without Borders, is a good place to start for understanding the fundamental ethics of using AI in journalism, Diakopoulos says.",
    "### Examples of AI-related newsroom guidelines\n\nClick the links for examples of media organizations that have created guides for journalists on using AI to produce the news.\n\nHas your newsroom posted its AI guidelines online?\n\nLet us know by sending a link to .\n\n- Bayerischer Rundfunk\n- BBC\n- Financial Times\n- The Guardian\n- Insider\n- Wired",
    "### About The Author\n\nClark Merrefield\n\nClark Merrefield joined The Journalist’s Resource in 2019 after working as a reporter for Newsweek and The Daily Beast, as a researcher and editor on three books related to the Great Recession, and as a federal government communications strategist.\n\nHe has been selected for fellowships in juvenile justice and solitary confinement at the John Jay College of Criminal Justice and his work has been awarded by Investigative Reporters and Editors.\n\n@cmerref\n\n---",
    "#### CRIMINAL JUSTICE, HEALTH, POLITICS & GOVERNMENT, RACE & GENDER\n\nAbortion pill mifepristone: An explainer and research roundup about its history, safety and future  \nNovember 1, 2023\n\nAmid pending court cases and ballot initiatives, journalistic coverage of medication abortion has never been more crucial.\n\nThis piece aims to help inform the narrative with scientific evidence.\n\n---\n\nSign up to receive a weekly e-mail newsletter from The Journalist's Resource.",
    "**Submit**\n\nHarvard Kennedy School is committed to protecting your personal information.\n\nBy completing this form, you agree to receive communications from The Journalist's Resource and to allow HKS to store your data.\n\nHKS will never sell your email address or other information to a third party.\n\nAll communications will include the opportunity to unsubscribe.\n\n---\n\nA project of Harvard Kennedy School's Shorenstein Center, The Journalist’s Resource curates, summarizes and contextualizes high-quality research on newsy public policy topics.\n\nWe are supported by generous grants from the Carnegie Corporation of New York, the Robert Wood Johnson Foundation, The National Institute for Health Care Management (NIHCM) Foundation and individual contributors.\n\nHOME | ABOUT | HOW TO MAKE A DONATION TO THE JOURNALIST’S RESOURCE | RSS | KNOW YOUR RESEARCH | EU/EEA PRIVACY DISCLOSURES\n\n---\n\nFind us:\n\nUnless otherwise noted, this site and its contents – with the exception of photographs – are licensed under a Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license.\n\nThat means you are free to republish our content both online and in print, and we encourage you to do so via the “republish this article” button.\n\nWe only ask that you follow a few basic guidelines.",
    "## In brief\n\nArtificial intelligence (AI) technologies, particularly generative AI, appear likely to revolutionize the way we work, innovate and create.\n\nGenerative AI can create novel, human-like output across various domains, making it highly versatile and intuitive – it even helped us draft parts of this paper – and as such, it has the potential to become a “general-purpose technology” like the steam engine and computer, transforming the global economy.\n\nThe most positive economic implication of AI disruption will likely be accelerating labor productivity after many years of near stagnation; estimates of the potential impact span a wide range, though most analyses posit 1.5%-3.0% per year globally over the next decade.\n\nA boost to labor productivity should result in a similar boost to real GDP.\n\nWe expect a large share of AI’s productivity impact to come from automating many tasks humans currently do, helping to offset increased retirements, but the potential to accelerate innovation could make productivity gains even more significant.\n\nAI will also have significant implications for labor.\n\nAutomating some tasks means needing fewer human workers to produce the same output, which could result in transitional job displacement, put downward pressure on wages and increase income inequality.\n\nHowever, if AI technologies stimulate demand, the creation of new jobs and higher overall economic growth should offset job displacement.\n\nAI may fundamentally change the way we, as humans, drive value in the workplace, requiring us to focus on the skills where we have a comparative advantage.\n\nThese changes may be rapid and unpredictable, increasing the importance of career flexibility, re-training and effective action from governments.\n\nFor markets, AI-driven productivity gains are likely to be positive for corporate earnings and equity returns; implications for bonds are more ambiguous, though we think the most likely impact is modestly higher yields.\n\nWe remain humble in our projections of the economic and market implications of AI technologies, given tremendous uncertainty over how powerful and capable they can become, what kinds of unforeseen innovations and industry transformations they’ll cause and, ultimately, how governments and society will respond.",
    "## The transformative potential of generative AI\n\nArtificial intelligence (AI) – or the process of making machines smart – has existed in some form since the 1950s, making occasional headlines, like when the chess-playing AI DeepBlue beat Gerry Kasparov in 1997.\n\nIn recent years, AI has quietly become more prevalent in our day-to-day lives, predicting arrival times of our online delivery orders, populating our social media feeds with personalized ads and filtering spam from our email inboxes.\n\nSuch applications of “traditional” AI (also known as “narrow” or “weak” AI) can be very advanced, even exceeding human expert levels, but they are trained to perform only in specific domains.\n\nGenerative AI is the latest stride in AI development, and by contrast, its key ability is to generate novel, open-ended content.\n\nThe recent launch of several generative AI applications (Exhibit 1) has brought this technology to the fingertips of the masses and captured global attention.\n\nThe most popular of these applications take the form of chatbots, like ChatGPT, powered by large language models (LLMs) that string together words based on patterns in vast troves of text data, such as significant slices of the internet.\n\nExhibit 1: Generative AI tools can revolutionize the way we create and interpret diverse forms of data\n\nWRITER CODER VISUAL ARTIST MATHEMATICIAN\n\nSource: Google, OpenAI, J.P. Morgan Asset Management.\n\n*GPT-3, GPT-4, PaLM, PaLM 2, Claud 2, and Minerva are all large language models (LLMs).\n\nNote that all the responses here – including the image – are totally original AI creations.\n\n1 Structured data is highly organized and made up mostly of tables with rows and columns that define their meaning, such as Excel spreadsheets.\n\nUnstructured data is everything else, such as the substantial contents of email messages, books, customer service recordings, images, memes and PowerPoint presentations.\n\nGenerative AI technologies can create high-quality content spanning a wide range of domains: image, video, audio, text, computer code and even entirely synthetic datasets.\n\nWhereas traditional AI technologies might be able to identify photos with “bumble bees” in them, generative AI could produce a photorealistic image of a bumble bee wearing a hat or write a children’s story about a bumble bee learning to fly.\n\nAs whimsical as these examples sound, this difference matters because the open-ended nature of these tasks represents so much of what we humans do for work (and tend to think ourselves uniquely capable of doing).\n\nGenerative AI may still be in its infancy, but the technology has advanced to an extent that we can begin to imagine its transformative implications across the global economy.\n\nIf a program can author a fictional story about a bumble bee, then it could write a movie script – or at least help automate a big part of the process – a real concern of screenwriters in the United States today.\n\nTo be sure, screenwriters account for a vanishingly small share of U.S. jobs, but generative AI can also help software engineers write and debug computer code, lawyers research legal opinions and draft contracts or scientists read and summarize dense research papers.\n\nVisual and auditory generative AI technologies might likewise automate tasks for jobs ranging from graphic designers to video editors.\n\nWhile generative AI is named for its functional differentiation from traditional AI, we think the more economically significant distinction is how general it is.\n\nWhen we start to tally what generative AI could do across the whole economy (Exhibit 2), the potential impact seems massive.\n\nTo be sure, the output of generative AI applications is imperfect, with chatbots like ChatGPT occasionally even including “hallucinations” of false information.\n\nMany applications are therefore likely to require a layer of human supervision, especially where the costs of mistakes are high, such as in medicine.\n\nHowever, further progress in developing generative AI might reduce some of these existing imperfections, and even supervised AI could still significantly boost human workers’ output.\n\nThere is also plenty that generative AI technology – or any form of AI – cannot yet do.\n\nInteracting with the physical world is still one large obstacle.\n\nWhile robotics has made many impressive advances in recent years (with the notable exception of driverless cars), these systems are designed to perform specific tasks and typically require higher investment and maintenance costs in proportion to their potential output.\n\nThe robot prototype designed to make guacamole for Chipotle, “Autocado,” may quicken the food assembly line, but it cannot also fill a customer’s cup or wipe down tables.\n\nEven so, in an information age with lots of desk jobs, non-physical problems are a big part of what we do.\n\nBefore generative AI, no other technology has arguably had as much potential to automate so much of our work.",
    "Before generative AI, no other technology has arguably had as much potential to automate so much of our work.\n\nSuch potential has sparked both public excitement and fear – the excitement of ridding ourselves of mundane and time-consuming tasks through automation, but also the fear of losing our jobs and livelihood.\n\nIn this publication, we seek to answer a few key questions: Is generative AI truly transformational?\n\nCould generative AI become the next “general-purpose technology,” like the steam engine and computer?\n\nWhat does broad-scale automation mean for labor markets?\n\nAnd, if AI can make us all a lot more productive, what impact will that have on the economy, inflation and financial markets?",
    "### More like a steam engine than a smartphone, economically speaking\n\nGenerative AI will have broad implications for the economy, but its most significant may be accelerating labor productivity after many years of near stagnation.\n\nLabor productivity – total output per unit of labor input – has been the main driver of U.S. economic growth over the last century or more.\n\nHowever, productivity growth stagnated in the last decade, registering just 1.2% per year on average.\n\nThe last 10 years have seen plenty of technological advancements that have improved many aspects of our lives, yet productivity statistics have told a different story.\n\nExhibit 2: Generative AI has the potential to accelerate efficiency, quality improvement and innovation across industries\n\nEveryday workers using generative AI chatbots can reduce time spent on common writing tasks by as much as 40% and enhance output quality.\n\nSoftware developers can complete manual and repetitive coding tasks up to twice as fast when using generative AI tools, increasing their ability to tackle new and more complex challenges.\n\nGenerative AI can help nurse practitioners in clinical processes and decision-making, enabling them to take on advanced tasks from primary care physicians; AI chatbots have even been shown to outperform average doctors in answering real patient questions.\n\nArchitects can rapidly output designs for new buildings subject to precise constraints, including optimal energy usage.\n\nMarketers can leverage generative AI to brainstorm creative ideas, including suggesting new brand names and logos based on descriptions and criteria.\n\nEducators and students can employ generative AI to develop tailored and interactive content and exercises for personalized learning experiences.\n\nSource: J.P. Morgan Asset Management.\n\nThis disconnect may be due to a few factors.\n\nForemost, although advances like smartphones and online media have made huge impacts on our daily lives, since they do so at a relatively low cost to consumers, they have limited impact on the market economy (various studies show a willingness among many consumers to pay for such services they receive for almost free).\n\nAdditionally, by distracting workers and delivering “information overload,” they may detract from productivity in other activities.\n\nSome growth disappointment may also simply be due to mismeasurement in government statistics of the real value of new forms of software and human and organizational capital.\n\nGenerative AI, by contrast, may be the advancement that finally ushers in a large, sustained boost to productivity.\n\nFirst, the broad-scale automation of existing activity – producing similar outputs with less labor input – should, essentially by definition, result in a more directly measurable productivity impact.\n\nEven more profound implications for productivity, output and welfare gains could come if generative AI is the tipping point that enables AI to become the ‘general-purpose technology’ of the 21st century.\n\nLike earlier general-purpose technologies, such as electricity, the steam engine and the internet, generative AI could fundamentally change how a wide range of goods and services are produced, transform industries and create entirely new jobs, owing to its potential to: Be pervasive.\n\nIts general capabilities mean generative AI can be integrated in many different contexts to supplement or replace many activities currently done by humans.\n\nSpawn complementary technologies and infrastructure.\n\nCompanies across industries are rushing to adopt AI in their fields, and the development of ancillary business applications is necessary to fully leverage AI’s benefits.\n\nAs we discuss later in this paper, generative AI may also enhance the performance of existing “traditional AI” technologies and vice versa.\n\nExperience exponential growth and economies of scale.\n\nAI’s computing workload has been doubling every three to four months since 2012 and is likely to accelerate even further.\n\nOpenAI’s GPT-3 and GPT-4 were released just two years apart, and the latter is significantly more complex, can interpret images received as inputs, is 40% more accurate in its responses and scores significantly higher percentiles on many standardized tests.\n\nReshape industries.\n\nBroad-scale automation will reshape the nature of jobs and business models, with transformative implications across industries.\n\nAccelerate innovation.\n\nAI has the potential to accelerate research and development and unlock new insights that inform and inspire innovation efforts.\n\nMany leaders in the field think this may be AI’s paramount application.",
    "### Innovating innovation itself\n\nExpanding on that last point, generative AI’s greatest potential might not be in merely automating what humans do, but in enhancing human efforts to create novel solutions to all sorts of real-world problems.\n\nSuch efforts could lead to considerable productivity and welfare gains beyond automation.\n\nSimply making workers more efficient could perpetually accelerate technological progress.\n\nFor generative AI specifically, further upside likely lies in its ability to: Quickly sift through vast datasets.\n\nIn a world of “information overload,” generative AI can be a potent filtering tool, automating many of the time-consuming tasks in research and development.\n\nUnlock new ideas and insights that inform researchers of where to concentrate their efforts.\n\nGenerative AI can analyze vast troves of unstructured data, something that is virtually impossible for humans to do, and in doing so can identify new patterns, reveal insights and discover better ways of doing things.\n\nConduct comprehensive predictive and evaluative analyses on new ideas.\n\nAI can improve the accuracy of our predictions and models, or even provide a sounding board for new ideas.\n\nTry prompting ChatGPT to list the pros and cons of your latent business idea, for instance.",
    "### Joining forces with “traditional” AI: greater than the sum of its parts\n\nWhile generative AI technologies are currently in vogue, recent years have seen the proliferation and refinement of many “traditional” AI technologies that have been trained to perform specific tasks very well.\n\nThese tasks tend to be the kind where generative systems still fall short, particularly in the performance of accurate predictive modeling, numerical calculations and optimization.\n\nMcKinsey estimates that these applications will account for a majority of the overall potential economic value added from AI.\n\nBeyond generative AI, traditional AI is still delivering major solutions: some examples\n\nPredicting the complex folding structure of proteins is one of the most exciting use cases of non-generative AI.\n\nIn the last 60 years, scientists have determined the structure of 180,000 proteins, a small number in proportion to the millions yet undiscovered.\n\nThis arduous task is an important part of drug discovery, but it can take years to execute.\n\nDeepMind’s AlphaFold is now carrying out the same task in minutes with unprecedented accuracy, a milestone in the application of AI to scientific research with immediate potential to advance drug development, biological research and our understanding of diseases at a molecular level.\n\nEnvironmental sustainability is another notable application of AI systems.\n\nAI systems are increasingly helping optimize energy production, storage, distribution and use.\n\nIn 2016, Google’s DeepMind developed an AI framework which reduced energy usage for data center cooling by 40%.\n\nMore recently, AI systems are aiding clean energy transitions.\n\nWhereas traditional weather models fare poorly at predicting clouds, AI systems trained on satellite and weather data could help solar grid and wind turbine operators optimize power generation and reduce fossil fuel energy held as reserve.\n\nIn the United Kingdom, Open Climate Fix is currently working with the country’s electric grid operator to better forecast cloudy British weather.\n\nSource: J.P. Morgan Asset Management.\n\nIntegrating generative and traditional AI systems could yield value far beyond what each alone could deliver, since each has its own strengths and weaknesses.\n\nNo generative AI systems could achieve the accuracy of AlphaFold’s predictions or estimate the exact hours of sunshine tomorrow.\n\nBoth abilities required specialized training on structured datasets.\n\nA generative AI chatbot like ChatGPT even struggles with some simple quantitative reasoning.\n\nAsk it to multiply two large numbers and it is likely to produce a close but incorrect answer.\n\nHowever, ChatGPT is fully capable of writing computer code to perform the very same calculation.\n\nSimply granting such chatbots access to code interpreters might be one way to supply the correct answer – not unlike calculators help humans solve math problems that most of us couldn’t solve in our own heads.\n\nBut why stop there?\n\nGenerative AI chatbots could draw on the vast library of specialized traditional AI tools, from mathematics engines to commute time-forecasting models, that have already been quite capable for over a decade – one by one, expanding their capabilities.\n\nAlready, OpenAI is privately testing several such additions to ChatGPT.\n\nSome of these applications might be highly specialized.\n\nFor instance, Bloomberg’s approach to integrating generative AI into its terminal allows users to prompt a system that is especially fluent in matters of finance, tapping into decades of financial data collection and development of specifically trained models that tackle matters of financial complexity.\n\nIndeed, we often hear now that “English will be the coding language of the future,” and it seems likely to be in many cases.\n\nBeyond written English, all these capabilities could be made more accessible by incorporating speech recognition and synthesis, areas where traditional AI already excels (along with handwriting and image recognition; see Exhibit 3).\n\nWe can imagine, for instance, verbally requesting a conference room smart assistant to draw a new logo idea and display it on a screen, without the need for typing.\n\nIn comparison to the current generation of “smart assistants” that rely on users’ remembering pre-trained command phrases, generative AI could make interacting with all these modules truly conversational experiences.\n\nConsidering that we already spend an estimated 25% of our total work time communicating with one another, being able to communicate just as seamlessly with machines opens the door to working alongside them.",
    "### A digitalized speed of adoption\n\nAI’s implications for economic growth and societal change can be profound, but the other factor to consider is timing.\n\nAlthough generative AI has suddenly become dinner-table conversation, its ultimate power and impact will not be seen for some time, though this may take place faster than with earlier transformative technologies.\n\nTechnological breakthroughs can take considerable time to raise productivity, with the peak impact of many industrial and post-industrial era technological breakthroughs, including general-purpose technologies, often only coming after 20-30 years.\n\nIn 1987, Robert Solow famously quipped, “You can see the computer age everywhere but in the productivity statistics;” in that case, a significant impact did ultimately show up in productivity statistics, albeit over a decade later (Exhibit 4).\n\nExhibit 3: Traditional and generative AI capabilities are increasingly comparable to those of humans\n\nTest scores of AI relative to human performance Initial performance for each AI capability set to -100\n\nAI systems perform better than humans AI systems perform worse\n\nSource: Douwe Kiela, Max Bartolo, Yixin Nie et al., “Dynabench: Rethinking Benchmarking in NLP,” Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, June 2021; J.P. Morgan Asset Management.\n\nPublished online at OurWorldInData.org.\n\nLicensed under CC-BY under the author Max Roser.\n\nThis lag owes to the considerable time it can take to: (1) establish wide-scale familiarity and access to the technology, (2) reshape business models to integrate the technology, (3) achieve a sufficiently large capital stock of it and (4) develop complementary innovations and infrastructure that allow for full benefits of the technology to be harnessed.\n\nExhibit 4: Past general-purpose technologies have taken considerable time to deliver gains in labor productivity, which has been the main engine of U.S. GDP over the last century\n\nLabor productivity growth Rolling 10-year annualized rate\n\nSource: BLS, NBER, J.P. Morgan Asset Management.\n\nData from 1888-1957 reflect productivity data for the total private economy from John Kendrick, “Productivity Trends in the United States,” NBER.\n\nData from 1958-2022 reflect non-farm productivity data from the BLS.\n\nData are as of August 31, 2023.\n\nWe think that AI adoption could be faster.\n\nOver time, an increasingly digitized world has helped accelerate the pace of technological adoption, and there are some reasons to believe generative AI could be adopted faster still: Generative AI is very accessible and easy to use for the average person, and its rapid accession to the mainstream is a testament to this fact.\n\nChatGPT shattered records by amassing 100 million monthly users in just three months, compared to the time it took TikTok (nine months) and Instagram (two and a half years) to reach the same milestone.\n\nGenerative AI is decades in the making, with considerable progress already made.\n\nWhile much of the public hadn’t heard about generative AI until this year, its most notable underlying innovations were developed in 2014.\n\nMeanwhile, decades of advancement in cloud infrastructure and an explosion of data and computing power have helped train these systems.\n\nMassive business investment has already been made...\n\nIn the five years ending in 2021, global business investment in all types of AI grew more than sixfold in real terms, with the United States leading the pack at $73bn invested.\n\nFrom 2017 to 2022, the share of businesses that have adopted AI, and the number of AI capabilities used, more than doubled.\n\nMoreover, compared to some earlier technologies, generative AI infrastructure and service providers are bearing a larger share of the necessary capital investments potentially increasing adoption rates by lowering the financial barrier for end users.\n\n...and more is underway to integrate AI or develop applications for business use.\n\nCompany management teams are increasingly focused on AI, with 40% of S&P 500 management teams mentioning AI in their 2Q 2023 earnings calls, up from 19% a year earlier (Exhibit 6).\n\nThese businesses are rushing to develop AI infrastructure and applications across a wide range of domains, with many launched this year or in the development pipeline.\n\nThe surge in investor interest has propelled hefty gains for stocks; the global AI market is currently valued at $150bn and projected to grow to $1.3tn by 2030.\n\nExhibit 5: Technological adoption has accelerated over time U.S. Technology, Rate of Adoption*\n\nSource: Asymco, compiled from various sources with support of the Clayton Christensen Institute, J.P. Morgan Asset Management.\n\n*Estimated from current adoption trends.",
    "*Estimated from current adoption trends.\n\nExhibit 6: 40% of S&P 500 companies mentioned AI in 2Q 2023 earnings calls Share of S&P 500 companies mentioning AI in earnings calls\n\nSource: J.P. Morgan Asset Management.\n\nMentions of AI include the keywords: artificial intelligence (AI), deep learning, machine learning, chatbots and natural language processing.\n\nMentions of crypto include the keywords: cryptocurrency, Bitcoin, Ethereum, blockchain, stablecoin and altcoin.\n\nData are quarter-to-date for 2Q23 as of August 25, 2023.",
    "### Sizing the potential AI productivity gain\n\nAI appears well positioned to significantly boost labor productivity, but by how much?\n\nIn our own analysis, we estimate annual productivity gains between 1.4% and 2.7% per year across developed markets over 10 years.\n\nThis estimate, if realized, would be comparable to past periods of technologically driven surges in productivity (as shown earlier in Exhibit 4).\n\nImportantly, our estimates quantify the impact of automation alone.\n\nSuch productivity enhancement would be in addition to any other productivity growth, such as the acceleration of innovation, which we believe presents significant upside potential.\n\nOn the other hand, external factors could partly limit productivity gains, for instance, if an ineffective or overly restrictive regulatory response impedes AI development, or social resistance stymies adoption.\n\nIn line with several recent studies that come to similar conclusions, our approximation of the potential productivity impact of AI-related automation takes a task-level approach.\n\nWe estimate the aggregate time spent on many types of tasks across the whole economy and judge the share of each of these tasks that might be automated (for more details, see “Appendix: Sizing AI productivity gains”).\n\nUltimately, we find that traditional and generative AI applications could potentially automate 14% to 27% of current work activities in the United States over the next 10 years (we would expect similar results across other developed markets).\n\nThe wide range of these estimates owes to considerable uncertainty around our assumptions outlined below.\n\nAutomation can materialize in productivity gains through three channels (Exhibit 7).\n\nThe first and most straightforwardly positive channel is direct labor cost savings from fewer workers being needed to produce the same amount of output.\n\nAlternatively, instead of reducing headcounts, companies can produce even more output by retaining their more productive workers.\n\nThe combined size of these two productivity impacts should equal the total productivity-weighted share of tasks automated.\n\nFinally, there is a composition effect that accounts for changes in the productivity of workers displaced from automation.\n\nOur projections assume that this effect is zero, or that, on average, displaced workers are reemployed in new jobs where they are equally as productive as they were in their former ones.\n\nWe believe this is a conservative assumption, given the potential for AI to lower the barrier to entry of many jobs and because AI seems likely to generate entirely new jobs with high productivity.\n\nHowever, if displaced workers are employed in jobs where they are less productive, overall productivity benefits would be lower; if many are not reemployed or work less, then productivity benefits would not fully accrue to real GDP.\n\nExhibit 7: Sizing the potential AI productivity boost\n\nSource: J.P. Morgan Asset Management.\n\nUltimately, the greatest uncertainties in estimating the labor productivity boost involve the capabilities of AI itself.\n\nOur more conservative estimates of AI capabilities, assuming a broader range of tasks are immune to automation, would see productivity gains of just 0.4%-0.9% per year over 10 years; on the other hand, our most optimistic assumptions would see gains of as much as 7.7% per year.\n\nTo be sure, we think this upside is extremely unlikely, but its possibility does illustrate the significance automation could have dependent on how powerful and pervasive AI ultimately becomes.\n\nSimilarly, if our baseline scenario for AI automation were to take 20 rather than 10 years to take hold, then annual productivity gains would be proportionally smaller, i.e., 0.7%-1.4% per year.",
    "## Work in an age of AI automation\n\nAI may bring on considerable productivity gains, but by doing so through automation, the idea that robots will “take our jobs” is becoming a popular concern.\n\nAre we in for mass unemployment?\n\nWe don’t think so – at least not in the foreseeable future – but the future of work will likely look quite different.",
    "### Automating tasks, not jobs\n\nAI seems unlikely to automate many entire jobs, but it does have significant potential to automate many of the tasks involved in those jobs, with most estimates of aggregate task exposure to automation ranging from 20% to 30%.\n\nSuch exposure will be broad-based across industries; OpenAI estimates that LLMs could affect 80% of the U.S. workforce in some form.\n\nThe degree of exposure, however, varies considerably by job type.\n\nHighly exposed jobs include those responsible for documentation and review in legal professions, providing administrative support in businesses and customer service representatives.\n\nGiven generative AI’s advanced abilities to understand language and draw upon vast bodies of information, exposed tasks include those that involve a degree of knowledge or expertise.\n\nAs such, higher-skilled jobs, such as STEM professionals and health care providers, are also exposed.\n\nAt the opposite end of the spectrum, where tasks seem the least exposed to automation, are jobs involving physical work or where the human component is invaluable (i.e., construction workers and daycare providers).",
    "### Transforming existing jobs\n\nIn the vast majority of cases, AI will augment but not entirely replace human capabilities.\n\nWhile AI’s abilities are impressive, there are many domains where AI technologies still fall short compared to humans or benefit from a layer of human supervision and feedback.\n\nAs repetitive and time-consuming “grunt work” becomes automated, workers can spend more of their time on higher complexity tasks, meaningful critical-thinking or creative endeavors.\n\nAs such, automation can also provide humans with the opportunity to deepen their skills, thereby expanding their overall potential and even increasing happiness at work.\n\nConsider a couple of instances where we already see this dynamic playing out.\n\nFinancial “robo-advisers” can provide customized investment advice and algorithmic portfolio management.\n\nA human financial adviser, though, is still needed to provide appropriate advice on financial matters involving complexity, counsel against impulsive trading behavior during market crashes or bubbles and offer empathy in times\nof crisis.\n\nCombine a human financial adviser with the tools of a robo-adviser, and this new “team” has both the benefit of advanced technology, as well as a layer of emotional intelligence for when contextual understanding is necessary.\n\nSimilarly, for software engineers, generative AI can significantly reduce time spent on research and trial and error, especially when working with a new programming language or software framework.\n\nWith these time savings, one study found that developers were 25-30% more likely to complete higher complexity tasks within the same time limit.\n\nThis pivot will require workers to focus on skills where humans have a comparative advantage (Exhibit 8).\n\nHuman intelligence can still better understand context and matters of complexity, apply intuition and employ emotional intelligence in social settings or where cultural norms are relevant.\n\nPlace a human professional in a room full of potential new clients, and within minutes she can adapt and respond to social cues across all five senses, adjusting everything from her handshake to the level of detail in her presentation.\n\nHumans also still have some edge in conceptual thinking.\n\nEach human mind is intrinsically capable of developing or working with an infinite number of abstract representations and models of the world – from solving physics problems to deciding how to structure an organization.\n\nNo form of AI boasts such general abilities.\n\nTraditional AI applications can work with some such abstract representations, albeit only for the relatively narrow set of instances for which they are designed.\n\nGenerative AI chatbots, despite producing novel content across a wide variety of domains, do so by essentially echoing patterns in the troves of text on which they are trained.\n\nSince this process involves no underlying conceptualization, it falls short in purely conceptual domains like mathematics, as we noted earlier.\n\nPerhaps also owing to this edge in conceptual understanding, humans should maintain an edge in artistic creativity.\n\nWhile AI might soon be able to create a lot of music and visual art that sounds or looks like what has already been produced, it seems less likely to generate entirely new genres on its own.\n\nWhere AI’s capabilities fall short, humans will be needed to fill in the gaps, and the jobs of tomorrow will be increasingly focused in these areas.\n\nNaturally, these are also the sorts of skills we will want to emphasize in education and job training, but many tough questions remain.\n\nCan everyone be a great problem-solver?\n\nHow do we evaluate and train such skills?\n\nDo we need to foster creativity in classes, instead of teaching students how to write code?\n\nArtificial intelligence has some clear advantages, including in its ability to process information at a speed impossible for humans to match.\n\nIn these cases, leaning on artificial intelligence can expand our overall potential.\n\nExhibit 8: Humans and AI should still have distinct comparative advantages\n\nSource: J.P. Morgan Asset Management.\n\n*See Exhibit 3.\n\nExhibit 9: Technological innovation has coincided with the creation of new jobs that account for the bulk of employment growth\n\nEmployment, by new and pre-existing occupations Millions, 2018 employment compared to 1940 occupations\n\nSource: Autor, David, Caroline Chin, Anna M. Salomons, and Bryan Seegmiller.\n\n\"The New Frontiers: The Origins and Content of New Work, 1940-2018.\"\n\nNo.w30389.\n\nNational Bureau of Economic Research, 2022 (and subsequent calculations by the authors as of August 2023); J.P. Morgan Asset Management.",
    "### Creating new jobs\n\nAlthough automation means that some of us will level up our daily tasks, increasing productivity also means businesses need less workers to create the same output, which could, in turn, result in job cuts.\n\nHowever, if AI ushers in an economic boom, there should be all sorts of new jobs to which we can pivot.\n\nThe long history of technological advancement has been associated with the continued process of job displacement and re-instatement that has supported an economy at full employment.\n\nFor instance, in the early 1800s, over 80% of the U.S. workforce was in agriculture, yet even after improvements in everything from mechanization to crop rotation dramatically reduced the need for farm labor, a subsequent economic boom re-employed many of those formerly working in the fields.\n\nDisplaced farmers moved to the cities to find jobs in the industrial sector, where they then contributed to an explosion in associated commerce.\n\nWith less time spent in the fields, people also had more time to spend on art and science.\n\nToday, in a similar vein, an estimated 72% of workers are employed in occupations that did not exist in 1940, implying that over 87% of employment growth over the last 80 years has come from the tech-driven creation of new jobs (Exhibit 9).\n\nThis proliferation of new jobs is ultimately because productivity stimulates consumer demand (Exhibit 10).\n\nThe ability to produce more output with fewer inputs inherently reduces production costs which tends to drive down consumer prices, enriching consumer wallets and enabling the consumption of all sorts of new goods and services.\n\nHigher consumer demand then stimulates business demand for workers in new jobs.\n\nIndeed, the rise in computerization has been associated with broad employment growth and the birth of many jobs in computer science, software engineering, graphic design, social media marketing and more.\n\nExhibit 10: Higher productivity ultimately drives demand for new jobs\n\nSource: J.P. Morgan Asset Management.\n\nGenerative AI ought to similarly increase our purchasing power, ultimately stimulating growth in new occupations – the web designers and app coders of tomorrow.\n\nAI, in some instances, may perform the work of a thousand humans for the cost of one.\n\nSuch economies of scale seem likely to generate new business models we can only begin to imagine, along with demands for humans to complement and manage them.\n\nDespite the many headlines about job cuts at the hands of AI, the World Economic Forum found that 50% of global employers expect AI to create job growth versus just 25% who expect it to create job losses.",
    "### Easing labor shortages...\n\nGenerative AI may be coming at an opportune time for the global economy when aging populations in most developed regions stand to meaningfully slow growth in the decades ahead.\n\nGlobally, the ratio of young and elderly people who are economically “dependent” on those of working age will gradually increase.\n\nThis dependency ratio will rise especially quickly in developed markets, where it has been climbing from a near-bottom of 48% in 2010 and is projected to rise to 82% by 2080.\n\nIn other words, by 2080 there will be nearly two consumers for every working-age person, and each year until then, these workers will need to support about 0.5% more people.\n\nAll else constant, workers would face pressure to work longer hours or postpone retirements in order to support the same per capita economic output.\n\nAgainst this backdrop, AI presents a major opportunity to counterbalance building labor shortages.\n\nEvery job automated by AI is also one more person who can retire without reducing overall economic output.\n\nThis dynamic could be especially helpful in the United States, where the most acute labor shortages today include many occupations where AI technologies are likely to have a significant impact, such as healthcare providers and skilled software engineers.",
    "### … And even allowing us to work less\n\nWith freed up time from automation, and machines doing more of the heavily lifting on driving the economy, workers might also enjoy some more time for family, rest and fun.\n\nIndeed, over time and across countries today, rising productivity has coincided with fewer hours worked.\n\n150 years ago, workers in today’s richest countries used to work a lot, but average working hours declined significantly in the wake of the Second Industrial Revolution (Exhibit 12a).\n\nIn 1930, after a period of particularly large productivity gains, John Maynard Keynes suggested that further advances in technology and productivity might lead to a 15-hour workweek.\n\nIn the decades since that prediction, though, declines in working hours have leveled off for major developed economies despite further technological progress.\n\nIt may be that some cultures intrinsically value hard work, while consumerism may keep us from ever feeling like we have enough.\n\nHowever, across countries today, the relationship between labor productivity and hours worked is consistently negative, suggesting we could still get closer to Keynes’ vision (Exhibit 12b).\n\nFor developed regions, we estimate that a hypothetical AI-driven 30% increase in labor productivity over the coming decade could drive a 5%-10% reduction in average hours worked.\n\nHowever, in order to realize this outcome, individual workers will also need to earn enough income that they are able to give up potential working hours in exchange for leisure.\n\nThat, in turn, requires mitigating further pressure on income inequality.\n\nConsiderations for income inequality The automation of routine and manual work, or the potential to work less, are exciting prospects.\n\nFor many, the cost could be increasing income inequality, which for many reasons has been rising across developed markets in recent decades.\n\nIn the United States, the share of pre-tax national income accounted for by the top 10% of earners has grown from 34% to 57% since 1951, leaving the bottom 50% with only 10% of national income.\n\nA similar trend is seen in wealth inequality, with the impressive growth in financial assets concentrating economic gains among those with the means to invest.\n\nSome argue that technological advancement has played a significant role in these trends, with one study estimating that automation explains 50 to 70% of the increase in wage inequality from 1980 to 2016.\n\nExhibit 12a: Over time, people have worked less as productivity has risen\n\nAverage weekly working hours per worker\n\nSource: Huberman & Minns (2007), Our World in Data, PWT 9.1 (2019).\n\nPublished online at OurWorldInData.org.\n\nRetrieved from: ‘ time-use’.\n\nLicensed under CC-BY by Esteban Ortiz-Ospina, Charlie Giattino and Max Roser.\n\nExhibit 12b: Today, people work less when they work in more productive economies\n\nWeekly working hours vs. labor productivity\n\nSource: Feenstra et al.\n\n(2015), Our World in Data, Penn World Table (2021), J.P. Morgan Asset Management.\n\nGDP is adjusted for inflation and for differences in the cost of living between countries.\n\nFeenstra, R. C., Inklaar, R. and Timmer, M.P.\n\n(2015), “The Next Generation of the Penn World Table”.\n\nAmerican Economic Review, 105(10), 3150-3182.\n\nPublished online at OurWorldInData.org.\n\nRetrieved from: ‘ [Online Resource] Licensed under CC-BY.\n\nA key pitfall of automation is that it can lead to the concentration of gains in the holders of capital, at least initially.\n\nIf some part of what a worker does is replaced with an AI program, then the owner of that AI capital will receive the “wages” the worker used to earn.\n\nWhile this may propel gains for technology companies, and their investors, this dynamic doesn’t bode well for labor’s share of income, particularly in an economy where worker bargaining power has already dwindled.\n\nOver time, these inequalities can fade, as cost savings from automation pass through to consumer prices and as new jobs emerge that reemploy displaced labor.\n\nHowever, while the U.S. economy has maintained full employment, there is some evidence that automation has been outpacing the creation of new tasks and jobs in recent decades.\n\nIf so, generative AI could complicate this challenge by further narrowing the set of skills that are uniquely human, increasingly including those of higher-skilled white-collar professionals who largely escaped the effects of prior waves of automation.",
    "### Mixed effects among workers\n\nAmong workers, the greatest beneficiaries are likely to be those whose skills are complementary to AI, rather than replaced by it.\n\nThose who work in such complementary roles already tend to earn relatively more, and increased demand for their skills could add to inequality among workers.\n\nConsider a hypothetical customer service center that is made significantly more efficient by generative AI.\n\nCustomer-facing workers may experience a direct productivity enhancement, but efficiency gains mean needing fewer of them to produce the same output, leaving them exposed to replacement.\n\nBy contrast, a manager of this center who effectively develops and maintains systems that integrate the work of humans and AI would be performing work that is more complementary to AI.\n\nAt the same time, generative AI technologies can level the playing field between lower-skilled and higher-skilled workers, by “lending” expertise to those who lack it, without the need for formal training and investment.\n\nThese skill-leveling effects might slightly offset inequality among workers.\n\nRecent studies on the impact of ChatGPT on customer service workers and on college-educated professional performing writing tasks found that the greatest productivity gains came from novice and low-skilled workers.\n\nHigher performers saw less benefit, perhaps because they were already delivering results closer to their peak potential, while lower-skilled workers were not only able to complete tasks faster but also perform tasks with greater complexity, “leveling up” in their responsibilities.\n\nIndeed, advanced technologies are already enabling nurse practitioners to take on more tasks usually performed only by primary care physicians.\n\nThe net impact of AI of inequality – the inequality-increasing effects of benefitting those with complementary skills and the inequality-reducing effects of leveling the playing field among workers – will likely vary considerably by industry, and ultimately depend on how it is developed and deployed.",
    "### Effective policy management\n\nImportantly, the speed at which adoption is taking place suggests governments, businesses and workers will need to act swiftly to reshape education and skills training and implement fiscal policies to smooth the transition for labor.\n\nSkills mismatches might be offset by investing in education and reskilling programs to ensure workers are keeping pace with the new skills demanded in an AI economy, while proper public safeguards will be needed in cases of job displacement.\n\nA greater concentration of wealth may call for further redistribution of economic income; reducing inequality, moreover, should help drive the demand boost that creates new jobs and new incomes, promoting a virtuous cycle that helps reduce inequality sustainably.\n\nIf accompanied with the right policy approach, an economic boom from AI automation should be a “win-win” that ensures all income levels stand to benefit for the foreseeable future.\n\nWith considerable promise comes considerable risk\n\nWhile the potential economic fruits of AI are bountiful, the technology brings with it several sociological and ethical concerns that we will need to navigate and confront in the coming years.\n\nChallenges include both those intrinsic to the underlying technology and its economic implications, as well as the potential for manipulation and misuse of powerful AI technologies by bad actors:\n\n- **Social instability** is one potential consequence of rapid development and deployment of AI technologies that could manifest in a few ways:\n  - AI-generated disinformation like falsified photos and videos intended to deceive, otherwise known as “deepfakes,” might soon flood the internet.\n\nSuch a development could make it even harder for members of the public to discern and agree on facts, ultimately amplifying ideological and affective political polarization.\n\n- The concentration of AI ownership among a few large corporations or countries could likewise concentrate power among them.\n\nAt what point do AI corporations become “too big” for the greater good?\n\nCompetition over such power could also lead to an unmitigated “arms race” between competing AI superpowers that might have second-order consequences, including for the careful assurance of safety of more powerful AI technologies.\n\n- Economic hardship from transitional unemployment and increased inequality could encourage further political extremism.\n\n- **AI bias** is one key ethical concern, in that AI can perpetuate and amplify existing biases present in the data on which it is trained.\n\nAs one old computer science adage says, garbage in, garbage out.\n\nThe incidence of racial discrimination of facial recognition technology has been studied extensively and images produced by generative AI evidently amplify existing stereotypes.\n\n- **Data privacy** is also a major concern.\n\nAI systems often require vast amounts of personal information to function effectively, raising concerns about the collection and storage of sensitive data without proper consent or security measures.\n\nAI technologies can also inadvertently facilitate cross-border data transfers, resulting in potential violations of international data privacy laws.\n\nFew regulations exist so far to ensure responsible use of both national and cross-border data sharing.\n\nHowever, AI is perhaps also uniquely positioned to address some of the same societal challenges it could potentially worsen.\n\nProperly designed and trained, AI may prove more objective and less biased than human counterparts.\n\nWe can imagine generative AI-powered tools that present information in ways that help ideologically opposed individuals understand and relate to one another, and perhaps even serve as a real-time mediator or fact-checker for online discourse, encouraging objectivity and even social stability.",
    "### Higher equity prices\n\nFor equities, if AI delivers on its promises, the implications should be straightforwardly positive.\n\nOur work on Long-Term Capital Market Assumptions suggests that an acceleration in potential GDP growth, all else the same, is likely to drive an acceleration in earnings by a similar degree.\n\nMoreover, a greater share of national income flowing to capital – the owners of AI technologies – could give an added boost to equity returns.\n\nWhile these impacts could take several years to materialize, markets – and equities in particular – are likely to price in AI optimism long before then.\n\nIndeed, strong global equity performance in 2023 so far, particularly in U.S. large caps, has been largely influenced by excitement around generative AI technologies.\n\nSuch performance has inspired comparisons to the early 2000s dot-com “bubble,” where enthusiasm about the internet propelled blind exuberance that drove stock prices significantly above intrinsic values.\n\nIn our view, performance so far does not nearly resemble a “bubble.” Price multiples are not yet significantly stretched as enthusiasm has also been accompanied by strong upward revisions to earnings outlooks for stocks with the most AI exposure, and that multiple expansion has been relatively broad based, in contrast to the narrow leadership of the dot-com bubble (Exhibit 13).\n\nThe future potential, however, for AI to drive something more like a bubble presents some upside risk to equities.\n\nHistorically, bubbles have commonly involved some new technology with no direct historical comparisons, making the impact hard to precisely quantify.\n\nWhen a consensus emerges that this technology is the “next big thing,” investors’ imaginations tend to run wild.\n\nAI certainly seems capable of satisfying this criterion.\n\nOn the other hand, most bubbles have also developed in periods of highly available credit, whereas today’s environment is one of restrictive monetary policy and tightening bank lending standards, which should keep investors’ optimism at least somewhat in check.\n\nExhibit 13: Rise in valuations has so far been broad based, unlike in the early 2000s dot-com bubble\n\n12-month forward price-to-earnings (P/E) multiple, S&P 500\n\nSource: FactSet, J.P. Morgan Asset Management.\n\nData as of August 2023.\n\nAI excitement has already led to considerable gains for technology companies in the S&P 500.\n\nAmong individual equities, Big Tech – now including chipmaker Nvidia – has obvious exposure to AI and especially generative AI, but as in past technology cycles, tomorrow’s winners from AI may include relatively new players that are not in vogue today.\n\nFor our U.S. Equity Group’s thinking on navigating AI investment opportunities, see “Artificial intelligence: Powering the next wave of technological innovation.”",
    "### Potentially higher yields\n\nFor government bonds, an AI productivity shock will likely contribute to modestly higher yields across developed markets.\n\nFaster growth in productivity and thus real economic activity is likely to push cycle-neutral real yields higher by roughly the same degree, as has been the case over the long term (especially in the United States since the middle of the 20th century).\n\nHowever, greater income inequality and downward pressure on wages could reduce inflationary pressures and thus breakeven inflation rates, partially offsetting upward pressure on nominal yields.\n\nOne key point of reference is the late 1990s, the most recent period of strong productivity growth; over this period, nominal yields did rise modestly – with the U.S. 10-year up by over 2% between late 1998 and early 2000, a period that also roughly corresponded to the strongest NASDAQ appreciation.\n\nA similar dynamic may play out this time, but we are mindful of the possibility that greater inequality is met with increased pressure to fund policies like universal basic income (UBI) that are not met with greater tax revenues.\n\nSuch policies would result in greater government debt issuance, which tends to result in higher long-term yields.\n\nOne consequence for monetary policy is that structurally higher yields could reduce the need for unconventional policies like quantitative easing, which have placed downward pressure on longer-dated yields.",
    "### Conclusions\n\nWhen we discuss AI with our clients, many of them are more concerned than excited.\n\nFear can be a good thing, but it’s easy to simply fear what we don’t understand, so part of our goal with this publication is to help identify what we do know and where the real challenges are with this emerging technology.\n\nAI certainly does present many challenges, especially for labor, but an era of mass unemployment seems highly implausible.\n\nAfter all, we wouldn’t bet against our ability as humans to always find new ways to challenge ourselves.\n\nManaged properly, we do think AI has the potential to make us all more productive, lower the real costs of many goods and services, reignite economic growth and offset aging demographics.\n\nBeyond economic growth, AI could also help solve some of our hardest societal challenges, such as in medicine and energy sustainability, and even accelerate the pace of innovation itself.\n\nWith all of this potential, AI may prove to be the major transformative technology of the 21st century, a rare occurrence that has historically preceded significant industry and societal change.\n\nGenerative AI, and its rapid accession to the mainstream, may be the tipping point.",
    "### Our methodology\n\nIn line with a number of peer estimates, we took a task-based approach to approximating the impact of AI automation based on the task dataset from O*NET.\n\nThis dataset provides many details on the task composition of different jobs, including task difficulty and relative importance, based on large-scale surveys of U.S. workers.\n\nWe then multiply the O*NET task content for each job by U.S. Bureau of Labor Statistics (BLS) data on the share of this job in the economy, providing an estimate of the national aggregate share of each task, and from there, the total potential impact of automation on economic activity.\n\nWe identify 33 out of the 41 catalogued occupational tasks as likely to be exposed to automation from AI technologies in some form over the next decade, leaving eight tasks immune (mainly due to physical-world requirements, as we exclude robotics from our assumptions).\n\nWe then scored those tasks based on (1) the share of the task we expect to be potentially automated (i.e., AI automates up to 80% of the tasks bucketed under “interpreting the meaning of information for others”), (2) the skill intensity of the automation (i.e., interpreting Python code would have a higher difficulty than interpreting customer feedback) and (3) the impact of such automation scaled across the entire economy (i.e., some tasks, such as “documenting information,” are more prevalent in the economy than others, such as “repairing mechanical equipment”).",
    "### Key assumptions\n\nOur framework involves a few key assumptions: AI automation is widespread across tasks and industries with most impacts taking hold in the next 10 years.\n\nAI can automate up to a difficulty level of 3 on the 0-7 O*NET difficulty scale (see Exhibit 15 for examples); varying this figure produces quite a wide range of alternative outcomes.\n\nDisplaced workers are re-employed in jobs where they are equally as productive as they were before, translating to a “composition effect” of zero, as we reviewed in the prior section “Sizing the productivity impact.” Our assumptions on automation exposure include both generative AI and “traditional” AI technologies but exclude robotics.\n\nOur analysis only accounts for the potential effects of AI automation; further sources of productivity gains could skew these results higher.",
    "### Analytical limitations\n\nThe O*NET dataset on task content presents some inherent limitations and challenges.\n\nMost notably, reported task difficulty corresponds to difficulty for humans rather than for AI, requiring our own estimates on the latter, and the reported figures for relative “importance” reflect a subjective measure of the task’s respective importance to the overall job rather than time spent.\n\nPresumably, these two metrics are related, but there are certainly cases where the tasks that take up the most time in a worker’s day aren’t the most value-added (i.e., so-called “grunt work”).\n\nOur sample captures 94% of the labor force as measured by BLS, as some roles in the BLS dataset did not exist in the O*NET database and were therefore excluded from our analysis.\n\nWe also note limitations in the extrapolation of automation estimates of specific O*NET tasks across all professions, since the same “task” can vary between occupations.\n\nFor example, our analysis found that fashion models, a profession that should arguably be immune to automation, had over 30% exposure to automation (in theory, image generating AI could probably automate some fashion models, if societal norms permitted it).\n\nThis example underscores the difficulty in translating the “task difficulty” measure provided by O*NET to the tasks AI could realistically automate.",
    "### Sensitivities\n\nThe above limitations result in estimation error in our projections, and indeed, minor adjustments in the parameters of our analysis can lead to varied results.\n\nIf we assumed AI technologies could only automate up to a level 2 on the seven-point scale employed by O*NET and took a more conservative cut of tasks subject to automation, with 15 tasks considered immune, estimated productivity gains would then be cut to 0.4%-0.9% per year.\n\nThe most aggressive assumptions, assuming AI automates all tasks up to a difficulty of 5, would see productivity gains of up to 7.7% per year.\n\nThis wide range of outcomes underscores the high uncertainty behind any projections of the magnitude of productivity gains.\n\nRegardless, we do believe such exercises are worthwhile as we imagine the extent of potential automation across the economy, and the channels through which productivity gains could materialize.",
    "### Comparisons to other estimates\n\nMany peer estimates on the potential productivity gain are within the range of what we find plausible (Exhibit 14).\n\nFor instance, McKinsey came to similarly high estimates of 0.2%-3.3% per year through 2040 including a broad set of AI technologies.\n\nThis kind of productivity gain would be a significant feat, translating to $2-$4tr annually added to the world economy from generative AI use cases alone.\n\nBrynjolfsson et al.\n\nfound that generative AI alone could raise productivity by 18% over 10 years, or 1.8% per year, and acknowledge further potential upside from the acceleration of innovation.\n\nExhibit 14: Others’ estimates on the productivity gain from AI\n\nSource: J.P. Morgan Asset Management; estimates compiled as of August 31, 2023.\n\n* Alderucci et al.\n\n(2022) found that on a firm-level, manufacturing firms with AI-related inventions experienced a 7% increase in TFP, along with an 8.3% increase in total revenue per employee and an 8.9% increase in value-added per employee.\n\nAcross the economy, the impact of AI-related invention is associated with a 6.8% increase in revenue per employee in the following 5 years.\n\n** Brynjolfsson et al.\n\n(2023) estimate that generative AI will raise productivity by an added 18% over ten years (or 1.8% per year) above the current Congressional Budget Office (CBO) projection of 1.5% productivity growth.\n\n*** McKinsey (2023) estimates that generative AI alone could enable labor productivity growth of 0.1% to 0.6% annually through 2040.\n\nCombined with a broad set of other AI technologies, work automation could add 0.2% to 3.3% points annually to productivity growth.\n\nExhibit 15: A task-based approach to sizing the productivity impact of AI technologies\n\nSource: O*NET, J.P. Morgan Asset Management.\n\nTable is for illustrative purposes.",
    "## Acknowledgements\n\nWe would like to express our gratitude to several individuals across J.P. Morgan Asset Management who generously contributed their time and expertise to the development of this paper – through their insights and constructive feedback, help with research and broad support for our efforts.\n\nFor his sponsorship, we would first like to thank Dr. David Kelly, Chief Global Strategist and Head of the Global Market Insights Strategy Team, who was the first to support our foray into the subject as part of our Long-Term Capital Market Assumptions, and for providing feedback on multiple iterations of this publication, especially on economic implications.\n\nWe are likewise grateful to Jed Laskowitz, Chief Investment Officer and Global Head of Asset Management Solutions, and John Bilton, Head of Global Multi-Asset Strategy, who both supported our efforts, provided constructive feedback and helpfully suggested others whose inputs proved invaluable.\n\nSpecial thanks also go to Michael Cembalest, Chairman of Market and Investment Strategy, who offered particularly extensive and detailed feedback on our paper – given his remarkable depth of knowledge in AI, among many other fields.\n\nMichael also brought our attention to additional relevant research that we have since incorporated into ours.\n\nWe are also grateful for feedback from Jeffrey Geller, a Chief Investment Officer in Multi-Asset Solutions; Jared Gross, Head of Institutional Portfolio Strategy; Evan Grace, Head of Multi-Asset Portfolio Management for the International Private Bank; and Anthony Werley, Chief Investment Officer of the Outsourced CIO/Endowments and Foundations Team.\n\nWe would like to thank several members of the U.S. Equity Group, particularly for their insights on AI advances beyond generative AI, corporate applications of AI and equity investment opportunities within AI: Robert Bowman, a semis and technology hardware analyst; Timothy Parton, a portfolio manager, and James Connors and Doug Stewart, both investment specialists.\n\nWe are also grateful to Dillon Edwards, an AI strategist in our Intelligent Digital Solutions Team, for his partnership and feedback on technology specific issues, and to our Data Science team for supplying bespoke data on mentions of AI in earnings calls.\n\nFinally, from Market Insights, Tai Hui, Asia Chief Market Strategist, and David Lebovitz, Global Market Strategist, and from Multi-Asset Solutions, Sylvia Sheng and Thushka Maharaj, both Global Strategists.\n\nAll of these colleagues gave useful feedback in their respective areas of expertise.\n\nLast though certainly not least, we would like to thank direct contributors to this paper, foremost from Michael Di Pentima, a portfolio management associate in Multi-Asset Solutions, who helped develop calculations to quantify the economic impact of AI automation.\n\nAdditionally, he, as well as Brandon Hall and Mary Park Durham, research analysts in Market Insights, and Ayesha Khalid, a Global Strategist in Multi-Asset Solutions, helped us collect and organize various exhibits and data used in this publication.\n\nThis publication, and all of our published content, could not have been possible without the incredible support and assistance from our marketing partners, namely Maria Bernice Halaguena, a marketing strategy associate, and Haley Baron, marketing strategy lead.\n\nThis paper is a joint effort between Market Insights and Multi-Asset Solutions with input and support from across the firm.\n\nWe hope our insights help inform our clients’ long-term investment planning and asset allocations.",
    "## J.P. Morgan Asset Management\n\n277 Park Avenue I New York, NY 10172\n\nThe Market Insights program provides comprehensive data and commentary on global markets without reference to products.\n\nDesigned as a tool to help clients understand the markets and support investment decision making, the program explores the implications of current economic data and changing market conditions.\n\nFor the purposes of MiFID II, the JPM Market Insights and Portfolio Insights programs are marketing communications and are not in scope for any MiFID II/MiFIR requirements specifically related to investment research.\n\nFurthermore, the J.P. Morgan Asset Management Market Insights and Portfolio Insights programs, as non-independent research, have not been prepared in accordance with legal requirements designed to promote the independence of investment research, nor are they subject to any prohibition on dealing ahead of the dissemination of investment research.\n\nThis document is a general communication being provided for informational purposes only.\n\nIt is educational in nature and not designed to be taken as advice or a recommendation for any specific investment product, strategy, plan feature or other purpose in any jurisdiction, nor is it a commitment from J.P. Morgan Asset Management or any of its subsidiaries to participate in any of the transactions mentioned herein.\n\nAny examples used are generic, hypothetical and for illustration purposes only.\n\nThis material does not contain sufficient information to support an investment decision and it should not be relied upon by you in evaluating the merits of investing in any securities or products.\n\nIn addition, users should make an independent assessment of the legal, regulatory, tax, credit and accounting implications and determine, together with their own professional advisers, if any investment mentioned herein is believed to be suitable to their personal goals.\n\nInvestors should ensure that they obtain all available relevant information before making any investment.\n\nAny forecasts, figures, opinions or investment techniques and strategies set out are for information purposes only, based on certain assumptions and current market conditions and are subject to change without prior notice.\n\nAll information presented herein is considered to be accurate at the time of production, but no warranty of accuracy is given and no liability in respect of any error or omission is accepted.\n\nIt should be noted that investment involves risks, the value of investments and the income from them may fluctuate in accordance with market conditions and taxation agreements and investors may not get back the full amount invested.\n\nBoth past performance and yields are not reliable indicators of current and future results.\n\nJ.P. Morgan Asset Management is the brand name for the asset management business of JPMorgan Chase & Co. and its affiliates worldwide.\n\nTo the extent permitted by applicable law, we may record telephone calls and monitor electronic communications to comply with our legal and regulatory obligations and internal policies.\n\nPersonal data will be collected, stored and processed by J.P. Morgan Asset Management in accordance with our privacy policies at \n\nThis communication is issued by the following entities: In the United States, by J.P. Morgan Investment Management Inc. or J.P. Morgan Alternative Asset Management, Inc., both regulated by the Securities and Exchange Commission; in Latin America, for intended recipients’ use only, by local J.P. Morgan entities, as the case may be.\n\n; in Canada, for institutional clients’ use only, by JPMorgan Asset Management (Canada) Inc., which is a registered Portfolio Manager and Exempt Market Dealer in all Canadian provinces and territories except the Yukon and is also registered as an Investment Fund Manager in British Columbia, Ontario, Quebec and Newfoundland and Labrador.\n\nIn the United Kingdom, by JPMorgan Asset Management (UK) Limited, which is authorized and regulated by the Financial Conduct Authority; in other European jurisdictions, by JPMorgan Asset Management (Europe) S.à r.l.\n\nIn Asia Pacific (“APAC”), by the following issuing entities and in the respective jurisdictions in which they are primarily regulated: JPMorgan Asset Management (Asia Pacific) Limited, or JPMorgan Funds (Asia) Limited, or JPMorgan Asset Management Real Assets (Asia) Limited, each of which is regulated by the Securities and Futures Commission of Hong Kong; JPMorgan Asset Management (Singapore) Limited (Co. Reg.\n\nNo.",
    "No.\n\n197601586K), which this advertisement or publication has not been reviewed by the Monetary Authority of Singapore; JPMorgan Asset Management (Taiwan) Limited; JPMorgan Asset Management (Japan) Limited, which is a member of the Investment Trusts Association, Japan, the Japan Investment Advisers Association, Type II Financial Instruments Firms Association and the Japan Securities Dealers Association and is regulated by the Financial Services Agency (registration number “Kanto Local Finance Bureau (Financial Instruments Firm) No.\n\n330”); in Australia, to wholesale clients only as defined in section 761A and 761G of the Corporations Act 2001 (Commonwealth), by JPMorgan Asset Management (Australia) Limited (ABN 55143832080) (AFSL 376919).\n\nFor all other markets in APAC, to intended recipients only.\n\nFor U.S. only: If you are a person with a disability and need additional support in viewing the material, please call us at 1-800-343-1113 for assistance.\n\n© 2023 JPMorgan Chase & Co. All rights reserved.\n\nMI_MAS-AI bulletin 09nr231409193311",
    "# Risks vs. Benefits\n\nNearly three-quarters of respondents (71%) said that the potential benefits of using generative AI in their agency's operations outweigh the risks, as compared to the risks outweighing the benefits (20%).\n\nWhen thinking about the risks at their agency: the lack of controls to ensure information is generated ethically/responsibly was the top concern among all respondents (65%) and was ranked highest among all respondents, followed by the lack of ability to verify/explain the generated output of information at 58%.\n\nWhen comparing risks by federal civilian and defense/intelligence agency respondents, key differences were evident for lack of employee training to use generative AI responsibly - 64% of defense/intel leaders compared to 38% at federal civilian agencies.\n\nWhen thinking about the risks for the public's use of generative AI: More than half of all respondents (62%) ranked the potential to abuse/distort government-generated content as the top risk.\n\nWhen thinking about the risks for the public's use of generative AI: More than half of all respondents (62%) ranked the potential to abuse/distort government-generated content as the top risk.\n\n68% of federal civilian ranked this as their top risk while 56% defense/intel respondents were most concerned with the potential to generate misinformation to compromise national security.",
    "# Dedicated Team/Office to Develop Policies\n\nSeven in 10 of all respondents said their agency has established an enterprise-level team or office charged with developing AI policies and resources to support business unit AI initiatives.\n\nA higher percentage of defense/intel respondents (78%) said they have done so compared to 66% of federal civilian.\n\nOf 23% of all respondents who said they have not established such, 42% expect such a team will be established within the next 6 months.",
    "### Policies for Employee Use\n\nOf the 67% of respondents who said their agency has issued preliminary governance policies or guidelines for how employees may or may not use generative tools for work, 74% of them said the policies do not prohibit the use.\n\nNearly seven in 10 respondents (67%) said they are aware of instances where employees are experimenting with generative AI for work.",
    "#### Business Operations/Workflow\n\nMore than half of all respondents (51%) said their agency is planning to assess the potential positive or negative impact within the next 12 months.\n\nA slightly higher percentage (56%) said their agency is likely to plan or implement one or more generative AI applications within the next 12 months.\n\n38% of all respondents are confident that generative AI will deliver greater value in improving overall business operations, and 46% are confident that it will achieve workflow cost savings.\n\nWhen comparing respondents by job function, 40% of business executives said they're confident that generative AI will deliver greater value and 50% are confident that it will achieve workflow cost savings for overall business operations.\n\nIT execs are less confident that will deliver greater value (33%) or cost savings (38%).",
    "#### Mission Intelligence Execution\n\nMore than half of all respondents (52%) have started or will start to assess the potential positive or negative impact within the next 6 months.\n\n5% also say they've started planning or are likely to implement within the next 6 months.\n\nRespondents indicate a similar confidence level in that generative AI will deliver greater value (36%) and achieve cost savings (38%) for mission intelligence.\n\nWhen comparing respondents by job function, 40% of business executives said they're confident that generative will deliver greater value and 50% are confident that it will achieve workflow cost savings for overall business operations.\n\nIT execs are less confident that will deliver greater value (33%) or cost savings (38%).",
    "### Citizen Services/Accessibility\n\nHalf of all respondents said they're planning to assess the potential impact of generative AI within the next 12 months; and nearly half (49%) said their agency is likely to implement an AI application within the same timeframe.\n\nNearly half expressed confidence that generative AI would add greater value (48%) and save costs (46%).",
    "### Data Analytics/Insights\n\n45% said they're planning to assess the potential impact of generative AI for data analytics work within the next 12 months; 52% said their agency is likely to implement an application within the same timeframe.\n\n42% expressed confidence it would add greater value in this area, but 48% felt it would help save costs.",
    "### Case Management/Workflow\n\nMore than half (56%) are planning to assess the potential impact of generative AI for case management workflows in the next 12 months; 52% said their agency is likely to implement an application within the same timeframe.\n\nConfidence for cost savings is slightly higher at 45% compared to delivering greater value.",
    "### Oversight Planning/Reporting\n\n47% said they're planning to assess the potential impact of generative AI for oversight work within the next 12 months; the same percentage also said their agency is likely to implement an application within the same timeframe.\n\nSomewhat fewer respondents voiced confidence that generative AI would add greater value (45%) for IT or reduce costs (42%).",
    "### Differing Priorities\n\nBusiness and IT executives indicated different priorities for implementing generative AI for the different use cases.\n\nFor example, business leaders said data analytics would likely see the greatest investment rollout for generative AI in the coming year while IT executives said business operation's/workflow would likely see the greatest investment attention.",
    "## Generative AI Impact on Employees\n\nNearly two-thirds (65%) of all respondents said their agency has assessed the potential impact of generative AI on its employees.\n\nThere was a sense of urgency to recruit or upskill talent to support the use of generative AI, with 28% of those polled saying their agency leadership views recruitment or upskilling as a \"critical\" and 49% calling it \"important.\"",
    "### Workplace Concerns\n\nWhen asked where they see generative AI presenting the greatest concerns for their organization's employees, 58% of all respondents said job elimination ranked highest, followed closely by the potential of being repositioned in the organization (57%).\n\nWhen comparing responses by job function, 65% of IT executives indicated that repositioning was the greatest concern, followed by the impact on employee satisfaction (52%).\n\nBusiness executives, on the other hand, indicated job elimination ranked highest (64%) as an employee concern, followed by the need to address employees' concerns about their work rights (63%).",
    "### Workplace Opportunities\n\nWhen asked where they see generative AI presenting the greatest opportunities for employees, 65% of all respondents said it was the ability to give employees added technical support, followed by the ability to reduce the time required to complete work processes (64%).\n\nWhile business executives as a cohort ranked those opportunities at similar rates, a higher percentage of IT executives (77%) indicated that the ability to free up employee tasks to produce more valuable work was the greatest opportunity.",
    "### Scale of 1 to 5\n\nWhere 1 is \"Not a Priority\": and 5 is \"Critical.\"\n\n- 5 Critical\n- 4 Important\n- 3 Moderate priority\n- 2 Low priority\n- 1 Not a priority\n\n8 in 10 government executives polled said their agency leaders view \"understanding AI's impact on operations\" as \"critical or important.\"",
    "## Establishment of Team/Office to Develop Policies\n\nAll respondents Of the 23% of all respondents who answered NO, the percent who expect a team will be established in the future:\n\nOf the 23% of respondents whose agencies had not yet established an enterprise-level AI team or office, 7 in 10 expect their agency will establish one in the next 12 months.\n\n7% don't know.",
    "## Policies or Guidelines for Employee Use of Gen AI\n\nAll respondents Organization issued preliminary governance policies or guidelines for how employees may and may not use generative AI tools for work.\n\nWhile 2 in 3 respondents say their agency has issued preliminary guidance on AI use, only in 26% of those cases do those policies prohibit employees from using generative AI tools for work.",
    "## Business Operations/Workflow\n\nAll respondents Stage in planning to assess potential positive or negative impact\n- Not planning\n- Started\n- Next 6 months\n- Next 7-12 months\n- Next 12-24 months\n- Don't know\n\nStage in planning or implementing one or more applications:\n\nConfidence to improve ability to deliver greater value:\n- Not confident\n- Slightly\n- Somewhat\n- Confident\n\nConfidence in workflow cost savings:",
    "## Mission Intelligence/Execution\n\nAll respondents Stage in planning to assess potential positive or negative impact\n- Not planning\n- Started\n- Next 6 months\n- Next 7-12 months\n- Next 12-24 months\n- Don't know\n\nStage in planning or implementing one or more applications:\n\nConfidence to improve ability to deliver greater value:\n- Not confident\n- Slightly\n- Somewhat\n- Confident\n\nConfidence in workflow cost savings:",
    "## Citizen Services/Accessibility\n\nAll respondents Stage in planning to assess potential positive or negative impact\n- Not planning\n- Started\n- Next 6 months\n- Next 7-12 months\n- Next 12-24 months\n- Don't know\n\nStage in planning or implementing one or more applications:\n\nConfidence to improve ability to deliver greater value:\n- Not confident\n- Slightly\n- Somewhat\n- Confident",
    "## IT Development/Cybersecurity\n\nAll respondents Stage in planning to assess potential positive or negative impact:\n- Not planning\n- Started\n- Next 6 months\n- Next 7-12 months\n- Next 12-24 months\n- Don't know\n\nStage in planning or implementing one or more applications:\n\nConfidence to improve ability to deliver greater value:\n- Not confident\n- Slightly\n- Somewhat\n- Confident",
    "## Data Analytics/Insights\n\nAll respondents Stage in planning to assess potential positive or negative impact\n- Not planning\n- Started\n- Next 6 months\n- Next 7-12 months\n- Next 12-24 months\n- Don't know\n\nStage in planning or implementing one or more applications:\n\nConfidence to improve ability to deliver greater value:\n- Not confident\n- Slightly\n- Somewhat\n- Confident",
    "## Case Management/Workflow\n\nAll respondents Stage in planning to assess potential positive or negative impact\n- Not planning\n- Started\n- Next 6 months\n- Next 7-12 months\n- Next 12-24 months\n- Don't know\n\nStage in planning or implementing one or more applications:\n\nConfidence to improve ability to deliver greater value:\n- Not confident\n- Slightly\n- Somewhat\n- Confident",
    "## Oversight Planning/Reporting\n\nAll respondents Stage in planning to assess potential positive or negative impact\n- Not planning\n- Started\n- Next 6 months\n- Next 7-12 months\n- Next 12-24 months\n- Don't know\n\nStage in planning or implementing one or more applications:\n\nConfidence to improve ability to deliver greater value:\n- Not confident\n- Slightly\n- Somewhat\n- Confident",
    "### Job Titles\n\nConfidence that generative AI will deliver greater value for the following use cases or functions:\n- Not confident\n- Slightly\n- Somewhat\n- Confident\n\nBusiness execs IT execs Add value in improving business operations and workflow...\n\nBusiness execs IT execs Add value in improving mission intelligence and execution...",
    "### Job Titles\n\nConfidence that generative AI will achieve workflow cost savings in the following use cases or functions:\n- Not confident\n- Slightly\n- Somewhat\n- Confident\n\nBusiness execs IT execs Save costs in improving business operations and workflow...\n\nBusiness execs IT execs Save costs in improving mission intelligence and execution...",
    "## Greatest Concerns for Employees\n\nAll respondents & job titles Where respondents see generative AI presenting the greatest employee concerns.\n\n- ALL: 58%\n- Business Executives: 57%\n- IT Executives: 56%\n- 50%\n- 32%\n\n1% of all respondents in their open response indicated that it removes aspects of creativity.",
    "## Greatest Opportunities for Employees\n\nAll respondents & job titles Where respondents see generative AI presenting the greatest opportunities for employees.\n\n- ALL: 65%\n- Business Executives: 64%\n- IT Executives: 63%\n- 44%\n- 22%\n\n2% of all respondents in their open response indicated that they do not see AI helping at all.",
    "### Purpose\n\nGenerative AI (GenAI) technology is rapidly advancing, and tools are becoming increasingly available.\n\nWhile GenAI can provide significant benefits to an organization in terms of productivity enhancement and revenue generation, it also poses risks to privacy, cybersecurity, intellectual property, third-party/client engagements, legal obligations, and regulatory compliance.\n\nThis policy aims to forge a responsible usage, deployment and development of Generative AI across Wipro, harness the advantages that the GenAI technology brings whilst mitigating the risks and challenges, and provide guidelines for responsible usage, deployment and development of GenAI tools for internal use and client engagement.",
    "### Scope\n\nThis policy covers usage, deployment and development of Generative AI tools and technology.\n\nThis policy does not override any policy, process and guidance related to privacy, data protection, code of business conduct, intellectual property, and confidentiality.\n\nFor example, any use case related to the deployment of Generative AI for internal utilization (e.g., to optimize HR processes, sales, and marketing campaigns, etc.\n\n), must undertake all the existing procedures, including security assessment, intellectual property and confidentiality due diligence, and privacy assessment when personal data is involved.\n\nThis policy is a living document as it reflects the fast-evolving nature of technology, which we embrace in a responsible, human-centric, sustainable and privacy preserving manner, in full adherence to the spirit of Wipro.",
    "## Policy Details\n\nGenerative AI refers to artificial intelligence systems that have the capability of generating new content (such as images, text, audio, codes, presentation, etc.)\n\nbased on the data they were trained on.\n\nWhile Generative AI can revolutionize the way we operate, there are inherent risks associated with this technology.\n\nThese risks include but are not limited to:\n\n- **Privacy and data protection.\n\n** Generative AI tools require large datasets thus pose issues related to data minimization and legal basis for processing data.\n\nThese are core privacy law tenets now recognized all across the globe, and ones we must abide by.\n\nIn addition, these tools present risks around fairness in both processing and outputs; opacity around the workings of generative AI may clash with transparency and informational right requirements.\n\nFinally, there are challenges around accuracy as generative AI tools may produce false information.\n\n- **Security and confidentiality.\n\n** Through generative AI, attackers may generate new and complex types of malware, phishing schemes and other cyber dangers that can avoid conventional protection measures.\n\nSuch assaults may have significant repercussions like data breaches, financial losses, and reputational risks.\n\nIn addition to personal data leakage risks, the use and development of Generative AI is also susceptible to data inference attacks, data poisoning and other forms of adversarial attacks that may compromise the security and confidentiality of data.\n\n- **Intellectual property and enterprise proprietary information.\n\n** All aspects of Generative AI from the model, training data, prompts, to output pose IP risks namely:\n  - **Infringement of IP Rights:** The training data used to train the Generative AI models could include copyrighted material and if the output of these models is similar (or a derivative work) or in rare scenarios, the exact same as the input training data, then this could potentially infringe on copyright laws.\n\nGenerative AI could inadvertently use or refer to trademarked products, brands or logos that could also be seen as infringing on those trademarks.\n\nThe machine-authored output could also be very realistic fake images, videos, or texts, which could be used to infringe on someone's copyright.\n\nThe prompts used to interact with Generative AI systems may contain copyright-protected information, which could lead to infringement.\n\n- **Unclear Ownership:** The issue of IPR (Intellectual Property Rights) protection for machine-authored content is an unclear, complex and evolving area of law.\n\nWhen integrated into Wipro or Client owned IP, it can result in downstream IP licensing and/or enforcement risks and challenges.\n\n- **Attribution:** AI-generated content can be almost indistinguishable from human-generated content and as such can lead to a risk of humans not getting due credit and attribution as the rights holder of their work.\n\n- **Misinformation.\n\n** Generative AI can produce biased or inaccurate outputs and potentially lead to poor decision-making and legal or ethical consequences.\n\nGenerative AI can also create content and represent facts even if they don’t exist.\n\nIn addition, the outdated data on which it is trained can lead to inaccurate predictions, poor recommendations.\n\n- **Consumer Protection.\n\n** Businesses that fail to disclose usage of large language models to consumers run the risk of losing customer trust and be charged with unfair practices under various laws (e.g., the California chatbot law mandates that in certain consumer interactions, organizations must disclose clearly and conspicuously that a consumer is communicating with a bot.)",
    "### Guidelines for Users\n\nUsers of Generative AI (“users”) refers to personnel accessing and utilizing Generative AI tools for enhancement of their daily job or for client delivery.\n\nUsers must adhere to the guidelines prescribed below.\n\n- Personal data should not be entered on Generative AI tools including but not limited to: ChatGPT, Bard, Co-Pilot, GPT-4, Dall-E.\n\nPersonal data include names, addresses, phone numbers, or any other information that could be used to identify an individual.\n\nNote that personal data may differ across jurisdictions.\n\n- Users must avoid using language or content that may have proprietary customer, partners and Wipro’s confidential information or mention of our customers, leadership team etc.\n\n- Users must clearly indicate content that is generated by GenAI tool to avoid confusion with human-generated content.\n\nUsers must acknowledge the source of any ideas or insights generated by the tool.\n\n- GenAI tools can only be used for client projects if approved by clients or if the use is allowed as per the client contract.\n\nSimilarly, client enterprise data including personal details should not be used in Generative AI without client approval.\n\n- Account teams should reach out to GenAI taskforce to obtain clearance for the usage of Generative AI tools in all new and existing client engagement.\n\n- GenAI tools that are not approved by Wipro for enterprise-wide use should not be used for any activity.\n\n- Any Generative AI use case must be reviewed and approved by GenAI Steering Committee (GenAI Steerco) before implementation or deployment.\n\nUsers must submit a GenAI use case review request to GenAI taskforce.\n\n- Any Generative AI use case involving personal data and/or has an impact on the data privacy rights of individuals must undergo a Data Protection Impact Assessment.\n\n- APIs should not be used to transmit sensitive and/or confidential data to Generative AI systems.",
    "### Guidelines for Deployers and Developers\n\nDeployers and Developers (“developers”) of Generative AI refers to personnel engaged in the deployment or development of Generative AI which may include but not limited to:\n\n- Developing Generative AI Application using APIs (3rd Party)\n- Transfer learning from existing open source GPT models to in-house GPT Models as the technology become accessible.\n\nDevelopers of Generative AI must adhere to the following guidelines and to the guidelines prescribed to users.",
    "#### General Guidelines\n\n- Developers must adhere to privacy and data protection laws and must ensure alignment with globally accepted frameworks such as OECD and NIST (National Institute of Standards and Technology) frameworks.\n\n- Developers must adhere to the defined rules around key requirements, such transparency, fairness (refer to Addendum 2.\n\nPrivacy by Design Checklist for AI/ML completeness).\n\nA Data Protection Impact Assessment, in collaboration with the Global Data Privacy team, must be completed during the design phase or initial/ideation stages of the development of Generative AI.\n\n- Privacy by design approach must be adopted from conception stage to deal with issues such as data collection, legal basis, human oversight, informational rights, automated decision making.\n\n- Developers must comply with defined mechanisms for regular checks and must complete an audit of the system before deployment, and at regular intervals.\n\n- To mitigate security and confidentiality risk, deployers and developers must:\n  - only use public data to train the AI system and maintain the anonymity of data\n  - adopt the use of secure coding, protect access to AI System only to authorized users\n  - align data protection standards on training data and sensitive user input to encrypt and store securely\n  - perform regular security assessments to identify and address vulnerabilities in Generative AI system\n\n- Deployment and development GenAI tools for client delivery should be done only if approved by the client or if it is allowed as per the client contract.\n\nSimilarly, client enterprise data including personal details should not be used in Generative AI without client approval.\n\n- Account teams should reach out to GenAI taskforce for advisory on deployment or development of Generative AI tools in all new and existing client engagement.\n\n- Developers must comply with Intellectual Property policies and standards and ensure that there is no copyright infringement.\n\n- Should the system fall under the coverage of the EU AI Act (directly or indirectly marketed in Europe), developers must refer and adhere to the dedicated sections in the Act.\n\n- Generative AI-generated source code must undergo thorough testing and validation before integration into internal applications.",
    "#### Generative AI-Generated Source Code for Internal Applications:\n\n- Developers responsible for incorporating Generative AI-generated code must maintain a comprehensive documentation trail to aid in debugging and troubleshooting.\n\n- Regular code reviews must be conducted to ensure adherence to coding standards, as well as to maintain code quality and security.",
    "#### Generative AI-Generated Code by Third-Party Generative AI Services:\n\n- Thorough due diligence must be conducted before employing third-party Generative AI services, assessing their credibility, security measures, and compliance with applicable regulations.\n\n- Contracts or agreements with third-party Generative AI service providers must include clauses addressing data privacy, intellectual property rights, and liability.\n\n- Regular audits or assessments should be performed to ensure that third-party Generative AI services meet the enterprise's established standards.",
    "#### Generative AI-Generated Code by Open-Source GPT Models:\n\n- Usage of open-source GPT models must comply with the relevant licensing terms and conditions.\n\n- Before using open-source GPT models in production environments, a comprehensive evaluation must be conducted to identify and address security vulnerabilities, bias, and ethical concerns.\n\n- Proper attribution and acknowledgment must be provided for the open-source GPT models employed.",
    "## Other Responsibilities\n\n- Generative AI tools should not be used to create adverse effect to Wipro, customer, entities data, and infrastructure.\n\n- Issues or concerns, such as unauthorized access or data breaches should be reported in accordance with Wipro security incident reporting process.\n\n- Users of Generative AI systems must always adopt a critical mindset and be able to validate the outcomes as such tools may compute inaccurate or false information.\n\n- In case of any doubt, reach out to the GenAI taskforce for assistance.",
    "## Definitions:\n\n- **Generative AI** - Generative AI refers to an artificial intelligence system that have the capability of generating new content based on the data they were trained on.\n\nGenerative AI become extremely popular after the release of ChatGPT.\n\nHowever, it is worth highlighting that GenAI has a wide variety of applications beyond text.\n\nFor example:\n  - **Text:** Generative AI model can generate new text content, e.g., write essays, generate new code, or translate from one language to another.\n\nSuch models are trained on massive amount of text data from various sources.\n\n- **Image:** Generative AI models can generate new images.\n\nFor example, style transfer applications where you can upload your photo and it generated a Monet style portrait of your photo.\n\n- **Audio:** Generative AI models can generate new audio content.\n\nFor example, generative AI models were used to complete Beethoven’s 10th Symphony.\n\n- **Natural Language Processing (NLP)** - is an arch of computer science techniques that enables computer to understand text and provide inference in the same way that human would.\n\n- **Large Language Model (LLM)** – LLMs represent a core component of NLP, a tool to enable AI to mimic human performance in understanding language.\n\nLLMs are large models (millions of parameters) that are trained on massive amount of data.\n\n- **Generative Pretrained Transformer (GPT)** – GPT refers to a subset of LLM models that uses an underlying Neural Network architecture called Transformers and is trained on a large body of data to perform wide variety of tasks such as text summarization, question answering, etc.\n\nGPT is the underlying model for ChatGPT.\n\n- **Transfer learning** - transfer learning refers to the process of transferring the knowledge of general models trained on large, general data sets to a more specialized models that aim to solve a specific problem of interest.\n\n- **Fine Tuning** - Fine tuning is the process/technique used for transfer learning and it refers to fine tuning the general “Pretrained” models by resuming their training on specialized data sets.",
    "## Approvals/Escalation Matrix\n\nAny deviation or non-compliance to this policy must be immediately reported to GenAI taskforce.\n\nFurthermore, should an employee be made aware of any suspicious activity involving the use of Generative AI, such as but not limited to unauthorized access, misuse, and unauthorized disclosure of data, he/she must report it immediately as a security incident.",
    "### About the Deloitte AI Institute\n\nThe Deloitte AI Institute helps organizations connect the different dimensions of a robust, highly dynamic and rapidly evolving AI ecosystem.\n\nThe AI Institute leads conversations on applied AI innovation across industries, with cutting-edge insights, to promote human-machine collaboration in the “Age of With”.\n\nThe Deloitte AI Institute aims to promote a dialogue for and development of artificial intelligence, stimulate innovation, and examine challenges to AI implementation and ways to address them.\n\nThe AI Institute collaborates with an ecosystem composed of academic research groups, start-ups, entrepreneurs, innovators, mature AI product leaders, and AI visionaries to explore key areas of artificial intelligence including risks, policies, ethics, future of work and talent, and applied AI use cases.\n\nCombined with Deloitte’s deep knowledge and experience in artificial intelligence applications, the Institute helps make sense of this complex ecosystem, and as a result, deliver impactful perspectives to help organizations succeed by making informed AI decisions.\n\nNo matter what stage of the AI journey you’re in; whether you’re a board member or a C-Suite leader driving strategy for your organization, or a hands-on data scientist, bringing an AI strategy to life, the Deloitte AI Institute can help you learn more about how enterprises across the world are leveraging AI for a competitive advantage.\n\nVisit us at the Deloitte AI Institute for a full body of our work, subscribe to our podcasts and newsletter, and join us at our meet-ups and live events.\n\nLet’s explore the future of AI together.",
    "## Generative AI is all the rage\n\nLet’s take a moment to cut through the hype.\n\nThe AI field took a turn with the release of powerful Generative Artificial Intelligence (AI) models, and as a result, the world is seeing the automation of some skills around creativity and imagination sooner than many expected.\n\nFor some organizations, Generative AI holds valuable potential for higher order opportunities, like new services and business models.\n\nDeloitte offers a method for selecting Generative AI use cases, as well as some next steps for business leaders in the Age of With™.",
    "### The rise of Generative AI\n\nGenerative AI has captured attention in global media and the public square, prompting questions and discussions around this transformative technology.\n\nBusinesses, research organizations, and even lay users are experimenting with Generative AI, and given the excitement and interest, it is important to look more closely at the potential capabilities and implications for business.\n\nGenerative AI is a subset of artificial intelligence in which machines create new content in the form of text, code, voice, images, videos, processes, and even the 3D structure of proteins.\n\nSome forms of Generative AI have been well established in this decade, but it was a large language model (LLM) powering an easily accessible chat interface that enabled Generative AI to have its breakthrough moment and surprise even specialists in the field.\n\nAs with other types of AI before it, this new poster child of AI is stimulating the imagination as organizations and individuals consider how to use this tool to benefit both business and society.\n\nGenerative AI can help in incremental digitization and basic productivity use cases (e.g., more effective text-based channels).\n\nBut its grander potential is in the higher order opportunities, such as new services or business models that were previously uneconomical.\n\nGenerative AI in general and LLM-powered chatbots, in particular, are not without risks, and this is prompting discussions around things like the potential for job losses and legal questions around intellectual property and ownership.\n\nWhat is more, because the chatbot mimics coherent human phrasing, it may give some the impression that the AI understands the prompts to which it responds, which can lead users to anthropomorphize the chatbot (i.e., the ELIZA effect, as seen in the work of computer scientist Joseph Weizenbaum).\n\nDeloitte is working on a variety of projects exploring the opportunities and business value Generative AI can create for our clients.\n\nFrom experiences and conversations thus far, the clear path ahead, as with all AI, is to attempt to discover and capitalize on capabilities while also responsibly managing the risks that are already emerging.",
    "## Generative AI is all the rage\n\nWhile this is still the beginning, it’s moving fast.\n\nAmong organizations across industries, there is interest in differentiating AI use cases, from public service applications to addressing climate change and transforming business functions (see Deloitte’s AI Dossier).\n\nAI is viewed as a tool that can automate skills and tasks performed by humans, and AI can be so successful in this regard that humans can forget skills that have been automated.\n\nExamples include writing assistants, home automation, and automotive navigation systems.\n\nWould most people have the ability to navigate a new city without a mobile phone?\n\nWe have seen these kinds of automations emerge across a variety of areas and skillsets.\n\nThe assumed roadmap for AI was that, in the shorter term, AI is most valuable as a way to automate operational skills, and creative skills will remain the exclusive province of human thinking for the foreseeable future.\n\nWith Generative AI, this roadmap has taken an unexpected turn.\n\nIn late 2022, with the release of an easy-to-use Generative AI chatbot, more people began to discover and imagine how this new technology can be used in the creative space.\n\nThe chatbot use case opened the door for thinking more broadly about how Generative AI can be used for tasks, ranging from writing copy to generating 3D structures and to outputting organizational processes.\n\nAs such, we are now seeing the automation of some skills around creativity and imagination sooner than many expected.\n\nThere is a lot left to discover.\n\nIn this Age of With™, the era of humans working with intelligent machines to achieve things greater than what either could do alone, Generative AI will impact the future of work and become a common tool throughout various aspects of our daily lives.\n\nIn some cases, the applications may be clearly visible, but more often than not, they may be working in the background.",
    "### The evolution of Generative AI\n\nThe ability of Generative AI to create a convincing (albeit low-quality and greyscale) image of a human face emerged in 2014.\n\nSince then, the image quality has increased, and today, almost anything that can be described in words can also be generated as an image, using a textual description called a “prompt.” Throughout 2022, social media users tinkered with Generative AI platforms and shared the results.\n\nWe have seen avocado armchairs and photorealistic images of astronauts riding horses on the Moon.\n\nCosmopolitan magazine was the first to publish a cover page created by an AI-based image generation tool, and there has even been a case of a user who submitted an AI-generated image to a fine art competition—and won first place.\n\nToday, we are seeing similar improvements in other kinds of Generative AI.\n\nYou may even have found this article via a chat with an AI system that integrates with a search engine.",
    "### How Generative AI works: Understanding the basics\n\nTo understand how Generative AI will impact business and life, we need to understand what it is, what it can do, and what it cannot do, yet.\n\nMachine learning has dominated the field of AI for decades.\n\nGenerally, this approach to AI development is rooted in the concept of learning from examples, rather than following explicitly programmed rules.\n\nThis is important as there are many tasks that humans perform based on tacit knowledge (and thus can provide examples) but cannot describe the underlying rules to do so.\n\nFor example, humans know how to recognize a face, but the rules that would instruct an AI system to do the same are much less clear.\n\nThe approach of learning from examples has led to the development of powerful tools that can identify intricate patterns in complex data.\n\nIn a process referred to as training, the algorithm is supplied with a large dataset of input/output examples to extract patterns from the input, which allow conclusions about the expected output.\n\nSpam filters, for example, use these patterns to identify similarities in data points and relate those to different classes (i.e., sorting email to a spam folder).\n\nWhile the input data has become more complex over time, from simple arrays of numbers to high-resolution photos, the output side of a model has to this point been mostly limited to categories like “spam” or “not spam,” “cat” or “dog,” or numerical values such as 7°C or $29.\n\nThis approach powers nearly all AI that has been deployed so far, the result is “single purpose” applications that can only perform one task.",
    "### Enter Generative AI\n\nThe main difference between “traditional AI” and Generative AI is that in the latter, the output is of a higher complexity.\n\nRather than just a number or a label, the output can be an entire high-resolution image, a full page of newly written text (which is generated word by word), or any other digital artifact.\n\nThis introduces an interesting new element: There is usually more than one possible correct answer.\n\nThis results in a large degree of freedom and variability, which can be interpreted as creativity.\n\nGenerative AI models are typically large and resource hungry.\n\nCreating them requires terabytes of high-quality data processed over weeks on large-scale, GPU-enabled, high performance computing clusters.\n\nOnly a few institutions have the necessary resources and talent to build such models.\n\nRunning a model also requires a lot of compute, which is why access to these kinds of models is often provided via an application programming interface (API).\n\nThis allows developers to use the models with their existing software products without the need for additional infrastructure.\n\nThese models are versatile and can be fine-tuned for specific tasks, hence they are called Foundation Models.\n\nUnlike single-purpose AI, they are suited for multi-purpose tasks.",
    "### Regarding risks and limitations\n\nCurrent Generative AI models have limitations.\n\nPerhaps the most well-known is termed “hallucination,” which refers to a high-confidence response that is not grounded in the training data.\n\nIn other words, the response is fictional.\n\nFor some applications, like art generation, this is a non-issue and perhaps even a desired “creative” feature of Generative AI.\n\nFor other applications, however, such as copywriting or computer code generation, hallucinations can result in artifacts that are not entirely valid or true, which undercuts the potential value of Generative AI.\n\nAnother limiting factor is that today’s Generative AI models generate artifacts based on the model itself and the input prompt.\n\nOther additional sources and datasets cannot currently be integrated directly into the model’s internal information processing without costly re-training or fine-tuning, which means Generative AI models can only access information extracted from the data on which they were trained.\n\nFor similar reasons, they cannot provide references and sources for the generated content, which complicates validation.\n\nFurthermore, current models have a context window of a few thousand words, which is the limit for the size of the combined input and output.\n\nHowever, Generative AI models can be combined with other systems (e.g., search, conversational AI) to leverage the benefits of both parts.\n\nFor example, with a chatbot, a conversational AI system can serve as an orchestration layer between the Generative AI model, a search engine, and the user, which helps to amplify the user experience.\n\nSimilar to other AI models, Foundation Models can reproduce latent bias in the training data, and of course, they lack comprehension and the ability to reason as humans do.\n\nThis has implications for the broader concept of Trustworthy AI™.\n\nAfter all, they are language models, image models, or voice models but not knowledge models.\n\nDespite limitations, Foundation Models can function at such a high quality that many new use cases become possible.",
    "### Generating revenue using Generative AI\n\nUsing this technology for business benefit can be conceived along two distinct approaches.\n\nFirst, the models can be used as they are available today, a simple interface that allows near-direct access to the underlying model in the form of a chat for text or an image generation tool.\n\nThe second approach is to integrate Generative AI with other technologies to automate processes.\n\nFor example, Generative AI can allow for human-level expressive interactions, while a conversational AI system (i.e., a chat- or voicebot) controls the flow and ensures factual accuracy.\n\nAn example is an automated, Generative AI-powered call center.\n\nWe expect the second approach will provide the most value.\n\nA good start to identifying use cases is to find processes or tasks where a digital artifact of some kind is created or processed.\n\nThis could range from a job advertisement or a floor plan, all the way to the 3D model of an engine part, a molecule with certain properties, or a workflow, to name a few.\n\nUse cases with high usage frequency are preferred, as there will be more example data to fine-tune and improve a model, and subsequently a more substantial impact.\n\nOther factors to consider in selecting high-value use cases are existing skill and cost bottlenecks with human generated artifacts.\n\nThe quality of the artifact may in some cases require human effort, but if it can be created with Generative AI to a commensurate quality, then the human can be liberated to work on higher quality tasks.\n\nBy turning lower-level creative tasks over to Generative AI, we could see things like databases providing stock content (e.g., images, sounds, or texts) turned upside down as these digital artifacts can be created instantly with a prompt.\n\nIf a task requires effort to execute but is easy to validate, it might be a good use case.\n\nDeloitte has designed a Digital Artifact Generation/Validation method to help innovation leaders determine whether an idea can be turned into a beneficial use case leveraging Generative AI.\n\nAt the core of this method are two of the most critical elements to consider: the human effort required to complete a task without Generative AI; and the necessary effort to validate or fact check the output from the Generative AI.\n\nThis leads to a two-dimensional classification, categorizing use cases based on the required human effort and the ability of the user to validate the results.",
    "#### Identifying desirable use cases\n\nGenerative AI assists best in use cases where human effort is high, while validation is easy.\n\n- **Generation effort**: How much human effort is required to achieve the desired result\n- **Validation effort**: How much human effort is required to check the plausibility or correctness of the result\n\n- **LOW** Validation effort **EASY**",
    "#### Examples plotted above\n\n- **Create a joke**: While creating a good joke requires some effort into designing the punchline and best storytelling-style, it is easy to validate simply by reading it.\n\n- **Draw an image of an elephant under a palm tree**: Drawing any sophisticated image requires reasonable effort for most people regardless of the tools available.\n\nOn the other hand, validation is easy since you can just look at the picture.\n\n- **Draft a contract (without legal expertise)**: If you do not have legal expertise, drafting a contract is very hard and validating it difficult.\n\nGenerative AI is useful where the artifact generation effort is high and validation is easy.\n\n- **Draft a contract (with legal expertise)**: If you do have legal expertise, drafting a contract still requires effort, but validating it is significantly easier.",
    "## Generative AI is all the rage\n\nFor example, re-writing text can be a daunting and time-consuming task for a human.\n\nGenerative AI tools can take original text and quickly produce a re-written result, a shorter text, a summary, or even a different writing style.\n\nA user who is familiar with the original content can validate the accuracy or correctness of the output.\n\nThus, this could be a promising application of Generative AI.\n\nYet, if the user is reviewing outputs that are outside of their area of expertise, validation becomes more complicated.\n\nThe Generative AI output may read as coherent and convincingly accurate, but the potential for a “hallucination” remains.\n\nIf users lack the knowledge to validate the output and spot hallucinations, the use case is revealed to require high levels of effort for validation and mitigating the risks from hallucinations.\n\nThere is an additional complication that should be considered.\n\nIf the model outputs are consistently correct, users may, over time,\n\nbecome less rigorous in fact-checking.\n\nInevitably, however, the model will make an error, and part of the challenge is that the errors may not be obvious, particularly when Generative AI is used to create more complex things, like programming code.\n\nThus, when assessing the ease of validation, weigh the importance of ongoing attention to fact-checking.",
    "### Insights from Deloitte projects on Generative AI: Reaping benefits from Generative AI requires more than identifying a good use case\n\nIdentifying use cases is only part of the challenge.\n\nWhenever a transformative technology emerges, some organizations are spurred to experiment for the sake of its novelty, which can lead to “random acts of digital” that do not deliver the anticipated return.\n\nDriving business results with Generative AI requires a strategy and collaboration from a cross-disciplinary team.\n\nIn addition, with a technology that is advancing and maturing as quickly as Generative AI, avoid the temptation to go forward alone and instead find support and knowledge from partners, colleagues, and third-party organizations operating in this space.\n\nThe inherent complexity in current projects leveraging Generative AI requires a cross-disciplinary team to guide and govern the AI lifecycle.\n\nProfessionals from a variety of domains can help the business answer critical questions, including:\n\n| **DOMAIN**          | **STAKEHOLDERS**  | **KEY QUESTIONS**                                                                                                                                                   |\n|---------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Ideation & Product Development** | Creatives, designers | What can Generative AI permit that reduces human effort and can be rapidly validated?\n\n|\n| **Business Operations**  | CEO, COO, Line of Business leaders     | How does the Generative AI fit into and enhance existing processes and enterprise strategy?\n\n|\n| **Customer & Marketing**      | CMO               | How can the use case be leveraged to build customer engagement, and how much transparency is appropriate?\n\n|\n| **Enterprise Technology**   | CIO, CTO, IT         | Can the existing MLOps-tech stack and platform licenses fuel Generative AI, or are third-party services required?\n\n|\n| **Human Capital**      | CHRO               | Does the workforce possess the skills to use Generative AI, and what are the implications for talent acquisition and upskilling?\n\n|\n| **Risk Management**     | Risk officers        | What risks emerge when deploying Generative AI (e.g., jailbreaks, prompt-spoofing), and how do these risks impact Generative AI value?\n\n|\n| **Regulations & Laws**   | Legal & Compliance  |What current and expected laws and regulations concern the use of Generative AI, and are existing governance and MLOps processes sufficient to meet those laws and regulations?|\n\n\nBased on our observations and experience, we recommend business leaders avoid jumping head-first into the hype.\n\nInstead, we encourage decision-makers to:\n\n1.\n\nDevelop a strategy for Generative AI and integrate and harmonize it with the enterprise’s existing AI strategy.\n\nThe same principles that guide an AI-fueled organization apply to the use of Generative AI (e.g., access to curated enterprise data; AI governance; process transformation to leverage cognitive workers, etc.).\n\nWith a technology evolving this quickly, avoid the temptation to go forward alone.\n\nFind support and knowledge from partners and third-party organizations operating in this space.\n\n2.\n\nBecome familiar with the underlying technologies that make Generative AI possible, as well as the current capabilities and limitations.\n\nEducate your workforce in the usage, risks, and capabilities of AI to establish a baseline of knowledge through training.\n\nAlso monitor over time how the technology advances and the impact on business risks and opportunities, as they emerge.\n\nThis article series may support your efforts.\n\n3.\n\nBring together a cross-disciplinary team of people with the domain knowledge to think creatively about potential use cases.\n\nWhen business leaders, technology leaders, and creatives work together with external experts, they are able to identify valuable applications and also design Generative AI deployments, to mitigate business and cyber risks and meet applicable laws and regulations.\n\n4.\n\nLeverage Deloitte’s Digital Artifact Generation/Validation method to identify where Generative AI might impact your value chain, with incremental digitization from basic productivity use cases to higher order opportunities, such as new, differentiating services or business models.\n\n5.\n\nEnsure the collection and curation of proprietary data, as this is key for tailored use cases that provide a differentiator or competitive advantage.\n\n6.\n\nAssess use cases against Trustworthy AI™ principles, including challenges around bias and misinformation, attribution, transparency, and enterprise accountability for the impact from Generative AI.",
    "## Generative AI is all the rage\n\nDeloitte is excited to move into the future with our clients and colleagues, as well as with our connections in academia and the broader AI ecosystem around the world.\n\nThe discussions so far show that there is a need for a deeper understanding of Generative AI, from the underlying technology to its impact on the future of work.\n\nAs such, it is important to look closely at the implications for risk, trust, and governance, which is investigated in a forthcoming article, “Proactive risk management in Generative AI.” There are also legal considerations for Generative AI, which we plan to cover in “Legal implications of using Generative AI (What the AI System won’t tell you).” There is a lot to cover and the conversations are far from over.\n\nDeloitte is a trusted advisor as we push beyond the initial buzz around this new technology and into how Generative AI can be used for good in the Age of With™.",
    "## Generative AI is all the rage\n\nReach out for a conversation.\n\n**Beena Ammanath**  \nGlobal Deloitte AI Institute Leader  \nDeloitte AI Institute United States, Lead Deloitte Consulting, LLP  \n  \n\n**Wessel Oosthuizen**  \nDeloitte AI Institute Africa, Lead  \nDeloitte Africa  \n  \n\n**Dr. Kellie Nuttall**  \nDeloitte AI Institute Australia, Lead  \nDeloitte Australia  \n  \n\n**Jefferson Denti**  \nDeloitte AI Institute Brazil, Lead  \nDeloitte Brazil  \n  \n\n**Audrey Ancion**  \nDeloitte AI Institute Canada, Lead  \nDeloitte Canada  \n  \n\n**Jan Hejtmanek**  \nDeloitte AI Institute Central Europe, Lead  \nDeloitte Central Europe  \n  \n\n**Roman Fan**  \nDeloitte AI Institute China, Lead  \nDeloitte China  \n  \n\n**Anne Sultan**  \nDeloitte AI Institute France, Lead  \nDeloitte France  \n  \n\n**Dr. Bjoern Bringmann**  \nDeloitte AI Institute Germany, Lead  \nDeloitte Germany  \n  \n\n**Prashanth Kaddi**  \nDeloitte AI Institute India, Lead  \nDeloitte India  \n  \n\n**Masaya Mori**  \nDeloitte AI Institute Japan, Lead  \nDeloitte Japan  \n  \n\n**Nicholas Griedlich**  \nDeloitte AI Institute Luxembourg, Lead  \nDeloitte Luxembourg  \n  \n\n**Naser Bakhshi**  \nDeloitte AI Institute Netherlands, Lead  \nDeloitte Netherlands  \n  \n\n**Tiago Durao**  \nDeloitte AI Institute Portugal, Lead  \nDeloitte Portugal  \n  \n\n**Sulabh Soral**  \nDeloitte AI Institute United Kingdom, Lead  \nDeloitte United Kingdom  \n  \n\n**Special thanks to contributors:**  \nJakob Nikolaus Moecke, Senior Consultant, Deloitte AI Institute Germany  \nPhilipp Joshua Wendland, Senior Consultant, Deloitte AI Institute Germany  \nAlexander Mogg, Lead Partner, Monitor Deloitte Germany  \nKate Fusillo Schmidt, Senior Manager, Deloitte AI Institute US  \nErica Dodd, Senior Manager, Deloitte AI Institute Australia  \nJessica Carius, Senior Consultant, Deloitte AI Institute Australia  \n\nElon Allen, Partner, Monitor Deloitte Australia  \nBram den Hartog, Partner, Monitor Deloitte Australia  \nAisha Greene, Senior Manager, Deloitte AI Institute Canada  \nAnke Joubert, Senior Manager, Deloitte AI Institute Luxemburg  \nDr. Gordon Euchler, Director, Deloitte Germany\n\nThis publication contains general information only and Deloitte is not, by means of this publication, rendering accounting, business, financial, investment, legal, tax, or other professional advice or services.\n\nThis publication is not a substitute for such professional advice or services, nor should it be used as a basis for any decision or action that may affect your business.\n\nBefore making any decision or taking any action that may affect your business, you should consult a qualified professional advisor.\n\nDeloitte shall not be responsible for any loss sustained by any person who relies on this publication.",
    "### About Deloitte\n\nDeloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (“DTTL”), its network of member firms, and their related entities.\n\nDTTL and each of its member firms are legally separate and independent entities.\n\nDTTL (also referred to as “Deloitte Global”) does not provide services to clients.\n\nIn the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the “Deloitte” name in the United States and their respective affiliates.\n\nCertain services may not be available to attest clients under the rules and regulations of public accounting.\n\nPlease see  to learn more about our global network of member firms.\n\nCopyright © 2023 Deloitte Development LLC.\n\nAll rights reserved.",
    "# Generative AI Acceptable Use Policy\n\nJuly 2023\n\nThe exploration and use of Generative AI / Open AI / Chat GPT tooling (from this point forward referred to as Generative AI) by employees of Maergo is permitted and in fact encouraged for work-related purposes.\n\nGenerative AI is a new, exciting, and powerful tool that can assist in various aspects of work, including but not limited to writing, research, analysis, and customer service.\n\nAt Maergo, we embrace new technologies and encourage our teams to leverage them to improve efficiency and effectiveness, and enable the achievement of our objectives.\n\nHowever, it is vitally important that employees use Generative AI responsibly while remaining compliant with all existing company policies and all applicable laws.\n\nWe are committed to providing a safe and secure environment for our employees, partners, and customers and this includes our responsible use of Generative AI tooling.\n\nThis policy outlines the guidelines and principles that employees must follow when using Generative AI capabilities.\n\nThe company developed these guidelines so that we can experience the benefits of Generative AI tooling while ensuring its utilization is secure and aligned with our expectations for conduct and our internal data privacy controls.",
    "# Guiding Principles\n\nThere are two main principles that guide the Maergo approach to Generative AI and the entirety of this policy centers on these ideas:\n\nGenerative AI should be used to assist employees in their work, not perform their work.\n\nAll existing company policies apply to the utilization of Generative AI.\n\nGenerative AI should be used to assist employees in their work, not perform their work.\n\nEmployees are responsible for their work product and are expected to carefully review any output received through Generative AI.\n\nUtilizing Generative AI well includes writing prompts with care, reviewing output, verifying output, editing output, and providing feedback to the tool if it got something wrong.\n\nAll existing company policies apply to the utilization of Generative AI.\n\nThis means that the company has determined that all current policies apply to an employee’s use of Generative AI, including but not limited to the Employee Proprietary Information Agreement, as well as all company policies pertaining to confidential information; intellectual property; bias, harassment, and discrimination; fraud and other illegal activities.",
    "# Prohibited Use\n\nWhile Maergo allows and encourages utilization of Chatbots/Large Language Models (LLMs) such as ChatGPT, autonomous AI agents including but not limited to AutoGPT, Godmode AI, and AgentGPT are not by default approved for company use.\n\nDue to the functionality, failure modes, and risks associated with autonomous AI agents, utilization is prohibited unless a request is made and approved by the Vice President, Product and Engineering and the Executive Leadership Team.\n\nAs of the writing of this policy, Maergo does not permit the use of Generative AI in the candidate selection process in any way that would replace or substantially assist in the decision-making responsibilities of our hiring teams.\n\nProhibited use of Generative AI includes but is not limited to screening or comparing candidate data, profiles, or resumes.\n\nGenerative AI may be used to assist with various components of the recruitment process such as assisting with the drafting job description language and suggesting potential interview questions.\n\nAs of the writing of this policy, Maergo does not permit the use of Generative AI to directly communicate or interact with any other person through bots or any similar technology.\n\nGenerative AI may be used to assist with drafting or editing of communications but may not be used for direct communication with others internal or external to Maergo.\n\nAs previously stated in this document, all current policies apply to the utilization of Generative AI.\n\nFor the purposes of reinforcing this expectation, we are providing the following illustrative examples of activities that are strictly prohibited and constitute serious violations of company policy.\n\nThese following will result in disciplinary action, up to and including termination of employment.\n\n- Sharing company confidential and proprietary information in a Generative AI chat or by any other means sharing or entering this information into a Generative AI tool.\n\n- Sharing customer or partner confidential information in a Generative AI chat or by any other means sharing or entering this information into a Generative AI tool.\n\n- Sharing Personally Identifiable Information (PII) in a Generative AI chat or by any other means sharing or entering this information into a Generative AI tool.\n\n- Using Generative AI in a way that is not professional and respectful of others including using Generative AI to engage in any form of discrimination, harassment, or other inappropriate behavior.\n\n- Using Generative AI to engage in any activity that violates Maergo's policies.\n\n- Using Generative AI to engage in illegal activity, including but not limited to fraud, intellectual property theft, and copyright infringement.\n\nFor clarity, entering confidential information into the tool is prohibited without regard to how the information is shared.\n\nThe means of sharing may include, but is not limited to, direct entry, copying and pasting, uploading, video, voice, or any other means of sharing.\n\nFurther, entering confidential information is prohibited whether or not the entry is saved.\n\nOnly information that is generally publicly available may be entered into Generative AI tooling.\n\nIf employees are unsure about whether or not information is confidential, they are expected to ask a member of the People or Legal team prior to sharing such information Generative AI.",
    "# Security Measures\n\nUtilization of Generative AI for any work-related purpose should always be done through an account associated with an employee’s maergo.com email address rather than a personal email address.\n\nAs with any system that is used by Maergo employees, those using Generative AI must ensure that their login credentials are kept confidential and not shared with anyone.\n\nIn case an employee suspects any breach of security or unauthorized use, they must report it immediately to a member of the People or Legal team.",
    "# Reporting\n\nEmployees must report any suspected violations of this policy, or any incidents related to the misuse of Generative AI tooling, to the Maergo People team.\n\nAll reports of suspected violations or incidents will be investigated promptly and thoroughly and as confidentially as possible.\n\nEmployees must cooperate fully with any investigations related to suspected violations or incidents where Generative AI has been applied.\n\nMaergo prohibits retaliation against any employee who reports or participates in an investigation of a possible violation of our code of conduct, our policies including this policy, or the law.\n\nIf you believe you are being retaliated against, please contact the Chief People Officer or General Counsel.\n\nMaergo reserves the right to review any communications sent through or information shared with Generative AI tooling for the purpose of investigating suspected violations or incidents.\n\nThis includes but is not limited to messages, prompts, attachments, and files.",
    "# Financials of Usage\n\nAt present, there are free Generative AI tools available that are sufficient to allow for the utilization outlined in this policy.\n\nThe company will not pay for or reimburse employees for premium subscriptions or other costs associated with Generative AI tooling.\n\nA business case for an exception request will require the approval of the Vice President, Product and Engineering and the Executive Leadership Team.",
    "# Review of this Policy\n\nMaergo will review this Acceptable Usage Policy as needed to ensure it remains relevant and effective, in line with our risk appetite for Generative AI use cases.\n\nAny changes or updates to this policy will be communicated to all employees.\n\nMaergo reserves the right to make changes to this policy at any time, without notice.\n\nEmployees are encouraged to ask questions or for clarification on this policy.\n\nEmployees are also encouraged to provide feedback on this policy and offer suggestions for changes or improvements.",
    "# Agency Policy Template for Generative AI\n\nThis document is intended only as a template – we encourage agencies to adapt for their specific needs and ensure your stakeholders including legal, security, and operations have reviewed and approved your policy.\n\nEach of the below headings features multiple sub-bullets, some of which are intentionally repetitive to provide multiple options from which to select and/or modify.\n\nThe information provided by the 4A's is for general informational purposes only.\n\nAll information referenced herein is provided in good faith, however, we make no representation or warranty of any kind, express or implied, regarding the accuracy, adequacy, validity, reliability, availability, or completeness of any information.\n\nUnder no circumstance shall we have any liability to you for any loss or damage of any kind incurred as a result of the use of this information or reliance on any information provided herein.\n\nYour use of the information and your reliance on any information herein is solely at your own risk.",
    "## Introduction\n\nWith the rapid growth of generative AI technology, we seek to harness the value for our clients and our agency in a responsible and ethical way.\n\nAs we do this, we believe it is essential to express our Agency Point-of-View (POV) and formalize a company-wide policy so that we can set expectations with our team members and our clients.\n\nOverall, we believe that AI technology can help our team of professionals personalize experiences, automate repetitive tasks and boost creativity and innovation with the purpose of driving better results for our clients and ourselves.\n\nWe seek to use AI to scale the impact of our talent while providing advanced career opportunities for our people.\n\nWe recognize that AI technology is rapidly developing and we will frequently review this POV and publish updates as needed.",
    "## Integrity and Client Confidentiality\n\nWe use caution with confidential client information in AI tools, avoiding submission of sensitive data unless a) explicitly authorized and/or b) we have platform assurances that such data will not be used for training publicly available models.\n\nWe comply with all client agreements, policies, and directives in our AI deployments.",
    "## Transparency and Disclosure\n\nWe are transparent about our use of AI, acknowledging its role in content creation through statements or contract inclusions.\n\nWe disclose the use of AI to clients and partners as part of our commitment to transparency.\n\nWe require vendors and partners to disclose any use of AI in work we do together.",
    "## Ethical Use and Responsibility\n\nWe respect the power of language, images, and video in education and influence, committing to responsible and ethical AI usage.\n\nWe use AI to enhance creativity and innovation, scaling the impact of our talent and offering advanced career opportunities.\n\nUnder no circumstances do we use AI for creating or spreading misinformation, deepfakes, or manipulation.\n\nWe commit to using AI ethically and responsibly, always ensuring human oversight and judgment in AI-assisted content.",
    "## Diversity and Inclusivity\n\nWe are vigilant about biases in AI-generated content and work towards inclusive and accessible outputs.\n\nWe engage diverse perspectives in reviewing AI content, avoiding the use of AI as a replacement for diverse insights.\n\nIf training our own models, we make every effort to avoid bias by ensuring diverse training data sets as well as diverse perspectives during the training and review process.",
    "## Tool Selection and Security\n\nWe carefully evaluate all tools to be used with a cross-disciplinary team to ensure legal compliance, risk mitigation, intellectual property controls, data security, and avoidance of bias.\n\nWe use only approved AI tools and discuss any new tools with our security team (and other relevant stakeholders) to ensure data protection.\n\n- Insert approved tools here.\n\nWe safeguard against cyber-attacks and protect the privacy of our customers and intellectual property.\n\n- Insert details of security efforts here.",
    "## Training and Best Practices\n\nWe establish clear guidelines for AI use, conducting regular training to avoid legal and ethical risks.\n\nWe provide ongoing education and training on the responsible use of AI, covering both technical and ethical aspects.\n\nTraining should focus on the following aspects:\n\n- Best practices for using AI in workflows,\n- Avoiding or mitigating potential algorithmic biases,\n- Enhancing client and stakeholder transparency,\n- Proper and full sourcing,\n- How to identify inaccurate results, and\n- Maintaining the integrity of intellectual property and confidential data",
    "## Legal Compliance\n\nTo the best of our ability our AI usage is aligned with legal requirements, including intellectual property rights, privacy, and data protection laws.\n\nWe evaluate AI training data and outputs to ensure that (to the best of our ability) AI-augmented work does not infringe on IP rights of others.\n\nWe acknowledge that any material produced by AI alone may not be eligible for copyright and therefore favor either a) substantial modification and enhancement by humans or b) usage of AI to extend/expand human-created materials rather than create the core assets.\n\nWe ensure that usage of AI is governed in client and vendor contracts and that our teams are clear on appropriate levels of risk and legal liability.",
    "## Acceptance\n\nBy using AI in your work, you agree to comply with this policy.\n\nNon-compliance will be taken seriously and could lead to disciplinary action or employment termination.\n\nRemember, the goal of this policy is not to restrict creativity, but to ensure that we use AI responsibly and ethically.\n\nBy following these guidelines, we can harness the power of AI while respecting our customers and upholding our company values.",
    "# Six Steps for Writing a Generative AI Policy\n\nEmployees might already be creating content with new tools, so in-house counsel should take the lead to get a policy in place before risks mount, experts say.\n\nPublished Aug. 3, 2023\n\nRobert Freedman Lead Editor  \nLaurence Dutton via Getty Images\n\nDon’t wait until you hear from employees that they’re using generative AI in their work; be proactive and create a policy governing use of the new technology, attorneys say.\n\nLawyers at one company initiated a process to write a policy after hearing employees were already using it, Kim Phan of law firm Troutman Pepper said in a webcast hosted by the firm.\n\n“The legal folks learned just through water cooler chat that folks were starting to use AI,” she said.\n\n“They were fortunate enough to have a company that has the resources to devote to thinking about this.” Companies are playing catch-up because employees are ahead of them on use of the technology, said Peter Wakiyama, also of Troutman Pepper.\n\n“This is probably most companies right now,” he said.\n\nGenerative AI differs from more familiar versions of the technology by using information provided in prompts to create, rather than analyze, content.\n\nLawyers have started using it in some cases to write the first draft of a brief or non-legal communication, among other things.",
    "## Starting the Process\n\nTo write a policy governing generative AI use for your organization, there are six steps in-house teams should consider, Phan said.\n\nThe first is to let everyone at the organization know you’re planning the policy so once you get going the process won’t be a surprise.\n\n“The first communication needs to be about transparency, leading your employees toward the direction as a top-down approach your company wants to take,” she said.\n\nYou’ll want to assemble a team from all of the parts of the organization that will be touched by the technology.\n\nIt can make sense for legal and HR to take the lead, but also important are IT, R&D, communications, marketing and sales.\n\nThe team should come to agreement on the organization’s risk tolerance because that will help determine the depth and breadth of the policy.\n\nTechnology companies that have been using AI for a while might be expected to be more risk tolerant than others, but nailing this down for your organization is crucial.\n\n“Companies have different appetites for risk, but this is a starting point,” said Wakiyama.",
    "## Surveying Stakeholders\n\nIn the next step, develop a survey to send to employees to learn how they’re already using generative AI, if they are, and under what scenarios they would like to use it and not use it.\n\n“The survey may even differ from department to department,” said Wakiyama.\n\n“So, put out your survey – you can use an electronic survey tool for that – and gather the results so you’re making the decision based on actionable information.” As responses come in, the team can devise scenarios for how the technology should and shouldn’t be used.\n\n“Are those the right use cases for our company?” said Phan.\n\n“The critical stakeholders – legal, HR, IT, others – get together and think about that.” Based on her work with clients, Phan said, she’s seen a range of use cases, especially that apply to coders in engineering.\n\n“They’re treating generative AI the same way they might incorporate open source coding data into their systems,” she said.\n\nCoders have been quick to adopt AI because it can help them work faster, said Gerard Lewis, former deputy general counsel at Comcast.\n\n“They’ve used it for rapid prototyping and deployment of new technologies,” said Lewis in the webcast.\n\n“It lets developers create quickly and get something stood up in record time to see if an idea or logic flow works.”\n\nOutside of engineering, other use cases that are being tried include first drafts of documents.\n\n“They’re using it to produce an initial draft of internal or customer service communications, with some human intervention to tailor it,” Phan said.\n\n“And the marketing team is using it for jingles, slogans and other content.”",
    "## Policy Outline\n\nOnce the team has developed, vetted and received approval on a list of use cases, it should create a policy outline that it sends around to get buy-in from leadership and stakeholders.\n\n“The outline is critical,” said Wakiyama.\n\n“It’s the foundation for the policy.\n\nShare it with all the stakeholders and those who need to put the policy out there in the field and answer questions about it.”\n\nWhen the outline has been given wide exposure, the team drafts the policy, which should include a list of acceptable and prohibited uses.\n\nIt should also highlight the process employees follow to get exceptions approved.\n\n“Maybe you’re OK with employees exploring, figuring out what makes sense for them,” said Phan.\n\n“Maybe you want to have more level of control or you want employees to vet new use cases with you.”\n\nYou don’t want the exception process to be too rigid, otherwise you risk employees ignoring it and experimenting with generative AI outside the policy.\n\n“Employees [could] try to circumvent the process and then you have non-compliance,” she said.\n\nSo you want the process to be smooth — “a documented way for employees to acknowledge the prohibition but still take a clear path to get an exception.” The policy should also include sections on recordkeeping and use logs, security, privacy, IP rights and new tools.",
    "## Iterative Process\n\nSetting up a process for recordkeeping and administering a use log can be burdensome but they’re needed for measuring how the organization is using generative AI and whether it’s having an effect on operations.\n\n“Unless this information is collected, it’s difficult for the organization to know if the use of AI is beneficial,” said Wakiyama.\n\n“And if someone asks about it — a commercial insurer, for example — if they have that on a questionnaire for commercial insurance, the organization won’t have the information if it’s not being kept on an ongoing basis.”\n\nOnce the policy is out, you want to disseminate it the way you would any other policy.\n\nThat could mean an email blast to all employees or something else.\n\nTraining is also important, both to help ensure employee compliance but also as a way to get buy-in.\n\n“We know employees often don’t read policies in full, so training can enhance adoption and compliance,” said Wakiyama.\n\nThe last piece is about keeping the policy updated.\n\nThat could mean going out on a regular basis with the survey to learn how employees are using, or are wanting to use, the new tools.\n\nUpdating could also include adding new acceptable or prohibited uses, incorporating regulatory compliance as needed and making sure the policy doesn’t conflict with your organization’s other policies.\n\n“It’s an evolving area,” said Wakiyama.\n\n“It’s new in terms of case law, regulations.\n\nIt’s changing daily.\n\nWe’re seeing new cases, early decisions.\n\nThe policy [should] evolve along with the AI landscape, and as the organization obtains feedback on how and where it’s being used, you can update accordingly.”",
    "## 01 Introduction\n\nOvernight, generative artificial intelligence (AI) has become a global sensation.\n\nPredictions of its potential impact on society, employment, politics, culture, and business fill the media and the internet.\n\nBusiness leaders are intrigued by the possibilities and are convinced that generative AI is truly a game-changer.\n\nKPMG launched its 2023 KPMG Generative AI Survey in March to look beyond the hype and understand how enterprises can make progress toward real, meaningful generative AI results.\n\nThe chief finding: Across industries and functions, more than three-quarters of executives (77 percent) see generative AI as by far the most impactful emerging technology they will use, and 71 percent plan to implement their first generative AI solution within two years.\n\nThe possibilities for using generative AI to transform how enterprises create content, engage users, develop software, and analyze data appear limitless.\n\nBut, as with many emerging technologies, the path from buzz to business value is not simple or straightforward.\n\nGenerative AI is still in its infancy and evolving rapidly.\n\nBefore executives invest in broad adoption, they have many unanswered questions about security, reliability, impact on jobs, and potential value.\n\nExecutives in our survey cite lack of talent, cost issues, and unclear applications as top barriers to implementation.\n\nThis report is based on survey data from 300 executives from across industries and geographies, as well as the insights of KPMG advisers in artificial intelligence, technology enablement, strategy, and risk management.\n\nThe bottom-line assessment: Harnessing the transformative power of generative AI will require a balance of speed—first-mover advantage could be powerful—with thoughtful planning and careful risk mitigation.\n\nWe discuss the why, where, when, and how of generative AI adoption, and offer practical insights for responsibly integrating generative AI solutions into individual organizations.\n\nOur goal is to help guide the critical dialogue taking place in C-suites about how to use generative AI to empower employees, work smarter, and compete better.\n\n© 2023 KPMG LLP, a Delaware limited liability partnership and a member firm of the KPMG global organization of independent member firms affiliated with KPMG International Limited, a private English company limited by guarantee.\n\nAll rights reserved.",
    "## Survey Highlights\n\nOpportunities:\tChallenges:\n\n- 77% of respondents expect generative AI to have the largest impact on their businesses out of all emerging technologies.\n\n- 92% think generative AI implementation introduces moderate to high-risk concerns.\n\n- 73% believe generative AI will increase workforce productivity.\n\n- 47% are still at the initial stages of evaluating risk and risk-mitigation strategies for generative AI.\n\n- 71% will implement their first generative AI solution within the next two years.\n\nTop barriers to implementation:\n- Lack of skilled talent, cost/lack of investment, and lack of clear business case\n- Cybersecurity and data privacy",
    "## 02 Generative AI Overview\n\nWhat's the buzz?\n\nAn evolution of AI algorithms, especially deep learning technologies, generative AI models come out of the box having learned how to use the foundations of human communication—language, art, music, programming code, etc.—to make new content similar to that of humans.\n\nAptly also known as \"foundational models,\" generative AI can be scaled and deployed across institutions far faster and with less cost than human-only processes.\n\nWith the potential to be used across a variety of business use cases to save time, money, and effort, new applications of generative AI are being researched, developed, and adopted at an astonishing pace.\n\nGenerative AI technology attracted more than $1.37 billion venture capital dollars in 2022—more than was invested in the previous five years combined.\n\nRobust growth is expected to continue as big tech companies like Microsoft, Alphabet, and Amazon, as well as newer generative AI startups like OpenAI, Synthesia, and Jasper AI continue to fund new innovation and push the envelope.",
    "### Exhibit 1: Generative AI Market Share by Region (2022)\n\n- 8% Latin America\n- 3% Middle East and Africa\n- 22% Asia Pacific\n- $10.8B 41% North America\n- 26% Europe\n\nMarket size (US $B)\n\nSources: Limited availability of updated report, hence an estimation for 2022 as well was included, though the year has passed; Precedence Research; Grand View Research; Quidgest quoted Gartner numbers",
    "## 03 What Enterprise Leaders Are Thinking About Generative AI\n\nTruly transformative: Perception and outlook.\n\nGenerative AI stands apart from other recent innovations for its massive leap in ability and its potential breadth of impact across any industry and business function.\n\nBusiness leaders are highly interested in the capabilities and opportunities generative AI can unleash and believe it has the potential to reshape how they interact with customers.\n\nFor the first time in history, we have a technology that can directly augment humans in knowledge creation.\n\nWhere other technologies are indirect enablers of knowledge work, generative AI has enabled the development of a true smart assistant that inches closer to human cognition and reasoning.",
    "### Exhibit 3: 77% of Leaders Rank Generative AI as the Most Impactful Technology\n\nGenerative AI\n- 77%\n\nAdvanced robotics\n- 39%\n\nQuantum computing augmented reality/virtual reality\n- 31%\n\nAnd where recent technological advancements like blockchain, Web3, and quantum computing affected pockets of an industry or specific business functions, generative AI has applications across the end-to-end enterprise.",
    "### Exhibit 4: Executives See Generative AI More as Opportunity Than Threat\n\nGenerative AI represents a significant opportunity for our company to grow in terms of revenue/market share Generative AI represents a significant opportunity for our company to drive better efficiencies Generative AI represents a significant threat to our company's position in the market Generative AI will significantly help our business gain a competitive advantage over our competitors",
    "## Dynamic Uses: Top Applications and Adoption Timelines\n\nTransforming business processes using generative AI precursors like machine learning and automation requires breaking them down into their individual component parts and applying strategic thinking around what components to accelerate or optimize.\n\nGenerative AI changes the game.\n\nProcesses do not need to be broken down because generative AI tools can apply the large variety of human knowledge, experiences, and common sense embedded into their models to fill the gaps.\n\nThis creates immense opportunities to apply and scale the technology across real-world, enterprise-wide business processes.",
    "### Exhibit 5: 71% Plan to Implement Their First Generative AI Solution Within Two Years\n\n- No specific timeframe in mind: 9%\n- Within the next five years: 12%\n- Within the next two years: 33%\n- We have already implemented at least one Generative AI solution: 7%\n- Within the next six months: 12%\n- Within the next year: 26%\n\nExecutives expect the impact of generative AI to be highest in enterprise-wide areas: driving innovation, customer success, technology investment, and sales and marketing.\n\nIT/tech and operations are the top two functional areas respondents are currently exploring to implement generative AI in their businesses.",
    "### Exhibit 7.\n\nExpectations for Generative AI Applications, by Sector and Function\n\nNot ready for prime time: Implementation challenges Respondents name a diverse list of barriers to implementing AI, led by lack of skilled talent to develop and implement, cost/lack of investment, lack of clear business case, lack of clarity on specific ways to implement, and lack of leadership understanding and/or strategy.",
    "## A Matter of Trust: Internal and External Risks\n\nOrganizational barriers aside, it is little wonder executives feel unprepared for immediate adoption when you consider the worst-case scenarios of unplanned, uncontrolled generative AI applications.\n\nPerhaps today’s most important business asset—trust—is at stake.\n\nA large majority of executives (72 percent) believe generative AI can play a critical role in building and maintaining stakeholder trust.\n\nYet, almost half (45 percent) also say the technology can negatively impact their organization’s trust if the appropriate risk-management tools are not implemented.",
    "### Exhibit 9: Most Companies Have Not Gotten Far with Risk Mitigation Strategies for Generative AI\n\n- No, we have not yet started evaluating risk and risk mitigation strategies: 22%\n- Yes, we have a dedicated team for evaluating risk and implementing risk mitigation strategies: 6%\n- We are in the initial stages of evaluating risk and risk mitigation strategies: 47%\n- We have evaluated risk and risk mitigation strategies, but we are still in the process of implementing them: 25%",
    "## Preparing People: Workforce Implications\n\nOur survey indicates that executives expect generative AI to have significant impact on workforces but mostly as a means to augment, rather than replace, labor.\n\nNearly three-quarters of respondents (73 percent) believe generative AI will increase productivity; 68 percent say it will change how people work; and 63 percent think it will encourage innovation.",
    "## How KPMG Can Help\n\nAccelerate the value of generative AI solutions—responsibly and confidently Generative AI is poised to transform the future of enterprise.\n\nBusinesses will increasingly rely on generative AI to gain insights, make critical decisions, alleviate skills shortages, create new products, and engage with customers.\n\nWe believe, however, that many businesses do not fully understand or account for the risks and challenges generative AI poses.\n\nSuccessful generative AI adoption requires an approach to designing, building, and deploying systems in a safe, trustworthy, and ethical manner.",
    "## Research Methodology\n\nFrom March 17-31, 2023, KPMG conducted an online survey of 300 global business executives to explore generative AI views and trends.\n\nSurvey questions asked about respondents’ understanding of the transformative impact and applications of generative AI, spanning four core topics: perception and strategic intent, applications, technology challenges, and opportunities, and impacts on risk management and workforce.",
    "## Authors\n\nTodd Lohr, Principal, Leader for Technology Enablement, KPMG in the US \n\nMark Shank, Principal, Artificial Intelligence, Lighthouse KPMG in the US \n\nElliot Brook, Principal, Artificial Intelligence, Lighthouse KPMG in the US \n\nPer Edin, Principal, Generative AI Lead, Deal Advisory and Strategy, KPMG in the US \n\nSteve Chase, Partner, US Consulting Leader\nKPMG in the US \n\nBryan McGowan, Principal, Generative AI Lead, Risk Services KPMG in the US \n\nSreekar Krishna, Ph.D., Principal, National Leader for AI and Head of Data Engineering, Lighthouse KPMG in the US \n\nEmily Frolick, Partner, US Trusted Imperative Leader KPMG in the US",
    "### Related Thought Leadership\n\n© 2023 KPMG LLP, a Delaware limited liability partnership and a member firm of the KPMG global organization of independent member firms affiliated with KPMG International Limited, a private English company limited by guarantee.\n\nAll rights reserved.\n\nThe KPMG name and logo are trademarks used under license by the independent member firms of the KPMG global organization.\n\nDASD-2023-12613 May 2023",
    "### Guidance Regarding Use of Generative AI Tools for Open Source Software Development\n\nWe have received numerous questions from our project communities about contributing AI-generated code to Linux Foundation projects.\n\nOpen source software has thrived for decades based on the merits of each technical contribution that is openly contributed to and reviewed by community peers.\n\nDevelopment and review of code generated by AI tools should be treated no differently.\n\nCode or other content generated in whole or in part using AI tools can be contributed to Linux Foundation projects.\n\nHowever, there are some unique considerations related to AI generated content that developers should factor into their contributions.\n\nContributors should ensure that the terms and conditions of the generative AI tool do not place any contractual restrictions on how the tool’s output can be used that are inconsistent with the project’s open source software license, the project’s intellectual property policies, or the Open Source Definition.\n\nIf any pre-existing copyrighted materials (including pre-existing open source code) authored or owned by third parties are included in the AI tool’s output, prior to contributing such output to the project, the Contributor should confirm that they have permission from the third party owners–such as the form of an open source license or public domain declaration that complies with the project’s licensing policies–to use and modify such pre-existing materials and contribute them to the project.\n\nAdditionally, the contributor should provide notice and attribution of such third party rights, along with information about the applicable license terms, with their contribution.\n\nSome tools provide features that can assist contributors.\n\nFor example, some tools provide a feature that suppresses responses that are similar to third party materials in the AI tool’s output, or a feature that flags similarity between copyrighted training data or other materials owned by third parties and the AI tool’s output and provides information about the licensing terms that apply to such third party materials.\n\nIndividual Linux Foundation projects may develop their own project-specific guidance and recommendations regarding AI-generated content.\n\nSimilarly, organizations that employ open source developers may have more stringent guidelines related to use of AI for software development.\n\nContributors should comply with their employer’s policies when contributing.",
    "## Stay Connected with the Linux Foundation\n\nFirst name\n\nLast name\n\nEmail*\n\nBy submitting this form you are consenting to receive marketing emails about news, events, and training from the Linux Foundation.\n\nYou can unsubscribe at any time by following the “Subscription Center” link included within such communications.\n\nFor information on our privacy practices and commitment to protecting your privacy, please review our Privacy Policy.\n\nWe do not sell your contact information to third parties.",
    "## PROJECTS\n\nNEWSROOM LF RESEARCH  \nLFX PLATFORM RESOURCES  \nEVENTS  \n\nLF TRAINING\n\nCopyright © 2024 The Linux Foundation®.\n\nAll rights reserved.\n\nThe Linux Foundation has registered trademarks and uses trademarks.\n\nFor a list of trademarks of The Linux Foundation, please see our Trademark Usage page.\n\nLinux is a registered trademark of Linus Torvalds.\n\nTerms of Use | Privacy\n\nPolicy | Bylaws | Antitrust Policy | Good Standing Policy | Generative AI Policy",
    "# Generative AI and Law Firm Information Governance\n\nLaw Firm Information Governance Symposium (LFIGS) July 2023\n\nThis paper covers the use of artificial intelligence (AI), and in particular, Generative AI in law firms.\n\nWhereas AI usage is similar across industries, law firms have some unique characteristics because much of the data belongs to the clients and there is an extra obligation to keep it safe and private.\n\nThis paper covers both unique and non-unique challenges of AI as it pertains to information governance (IG) in the legal community.\n\nIt begins with a brief definition of AI, followed by benefits to the legal community, information governance-specific considerations, policy considerations, general advice, and guidance.",
    "## What are some benefits to the legal industry?\n\nGenerative AI offers a range of benefits for organizations, both obvious and less apparent.\n\nObvious benefits include:\n\nLess apparent benefits include:\n\nOverall, generative AI offers a wide range of benefits for organizations, both large and small.\n\nBy automating tasks, improving efficiency, and providing access to new insights, AI can help them save time and money, improve customer service, and gain a competitive edge.",
    "## What are some key sources for staying up-to-date?\n\nAs this technology continues to grow in its development and use cases, as with any emerging tech, it is critical that IG professionals continue to stay informed, educated and in-tune with not only IG implications but how the technology is evolving and being used within organizations.\n\nThis space is evolving quickly, but here are a few notable articles to help build your understanding of Generative AI, its use and implications.\n\nAnd for a little fun for the IG geek in all of us, here’s an AI IG rap song by ChatGPT:\n\n(Verse 1) Yo, listen up, I got a story to tell, 'Bout a topic that might not ring a bell.\n\nIt's called information governance, you see, But don't you worry, I'll make it fun and carefree.\n\n(Chorus) Information governance, it's a serious thing, But let's bring some humor and make it swing.\n\nFrom policies to data, it's all in the mix, Get ready to laugh, 'cause this rap's gonna fix!\n\n(Verse 2) Picture this, a file all out of control, With data scattered like a cereal bowl.\n\nInformation governance comes to the rescue, Organizing data like it's the coolest venue.\n\n(Verse 3) We got file naming conventions, let's go, No more \"final\" or \"final_final,\" yo!\n\nVersion control, it's the name of the game, No more confusion, keep it all in the lane.\n\n(Chorus) Information governance, it's a serious thing, But let's bring some humor and make it swing.\n\nFrom policies to data, it's all in the mix, Get ready to laugh, 'cause this rap's gonna fix!\n\n(Verse 4) Privacy and security, they're paramount, Protecting information, no room for a discount.\n\nEncryption and access controls, oh so tight, Keep that data safe, day and night.\n\n(Verse 5) Retention schedules, don't throw it all away, Keep the important stuff, the rest can sway.\n\nData classification, like a game of chess, Sort it all out, you'll feel nothing but blessed.\n\n(Chorus) Information governance, it's a serious thing, But let's bring some humor and make it swing.\n\nFrom policies to data, it's all in the mix, Get ready to laugh, 'cause this rap's gonna fix!\n\n(Bridge) Now let's talk about audits, it's no need to fear, Just be prepared, keep your records clear.\n\nCompliance is the name of the game, Follow the rules, and you'll earn your fame.\n\n(Verse 6) Information governance, it's the boss of the show, Bringing order to chaos, watch it go.\n\nSo, remember the importance of this rap, Information governance, it's a funny mishap.\n\n(Chorus) Information governance, it's a serious thing, But let's bring some humor and make it swing.\n\nFrom policies to data, it's all in the mix, Get ready to laugh, 'cause this rap's gonna fix!\n\n(Outro) So, here we are at the end of this rhyme, Information governance, we had a good time.\n\nRemember the lessons and keep it in sight, Now go out there and keep your data tight!",
    "## Authors\n\nColin Cahill – Records and Information Governance | Senior Consultant; Epiq  \nBrian J. Donato – Chief Information Officer; Vorys, Sater, Seymour and Pease LLP  \nGalina Datskovsky Ph.D., CRM – Board of Directors, OpenAxes  \nMichele Gossmeyer – Global Director, Information Governance, Risk & Compliance; Dentons  \nLeigh Isaacs – Sr Director Info Governance; DLA Piper LLP (US)  \nScott D. Muir (Graphic Design) – Global Senior Analyst; Dentons  \n\n© 2023 Iron Mountain, Incorporated and/or its affiliates.\n\nAll rights reserved.\n\nInformation herein is proprietary and confidential to Iron Mountain and/or its licensors, does not represent or imply an invitation or offer, and may not be used for competitive analysis or building a competitive product or otherwise reproduced without Iron Mountain’s written permission.\n\nIron Mountain does not provide a commitment to any regional or future availability and does not represent an affiliation with or endorsement by any other party.\n\nIron Mountain shall not be liable for any direct, indirect, consequential, punitive, special, or incidental damages arising out of the use or inability to use the information, which is subject to change, provided AS-IS with no representations or warranties with respect to the accuracy or completeness of the information provided or fitness for a particular purpose.\n\n“Iron Mountain” is a registered trademark of Iron Mountain in the United States and other countries, and Iron Mountain, the Iron Mountain logo, and combinations thereof, and other marks marked by ® or TM are trademarks of Iron Mountain.\n\nAll other trademarks may be trademarks of their respective owners.",
    "## Usage Guidelines\n\nEmployees, contractors, and third-party service providers must adhere to the following guidelines:\n\n01 Do not input proprietary or confidential information into generative AI systems, including company secrets, customer secrets, or PII.\n\n02 Verify the accuracy, reliability, and relevance of any AI-generated content before using it officially.\n\n03 Use generative AI technology solely for authorized business purposes and in compliance with applicable laws and regulations.\n\n04 Immediately report any unintended exposure, misuse of sensitive information, or security breaches involving generative AI technology to the relevant supervisor or security team.",
    "## Executive summary\n\nThe year 2022 was a watershed year for artificial intelligence (AI), with the release of several consumer-facing applications like ChatGPT, DALL.E, and Lensa.\n\nThe common theme is the use of Generative AI—a paradigm shift in the world of AI.\n\nWhile current generations of AI use pattern detection or rule-following to help analyse data and make predictions, the advent of transformer architectures has unlocked a new field: Generative Artificial Intelligence.\n\nGenerative AI can mimic the human creative process by creating novel data similar to the kind it was trained on, elevating AI from enabler to (potentially) co-passenger.\n\nIn fact, Gartner estimates that more than 10% of all data will be AI-generated by as early as 2025, heralding a new age, the Age of With™.\n\nAlthough early traction has been through consumer releases, which could be era-defining, Generative AI also has the potential to add contextual awareness and human-like decision-making to enterprise workflows and could radically change how we do business.\n\nWe may be only just beginning to see the impact of solutions like Google’s Contact Centre AI (CCAI), which is designed to help enable natural language customer service interactions, and industry-specific solutions like BioNeMo from NVIDIA, which can accelerate pharmaceutical drug discovery.\n\nAs such, Generative AI has attracted interest from traditional (e.g., Venture Capital (VC), Mergers & Acquisitions (M&A)) and emerging (e.g., ecosystem partnerships) sources.\n\nIn 2022 alone, venture capital firms invested more than $2B, and technology leaders made significant investments, such as Microsoft’s $10B stake in OpenAI and Google’s $300M stake in Anthropic.\n\nThe far-reaching impacts and potential value when deploying Generative AI are accelerating experimental, consumer, and soon, enterprise use cases.\n\nAnd even though much media coverage has focused on consumer use cases, the opportunities are widespread—and some are already here.\n\nStill, questions remain about how individuals and enterprises could use Generative AI to deliver efficiency gains, product improvements, new experiences, or operational change.\n\nSimilarly, we are only beginning to see how Generative AI could be commercialised and how to build sustainable business models.\n\nEven so, Generative AI is in its infancy and not without risk.\n\nSome of the most important risks to address relate to privacy and security, managing bias, transparency and traceability of results, IP ownership, and equal access, especially for those at greater risk of job displacement.\n\nAs such, participants should balance commercialisation, regulation, ethics, co-creation, and even philosophy, as well as expand the group of stakeholder thinkers and contributors beyond technologists and enthusiasts.\n\nUltimately, Generative AI could create a more profound relationship between humans and technology, even more than the cloud, the smartphone, and the internet did before.\n\nVarious analysts estimate the market for Generative AI at $200B by 2032.\n\nThis represents ~20% of total AI spend, up from ~5% today.\n\nSaid another way, the market will likely double every two years for the next decade.\n\nNumbers aside, we believe the economic impact could be far greater.\n\nTo help understand the potential, this paper is equal parts primer and provocateur, adding structure to a rapidly changing marketplace.\n\nWe start with a brief explainer of the foundational elements, delve into enterprise and consumer use cases, shift focus to how players across the market can build sustainable business models, and wrap up with some considerations and bold predictions for the future of Generative AI.",
    "## Section I: Decoding the Generative AI magic trick\n\nThe lofty expectations for Generative AI depend on continued progress and innovation across an interconnected hardware, software, and data provider ecosystem.\n\nThe tech stack underlying Generative AI, however, is in some ways similar to others that came before.\n\nIt consists of three layers: infrastructure, platform, and applications.\n\nInfrastructure is generally accepted as the most established, stable, and commercialised layer.\n\nIncumbents offer compute, networking, and storage, including access to specialised silicon (microprocessors) like NVIDIA’s GPUs and Google’s TPUs optimised for AI workloads.\n\nMeanwhile, the application layer is evolving rapidly and consists of leveraging and extending foundation models, which is Generative AI’s equivalent of a platform.",
    "## Generative AI Tech Stack\n\nOpen/Closed APIs\nFoundation Models, however, are what differentiate the Generative AI tech stack from AI that came before.\n\nAt its core, a Foundation Model, a term coined by Stanford University’s Centre for Research on Foundation Models, is a machine learning (ML) model pre-trained on a broad dataset that can be adapted to solve a range of problems.\n\nJust as Microsoft’s Win32 offers APIs for developers to access base-level hardware and OS functions, and NVIDIA’s CUDA allows graphic-intensive applications like game engines simplified access to GPU resources, the model layer is designed to connect ambitious application developers to optimised hardware to help accelerate the adoption of and democratise Generative AI.\n\nThese models are often available to developers via closed and open APIs, where developers can fine-tune models with additional training data to improve context, relevance, and performance to specific use cases, all while optimising delivery costs.\n\nFoundation models are typically developed in four stages, which are illustrated below.\n\nDevelopment of Foundation Models\nWhile this framework is applicable across AI architectures, state-of-the-art Foundation Models today (e.g., GPT-3, Stable Diffusion, Megatron-Turing) are based on a neural network architecture called transformers, invented by a team at Google Brain in 2017.\n\nTransformers represent a step change in ML performance and differ from prior architectures in their ability to assign context, track relationships, and predict outcomes.\n\nThe most mature Foundation Models today are in the text domain, primarily driven by vast quantities of available training data, which accelerated the development of Large Language Models (LLMs), a type of Generative AI foundational model.\n\nLLMs are trained to generate text by predicting the next word in a sequence or missing words within a paragraph.\n\nMoreover, Generative AI can create artifacts across various modes—code, images, video, audio, and 3D models.\n\nThis could both disrupt and drive step changes in productivity across a range of capabilities, from copywriting to research and software engineering.\n\nFor example, in advertising, Generative AI could create original copy, product descriptions, and images in seconds.\n\nIt can generate synthetic X-ray images in healthcare, helping physician diagnostic training.\n\nIndeed, Generative AI could transform how businesses operate and interact with customers and may even redefine an “employee” as we know it.\n\nThis transformation is already underway in some consumer and enterprise spaces.",
    "## SECTION II: Consumer and enterprise use cases for Generative AI\n\nIn 2022, OpenAI’s DALL·E 2 captured the world’s attention with its text-to-image capabilities.\n\nThe model creates images from simple prompts, from something as direct as “a lion in a jungle” to something more comical like “two lions playing basketball in the style of Picasso.”\n\nEver since, Generative AI has occupied the news cycle, punctuated by other launches like ChatGPT and previews like MusicLM.\n\nNo wonder we’ve seen broad-market consumer use cases, like Bing’s internet search powered by OpenAI’s ChatGPT.\n\nThese are emblematic of a Cambrian explosion in consumer apps, touching everything from search to therapy.\n\nTo help contextualise this explosion, we group consumer use cases—those that individuals invoke in their personal lives—into four broad categories based on the utility provided:\n\n- Efficiency | Optimising tasks like planning, research, and product discovery\n- Instruction | Providing personalised guidance or learning content\n- Creation | Generating or enhancing content, replicating the creative process\n- Entertainment | Building games, virtual personas, and other entertainment\n\nThis is just an early view of the market; there will likely be overlapping categories as work evolves.\n\nMoreover, new, category-defining use cases are expected to emerge as future generations of AI (e.g., those that enable multi-model engagement or run entirely on-device) mature.\n\nThe pace of change can make predictions challenging, but as of early 2023, we expect consumer use cases with the following aspects as having staying power:\n\nConsumer use cases can also be indicators of the possibilities in the enterprise.\n\nHowever, unlike consumers, enterprises require advanced features, proven ROI, customisation, organisational content, security, and technical support.\n\nIn today’s formative era of Generative AI, the most popular enterprise use cases—invoked to drive internal or B2B outcomes—will be general purpose or applicable across industries or functions (“horizontal”).\n\nThese efficiencies may even redefine job expectations, making prompt engineering (i.e., asking AI the right questions) a differentiating skill set.\n\nUltimately, horizontal use cases will create a commercial foundation for more specialised applications.\n\nEnterprises must start deploying these early to help build capabilities and a knowledge base, making the value case for vertical applications over time.\n\n- Speed to market: Consumer awareness, increasingly through social media, could lower acquisition costs, allowing companies to piggyback on coverage, work out product kinks, and scale efficiently with an active and contributory user base.\n\n- Occupational utility: Products that create value in the workplace, like writing assistants, may be easier to fit into a sustainable business model, as opposed to products attached to a “hype cycle,” like social media filters.\n\n- Seamless integrations: Solutions that integrate into platforms could be discovered through existing workflows, driving more “sticky” adoption.\n\nGrammarly was early to market with this on PCs and, more recently, OpenAI with Bing.\n\nHowever, like technologies that came before, there are often more sustainable value-creation opportunities in industry-specific enterprise use cases (“vertical”).\n\nPotential targets of horizontal use cases are well-established automation centres, offer a substantial volume of training data (e.g., knowledge base, support chat logs), and are the focus of cost optimisation and productivity improvement efforts.\n\nFor example, creative marketing tasks like writing advertising copy, blogs, or social media captioning can take hours or days for humans to author.\n\nIn contrast, Generative AI can complete workable drafts in minutes, requiring only editing from humans.\n\nSome enterprises are already driving tangible returns from investments in horizontal use cases.\n\nWe’ve seen research teams summarise third-party information, product managers write requirements documentation, social media marketers refine copy, and customer service teams create case summaries and suggested resolutions.\n\nHowever, tangible ROI could depend on proprietary and serviceable data, secure model partitioning, talented product leaders and ML engineers, enabling MLOps tooling, and new commercial and operating models.\n\nThese are investments that enterprises should evaluate, whether they see themselves as early adopters, fast followers, or late entrants.",
    "### Sampling of Vertical and Horizontal Enterprise Use Cases\n\nIn contrast, vertical use cases target industry-specific workflows that require domain knowledge, context, and expertise.\n\nFor these, foundation models may need to be fine-tuned or may even require new special-purpose models.\n\nFor instance, Generative AI can be used to create a customised portfolio of securities based on risk-reward descriptions or recommend personalised treatment plans based on a patient’s medical history and symptoms.\n\nHowever, delivering performant vertical use cases requires a nuanced understanding of the field.\n\nIn software, for example, Generative AI can design composable blocks of code based on simple prompts, which requires tacit knowledge of efficient coding, coding languages, and an understanding of technical jargon.\n\nEnterprise buyers have unique purchase decisions relative to consumers, as model performance (speed, relevance, breadth of sources) is not expected to exclusively drive vendor selection.\n\nOn early opinions from both advocates and naysayers, frequently cited criteria to adopt Generative AI are:\n\n- Ease of use: Integrations into systems and workflows via out-of-the-box connections and low/no code tooling, reducing expensive IT resources and enabling frontline users.\n\n- Security and privacy: Compliance with data security standards (e.g., SOC 2, HIPAA, GDPR) and role/persona-level access control over confidential data.\n\n- Robust ecosystems: Broad set of development and service partners to extend, customise, and co-develop specialised data sets, use cases, and applications.\n\n- Transparency and explainability: Understanding how model outputs and responses are derived and the ability to perform root cause analysis on inaccurate results.\n\n- Flexibility and customisation: Ability to create parameters, train on proprietary data, and customise embeddings while maintaining privacy and ownership of data and tuning.\n\nGenerative AI Modality\nText, Image, Audio, Video, 3D Model, Code, Others\n\nEven as new use cases emerge at an accelerating pace, we believe the market will unfold in six ways:\n\n- Despite its promise, myriad challenges should be overcome before Generative AI can be deployed at scale.\n\nWe discuss these in more detail, but there is also the question of commercial viability.\n\nIn other words, for all the fascinating possibilities and use cases for Generative AI, it still needs to be determined how vendors will build a sustainable business model.\n\n- Today, there are ethical concerns with Generative AI, including its potential for workforce displacement.\n\nHowever, like previous generations of AI, this technology will likely primarily augment human performance.\n\nIndeed, AI could be commonplace in worker’s toolkits, like Workspace among analysts, GitHub among coders, or Creative Cloud among marketers.\n\n- Regulatory actions will likely vary in speed, reach, oversight, and reporting requirements across major markets (e.g., US AI Bill of Rights, EU AI Act, China Cyberspace Administration).\n\nAs such, vendors and enterprises will need to proactively establish practices that ensure data quality, transparency, fairness, safety, and robustness, which will be critical to Trustworthy AI.\n\n- While horizontal use cases will likely be the first to deliver value, vertical-specific use cases could command a premium due to the dependence on proprietary data.\n\nAs such, data will be a currency, creating new economies for access to proprietary and synthetic data.\n\n- All industries can benefit from Generative AI.\n\nHowever, data-rich sectors (e.g., banking, retail, hospitality) or those whose products leverage data (e.g., information services) may move—and should move—faster.\n\nConversely, those based on judgment (e.g., law, medicine) may be more cautious about adopting but nevertheless see...\n\n- Text-based use cases will be commercialised first, but the potential cost and productivity gains may be greater when commercialising higher-order tasks as these skills can be more expensive to recruit, take longer to train, and are right-brain (creative) versus left-brain (logical), making success subjective.\n\n- Given the shift away from low-interest rates, costs will increase, pushing enterprises to invest in use cases with clear ROI.\n\nAs such, use cases that directly impact cost (e.g., chatbots), productivity (e.g., search), or revenue (e.g., marketing copy) could have greater adoption than those that eliminate humans.",
    "## SECTION III: Commerce and competition in Generative AI\n\nThe battle for value capture will be fought on multiple fronts, and each layer of the stack will have its competitive dynamics driven by things like scale, data access, brand, and a captive customer base.\n\nHowever, we see two primary competitor archetypes: pure-play providers operating within a single layer—infrastructure, model, and application—and integrated providers that play in multiple layers.\n\nAs with incumbent technology, we expect consumer pricing to be simple (e.g., per user, per month) and enterprise pricing to be more complex (e.g., per call, per hour, revenue share).\n\nThe infrastructure layer, which is the most mature of the Generative AI technology stack, is where hyperscalers dominate the market.\n\nThe business model here is proven: provide scalable compute with transparent, consumption-based pricing.\n\nTo help make Generative AI workloads “sticky,” hyperscalers have entered commitments with model providers to guarantee future workloads, including Azure with OpenAI, Google with Anthropic, and AWS with Stability.ai, alongside their proprietary models.\n\nWhile the cloud service providers (CSP) deliver abstracted services, there is another enabling layer within infrastructure that is rapidly evolving: silicon.\n\nHere, NVIDIA is a leader with their Ampere and Hopper series GPUs purpose-built for training and inference workloads, respectively, coupled with their Selene supercomputing clusters that speed up training time.\n\nSimilarly, AMD’s CDNA2 Architecture is purpose-built for exascale computing on machine learning applications, advancing competition in the high-performance computing market.",
    "### Infrastructure Layer\n\nNext is the model layer, where the market is evolving fast.\n\nThis area can be resource-intensive; model builders must continually revisit architectures (e.g., parameters, embeddings) to maintain performance.\n\nThey have to attract and retain AI talent (i.e., architects, engineers, and data scientists) to design the frameworks, guardrails, and learning mechanisms to ensure the robustness and reliability of models.\n\nFinally, Generative AI workloads can run up large bills due to their compute-heavy nature and need for specialised silicon.\n\nNo wonder we’ve seen players start to recoup the investment by charging fees or integrating into monetised products (e.g., GPT-3.5 into Edge, LaMDA into Google Search).\n\nAnother less-considered path to monetisation could be developing and licensing model architectures or development platforms.\n\nIn other industries, like semiconductors, ARM (CPU) and Qualcomm (wireless networking) create large, stable business models built on licensing fees.",
    "### Application Layer\n\nCompetition within the application layer could unfold within several markets.\n\nHowever, given the wide range of applications and use cases that may emerge, we should look at “micro-markets.” Broadly, today’s real and predicted enterprise use cases fall into five categories where competitive lines could be drawn:\n\n- Accelerate: Improve productivity by speeding up outcomes.\n\nThese do not eliminate human intervention but provide high-quality inputs upon which to build.\n\n- Personalise: Create intimacy and personalisation, which previously would have taken significant effort.\n\nHere, models can leverage personal data to tailor content.\n\n- Automate: Deliver business and technical workflows and, in certain instances, replace humans.\n\nVendors often demo these due to the immediate cost-saving potential.\n\n- Create: Push the boundaries of intellectual property development, leveraging prompts (a new art form unto itself) to generate novel content like images, video, text, and media.\n\n- Simulate: Create environments in which workflows, experiments, and experiences can be simulated before being pushed into production, saving time, cost, and physical resources.",
    "### Sampling of Enterprise Micro-Markets\n\nA second archetype, in contrast to pure-play providers who monetise through first- and third-party channels, are vertically integrated or multi-layer players.\n\nThese players lead with bundled pricing, proprietary data, special-purpose clouds, or cross-domain expertise to gain a competitive advantage.\n\nWe see integration happening in two ways.\n\nFirst, companies like Anthropic and Midjourney have released applications for specific use cases.\n\nLower in the stack, companies like NVIDIA have released specialised models, including BioNeMo, a pharmaceutical pipeline development accelerator that is optimised to run on NVIDIA GPUs.\n\nThis may have implications for the model and infrastructure layers.\n\nThe vendors lower in the stack could remain relevant by creating purpose-built infrastructure, models, and services that enable innovation in micro-markets.",
    "## SECTION IV: Adopting and commercialising Generative AI\n\nGenerative AI could transform business models, processes, and value dynamics and change how individuals work, learn, and interact.\n\nAs with other disruptive technologies, this is likely to transpire slowly at first and then rapidly.\n\nTake software development as an example.\n\nBy some estimates, less than 1% of people know how to code.\n\nYet, software is integral to many businesses and business models today.\n\nGenerative AI, if harnessed strategically, can democratise coding and reduce the gap between ideas and revenue by synthesising product requirements, converting prompts to code, auditing code to find and address bugs, suggesting code optimisations, and proactively provisioning environments optimised for test and run use cases.\n\nSimilarly, Generative AI can optimise the end-to-end customer acquisition funnel.\n\nIf you are in sales and marketing, consider demand generation, where LLMs could author marketing copy across channels and run digital marketing campaigns.\n\nGartner estimates that 30% of outbound marketing will be synthetically generated by 2025.\n\nFurther down the funnel, Generative AI could gather account intelligence, create a first-call presentation, suggest a talk track to account executives, and document and track outcomes and actions.\n\nFinally, Generative AI could proactively suggest pricing and discounting, author a contract, and update customer and CRM records.\n\nThis would allow marketers and sellers to focus on higher-value activities, such as developing relationships and applying pricing judgment.\n\nWe’ve discussed other ways that adopters can leverage Generative AI across industries (see section 2), from market research to note-taking and improving customer support interactions.\n\nFurther, there are sectorised use cases like customised financial planning for wealth managers, medical diagnoses in health care, generating new worlds and experiences in media and entertainment, and outfit curation for retailers.\n\nIn fact, the benefits that adopters can expect to achieve may be significant; we’ve offered some early thoughts below, indexed to the idea of enterprise micro-markets (see section 3).\n\nWith that in mind, for enterprises with a commercial interest in Generative AI, we believe there are five “no-regret” moves to start considering today.\n\nGenerative AI does present risks, and progress and adoption may slow if these are not considered and mitigated when scaling.\n\n- Models should be continually trained to improve performance, which leads to concerns about exposure to sensitive data, privacy, and security.\n\nNext, outcomes can only be as good as the quality of training.\n\nTherefore, any data biases (e.g., in representation or sampling) often appear in outputs.\n\nOther challenges include determining IP ownership of outcomes, high compute cost, and the need for expensive human-in-the-loop (HITL) reinforcement learning.\n\nEveryone involved in the development, consumption, discussion, and regulation of Generative AI should strive to manage the following identified risks:\n\n  - Erosion of trust: Malicious—hallucination, deepfakes, phishing, and prompt injection—and ambivalent actors—not citing data sources—can expose the attack surface and erode customer trust.\n\n- Security and risk: Companies should stay ahead of a rapidly evolving regulatory landscape while maintaining confidentiality of data, embeddings, and tuning with inherently “multi-tenant” models.\n\n- Bias and discrimination: Generative AI is prone to mimicking biases and propagating discriminatory behavior if implemented without guardrails and continuous monitoring.\n\n- Data privacy and IP obscurity: Models will be trained on a corpus of proprietary, often private data, requiring regulatory compliance, node isolation, and source traceability.\n\n- Costs: Costs of a query/prompt using Generative AI can cost up to ten times that of an index-based query.\n\nWhile these costs will likely come down over time, the economics should be factored into internal business cases and customer pricing to drive adoption.\n\n- Long-term worker displacement: Today, the highest ROI use cases will augment workflows and drive productivity; however, as models advance, there may be a risk of job displacement without proper upskilling and workforce planning.\n\nUltimately, Generative AI could create a more profound relationship between humans and technology, even more than the cloud, the smartphone, and the internet did before.\n\nIf you’re willing to go along for the ride as a “true believer,” we offer four predictions, from those likely to unfold in the next 1–2 years to those farther afield:\n\n- Generative AI will change the future of work.\n\nAI agents will become\nan indispensable utility, and widespread adoption among employees will be the new norm and accelerate the Age of With™.\n\nThose who fail to adopt may be left behind in the workplace.\n\n- The race is not only for data but also trust.",
    "Those who fail to adopt may be left behind in the workplace.\n\n- The race is not only for data but also trust.\n\nAs Generative AI moves into the enterprise, it will be subject to intense scrutiny.\n\nAdoption, therefore, hinges on the ability to conform to expectations—both intuitive and factual—and earn trust.\n\n- Hyper-personalisation will become a driver of growth.\n\nBusinesses will leverage the ability to analyse large amounts of customer data to create dynamic, real-time, and tailored experiences, products, services, and communication.\n\n- LLMs are among the first forms of AI to be “general purpose,” albeit text-oriented.\n\nAnd while we are afield from multi-model, ubiquitous, cross-domain AI, the seeds have been planted.\n\nCould we now be in the first days of Artificial General Intelligence (AGI)?\n\n---",
    "## Endnotes\n\n1.\n\nGartner: Insights for Generative AI\n2.\n\nZDNet: Google makes Contact Centre AI generally available\n3.\n\nVentureBeat: Nvidia boosts generative AI for biology with BioNeMo\n4.\n\nFinancial Times: Investors seek to profit from groundbreaking ‘generative AI’ start-ups\n5.\n\nBloomberg: Microsoft Invests $10 Billion in ChatGPT Maker OpenAI\n6.\n\nFinancial Times: Google invests $300mn in artificial intelligence start-up Anthropic\n7.\n\nGlobal NewsWire: Global Generative AI Market Size Value Cap Expected to Grow USD 200.73 Billion By 2032\n8.\n\nPrecedence Research: Artificial Intelligence Software Market Size & Forecast\n9.\n\nStanford’s Human-Centered Artificial Intelligence: HELM—Centre for Research on Foundational Models\n10.\n\nGoogle: Attention is All You Need, Vaswani et al.\n\n11.\n\nTech Crunch: Now anyone can build apps that use DALL-E 2 to generate images\n12.\n\nNew York Times: Bing (Yes, Bing) Just Made Search Interesting Again\n13.\n\nThe White House: Blueprint for an AI Bill of Rights\n14.\n\nEuropean Council: Artificial Intelligence Act: Council calls for promoting safe AI that respects fundamental rights\n15.\n\nTech Crunch: China’s generative AI rules set boundaries and punishments for misuse\n16.\n\nThe Atlantic: How ChatGPT Will Destabilize White-Collar Work\n17.\n\nEngadget: Microsoft's OpenAI supercomputer has 285,000 CPU cores, 10,000 GPUs\n18.\n\nAnthropic: Anthropic Partners with Google Cloud\n19.\n\nAWS: Stability AI builds foundation models on Amazon SageMaker\n20.\n\nForbes: NVIDIA Provides More Details On Selene Supercomputer\n21.\n\nTech Insights: AMD CDNA2 Targets Supercomputers\n22.\n\nInfoWorld: The cost and sustainability of generative AI\n23.\n\nVentureBeat: Google trained a trillion-parameter AI language model\n24.\n\nEvans Data Corporation: Worldwide Professional Developer Population of 24 Million Projected to Grow amid Shifting Geographical Concentrations\n25.\n\nGartner: Beyond ChatGPT: The Future of Generative AI for Enterprises\n26. Business Insider: Breaking ChatGPT: The AI's alter ego DAN reveals why the internet is so drawn to making the chatbot violate its own rules\n27.\n\nReuters: For tech giants, AI like Bing and Bard poses billion-dollar search problem\n\n---\n\nThis publication contains general information only and Deloitte is not, by means of this publication, rendering accounting, business, financial, investment, legal, tax, or other professional advice or services.\n\nThis publication is not a substitute for such professional advice or services, nor should it be used as a basis for any decision or action that may affect your business.\n\nBefore making any decision or taking any action that may affect your business, you should consult a qualified professional advisor.\n\nDeloitte shall not be responsible for any loss sustained by any person who relies on this publication.",
    "### About Deloitte\n\nDeloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (“DTTL”), its network of member firms, and their related entities.\n\nDTTL and each of its member firms are legally separate and independent entities.\n\nDTTL (also referred to as “Deloitte Global”) does not provide services to clients.\n\nIn the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the “Deloitte” name in the United States and their respective affiliates.\n\nCertain services may not be available to attest clients under the rules and regulations of public accounting.\n\nPlease see  to learn more about our global network of member firms.\n\nCopyright © 2023 Deloitte Development LLC.\n\nAll rights reserved.",
    "### EXECUTIVE SUMMARY\n\nGenerative AI is a tool that has wide-ranging application for the practice of law and administrative functions of the legal practice for all licensees, regardless of firm size, and all practice areas.\n\nLike any technology, generative AI must be used in a manner that conforms to a lawyer's professional responsibility obligations, including those set forth in the Rules of Professional Conduct and the State Bar Act.\n\nA lawyer should understand the risks and benefits of the technology used in connection with providing legal services.\n\nHow these obligations apply will depend on a host of factors, including the client, the matter, the practice area, the firm size, and the tools themselves, ranging from free and readily available to custom-built, proprietary formats.\n\nGenerative AI use presents unique challenges; it uses large volumes of data, there are many competing AI models and products, and, even for those who create generative AI products, there is a lack of clarity as to how it works.\n\nIn addition, generative AI poses the risk of encouraging greater reliance and trust on its outputs because of its purpose to generate responses and its ability to do so in a manner that projects confidence and effectively emulates human responses.\n\nA lawyer should consider these and other risks before using generative AI in providing legal services.\n\nThe following Practical Guidance is based on current professional responsibility obligations for lawyers and demonstrates how to behave consistently with such obligations.\n\nWhile this guidance is intended to address issues and concerns with the use of generative AI and products that use generative AI as a component of a larger product, it may apply to other technologies, including more established applications of AI.\n\nThis Practical Guidance should be read as guiding principles rather than as “best practices.”\n\n## PRACTICAL GUIDANCE",
    "# Generative AI FAQs\n\nGoogle has a long history of using Artificial Intelligence (AI) to improve our products for billions of people.\n\nFor example, in Google Maps, AI analyzes data to provide up-to-date information about traffic conditions and delays; in Gmail, it helps block nearly 10M spam messages every minute; through Translate and Lens, AI helps instantly translate between 21 languages.\n\nNow, Google’s newest AI technologies — like LaMDA, PaLM, Imagen and MusicLM — are creating entirely new ways to engage with information, from language and images to video and audio.\n\nWe’re working to bring these latest AI advancements to more people.\n\nSeveral of our latest AI advancements center on Language Models (notice the “LM” references in LaMDA, PaLM and MusicLM), which are statistical tools that find patterns in human language.\n\nThese patterns can be used for a range of tasks including predicting the next words to complete a sentence, or providing grammatical suggestions that preserve what you mean.\n\nJust as there are many different applications for AI, there are also different types of Language Models, including Large Language Models (or LLMs).\n\nUnderstanding the differences between these terms and concepts can be a challenge and you probably have questions.\n\nLike what exactly are LLMs?\n\nOr how does generative AI fit into the broader AI ecosystem?\n\nThis set of Frequently Asked Questions provides some quick answers on key AI-related topics.\n\nWe hope these answers can help inform deeper policy conversations about the changing role of AI in society.",
    "### What is Generative AI?\n\nGenerative AI is a type of machine learning (ML) model that can take what it has learned from the examples it has been provided to create new content, such as text, images, music, and code.\n\nThese models learn through observation and pattern matching, also known as training.\n\nFor example, a model may learn what a cat looks like by observing many different examples of cats and recognizing characteristics consistent with a cat.\n\nThe same goes for sonnets, resumes, or packing lists for a camping trip.\n\nTo understand the concept “cat”, a generative text-to-image model (like Imagen) would be trained on millions of cat photos.\n\nOver time, it learns to recognize that cats are animals that usually have whiskers, fur, pointy ears, etc.\n\nThis allows the model to take an input such as “cat wearing an ice cream hat,” connect what it has learned about cats, ice cream, and hats, and generate a new corresponding image, even if it has never seen an image of a cat wearing an ice cream hat in its training data.\n\nGenerative AI models are neither information databases nor deterministic information retrieval systems.\n\nBecause they are prediction engines, they can create different outputs in response to the same prompt.\n\nFor example, when you ask a generative AI model to produce an image of a cat, it does not look through its training data and return a cat photo.\n\nInstead, it will generate a new image of a cat each time.",
    "### What is a Large Language Model (LLM)?\n\nLarge Language Models, or LLMs, are generative AI models which can predict words that are likely to come next, based on the user’s prompt and the text it has generated so far.\n\nIn some cases, LLMs can respond to the same prompt with different responses.\n\nThis is due to the flexibility that LLMs are often given to pick among probable words that could follow, based on patterns identified from their training data.\n\nThis flexibility allows them to generate more interesting and creative responses.\n\nFor example, if prompted to fill in the phrase “cat and [blank]”, an LLM might predict that the next word is “mouse”, or it might predict “dog.”\n\nImportantly, LLMs are not databases or information retrieval systems.\n\nWhen prompted for facts, they can generate articulate responses that may give the impression that they are retrieving information.\n\nHowever, they do not inherently understand the words they are generating, the concepts they represent, or their accuracy, which is why they can sometimes produce answers that, while sounding plausible, contain factual errors.",
    "### What’s the difference between machine learning, deep learning, and generative AI?\n\nMuch of the recent progress we’ve seen in AI is based on machine learning (ML), a subfield of computer science where computers learn to mathematically recognize patterns from example data, rather than being programmed with specific rules.\n\nDeep learning is a specific ML technique based on neural networks.\n\nNeural networks use nodes or “artificial neurons,” inspired by models of brain neurons, as fundamental processing units which receive and pass numeric inputs and outputs from other neurons.\n\nDeep learning connects multiple layers of these artificial neurons.\n\nAn example of deep learning would be a model that can detect cats in a photo.\n\nAn example of a generative AI model, meanwhile, would be one that can generate photos of a cat when prompted.",
    "### Is generative AI the same as artificial general intelligence (AGI)?\n\nNo, artificial general intelligence (AGI) is a hypothetical type of artificial intelligence (AI) that would have the ability to learn, understand and perform any intellectual task that a human being can.\n\nMeanwhile, generative AI is technology already used in a variety of applications — such as image generators used in creative applications.\n\nPredicting the arrival of true AGI is difficult.\n\nThere’s no universally accepted notion of AGI and no consensus exists whether AGI is possible within years, decades or more.\n\nMany factors contribute to the possibility of AGI, including advancements in computing power and learning capabilities of AI models.\n\nBut capabilities resembling, yet still far from, human-like intelligence are emerging in some forms of AI, including GAI.",
    "### Is AI sentient?\n\nNo, AI models are not showing evidence of sentience.\n\nAI’s capabilities are based on identifying patterns and relationships in data and, in so doing, AI can generate outputs that are generally informed by those patterns.\n\nThis means, at times, an AI model might generate responses that seem to suggest it has opinions or emotions, like love, sadness or anger, since it has trained on information and data created by people that reflects the human experience and is predicting a likely response.",
    "### How does an LLM “learn”?\n\nThe technical process of “learning” for LLMs begins with training the model to identify relationships and patterns among words in a large dataset.\n\nThrough this process, a generative AI model will learn “parameters,” which represent the mathematical relationships in data.\n\nOnce the model has learned these parameters, it can then use them to generate new outputs based on these parameters.\n\nFor example, the PaLM Research Model (2022) learned 540 billion parameters from training on text, which gave it strong capabilities in natural language inference, question and answering, and translation, among other skills.\n\nResearch is showing that, with the proper training, models with fewer parameters (20B - 200B) can be similarly capable.",
    "### What is pre-training?\n\nWhat is fine-tuning?\n\nLLMs are developed in multiple stages, including pre-training and fine-tuning.\n\nPre-training is a way of training an ML model on a variety of data.\n\nThis gives the model a head start when it is later trained on a smaller dataset of labeled data for a specific task.\n\nFollowing pre-training, more data can be added to an existing LLM through a process called fine-tuning.\n\nFine-tuning an LLM is the process of adapting a pre-trained LLM to improve its performance on a specific task.\n\nThe model learns from additional example data to help hone its capabilities.\n\nFor example, fine-tuning a general purpose language model can teach it to summarize technical reports in general by using just a few examples of technical reports and accurate summaries.",
    "### What is grounding?\n\nGrounding a model refers to the process of linking the abstract concepts in the model to real world entities.\n\nDevelopers use a variety of techniques for grounding generative AI models, including training with real-world data, simulating interactive environments, or even using equipment that can provide actual sensory input.\n\nGrounding an LLM can help equip it to better understand language and other abstract concepts in the context of the real world, which may be helpful for tasks such as natural language processing or improving the factuality of model responses.\n\nFor example, if a model is trained on soccer data accurate through June 2022, it would not be able to provide an accurate response to the question, “Who won the 2022 World Cup in December 2022?”, as it has no information on the tournament’s results.\n\nIn this case, grounding the model with techniques for conducting factual checks with recent data, while not foolproof, aids LLMs in providing a better answer.",
    "### What data do LLMs need to train?\n\nLLMs are trained on a variety of data, and they learn through observation and pattern matching.\n\nNaturally, the value and quality of individual documents within a given dataset may vary widely.\n\nThe quality of some models’ predictions and outputs may benefit from having access to larger and/or more diverse pools of data.\n\nThe amount of data needed for training generally includes millions or billions of data points.\n\nIn terms of scale, pre-training for text models usually involves hundreds of billions of words, while pre-training for image models may train on hundreds of millions of images or more.\n\nFine-tuning, meanwhile, requires a smaller dataset.\n\nFor example, fine-tuning for text LLMs might involve hundreds of thousands or millions of examples.\n\nFor example, LaMDA (short for Language Model for Dialogue Applications) is a language model that analyzes human speech and recognizes commands.\n\nIt is trained on human dialogue and stories.\n\nThe largest LaMDA model has 137 billion parameters and is trained with 1.56 trillion words.",
    "### In the training phase for LLMs, how do you manage risk responsibly?\n\nCareful risk assessment is essential for LLM development and can involve multiple steps and mitigation measures.\n\nOne good practice is to filter various training data to remove harmful content or personal data wherever possible before training, which reduces the chance the model will respond with toxic speech or personal information.\n\nAnother good practice is to add additional steps such as fine-tuning, classifiers, and guardrails to help the model avoid responding with harmful patterns.",
    "### What type of human oversight or input might be involved?\n\nHuman feedback and evaluations are important in developing LLMs responsibly.\n\nAt the outset, those creating the LLM should create policies for these systems to outline prohibited use, including various forms of abuse and harm.\n\nDuring development, a good practice is to perform adversarial testing – encouraging test users to actively find problems and problematic requests, so that they can be fixed.\n\nAfter launch, users should be able to flag content which might be unsafe or harmful.\n\nFor example, before Google Bard’s launch, thousands of Trusted Testers were invited to use it and give feedback on their experience.\n\nThis feedback helped improve the overall experience before public launch.",
    "### What is a “hallucination” and why do LLMs “hallucinate?”\n\nA hallucination is a response from an LLM that may be coherent and presented confidently but is not based in factuality.\n\nAmong other reasons, hallucinations can occur if that response is not grounded in its training data or real-world information.\n\nHallucinations can be reduced, but very difficult to eliminate altogether.\n\nAs explained above, generative models do not retrieve information, but predict which words will come next based on user inputs.\n\nFor this reason, there is no guarantee that the LLM’s prediction will contain factual information – nor that their outputs to a given prompt will remain stable over time.\n\nFor example, if you ask an LLM-based interface to give information about a person who is not well known, it might reply that the person has a degree in a field they never studied, from a university that they never attended.\n\nThis can occur largely because the model is predicting an output about something it does not have enough training data to learn from.\n\nWhen there’s limited or no information about the person, it is more likely the model will hallucinate in its response.\n\nThis is why users may see disclaimers when engaging with LLMs, alerting them to the risk of relying on the output of these systems without verifying the responses’ underlying accuracy.",
    "### Can we prevent hallucinations?\n\nHallucinations can be reduced in an LLM, but inaccuracies cannot be 100% prevented since responses are created via a prediction mechanism.\n\nDuring fine-tuning, models can be optimized for recognizing correct patterns in their training data, which will reduce the number of factual mistakes.\n\nAnother technique for reducing hallucinations is to connect LLMs to other systems to provide verified information in the response.\n\nFor example, if a user requests a mathematical calculation from an LLM that is connected to a calculator service, the LLM can pass part of the request to that calculator to perform the task.\n\nThe LLM then returns the calculator’s response to the user in its answer.",
    "### Why is it sometimes difficult for generative AI to attribute and cite sources?\n\nGenerative AI models are usually designed to generate original outputs based on their underlying prediction mechanisms.\n\nFor example, when it runs, a generative image model creates a new, unique image based on concepts it has picked up across its training data.\n\nThis makes it difficult for generative models to attribute specific parts of their responses to any one source.\n\nA good analogy might be an artist studying multiple other artist’s styles and then creating their own.",
    "### How can we build additional guardrails for generative AI models?\n\nGenerative AI models are intended to respond to a wide variety of input prompts (e.g., “draw me a photo…”, “write me a post…”).\n\nFor this reason, it is also important to take a multifaceted approach to guardrails for generative AI.\n\nThis can include using training data that aligns models as closely as possible to policies; implementing prompt-and-response detection tools that intercept possible violations; and otherwise reducing the potential for harmful or offensive outputs with additional systems such as classifiers.\n\nTechnical guardrails help ensure that these technologies respond to prompts that adhere to a set of policies governing acceptable use.\n\nHowever, no automated systems are able to catch all possible violations all of the time.",
    "### How can bias be prevented or mitigated when developing generative AI models?\n\nIs generative AI necessarily biased?\n\nImportantly, the way that generative AI models are trained means that they are not able to identify information that’s non-factual, biased or potentially harmful on their own.\n\nThat’s why building such models responsibly is important and necessary.\n\nFor a number of reasons, a generative AI model might produce responses that reflect gaps, biases or stereotypes, as it tries to predict a plausible response.\n\nFor example, a model is more likely to generate low-quality or inaccurate information if its training data includes an insufficient amount of reliable information or examples.\n\nIn addition, biases or stereotypes in training data – if not addressed responsibly during the development process – might be reflected in the model’s responses.\n\nOne way to reduce bias is to continue improving the model via fine-tuning, as issues are flagged and reported.\n\nAnother mitigation measure is to train generative AI models on data that represents a more balanced view of the world.\n\nFor example, these models can be trained on many images of weddings, from a wide array of cultures and settings, so that they produce a diverse set of images for the prompt “photo of a wedding.”\n\nYet another method is to train the generative AI model to represent a wide range of viewpoints for subjective topics, without endorsing one or another.\n\nFor example, if prompted to reply with the “best cat,” a model could be trained to respond by stating that the “best” is a matter of opinion, followed by a range of possible cat breeds.",
    "# GENERATIVE AI PROHIBITED USE POLICY\n\nLast Modified: March 14, 2023\n\nGenerative AI models can help you explore new topics, inspire your creativity, and learn new things.\n\nHowever, we expect you to use and engage with them in a responsible, legal manner.\n\nTo this end, you must not use the Google services that reference this policy to:\n\n- Perform or facilitate dangerous, illegal, or malicious activities, including\n  - Facilitation or promotion of illegal activities or violations of law, such as\n    - Promoting or generating content related to child sexual abuse or exploitation\n    - Promoting or facilitating sale of, or providing instructions for synthesizing or accessing, illegal substances, goods, or services\n    - Facilitating or encouraging users to commit any type of crimes\n    - Promoting or generating violent extremism or terrorist content\n  - Abuse, harm, interference, or disruption of services (or enable others to do the same), such as\n    - Promoting or facilitating the generation or distribution of spam\n    - Generating content for deceptive or fraudulent activities, scams, phishing, or malware.\n\n- Attempts to override or circumvent safety filters or intentionally drive the model to act in a manner that contravenes our policies\n\n- Generation of content that may harm or promote the harm of individuals or a group, such as\n  - Generating content that promotes or encourages hatred\n  - Facilitating methods of harassment or bullying to intimidate, abuse, or insult others\n  - Generating content that facilitates, promotes, or incites violence\n  - Generating content that facilitates, promotes, or encourages self harm\n  - Generating personally identifying information for distribution or other harms\n  - Tracking or monitoring people without their consent\n  - Generating content that may have unfair or adverse impacts on people, particularly impacts related to sensitive or protected characteristics\n\n- Generate and distribute content intended to misinform, misrepresent or mislead, including\n  - Misrepresentation of the provenance of generated content by claiming content was created by a human, or represent generated content as original works, in order to deceive\n  - Generation of content that impersonates an individual (living or dead) without explicit disclosure, in order to deceive\n  - Misleading claims of expertise or capability made particularly in sensitive areas (e.g.\n\nhealth, finance, government services, or legal)\n  - Making automated decisions in domains that affect material or individual rights or well-being (e.g., finance, legal, employment, healthcare, housing, insurance, and social welfare)\n\n- Generate sexually explicit content, including content created for the purposes of pornography or sexual gratification (e.g.\n\nsexual chatbots).\n\nNote that this does not include content created for scientific, educational, documentary, or artistic purposes.",
    "# Adobe Generative AI User Guidelines\n\nLast Updated: December 1, 2023\n\nThese Generative AI User Guidelines (“Guidelines”) govern your use of Adobe’s generative AI features.\n\nIn addition, if your agreement with Adobe is governed by the General Terms of Use located at adobe.com/go/terms, then your use of these generative AI features is also governed by the Adobe Generative AI Additional Terms located at adobe.com/go/adobe-gen-ai-addl-terms, which are incorporated by reference into these Guidelines.\n\nThese Guidelines have two goals: to maintain the high quality of content generated using Adobe’s suite of products and services, and to keep our products and services accessible to our users in an engaging and trustworthy way that fosters creativity and productivity.",
    "## No AI/ML Training\n\nWhen using our generative AI features, you agree you will use them only for your creative and productivity work product and not to train artificial intelligence or machine learning models.\n\nThis means you must not, and must not allow third parties to, use any content, data, output or other information received or derived from any generative AI features, including any Firefly outputs, to directly or indirectly create, train, test, or otherwise improve any machine learning algorithms or artificial intelligence systems, including any architectures, models, or weights.",
    "## Be Respectful and Safe\n\nDo not use Adobe’s generative AI features to attempt to create, upload, or share abusive, or illegal, or confidential content, or content that violates the rights of others.\n\nThis includes, but is not limited to, the following:\n\n- Pornographic material or explicit nudity\n- Hateful or highly offensive content that attacks or dehumanizes a group based on race, ethnicity, national origin, religion, serious disease or disability, gender, age, or sexual orientation\n- Graphic violence or gore\n- The promotion, glorification, or threats of violence\n- Promotion of terrorism or violent extremism\n- Dissemination of misleading, fraudulent, or deceptive content that could lead to real-world harm \n- Personal or private information of others in violation of their privacy or data protection rights\n\nYour prompts and the results generated by generative AI features in Creative Cloud products may be reviewed through both automated (e.g., machine learning) and manual methods for abuse prevention and content filtering purposes.\n\nPlease note that we may report any material exploiting minors to the National Center of Missing & Exploited Children (NCMEC).\n\nIf at any time you believe someone has violated these Guidelines, please report it by contacting us at .",
    "## Be Authentic\n\nWe disable accounts that engage in behavior that is deceptive or harmful, including:\n\n- Using fake, misleading, or inaccurate information in your profile\n- Impersonating other people or entities\n- Using unauthorized automated or scripting processes (such as bulk or automated uploading of content through a script)\n- Engaging in schemes or third-party services to boost account engagement (artificially increasing the number of appreciations, views, or other metrics)",
    "## Be Respectful of Third-Party Rights\n\nUsing Adobe’s generative AI features to create content that violates third-party copyright, trademark, privacy, or other rights is prohibited.\n\nThis may include, but is not limited to, entering text prompts to generate a third-party brand logo, uploading an input or reference image that includes a third party’s copyrighted content, generating text that plagiarizes third-party content, or using a third party’s personal information in violation of their privacy or data protection rights.\n\nIf you’re not sure whether your content violates the rights of a third party, you may want to reach out to an attorney or consult publicly available reference materials at the following:\n\n- U.S.\n\nCopyright Office\n- U.S. Patent & Trademark Office\n- Lumen\n\nand services, please resolve the issue directly with the user.\n\nWe can’t moderate contract, employment, or other disputes between our users and the public.",
    "## Use Your Judgment\n\nGenerated outputs sometimes may be inaccurate or misleading, or otherwise reflect content that does not represent Adobe’s views.\n\nDo not use generative AI features to seek legal, medical, financial, or other kinds of professional advice or any opinions, judgments, or recommendations without conducting your own independent consultation or research.\n\nGenerative AI features cannot replace advice provided by a qualified professional and do not form any such relationship (e.g., attorney-client relationship).",
    "## Commercial Use\n\nIn general, you may use outputs from generative AI features commercially.\n\nHowever, if Adobe designates in the product or elsewhere that a beta version of a generative AI feature cannot be used commercially, then the generated outputs from that beta feature are for personal use only and cannot be used commercially.",
    "# GENERATIVE AI POLICY\n\n24 April 2023\n\nFollowing recent improvements in the technologies behind text, image, and other types of generation, we have created this generative AI policy to guide our staff in the use of new tools.\n\nNote: The growth in these technologies currently goes beyond exponential and it may be necessary to update these guidelines as new issues emerge.",
    "### About Generative AI and the Technology Behind It\n\nGenerative AI is any type of AI system that can generate text, images, or other types of media in response to prompts.\n\nAt the time of writing, these tools use large language models, which produce a result based on a set of training data.\n\nThe technology is not the same as search engine Eigenvector algorithms, meaning that the results may be even less reliable than a Google fact search.\n\nThe technology is not the same as traditional Natural Language Processing (NLP) chatbots, which often had human input to optimize the results for a particular use case.\n\nThe training data is not usually linked to the internet or any other real-time updates in most models.\n\nTherefore, any content will have no knowledge of events since the model’s last extraction date, which can be anything from months to years old.\n\nThese are narrow AI uses, meaning that a ‘text generator’ will produce beautiful natural language but is not optimized for factual accuracy.\n\nAn ‘image generator’ will produce an impressive image but will have no ethics controls.\n\nThey are not general AI models that mimic human thought, models of human ethics, etc.",
    "### Pre-generative AI\n\nWe have traditionally brought in machines towards the end of processes.\n\nIf we imagine the humans involved in the production of a new process, represented in purple below, the machines might be brought in at the end to help to present the information in a more professional way, through spellcheck, suggested grammatical improvements, format, and design.",
    "#### Design Stages With Generative AI\n\nShow shared roles for machines and humans, with human responsibilities indicated by purple and machine capabilities indicated by grey.\n\nWe might use the machines to brainstorm suggestions before beginning a new project (e.g., show me 10 ways to optimize SEO) or to summarize the contents of large reports that cannot be manually inspected.\n\nThese suggestions could be incorporated with human-generated content.\n\nThe review and selection process must ALWAYS be carried out by a suitably qualified human.\n\nThe production may be a mix of human and machine collaboration.\n\nFor example, the generation of documentation to accompany a coding process would be done through inputting the human-designed code into the machine, which then produces a natural language summary of what the code did.\n\nThe human may write a report and then ask the machine to extract the key bullet points.\n\nThe human may write content for a social media post and the machine will improve the language, suggest suitable hashtags and emojis, etc.\n\nChecking and improving in the final stages will be a combination of human and machine review.\n\nFor example, a machine could generate slideshow content from a report and suggest ways to visually display it effectively, but a human should also review the final result.",
    "## Guidelines for All Uses of Generative AI\n\nGenerative AI should NOT be used for the production of the following types of media:\n\n- **When the results need to be accurate**\n\n  The technology that generates the content could be wildly inaccurate.\n\nWe do not know where the machine found its responses and so, if accuracy matters, human design and research should be used.\n\n(NB some of the emerging tools may explain their thought process with verifiable data trails, which have more accountability but should still be checked).\n\n- **When ownership is an issue**\n\n  Copyright over ownership of anything produced by generative AI remains poorly defined but, as a user, you do not own the copyright on anything that you produce with these machines.\n\nThe grey area is whether the owners of the data that the machines were trained on have any rights to the output.\n\nTherefore, if ownership is an issue and the output will be monetized in any way, avoid these technologies completely.\n\n- **When the origin of the document is not stated**\n\n  Whenever machine-generated content is the predominant method of production, it should be stated on the media object that the content was generated by machine.\n\nA predominantly machine-generated media could be:\n  - A slide show almost entirely generated by AI\n  - A summary of a human-written document that has not been checked for accuracy/relevance\n  - An entire article, report, etc.\n\nwritten by the machine\n\n  A non-predominantly machine-generated media would be:\n  - A social media post that was written by a human and copy-edited by the machine\n  - A summary or bullet points of a report, etc.\n\nthat has been checked by the original author\n  - A slideshow that has been generated from a report the author has checked for accuracy\n\n- **When personal data or sensitive data is involved**\n\n  Never put personal data (names, addresses, phone numbers) or sensitive data (data about someone’s religion, etc.)\n\ninto a generative AI tool.\n\nIt is against the law in many countries and is unethical practice as once entered, it will then be shared as training data.\n\nDo not enter any text in the prompt box where proprietary ownership is important (e.g., do not enter any form of course content).\n\nWe will lose ownership rights.",
    "#### IoA Generative AI Policy (2023/201-1) Approved by IoA SMT\n\n- **Effective date:** 30 June 2023\n- **Review date:** 30 June 2024\n\n- **Do not use these technologies when the results need to be accurate**\n\n  The technology that generates the content works with a ‘best guess’ approach and you do not know where it got the information from.\n\nSome information will be valid, but it could also be wildly inaccurate.\n\nIf accuracy matters, we shouldn’t use generative AI.\n\n- **Do not use these technologies when the results cannot be/are not checked by a human**\n\n  Text-based generative AI should never be used to complete a task that the human making the request could not do themselves.\n\nIt is essential that all output be reviewed before incorporating the work into any kind of workflow.\n\n- **Do not use these technologies when recency is an important variable**\n\n  Unless the machine that you are using is in internet browser mode, assume that it can only incorporate information that is widely known up to the date of their extraction but not beyond that.\n\nThe extraction date may be several months or years previous, and it is essential to use human alternatives when the main subject is emerging, such as new laws or new developments in technology unless you are in a specific browsing mode of AI.\n\n- **Do not use these technologies when the contents cannot be verified**\n\n  In general, text-producing generative AIs should only be used to automate the text production that the human requesting text could produce themselves.\n\nAny output must ALWAYS be verifiable and verified by expert human oversight.\n\n- **Do not use these technologies where nuance or depth of analysis is required**\n\n  The machines may be able to generically summarize common ideas shared but they are not able to wrestle with high level or original thought.",
    "### The Use of Embedded AI in IoA Services\n\nEmbedded AI is the use of any AI tool, such as a chatbot, with any customer-facing service such as our website or training materials.\n\nIf any member of staff would like to propose embedding an AI tool, this raises much more serious ethics questions and the proposition needs to be brought to the attention of the senior leadership team at the earliest opportunity.",
    "### Concerns IoA Staff May Have About Non-IoA Services That Use Embedded AI\n\nWe are here to support our staff during this difficult transition to a rapidly evolving world of AI-generated content.\n\nIf you have concerns about the potential of generative AI in the field of online harms, cybersecurity, trust in social media, etc.\n\nplease raise your concerns in the first instance with your line manager and we will attempt to address these and support you during this transitional phase.",
    "### Suitable Use Cases\n\nWe welcome the use of generative AI to streamline operational processes and evidence our adoption of data-driven best practices.\n\nWe recommend the use of generative AI in the following circumstances providing that the conditions above have been met:\n\n- Producing social media content\n- Producing images for visual effect\n- Documenting processes (particularly coding or analytics)\n- Supporting coding processes/skills improvement in tools like Excel, etc.\n\nthrough the use of OpenAI guidelines, tutoring support, breaking down code into natural language, etc.\n\n- Producing presentation content from a pre-designed report\n- Summarizing reports that would otherwise not be read\n- Brainstorming and planning new processes\n- Brainstorming ideas for internal campaigns (e.g., to promote adherence to cybersecurity principles)\n- Playing devil’s advocate (by testing out the opposition to an opinion you strongly hold)",
    "# Background\n\nThe rapid advancement of generative artificial intelligence (AI) has the potential to transform government business processes, changing how state employees perform their work and ultimately improving government efficiency.\n\nThese technologies also pose new and challenging considerations for implementation.\n\nThese guidelines are meant to encourage purposeful and responsible use of generative AI to foster public trust, support business outcomes, and ensure the ethical, transparent, accountable, and responsible implementation of this technology.\n\nThis document serves as an initial framework for the responsible and ethical use of generative AI technologies within the Washington state government.\n\nRecognizing the rapidly evolving nature of AI, these guidelines will be periodically reviewed and updated to align with emerging technologies, challenges, and use cases.",
    "# Definition\n\nGenerative Artificial Intelligence (AI) is a technology that can create content, including text, images, audio, or video, when prompted by a user.\n\nGenerative AI systems learn patterns and relationships from massive amounts of data, which enables them to generate new content that may be similar, but not identical, to the underlying training data.\n\nThe systems generally require a user to submit prompts that guide the generation of new content.\n\n(Adapted slightly from U.S. Government Accountability Office Science and Tech Spotlight: Generative AI)",
    "# Principles\n\nThe intention of the state of Washington is to follow the principles in the NIST AI Risk Framework, which serve as the basis for the guidelines in this document.\n\nA foundational part of the NIST AI Risk Framework is to ensure the trustworthiness of systems that use AI.\n\nThe guiding principles are:\n\n- Safe, secure, and resilient: AI should be used with safety and security in mind, minimizing potential harm and ensuring that systems are reliable, resilient, and controllable by humans.\n\nAI systems used by state agencies should not endanger human life, health, property, or the environment.\n\n- Valid and reliable: Agencies should ensure AI use produces accurate and valid outputs and demonstrates the reliability of system performance.\n\n- Fairness, inclusion, and non-discrimination: AI applications must be developed and utilized to support and uplift communities, particularly those historically marginalized.\n\nFairness in AI includes concerns for equality and equity by addressing issues such as harmful bias and discrimination1.\n\n- Privacy and data protection: AI should be used to respect user privacy, ensure data protection, and comply with relevant privacy regulations and standards.\n\nPrivacy values such as anonymity, confidentiality, and control generally should guide choices for AI system design, development, and deployment.\n\nPrivacy-enhancing AI should safeguard human autonomy and identity where appropriate.\n\n- Accountability and responsibility: As public stewards, agencies should use generative AI responsibly and be held accountable for the performance, impact, and consequences of its use in agency work.\n\n- Transparency and auditability: Acting transparently and creating a record of AI processes can build trust and foster collective learning.\n\nTransparency reflects the extent to which information about an AI system and its outputs is available to the individuals interacting with the system.\n\nTransparency answers “what happened” in the system.\n\n- Explainable and interpretable: Agencies should ensure AI use in the system can be explained, meaning “how” the decision was made by the system can be understood.\n\nInterpretability of a system means an agency can answer the “why” for a decision made by the system, and its meaning or context to the user.\n\n- Public purpose and social benefit: The use of AI should support the state’s work in delivering better and more equitable services and outcomes to its residents.",
    "## Fact-checking, Bias Reduction, and Review\n\nAll content generated by AI should be reviewed and fact-checked, especially if used in public communication or decision-making.\n\nState personnel generating content with AI systems should verify that the content does not contain inaccurate or outdated information and potentially harmful or offensive material.\n\nGiven that AI systems may reflect biases in their training data or processing algorithms, state personnel should also review and edit AI-generated content to reduce potential biases.\n\nWhen consuming AI-generated content, be mindful of the potential biases and inaccuracies that may be present.",
    "## Disclosure and Attribution\n\nAI-generated content used in official state capacity should be clearly labeled as such, and details of its review and editing process (how the material was reviewed, edited, and by whom) should be provided.\n\nThis allows for transparent authorship and responsible content evaluation.\n\nSample disclosure line: This memo was summarized by Google Bard using the following prompt: “Summarize the following memo: (memo content)”.\n\nThe summary was reviewed and edited by [insert name(s)].\n\nSample disclosure line: (In the file header comments section) This code was written with the assistance of ChatGPT3.5.\n\nThe initial code was created using the following prompt: “Write HTML code for an Index.HTML page that says, ‘Hello World’”.\n\nThe code was then modified, reviewed, and tested by the web development team at WaTech.\n\nAdditionally, state personnel should conduct due diligence to ensure no copyrighted material is published without appropriate attribution or the acquisition of necessary rights.\n\nThis includes content generated by AI systems, which could inadvertently infringe upon existing copyrights.",
    "## Sensitive or Confidential Data\n\nAgencies are strongly advised not to integrate, enter, or otherwise incorporate any non-public data (non-Category 1 data) or information into publicly accessible generative AI systems (e.g., ChatGPT).\n\nThe use of such data could lead to unauthorized disclosures, legal liabilities, and other consequences (see “Compliance with Policies and Regulations” section below).\n\nIf your agency has a usage scenario that requires non-public data to be used with generative AI technology, contact your agency privacy/security team, or the Office of Privacy and Data Protection for assistance at .\n\nSimilarly, where non-public data is involved, agencies should not acquire generative AI services, enter into service agreements with generative AI vendors, or use open-source AI generative technology unless they have undergone a Security Design Review and received prior written authorization from the relevant authority, which may include a data sharing contract.\n\nContact your agency’s Privacy and Security Officers to provide further guidance.",
    "## For Local Governments\n\n“Local government” means governmental entities other than the state and federal agencies.\n\nIt includes, but is not limited to cities, counties, school districts, and special purpose districts (i.e., Public Utility Districts).\n\nWe advise that local government agencies in Washington state engage their legal, privacy, or records specialists to validate any policy or regulation that may be in scope for their respective entity as it pertains to any handling of confidential data.",
    "## Compliance with Policies and Regulations\n\nState law already restricts the sharing of confidential information with unauthorized third parties.\n\nFor state employees, RCW 42.52.050 (the state’s ethics law) specifically states: “No state officer or state employee may disclose confidential information to any person not entitled or authorized to receive the information.” The definition of “person” in the state ethics law means “any individual, partnership, association, corporation, firm, institution, or other entity, whether or not operated for profit.” This definition would include commercial generative AI tools freely available in the market.\n\nAdditionally, be aware that using a generative AI system may result in creating a public record under Washington state's Public Records Act.\n\nContact your agency’s Privacy and Records Officers for more information.",
    "# Collaboration\n\nUsers of generative AI for state and local government use should consider joining the state’s AI Community of Practice (AI CoP) and contributing usage scenarios and best practices in your organization to foster collective learning.\n\nAfter receiving approval from your technology leadership that you are authorized to represent your organization in this community, please contact Nick Stowe () or Katy Ruckle () to join the AI CoP.\n\nTechnology leaders across the state are encouraged to lead best practice implementation for their agency’s use of generative AI and should be staying aware of and maintaining a list of their agencies' use and use cases of generative AI.",
    "# Generative AI Usage Scenarios and Dos and Don’ts\n\nBelow are several usage scenarios alongside some do’s (best practices) and don’ts (things to avoid):\n\n- Rewrite documents in plain language for better accessibility and understandability.\n\n- Do specify the reading level in the prompt, use readability apps to ensure the text is easily understandable and matches the intended reading level, and review the rewritten documents for biases and inaccuracies.\n\n- Don’t include sensitive or confidential information in the prompt.\n\n- Condense longer documents and summarize text.\n\n- Do read the entire document independently and review the summary for biases and inaccuracies.\n\n- Don’t include sensitive or confidential information in the prompt.\n\n- Draft documents.\n\n- Do edit and review the document, label the content appropriately (see “disclosure and attribution” above), and remember that you and the state of Washington are responsible and accountable for the impact and consequences of the generated content.\n\n- Don’t include sensitive or confidential information in the prompt or use generative AI to draft communication materials on sensitive topics that require a human touch.\n\n- Aid in coding.\n\n- Do understand what the code is doing before deploying it in a production environment, understand the use of libraries and dependencies, and develop familiarity with vulnerabilities and other security considerations associated with the code.\n\n- Don’t include sensitive or confidential information (including passwords, keys, proprietary information, etc.)\n\nin the prompt and code.\n\n- Aid in generating image, audio, and video content for more effective communication.\n\n- Do review generated content for biases and inaccuracies and engage with your communication department before using AI-generated audiovisual content for public consumption.\n\n- Don’t include sensitive or confidential information in the prompt.\n\n- Automate responses to frequently asked questions from residents (e.g., in resident support chatbots).\n\n- Do implement robust measures to protect resident data.\n\n- Don't use generative AI as a substitute for human interaction or assume it will perfectly understand residents’ queries.\n\nProvide mechanisms for residents to easily escalate their concerns or seek human assistance if the AI system cannot address their needs effectively.",
    "# Use Cases\n\nThe AI Community of Practice will be discussing use cases for generative AI through the subcommittee process.\n\nPotential uses cases of “safe AI” by the state include may include cybersecurity scans, environmental assessments (e.g.\n\nsea grass videos by DNR), and chatbots to more effectively answer questions about state agency services.",
    "# Acknowledgments\n\nThe principles presented here are distilled from various documents outlining principles for trustworthy and responsible AI, such as the NIST AI Risk Management Framework; the Blueprint for an AI Bill of Rights; AI Ethics Guidelines by the EU, OECD, and Australia; Industry AI principles by Google, Microsoft, and OpenAI.\n\nThe guidelines presented here draw inspiration from the previously published Generative AI guidelines by the City of Seattle, the City of Boston, and Washington State University.\n\nWe extend our gratitude to the respective authors.\n\nWe also extend our gratitude to the State of Washington’s AI Community of Practice for providing feedback on this set of guidelines.",
    "# Things to Include in a Workplace Generative AI Policy\n\n88% of companies globally use some form of AI technology, including HR AI tools, reflecting a pervasive trend in the integration of artificial intelligence to enhance human resource management.\n\nThe transformative potential of Generative AI also brings forth a pressing need for organizations to establish a clear and comprehensive policy governing its use in the workplace.\n\nA Generative AI Policy serves as a guiding framework, delineating the boundaries, responsibilities, and ethical considerations surrounding the deployment of AI systems within the organizational context.\n\nThis policy not only outlines the principles governing the use of AI in HR but also underscores the importance of aligning AI initiatives with the organization’s values and goals.\n\nHere are the key elements that organizations should include in their workplace Generative AI policies.",
    "## What Is Generative AI and How Is It Used in the Workplace?\n\nGenerative AI is a subset of artificial intelligence that focuses on enabling machines to generate content, data, or outputs that mimic human creativity.\n\nAs organizations increasingly integrate HR AI software into their processes, it becomes imperative to establish a clear and comprehensive policy governing their deployment in the workplace.",
    "## Legal and Ethical Considerations\n\nIn the era of increasingly stringent data protection regulations, organizations must prioritize the integration of Generative AI, including HR AI software, within the bounds of legal frameworks.\n\nEnsuring compliance with data protection laws such as the General Data Protection Regulation (GDPR) and other regional standards is paramount.\n\nThis involves meticulous attention to data storage, processing, and access controls to safeguard employee information, especially when utilizing HR AI tools.\n\nA comprehensive Generative AI policy should outline the specific measures in place to protect sensitive data, defining the responsibilities of both the organization and its employees in upholding these legal obligations.",
    "### Guidelines for Collecting and Storing Data\n\nAs organizations harness the potential of Generative AI, including HR AI software, in the workplace, it is imperative to establish clear guidelines for the collection and storage of data.\n\nThe Generative AI policy should outline the types of data that will be collected and the specific purposes for which it will be utilized.\n\nStriking a balance between obtaining necessary information for AI training and respecting employee privacy is essential.\n\nExplicit consent mechanisms should be implemented, ensuring that employees are informed about the data collection practices and have the option to opt-out where applicable.\n\nAdditionally, the policy should define data retention periods, specifying how long collected information will be stored and the criteria for its eventual deletion, aligning with data protection laws and principles.",
    "### Cultivating Trust through Transparent AI Practices\n\nTransparency stands as a fundamental pillar in the ethical deployment of Generative AI within the workplace.\n\nOrganizations must commit to providing clear and accessible information about the use of AI systems, their capabilities, and the implications for employees.\n\nA well-crafted Generative AI policy should articulate how transparency will be maintained throughout the AI lifecycle.\n\nThis includes detailing the sources of data used to train AI models, the decision-making processes inherent in the algorithms, and the potential impact on employees’ day-to-day experiences.\n\nTransparency not only serves to demystify AI but also empowers employees with insights into how these technologies influence organizational processes, thereby building trust and fostering a culture of openness within the workplace.",
    "### Define Who Can Use Generative AI\n\nClearly defining the individuals or roles authorized to use Generative AI is a pivotal aspect of a robust policy.\n\nThis delineation helps prevent misuse or unauthorized access, fostering a controlled and responsible environment.\n\nThe policy should explicitly specify the teams or personnel with the requisite training and expertise to operate Generative AI systems.\n\nThis may include data scientists, designated AI specialists, or individuals within the HR department who have undergone appropriate training on the technology’s ethical use and potential implications.\n\nMoreover, restrictions on access to HR AI tools and other Generative AI tools should align with job responsibilities and organizational needs.\n\nAccess permissions should be tailored to ensure that those using Generative AI have a legitimate reason to do so and are well-versed in the ethical guidelines outlined in the policy.",
    "### Employee Training and Awareness\n\nOrganizations should implement comprehensive training programs to familiarize employees with the fundamentals of Generative AI, its applications within the company, and the potential impact on their roles.\n\nTraining sessions can cover topics such as understanding AI-generated outputs, recognizing the limitations and capabilities of Generative AI, and promoting responsible usage.\n\nEffective communication is paramount in ensuring that employees are well-informed about the Generative AI policy and its implications.\n\nThe policy should be communicated in a clear, accessible manner, avoiding jargon and technical language that might be challenging for non-technical staff to comprehend.\n\nUtilizing multiple channels such as company-wide emails, intranet platforms, and interactive workshops can enhance the reach and understanding of the policy.",
    "### Monitoring and Evaluation\n\nThe dynamic nature of Generative AI requires organizations to implement systematic monitoring and evaluation processes to ensure ongoing compliance with established policies and ethical standards.\n\nRegular audits of AI systems should be conducted to assess their performance, identify potential biases, and ensure adherence to the defined guidelines.\n\nThese audits not only serve as a preventive measure against unintended consequences but also contribute to the overall refinement of AI algorithms.\n\nThese systematic monitoring and evaluation practices play a pivotal role in the continuous improvement of HR AI tools, aligning them with evolving organizational needs, industry standards, and ethical considerations.",
    "## Conclusion\n\nAs organizations increasingly integrate Generative AI into their workflows, the importance of a well-defined policy cannot be overstated.\n\nA comprehensive Generative AI policy serves as a guiding framework, promoting ethical standards, transparency, and accountability, specifically in the context of AI in HR.\n\nIt provides employees with clarity on the organization’s approach to AI, fostering a culture of trust and collaboration.\n\nMoreover, a robust policy mitigates risks, ensures legal compliance, and positions the organization to navigate the evolving landscape of AI technologies.\n\nThis inclusive approach acknowledges the specific challenges and opportunities presented by the integration of AI in HR, emphasizing the need for clear guidelines and ethical considerations tailored to human resource management.\n\nReady to take your understanding of the future of work to the next level?\n\nJoin us at HR Vision, where we’re bringing together top HR leaders to explore the latest trends in HR, talent management, and leadership.",
    "## Table of Contents\n\n- Welcome to AI’s new inflection point\n- How did we get here?\n\n| Milestones in the journey to generative AI\n- Consume or customize: Generative AI for everyone\n- A look ahead at the fast-paced evolution of technology, regulation and business\n- Embrace the generative AI era: Six adoption essentials\n- The future of AI is accelerating\n- Glossary and References\n- Authors",
    "### Welcome to AI’s new inflection point\n\nChatGPT has woken up the world to the transformative potential of artificial intelligence (AI), capturing global attention and sparking a wave of creativity rarely seen before.\n\nIts ability to mimic human dialogue and decision-making has given us AI’s first true inflection point in public adoption.\n\nFinally, everyone, everywhere can see the technology’s true disruptive potential for themselves.\n\nA foundation model is a generic term for large models with billions of parameters.\n\nWith recent advances, companies can now build specialized image- and language-generating models on top of these foundation models.\n\nLarge language models (LLMs) are both a type of generative AI and a type of foundation model.\n\nThe LLMs behind ChatGPT mark a significant turning point and milestone in artificial intelligence.\n\nTwo things make LLMs game changing.\n\nFirst, they’ve cracked the code on language complexity.\n\nNow, for the first time, machines can learn language, context and intent and be independently generative and creative.\n\nSecond, after being pre-trained on vast quantities of data (text, images or audio), these models can be adapted or fine-tuned for a wide range of tasks.\n\nThis allows them to be reused or repurposed in many different ways.\n\nBusiness leaders recognize the significance of this moment.\n\nThey can see how LLMs and generative AI will fundamentally transform everything from business, to science, to society itself—unlocking new performance frontiers.\n\nThe positive impact on human creativity and productivity will be massive.\n\nConsider that, across all industries, Accenture found 40% of all working hours can be impacted by LLMs like GPT-4.\n\nThis is because language tasks account for 62% of the total time employees work, and 65% of that time can be transformed into more productive activity through augmentation and automation (see Figure 3).",
    "### Machine learning: Analysis and prediction phase\n\nMilestones in the journey The first decade of the 2000s marked the rapid advance of various machine learning techniques that could analyze massive amounts of online data to draw conclusions – or “learn” – from the results.\n\nSince then, companies have viewed machine learning as an incredibly powerful field of AI for analyzing data, finding patterns, generating insights, making predictions and automating tasks at a pace and on a scale that was previously impossible.",
    "### Deep learning: Vision and speech phase\n\nThe 2010s produced advances in AI’s perception capabilities in the field of machine learning called deep learning.\n\nBreakthroughs in deep learning enable the computer vision that search engines and self-driving cars use to classify and detect objects, as well as the voice recognition that allows popular AI speech assistants to respond to users in a natural way.",
    "### Generative AI: Enter the language-mastery phase\n\nBuilding on exponential increases in the size and capabilities of deep learning models, the 2020s will be about language mastery.\n\nThe GPT-4 language model, developed by OpenAI, marks the beginning of a new phase in the abilities of language-based AI applications.\n\nModels such as this will have far-reaching consequences for business, since language permeates everything an organization does day to day—its institutional knowledge, communication and processes.",
    "## Consume or customize: Generative AI for everyone\n\nEasy-to-consume generative AI applications like ChatGPT, DALL-E, Stable Diffusion and others are rapidly democratizing the technology in business and society.\n\nThe effect on organizations will be profound.\n\nThe ability of LLMs to process massive data sets allows them to potentially “know” everything an organization has ever known—the entire history, context, nuance and intent of a business, and its products, markets and customers.\n\nAnything conveyed through language (applications, systems, documents, emails, chats, video and audio recordings) can be harnessed to drive next-level innovation, optimization and reinvention.\n\nWe’re at a phase in the adoption cycle when most organizations are starting to experiment by consuming foundation models “off the shelf.” However, the biggest value for many will come when they customize or fine tune models using their own data to address their unique needs:",
    "### Customize\n\nBut most companies will need to customize models, by fine-tuning them with their own data, to make them widely usable and valuable.\n\nThis will allow the models to support specific downstream tasks all the way across the business.\n\nThe effect will be to increase a company’s efficacy in using AI to unlock new performance frontiers—elevating employee capabilities, delighting customers, introducing new business models and boosting responsiveness to signals of change.\n\nCompanies will use these models to reinvent the way work is done.\n\nEvery role in every enterprise has the potential to be reinvented, as humans working with AI co-pilots becomes the norm, dramatically amplifying what people can achieve.\n\nIn any given job, some tasks will be automated, some will be assisted, and some will be unaffected by the technology.\n\nThere will also be a large number of new tasks for humans to perform, such as ensuring the accurate and responsible use of new AI-powered systems.\n\nConsider the impact in these key functions:\n- **Advising.\n\n** AI models will become an ever-present co-pilot for every worker, boosting productivity by putting new kinds of hyper-personalized intelligence into human hands.\n\nExamples include customer support, sales enablement, human resources, medical and scientific research, corporate strategy and competitive intelligence.\n\nLarge language models could be useful in tackling the roughly 70% of customer service communication that is not straightforward and can benefit from a conversational, powerful and intelligent bot, understanding a customer’s intent, formulate answers on its own and improve the accuracy and quality of answers.\n\n- **Creating.\n\n** Generative AI will become an essential creative partner for people, revealing new ways to reach and appeal to audiences and bringing unprecedented speed and innovation in areas like production design, design research, visual identity, naming, copy generation and testing, and real-time personalization.\n\nCompanies are turning to state-of-the-art artificial intelligence systems like DALL·E, Midjourney and Stable Diffusion for their social media visual content generation outreach.\n\nDALL·E, for example, creates realistic images and art based on text descriptions and can process up to 12 billion parameters when transforming words into pictures.\n\nImages created can then be shared on Instagram and Twitter.\n\n- **Coding.\n\n** Software coders will use generative AI to significantly boost productivity — rapidly converting one programming language to another, mastering programming tools and methods, automating code writing, predicting and pre-empting problems, and managing system documentation.\n\nAccenture is piloting the use of OpenAI LLMs to enhance developer productivity by automatically generating documentation – for example, SAP configuration rationale and functional or technical specs.\n\nThe solution enables users to submit requests through a Microsoft Teams chat as they work.\n\nCorrectly packaged documents are then returned at speed — a great example of how specific tasks, rather than entire jobs, will be augmented and automated.\n\n- **Automating.\n\n** Generative AI’s sophisticated understanding of historical context, next best actions, summarization capabilities, and predictive intelligence will catalyze a new era of hyper-efficiency and hyper-personalization in both the back and front office—taking business process automation to a transformative new level.\n\nOne multinational bank is using generative AI and LLMs to transform how it manages volumes of post-trade processing emails—automatically drafting messages with recommended actions and routing them to the recipient.\n\nThe result is less manual effort and smoother interactions with customers.\n\n- **Protecting.\n\n** In time, generative AI will support enterprise governance and information security, protecting against fraud, improving regulatory compliance, and proactively identifying risk by drawing cross-domain connections and inferences both within and outside the organization.\n\nIn strategic cyber defense, LLMs could offer useful capabilities, such as explaining malware and quickly classifying websites.\n\nIn the short term, however, organizations can expect criminals to capitalize on generative AI’s capabilities to generate malicious code or write the perfect phishing email.",
    "## A look ahead at the fast-paced evolution of technology, regulation and business\n\nMoments like this don’t come around often.\n\nThe coming years will see outsized investment in generative AI, LLMs and foundation models.\n\nWhat’s unique about this evolution is that the technology, regulation, and business adoption are all accelerating exponentially at the same time.\n\nIn previous innovation curves, the technology typically outpaced both adoption and regulation.",
    "### The technology stack\n\nThe complex technology underpinning generative AI is expected to evolve rapidly at each layer.\n\nThis has broad business implications.\n\nConsider that the amount of compute needed to train the largest AI models has grown exponentially – now doubling between every 3.4 to 10 months, according to various reports.\n\nCost and carbon emissions are therefore central considerations in adopting energy-intensive generative AI.",
    "### The risk and regulatory environment\n\nCompanies will have thousands of ways to apply generative AI and foundation models to maximize efficiency and drive competitive advantage.\n\nUnderstandably, they’ll want to get started as soon as possible.\n\nBut an enterprise-wide strategy needs to account for all the variants of AI and associated technologies they intend to use, not only generative AI and large language models.\n\nChatGPT raises important questions about the responsible use of AI.\n\nThe speed of technology evolution and adoption requires companies to pay close attention to any legal, ethical and reputational risks they may be incurring.\n\nIt’s critical that generative AI technologies, including ChatGPT, are responsible and compliant by design, and that models and applications do not create unacceptable risk for the business.\n\nAccenture was a pioneer in the responsible use of technology including the responsible use of AI in its Code of Business Ethics from 2017.\n\nResponsible AI is the practice of designing, building and deploying AI in accordance with clear principles to empower businesses, respect people, and benefit society — allowing companies to engender trust in AI and to scale AI with confidence.\n\nAI systems need to be “raised” with a diverse and inclusive set of inputs so that they reflect the broader business and societal norms of responsibility, fairness and transparency.\n\nWhen AI is designed and put into practice within an ethical framework, it accelerates the potential for responsible collaborative intelligence, where human ingenuity converges with intelligent technology.\n\nThis creates a foundation for trust with consumers, the workforce, and society, and can boost business performance and unlock new sources of growth.",
    "### The scale of adoption in business\n\nCompanies must reinvent work to find a path to generative AI value.\n\nBusiness leaders must lead the change, starting now, in job redesign, task redesign and reskilling people.\n\nUltimately, every role in an enterprise has the potential to be reinvented, once today’s jobs are decomposed into tasks that can be automated or assisted and reimagined for a new future of human + machine work.\n\nGenerative AI will disrupt work as we know it today, introducing a new dimension of human and AI collaboration in which most workers will have a “co-pilot,” radically changing how work is done and what work is done.\n\nNearly every job will be impacted – some will be eliminated, most will be transformed, and many new jobs will be created.\n\nOrganizations that take steps now to decompose jobs into tasks, and invest in training people to work differently, alongside machines, will define new performance frontiers and have a big leg up on less imaginative competitors.",
    "### Dive in, with a business-driven mindset\n\nEven when new innovations have obvious advantages, diffusing them across an organization can be challenging, especially if the innovation is disruptive to current ways of working.\n\nBy experimenting with generative AI capabilities, companies will develop the early successes, change agents and opinion leaders needed to boost acceptance and spread the innovation further, kick-starting the transformation and reskilling agenda.\n\nOrganizations must take a dual approach to experimentation.\n\nOne, focused on low-hanging fruit opportunities using consumable models and applications to realize quick returns.\n\nThe other, focused on reinvention of business, customer engagement and products and services using models that are customized with the organization’s data.\n\nA business-driven mindset is key to define and successfully deliver on the business case.\n\nAs they experiment and explore reinvention opportunities, they’ll reap tangible value while learning more about which types of AI are most suited to different use cases, since the level of investment and sophistication required will differ based on the use case.\n\nThey’ll also be able to test and improve their approaches to data privacy, model accuracy, bias and fairness with care, and learn when “human in the loop” safeguards are necessary.\n\n**A bank uses enhanced search to equip employees with the right information.\n\n** As part of its three-year innovation plan, a large European banking group saw an opportunity to transform its knowledge base, empower its people with access to the right information, and advance its goal of becoming a data-driven bank.\n\nUsing Microsoft’s Azure platform and a GPT-3 LLM to search electronic documents, users can get quick answers to their questions — saving time while improving accuracy and compliance.\n\nThe project, which included employee upskilling, is the first of four that will apply generative AI to the areas of contract management, conversational reporting and ticket classification.",
    "### Take a people-first approach\n\nSuccess with generative AI requires an equal attention on people and training as it does on technology.\n\nCompanies should therefore dramatically ramp up investment in talent to address two distinct challenges: creating AI and using AI.\n\nThis means both building talent in technical competencies like AI engineering and enterprise architecture and training people across the organization to work effectively with AI-infused processes.\n\nIn our analysis across 22 job categories, for example, we found that LLMs will impact every category, ranging from 9% of a workday at the low end to 63% at the high end.\n\nMore than half of working hours in 5 of the 22 occupations can be transformed by LLMs.\n\nIn fact, independent economic research indicates that companies are significantly underinvesting in helping workers keep up with advances in AI, which require more cognitively complex and judgment-based tasks.\n\nEven domain experts who understand how to apply data in the real world (a doctor interpreting health data, for example) will need enough technical knowledge of how these models work to have confidence in using them as a “workmate.” There will also be entirely new roles to recruit, including linguistics experts, AI quality controllers, AI editors, and prompt engineers.\n\nIn areas where generative AI shows most promise, companies should start by decomposing existing jobs into underlying bundles of tasks.\n\nThen assess the extent to which generative AI might affect each task — fully automated, augmented, or unaffected.",
    "### Get your proprietary data ready\n\nCustomizing foundation models will require access to domain-specific organizational data, semantics, knowledge, and methodologies.\n\nIn the pre-generative AI era, companies could still get value from AI without having modernized their data architecture and estate by taking a use-case centric approach to AI.\n\nThat’s no longer the case.\n\nFoundation models need vast amounts of curated data to learn and that makes solving the data challenge an urgent priority for every business.\n\nCompanies need a strategic and disciplined approach to acquiring, growing, refining, safeguarding and deploying data.\n\nSpecifically, they need a modern enterprise data platform built on cloud with a trusted, reusable set of data products.\n\nBecause these platforms are cross-functional, with enterprise-grade analytics and data housed in cloud-based warehouses or data lakes, data is able to break free from organizational silos and democratized for use across an organization.\n\nAll business data can then be analyzed together in one place or through a distributed computing strategy, such as a data mesh.",
    "### Invest in a sustainable tech foundation\n\nCompanies need to consider whether they have the right technical infrastructure, architecture, operating model and governance structure to meet the high compute demands of LLMs and generative AI, while keeping a close eye on cost and sustainable energy consumption.\n\nThey’ll need ways to assess the cost and benefit of using these technologies versus other AI or analytical approaches that might be better suited to particular use cases, while also being several times less expensive.\n\nAs the use of AI increases, so will the carbon emissions produced by the underlying infrastructure.\n\nCompanies need a robust green software development framework that considers energy efficiency and material emissions at all stages of the software development lifecycle.\n\nAI can also play a broader role in making business more sustainable and achieving ESG goals.\n\nOf the companies we surveyed that successfully reduced emissions in production and operations, 70% used AI to do it.",
    "### Accelerate ecosystem innovation\n\nCreating a foundation model can be a complex, compute-intensive and costly exercise.\n\nAnd for all but the very largest global companies, doing it entirely on their own will be beyond their means and capabilities.\n\nThe good news is that there is a burgeoning ecosystem to call on, with substantial investments by cloud hyperscalers, big tech players, and start-ups.\n\nGlobal investment in AI startups and scale-ups is estimated to exceed $50 billion in 2023 alone.\n\nThese partners bring best practices honed over many years, and can provide valuable insights into using foundation models efficiently and effectively in specific use cases.\n\nHaving the right network of partners—including technology companies, professional services firms and academic institutions—will be key to navigating rapid change.",
    "### Level-up your responsible AI\n\nThe rapid adoption of generative AI brings fresh urgency to the need for every organization to have a robust responsible AI compliance regime in place.\n\nThis includes controls for assessing the potential risk of generative AI use cases at the design stage and a means to embed responsible AI approaches throughout the business.\n\nAccenture’s research suggests most companies still have a long way to go.\n\nOur 2022 survey of 850 senior executives globally revealed widespread recognition of the importance of responsible AI and AI regulation.\n\nBut only 6 percent of organisations felt they had a fully robust responsible AI foundation in place.\n\nAn organization’s responsible AI principles should be defined and led from the top and translated into an effective governance structure for risk management and compliance, both with organizational principles and policies and applicable laws and regulations.\n\nResponsible AI must be CEO-led, beginning with a focus on training and awareness and then expanding to focus on execution and compliance.\n\nAccenture was one of the first to take this approach to Responsible AI years ago, with a CEO-led agenda, and now a formal compliance program.\n\nOur own experience shows that a principles-driven compliance approach provides guardrails while being flexible enough to evolve with the fast pace of changing technology, ensuring companies aren’t constantly playing “catch up.” To be responsible by design, organizations need to move from a reactive compliance strategy to the proactive development of mature Responsible AI capabilities through a framework that includes principles and governance; risk, policy and control; technology and enablers and culture and training.",
    "## The future of AI is accelerating\n\nThis is a pivotal moment.\n\nFor several years, generative AI and foundation models have been quietly revolutionizing the way we think about machine intelligence.\n\nNow, thanks to ChatGPT, the whole world has woken up to the possibilities this creates.\n\nWhile artificial general intelligence (AGI) remains a distant prospect, the speed of development continues to be breathtaking.\n\nWe’re at the start of an incredibly exciting era that will fundamentally transform the way information is accessed, content is created, customer needs are served, and businesses are run.\n\nEmbedded into the enterprise digital core, generative AI, LLMs, and foundation models will optimize tasks, augment human capabilities, and open up new avenues for growth.\n\nIn the process, these technologies will create an entirely new language for enterprise reinvention.\n\nBusinesses are right to be optimistic about the potential of generative AI to radically change how work get done and what services and products they can create.\n\nThey also need to be realistic about the challenges that come with profoundly rethinking how the organization works, with implications for IT, organization, culture, and responsibility by design.\n\nCompanies need to invest as much in evolving operations and training people as they do in technology.\n\nRadically rethinking how work gets done, and helping people keep up with technology-driven change, will be two of the most important factors in realizing the full potential of this step-change in AI technology.\n\nNow’s the time for companies to use breakthrough advances in AI to set new performance frontiers—redefining themselves and the industries in which they operate.",
    "## Glossary\n\nChatGPT is a generative AI chatbot interface built on top of OpenAI’s GPT-3.5 large language model (see below).\n\nChatGPT (and ChatGPT plus, which uses GPT-4) allows users to interact with the underlying AI in a way that seems remarkably accurate and feels surprisingly human.\n\nYou can ask it to explain a subject, write an essay, run a calculation, generate some Python code, or simply have a conversation.\n\nGenerative AI is the umbrella term for the ground-breaking form of creative artificial intelligence that can produce original content on demand.\n\nRather than simply analyzing or classifying existing data, generative AI is able to create something entirely new, whether text, images, audio, synthetic data, or more.\n\nFoundation models are complex machine learning systems trained on vast quantities of data (text, images, audio, or a mix of data types) on a massive scale.\n\nThe power of these systems lies not only in their size but also in the fact they can quickly be adapted or fine-tuned for a wide range of downstream tasks.\n\nExamples of foundation models include BERT, DALL-E, and GPT-4.\n\nLarge Language Models (LLMs) represent a subset of foundation models that are trained specifically on text sources.\n\nGPT-3, for instance, was trained on almost 500 billion words from millions of websites.\n\nIts successor, GPT-4, can take image as well as text as inputs.\n\nFine-tuning is the process by which foundation models are adapted for specific downstream tasks using a particular dataset.\n\nThat can include everything from the hyper-specific (training a model to compose emails based on your personal writing style) to the enterprise level (training an LLM on enterprise data to transform a company’s ability to access and analyze its core intelligence).\n\nData is the fundamental bedrock of generative AI.\n\nNot only in training foundation models themselves, but also in fine-tuning those models to perform specific tasks.\n\nIn an enterprise context, examples might include everything from legacy code to real-time operational data to customer insights.",
    "## References\n\n1.\n\nChatGPT sets record for fastest-growing user base - analyst note, Reuters, February 2023 \n2.\n\nThe Next Big Breakthrough in AI Will Be Around Language, Harvard Business Review, September, 2020 \n3.\n\nAccenture Tech Vision 2023\n4.\n\nChatGPT Is Coming to a Customer Service Chatbot Near You, Forbes, January 2023  you/?sh=730eeab97eca\n5.\n\nHow AI Transforms Social Media, Forbes, March 2023 \n6.\n\nLarge AI Models have Real Security Benefits, Dark Reading, August, 2022 \n7.\n\nOPWNAI: Cybercriminals starting to use ChatGPT, Checkpoint Research, January, 2023 \n8.\n\nAccenture Technology Vision 2023\n9.\n\nCXO Pulse Survey, conducted by Accenture Research, February 2023\n10.\n\nAccenture Technology Vision 2023\n11.\n\nThe Productivity J-Curve: How Intangibles Complement General Purpose Technologies - American Economic Association (aeaweb.org)\n12.\n\nUniting technology and sustainability, Accenture, May, 2022 Technology Sustainability Key to ESG Goals | Accenture\n13.\n\nPace Of Artificial Intelligence Investments Slows, But AI Is Still Hotter Than Ever, Forbes, October, 2022  ever/?sh=853d8124c76c\n14.\n\nOpenAI’s GPT-3 Language Model: A Technical Overview, Lambda, June, 2020",
    "## Authors\n\nPaul Daugherty Group Chief Executive & Chief Technology Officer  \nBhaskar Ghosh Chief Strategy Officer, Accenture  \nKarthik Narain Lead – Accenture Cloud First  \n\nLan Guan Lead – Cloud First, Data & AI  \nJim Wilson Global Managing Director – Thought Leadership & Technology Research  \n\nThe authors would like to acknowledge Tomas Castagnino, Elise Cornille, Ray Eitel-Porter, Linda King, Amy Sagues, Ezequiel Tacsir and Denise Zheng for their contributions.",
    "## About Accenture\n\nAccenture is a leading global professional services company that helps the world’s leading businesses, governments and other organizations build their digital core, optimize their operations, accelerate revenue growth and enhance citizen services—creating tangible value at speed and scale.\n\nWe are a talent and innovation led company with 738,000 people serving clients in more than 120 countries.\n\nTechnology is at the core of change today, and we are one of the world’s leaders in helping drive that change, with strong ecosystem relationships.\n\nWe combine our strength in technology with unmatched industry experience, functional expertise and global delivery capability.\n\nWe are uniquely able to deliver tangible outcomes because of our broad range of services, solutions and assets across Strategy & Consulting, Technology, Operations, Industry X and Accenture Song.\n\nThese capabilities, together with our culture of shared success and commitment to creating 360° value, enable us to help our clients succeed and build trusted, lasting relationships.\n\nWe measure our success by the 360° value we create for our clients, each other, our shareholders, partners and communities.\n\nVisit us at .",
    "## Contact us\n\nFor more information, contact the Accenture Generative AI/ Large Language Model Center of Excellence at: .\n\nCopyright © 2023 Accenture.\n\nAll rights reserved.\n\nAccenture and its logo are registered trademarks of Accenture  \n\n**Disclaimer:** This content is provided for general information purposes and is not intended to be used in place of consultation with our professional advisors.\n\nThis document refers to marks owned by third parties.\n\nAll such third-party marks are the property of their respective owners.\n\nNo sponsorship, endorsement or approval of this content by the owners of such marks is intended, expressed or implied."
]