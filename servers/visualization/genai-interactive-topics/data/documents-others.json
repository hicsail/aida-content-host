[
    "# The Shortcomings of Generative AI Detection: How Schools Should Approach Declining Teacher Trust In Students\n\nGenerative AI – systems that use machine learning to produce new content (e.g., text or images) in response to user prompts – has infiltrated the education system and fundamentally shifted the relationships between teachers and their students.\n\nAcross the country, educators have expressed high levels of anxiety about students using generative AI tools, like ChatGPT, to cheat on assignments, exams, and essays in addition to fears of students losing critical thinking skills.\n\nOne professor even described it as having “infected [the education system] like a deathwatch beetle, hollowing out sound structures from the inside until the imminent collapse.” In response to these fears, school districts, like New York City and Los Angeles, quickly imposed bans on its use by both educators and students.\n\nSchools have turned to tools like generative AI detectors to attempt to restore educator control and trust; however, detection efforts have fallen short in both their implementation and efficacy.",
    "## CDT Research Affirms Declining Trust\n\nOne significant finding from our polling research of teachers, parents, and students is that teacher perception of widespread generative AI use for cheating appears to be largely unfounded.\n\nForty percent of teachers who say that their students have used generative AI for school think their students have used it to write and submit a paper.\n\nBut only 19 percent of students who report having used generative AI say they have used it to write and submit a paper – a finding that is supported by other survey research.\n\nEven despite the reality that a large majority of students are not using generative AI for nefarious academic purposes, teachers have still become more mistrustful of students’ work – perhaps due to the widespread, fear-stoking coverage of cheating instances.\n\nSixty two percent of teachers agreed with the statement that “[g]enerative AI has made me more distrustful of whether my students’ work is actually theirs.” And this mistrust is bleeding into certain groups of students being disciplined at disproportionate rates for using or being accused of using generative AI – Title I and licensed special education teachers report higher rates of disciplinary actions for generative AI use among their students.\n\nThese high levels of mistrust among teachers and subsequent disciplinary action have led to frustration among students and parents about erroneous accusations of cheating, which can cause an even further rift between teachers and students.\n\nThis erosion of trust is potentially damaging to school communities where strong relationships between educators and their students are imperative in providing a safe, quality learning environment.",
    "## ...And Insufficient Detection Tools And Training\n\nTools designed to detect when generative AI was used to produce content are the only technological solutions currently available to help teachers attempt to combat generative AI-based cheating; however, they fall short of solving existing trust issues.\n\nTo begin, school policies on content detection tool use is spotty – only 17 percent of teachers say that their school provides a content detection tool as part of its larger technology platform, and 26 percent say their school recommends their use, but leaves it up to the educator to choose one and implement it.\n\nWithout strong guidance on the use and implementation of content detection tools, teachers appear uneasy about utilizing them as a defense mechanism for cheating.\n\nOnly 38 percent of teachers report using a generative AI content detection tool regularly, and just 18 percent of teachers strongly agree that these tools “are an accurate and effective way to determine whether a student is using AI-generated content.” Teachers’ lack of confidence is well-founded as, at least at this point, these tools are not consistently effective at differentiating between AI-generated and human-written text.\n\nBeyond using tools for detection, teacher confidence in their own effectiveness at detecting generative AI created writing is low – 22 percent say they are very effective and 43 percent say they are somewhat effective.\n\nThis is particularly concerning given that most teachers have not received guidance on how to detect cheating.\n\nOnly 23 percent of teachers who have received training on their schools’ policies and procedures regarding generative AI have gotten guidance on how to detect student use of ChatGPT (or another generative AI tool) when submitting school assignments.",
    "## How Should Schools Approach Declining Teacher Trust?\n\nGiven our research and what we know about generative AI content detection tools, they are not the answer, at least for now.\n\nThese tools suffer from accuracy issues, and may disproportionately flag non-native speakers.\n\nInstead, schools need to:\n\n- Offer teacher training on how to assess student work in light of generative AI.\n\nTo help teachers feel like they have more control over academic integrity in the classroom, schools must properly equip them to deal with the new reality of generative AI.\n\nThis means providing them with training on the limitations of detectors and how to respond if they reasonably suspect that a student is cheating.\n\n- Craft and implement clear policies about which uses are allowed and prohibited.\n\nOur polling from this past summer shows that schools are failing to provide guidance on what is defined as “improper use” of generative AI, with 37 percent of teachers reporting that their school has no policy or they are not sure if there is a policy in place on generative AI.\n\nIt is imperative for both teachers and students to know this, so that everyone is on the same page about responsible generative AI use.\n\n- Encourage teachers to modify assignments to minimize the effectiveness of generative AI.\n\nUnderstanding what generative AI systems are not good at can help teachers design assignments where using generative AI will not be helpful to students.\n\nFor instance, generative AI systems are often ineffective at providing accurate sources for their claims.\n\nRequiring students to provide citations for any claims they make will likely require students to go far beyond a generated response.",
    "# Responsible AI Guidelines for Generative AI\n\nThis document contains a description of the normative obligations for researchers, developers, and users of generative AI models and applications (hereinafter \"guidelines\"), with references to corresponding socio-technical recommendations and aids, where possible, to help ensure responsible adoption of generative AI.\n\nThese guidelines are the result of consultations with a multi-disciplinary group of AI experts, researchers, and practitioners, with representations from the industry, academia, and civil society.\n\nRecognising the fast-paced nature of the technological and regulatory developments impacting generative AI practice, we plan on releasing future editions of this document with revisions and updates that could serve stakeholders better over time.",
    "## Preamble\n\nThe recent powerful developments in generative artificial intelligence (hereinafter \"GenAI\") and the projected pace of adoption of this revolutionary technology across industries have got people both excited and worried about the future.\n\nThe excitement comes from the promise of GenAI to massively improve industrial productivity, boost economic growth, and scale humanitarian efforts.\n\nThe worry comes from our sheer unpreparedness to effectively tackle the fast-evolving universe of societal harms that are anticipated with the adoption of GenAI.\n\nStakeholders, largely, do not yet align on a set of robust, common standards and protocols for researching, developing, and using GenAI responsibly.\n\nThis heightens the risk of GenAI being deployed without appropriate safeguards in place, impacting public safety and potentially pausing the pace of innovation in GenAI.\n\nAt nasscom, we believe this calls for urgent action from all concerned stakeholders to cooperate and co-develop responsible approaches for researching, developing, and using generative AI models, applications, and tools in alignment with positive human values, for the benefit of humanity as a whole.\n\nIn line with that spirit, we propose this body of guidelines for those engaged in researching, developing, and using generative AI technologies, with the following provisos: This document is not an operational manual or guidebook.\n\nIt is rather intended to build stakeholder consensus on the core normative obligations of those engaged in researching, developing, or using GenAI technologies.\n\nWe expect the guidelines to become instrumental in defining frameworks for the development of standards, protocols, audit checklists, certifications, monitoring and evaluation mechanisms, and other forms of actionable guidance, tools, and best practices to effectively mitigate potential harms from the adoption of GenAI.",
    "## Guideline Focus\n\nThe guidelines focus on research, development, and use in relation to GenAI.\n\nThe guidelines define GenAI as a type of artificial intelligence technology that can create artefacts such as image, text, audio, video, and various forms of multi-modal content.\n\nThe object of these guidelines is to promote and facilitate the responsible development and use of GenAI solutions by different stakeholders.\n\nThe guidelines also intend to achieve a robust, common understanding of normative obligations amongst stakeholders to act with responsibility during the adoption of GenAI technologies across industries.\n\nNote that the categories of \"research\", \"development\", and \"use\" are not mutually exclusive; a given stakeholder could fit into all three categories.\n\nStakeholders may include, but not be limited to, technology companies, startups, open source developers, and researchers.",
    "## Harms Mitigation\n\nThe guidelines seek to aid mitigation of the following harms associated with the research, development, and use of GenAI technologies:\n\n- Proliferation of misinformation, disinformation, hateful (seditious, defamatory, socially disharmonising) content.\n\n- Infringement of intellectual property (e.g., copyrighted works of art, patented designs and inventions, trademarks, trade secrets) and academic malpractice (e.g., plagiarism in research).\n\n- Privacy harms through violations of data protection norms and standards.\n\n- Propagation of harmful social, economic, and political biases.",
    "## Obligations for Researchers\n\n- Demonstrate reasonable caution and foresight by systematically and rigorously anticipating and evaluating both positive and negative contingencies that might arise from the conduct of research.\n\n- Demonstrate transparency and accountability by releasing public disclosures about the values, goals, and motivations for driving or funding a research project and by describing the methodologies, model training datasets, and tools adopted for the conduct of research.\n\n- Demonstrate reliability and safety by adhering to established privacy-preserving norms and standards in research data collection, processing, and usage, and conducting safety testing of GenAI models in regulated environments.\n\n- Demonstrate inclusion by accounting for the risk of harmful bias in research and deploying protocols and measures to mitigate it, and by publishing research findings in open-source formats to democratise the framing of new problem statements.\n\n- Support progress of humanity as a whole by prioritising research on GenAI applications, tools, and techniques that hold the maximum potential to enhance human agency and improve the human condition, and by advancing research in technical AI safety.",
    "## Obligations for Developers\n\n- Demonstrate reasonable caution and foresight by evaluating potential harms from the development, deployment, and use of a GenAI solution through comprehensive risk assessments and internal oversight throughout the lifecycle of the solution.\n\n- Demonstrate transparency by publicly disclosing the data and algorithm sources used for modelling and all other technical, non-proprietary information about the solution's development process, capabilities, and limitations.\n\n- Demonstrate reliability and safety by practising due diligence in the adoption of means and methods for solution development and deployment through strict adherence to applicable data protection and intellectual property rules and norms.\n\n- Demonstrate accountability by devising technical means for furnishing explanations for outputs generated by GenAI solutions in high-stake contexts.\n\n- Support progress of humanity as a whole by developing, deploying, and using GenAI solutions in alignment with the positive goals of human progress and societal well-being.",
    "## Compliance with Guidelines\n\nAs researchers, developers, and users of GenAI technologies, we resolve to contribute to the co-development and adoption of actionable guidance, tools, and best practices to enable all stakeholders to successfully comply with the guidelines and help humanity advance with the safe and responsible use of GenAI.",
    "### CO-SPONSORS:\n\n- Agencia de Acceso a la Información Pública (AAIP, Argentina);\n- Defensoría del Pueblo de la Ciudad de Buenos Aires (Argentina)\n- Der Bundesbeauftragte für den Datenschutz und die Informationsfreiheit (BfDI, Germany);\n- Der Landesbeauftragte für den Datenschutz und die Informationsfreiheit Rheinland-Pfalz (DRLP, Germany);\n- Commission for the Control and the Protection of Personal Data (CNDP, Morocco);\n- Commission nationale de l'informatique et des libertés (CNIL, France);\n- Council of Europe (CoE);\n- Federal Data Protection and Information Commissioner (FDPIC, Switzerland);\n- Garante per la Protezione dei Dati Personali (GPDP, Italy);\n- Information Commissioner’s Office (ICO, United Kingdom)\n- National Institute for Transparency, Access to Information, and Personal Data Protection of México (INAI, Mexico);\n- Office of the Information and Data Protection Commissioner of Malta (IDPC, Malta);\n\n- Office of the Information and Privacy Commissioner of Ontario (IPC, Canada);\n- Office of the Privacy Commissioner of Canada (OPC, Canada);\n- Office of the Privacy Commissioner for Personal Data (Hong Kong, China);\n- Personal Information Protection Commission (PPC, Japan);\n- Personal Information Protection Commission (PIPC, Korea);\n- Privacy Protection Authority of Israel (PPA, Israel);\n- Unidad Reguladora y de Control de Datos Personales (AGESIC, Uruguay)\n\nThe 45th Annual Closed Session of the Global Privacy Assembly:",
    "### CONCERNED\n\nBy the release - often with insufficient pre-deployment assessment - of generative AI systems to the wider public, which may present risks and potential harms to data protection, privacy and other fundamental human rights if not properly developed and regulated;\nCONCERNED about the indiscriminate collection of personal data from publicly accessible areas and sources to be fed into such technologies without legal authority;\nRECOGNISING about the environmental impact of developing and deploying generative AI models that dedicates substantial computational resources to process large amounts of data;",
    "### RECALLING\n\nThat data protection and privacy principles and current laws, including data protection and privacy laws, bills, statutes and regulations, apply to generative AI products and services, even as different jurisdictions continue to develop AI-specific laws and policies;\nACKNOWLEDGING the ongoing policy making, investigative and enforcement actions taken by various data protection authorities in relation to services using generative AI, in particular with regard to the processing of personal data and transparency for data subjects, including minors;\nWELCOMING the Roundtable of G7 Data Protection and Privacy Authorities (DPAs) statement on Generative AI of 21 June 2023, which draws specific attention to key areas where privacy and data protection risks may arise and data protection and privacy principles apply to the development and use of generative AI tools;\nACKNOWLEDGING that continuous global discussion and collaboration is needed on promoting common values in the context of generative AI, not only from a legal perspective but also from an ethical, social and technical one;\n\n\\[1\\]",
    "### STRESSING\n\nThat public entities, including DPAs, around the world have an essential role to play to ensure that the foundation models giving origin to generative AI tools are built in full respect for the individual’s rights and freedoms, including data protection and privacy, as well as preventing unfair, unethical or discriminatory treatment contrary to human rights law;\nRECOGNIZING the unique risks and potential harms of generative AI systems in the context of automated decision-making or in high-risk usages, particularly on vulnerable populations and children;",
    "### REAFFIRMING\n\nThe 41st Global Privacy Assembly’s International Resolution on Privacy as a Fundamental Human Right and Precondition For Exercising Other Fundamental Rights that to build trust in our digital society, accelerate innovation, and protect human dignity, generative AI be human-centric, based on democratic values, and should recognize privacy as a fundamental human right, vital to the protection of other rights and freedoms.",
    "### The 45th Global Privacy Assembly emphasises that:\n\n- As any other AI systems, generative AI must be designed, developed and deployed in a manner that is responsible and trustworthy, based on the principles of data protection, privacy, human control, transparency and democratic values;\n- Developers, providers and deployers of generative AI systems should embed data protection and privacy in the conception, design, operation and management of new products and services using generative AI systems, based on the principles of data protection and privacy by design and document their choices and analyses in a data protection and privacy impact assessment;\n- Moreover, developers, providers and deployers of generative AI systems should understand the risks, harms, and potential impact on affected individuals and society at large as the basis to develop ethical, trustworthy and responsible generative AI;\n\n- In addition, developers, providers and deployers of generative AI systems should also put in place measures to ensure compliance with data protection and privacy obligations.\n\nThese stakeholders should cooperate to ensure that individuals whose data are processed by generative AI systems have the ability to exercise their data protection and privacy rights;\n- Close attention should be paid by developers, providers and deployers of generative AI systems to the legal requirements and guidance from DPAs on how to interpret such legal requirements.\n\n- Where appropriate, close communication with DPAs can contribute to the responsible design, development and deployment of products and services based on generative AI systems.",
    "#### Lawful basis for processing\n\nGenerative AI systems must have a legal basis and be lawful in accordance with applicable legislation even when personal data is publicly accessible.\n\nDevelopers must establish that generative AI systems are both legal and safe before the systems are launched.\n\nWhere required under relevant legislation, developers, providers and deployers of generative AI systems must identify at the outset the legal basis for the processing of personal data related to:\n- collection of data used to develop generative AI systems;\n- training, validation and testing datasets used to develop or improve generative AI systems;\n- individuals’ interactions with generative AI systems;\n- content generated by generative AI systems.",
    "#### Purpose specification and use limitation\n\nDevelopers, providers and deployers of generative AI systems shall ensure that they process personal data for specific, explicit and legitimate purpose(s) and not process them further for incompatible purpose(s) or beyond the affected individuals’ reasonable expectations.\n\nThese purpose(s) must be appropriate, reasonable or legitimate in the circumstances.\n\nDevelopers, providers and deployers of generative AI systems must neither develop nor put into operation generative AI, the use of which is illegal or has a considerable potential to lead to unfair, unethical or discriminatory treatment, particularly where this would lead to significant violations of fundamental rights and freedoms.\n\nThis is even more fundamental where AI systems are used to make or assist in decision-making about individuals.\n\nDevelopers, providers and deployers of generative AI systems should carefully evaluate the compatibility —ethical, legal, social, and technical—with the purpose for which the personal data used in their development were collected.",
    "#### Data minimisation\n\nDevelopers, providers and deployers of generative AI systems should limit the collection, sharing, aggregation, retention and further processing of personal data only to what is necessary to fulfil\n\nthe legitimate identified purpose(s).\n\nPersonal data should not be collected and processed indiscriminately.\n\nIn addition, inclusion of personal data in training sets poses privacy and other risks to individuals, including, inter alia, that information in training data could foreseeably be produced as part of a generative AI system's output.\n\nTherefore, personal data must only be used as training data if required to achieve the intended purpose(s) of the generative AI system and only after a Data Protection and Privacy Impact Assessment has been carried out.\n\nDevelopers, providers and deployers of generative AI systems should aim to support environmental objectives by developing computational targets that reduce energy consumption, through tactics like data minimization, more efficient computational methods, and architectures that are less reliant on data growth.",
    "#### Accuracy\n\nIn their development stage, generative AI systems often use vast amounts of training, testing and validation data, including personal data.\n\nThe accuracy of the output of generative AI systems highly depends on the quality and representativeness of such large datasets.\n\nTo safeguard affected individuals from discriminatory, unlawful or otherwise detrimental consequences, it is paramount that developers, providers and deployers of generative AI systems rely on accurate, reliable and representative data.\n\nDevelopers, providers, and deployers must take measures to review and filter the content of the data to exclude information that is false or misleading.\n\nIn addition, developers must refrain from making unsupported or premature claims related to the accuracy of their systems.\n\nEven when trained with representative high-quality data, the content generated by generative AI systems may contain inaccurate or false information including personal data, commonly known as “hallucinations.”\nTo mitigate the risks posed by the potential lack of accuracy of generative AI systems, developers, providers and deployers of generative AI systems should implement appropriate data governance procedures (e.g.\n\nrecording of training dataset sources) and technical safeguards (e.g.\n\nuse of filters in input and output data).\n\nMoreover, once deployed, it is essential to ensure adequate cooperation to monitor the behaviour and responses of generative AI systems and fine-tune the behaviour of generative AI systems to produce accurate responses.",
    "#### Transparency\n\nLack of transparency as to training data used has already fuelled concerns over the impact on individuals’ data protection and privacy.\n\nDeployers of generative AI systems must implement adequate transparency measures ensuring the openness of generative AI tools, including\n\ninformation on how, when, and why personal data is collected and used in the process of training generative AI systems.\n\nProviders who put generative AI systems on the market or in operation must inform deployers about the potential data protection and privacy risks for using such systems and how providers address these issues through adequate policies and practices.\n\nThese risks and policies must be clear, easy to understand, and readily available to deployers, both before and during use of the system.\n\nIf a generative AI system is being used to make or assist in decision-making, developers, providers and deployers must clearly communicate these practices to the affected parties.\n\nDevelopers, providers and deployers of generative AI systems should provide transparent documentation about their datasets, including the sources of the datasets, the legal authority and licenses of the datasets, and any modification, filtering or other curation practices on the datasets.",
    "#### Security\n\nIn the design, conception and operation of generative AI systems, developers, providers and deployers must put in place effective security measures, especially where the system has access to external data sources.\n\nIn particular, these measures should aim at:\n\n- Integrating traditional cybersecurity controls with specific security controls tailored to generative AI system vulnerabilities (e.g.\n\nindirect prompt injection attacks);\n- Preventing model inversion attacks that could allow an attacker to extract and reproduce personal data included in the datasets used to train the model;\n- Ensuring that the safeguards put in place to foster compliance with data protection and privacy requirements are not undermined.\n\nDevelopers, providers and deployers of generative AI systems should assess and mitigate the risk of misuse of their systems, such as creating deep fakes or generating text for phishing attacks.\n\nPriority should be given to identifying and mitigating the root cause of these risks in order to prevent future harm.",
    "#### Privacy by Design and Default\n\nThe capacities and limitations of generative AI systems are evolving rapidly.\n\nFor example, certain generative AI systems that previously accepted just text, images or sound as input recently have been expanded to become multimodal and accept different types of input.\n\nNew risks will arise as a result of these evolutions in the technology.\n\nIn line with the privacy by design and by default principle, developers, providers and deployers of generative AI systems should conduct a data\n\nprotection and privacy impact assessment to identify, assess and address the risks posed by these systems at every stage of their life cycle.\n\nPrivacy by design and by default aims at protecting data throughout the entire life cycle of data processing, starting from the design stage.\n\nBy complying with this principle, based on a risk- oriented approach, the threats and risks that AI may create can be minimised by considering them sufficiently in advance.\n\nDevelopers, providers and deployers of AI systems need to carefully assess the envisaged processing activities, the risks they may pose for the data subjects, the possible measures available to ensure compliance with data protection principles and the protection of individual rights.\n\nPrivacy-oriented approaches should be favoured at all stages, including in particular through strong privacy by-default options and user-friendly options and controls.\n\nAny major change to the functionality of generative AI systems represents a ‘stage’ in their lifecycle and would warrant a Data Protection and Privacy Impact Assessment.",
    "#### Rights of data subjects\n\nDevelopers, providers and deployers of generative AI systems must ensure that individuals are granted the right to be informed about the collection and use of their personal data, particularly where such data is obtained from a variety of sources and when personal data is being used to make or assist in decisions.\n\nIn addition, developers, providers and deployers of generative AI systems shall implement appropriate technical and organisational measures in order to ensure that affected individuals are able to exercise their rights, where provided by law, including:\n\n- The right to access their personal data;\n- The right to rectify any inaccurate personal data;\n- The right to erase their personal data and;\n- The right not to be subject to automated decisions that result in a significant effect for the individuals.\n\nThe capacity of affected individuals to exercise their rights is especially relevant when generative AI systems process special categories of data or personal data of minors.",
    "#### Accountability\n\nDevelopers, providers and deployers of generative AI systems shall be responsible for and must be able to demonstrate compliance with applicable national legislation and international agreements.\n\nThe principle of accountability requires responsibility to be clearly identified and respected among the various actors involved in the generative AI model supply chain.\n\nSuch compliance should be demonstrated by making available technical documentation throughout the lifecycle of systems in order to enable data protection and privacy authorities to\n\nassess compliance of generative AI tools with data protection and privacy requirements.\n\nDevelopers, providers and deployers of generative AI systems should include in their documentation how their models work, what data was used to train their models, and the potential data protection and privacy impacts before putting their services on the market or in operation.\n\nDevelopers, providers and deployers of generative AI systems should also allow for external audits that can independently assess how a model works, test outputs for inaccuracies and biases, and recommend effective measures to mitigate potential risks.\n\nAccountability also requires developers, providers and deployers of generative AI systems to implement sound data governance procedures and tools.",
    "### The 45th Global Privacy Assembly therefore resolves to:\n\n- Commit to ensure the application and enforcement of data protection and privacy legislation in the context of generative AI technologies, including the applicable principles and rights set out in this resolution;\n\n- Commit to collaborate on ensuring personal data protection and privacy within the context of generative AI from an ethical, legal, social, and technical perspective;\n\n- Commit to sharing ongoing developments within jurisdictions regarding the data protection and privacy risks of generative AI systems within the Ethics and Data Protection in Artificial Intelligence Working Group;\n\n- Call on developers, providers and deployers of generative AI systems to recognise data protection and privacy as a fundamental human right and to build responsible and trustworthy generative AI technologies that protects data protection, privacy, human dignity and other fundamental rights and freedoms;\n\n- Encourage developers, providers and deployers of generative AI systems to provide training to employees and personnel to understand development and deployment of the generative AI systems regarding data protection, privacy and the rights of data subjects;\n\n- Encourage GPA members to raise awareness of the risks to data protection, privacy and other human rights as well as the applicable legal obligations and principles of data protection and privacy in the context of the generative AI systems;\n\n- Continue monitoring emerging risks and potential harms to fundamental rights and freedoms as they arise in the context of generative AI;\n\n- Aim to advocate and advise on ongoing and forthcoming legislative and regulatory initiatives and approaches;\n\n- Call on GPA members to coordinate their enforcement efforts on generative AI systems;\n\n- Consider presenting, at the 46th Global Privacy Assembly, an interim report on the work conducted by the GPA AIWG members on generative AI systems, and further consider additional policy documents or resolutions to be presented at the 47th Global Privacy Assembly.",
    "# The Impact of Generative Artificial Intelligence on Socioeconomic Inequalities and Policymaking\n\nValerio Capraro, Austin Lentsch, Daron Acemoglu, Selin Akgun, Aisel Akhmedova, Ennio Bilancini, Jean-François Bonnefon, Pablo Brañas-Garza, Luigi Butera, Karen M. Douglas, Jim A.C. Everett, Gerd Gigerenzer, Christine Greenhow, Daniel A. Hashimoto, Julianne Holt-Lunstad, Jolanda Jetten, Simon Johnson, Chiara Longoni, Pete Lunn, Simone Natale, Iyad Rahwan, Neil Selwyn, Vivek Singh, Siddharth Suri, Jennifer Sutcliffe, Joe Tomlinson, Sander van der Linden, Paul A. M. Van Lange, Friederike Wall, Jay J.\n\nVan Bavel, Riccardo Viale\n\n1 Department of Psychology, University of Milan-Bicocca, Italy.\n\n2 Department of Economics, MIT, USA.\n\n3 Institute Professor and Department of Economics, MIT, USA.\n\n4 College of Education, Michigan State University.\n\n5 IMT School of Advanced Studies Lucca, Italy.\n\n6 Toulouse School of Economics, France.\n\n7 Department of Economics, Loyola Andalucia University, Spain.\n\n8 Copenhagen Business School, Denmark.\n\n9 School of Psychology, University of Kent, UK.\n\n10 School of Psychology, University of Kent, UK.\n\n11 Harding Center for Risk Literacy, University of Potsdam, Germany.\n\n12 College of Education, Michigan State University.\n\n13 Penn Computer Assisted Surgery and Outcomes Laboratory, Department of Surgery, Perelman School of Medicine, University of Pennsylvania.\n\n14 Department of Computer and Information Science, School of Engineering and Applied Science, University of Pennsylvania.\n\n15 Department of Psychology and Neuroscience, Brigham Young University, USA.\n\n16 School of Psychology, University of Queensland, Australia.\n\n17 MIT Sloan School of Management, USA.\n\n18 Department of Marketing, Bocconi University, Italy.\n\n19 Economic & Social Research Institute, Dublin, Ireland.\n\n20 Department of Humanities, University of Turin, Italy.\n\n21 Center for Humans and Machines, Max Planck Institute for Human Development, Berlin, Germany.\n\n22 Faculty of Education, Monash University, Australia.\n\n23 Microsoft, USA.\n\n24 York Law School, United Kingdom.\n\n25 Department of Psychology, University of Cambridge, UK.\n\n26 Department of Experimental and Applied Psychology, Vrije Universiteit, Amsterdam, The Netherlands.\n\n27 Department of Management Control and Strategic Management, University of Klagenfurt, Austria.\n\n28 Department of Psychology & Center for Neural Science, New York University, USA.\n\n29 Norwegian School of Economics, Bergen, Norway.\n\n30 Department of Economics, University of Milan-Bicocca, Italy.\n\n* Corresponding author:",
    "## Abstract\n\nGenerative artificial intelligence, including chatbots like ChatGPT, has the potential to both exacerbate and ameliorate existing socioeconomic inequalities.\n\nIn this article, we provide a state-of-the-art interdisciplinary overview of the probable impacts of generative AI on four critical domains: work, education, health, and information.\n\nOur goal is to warn about how generative AI could worsen existing inequalities while illuminating directions for using AI to resolve pervasive social problems.\n\nGenerative AI in the workplace can boost productivity and create new jobs, but the benefits will likely be distributed unevenly.\n\nIn education, it offers personalized learning but may widen the digital divide.\n\nIn healthcare, it improves diagnostics and accessibility but could deepen pre-existing inequalities.\n\nFor information, it democratizes content creation and access but also dramatically expands the production and proliferation of misinformation.\n\nEach section covers a specific topic, evaluates existing research, identifies critical gaps, and recommends research directions.\n\nWe conclude with a section highlighting the role of policymaking to maximize generative AI’s potential to reduce inequalities while mitigating its harmful effects.\n\nWe discuss strengths and weaknesses of existing policy frameworks in the European Union, the United States, and the United Kingdom, observing that each fails to fully confront the socioeconomic challenges we have identified.\n\nWe contend that these policies should promote shared prosperity through the advancement of generative AI.\n\nWe suggest several concrete policies to encourage further research and debate.\n\nThis article emphasizes the need for interdisciplinary collaborations to understand and address the complex challenges of generative AI.",
    "## Introduction\n\n“The rise of powerful AI will be either the best, or the worst thing, ever to happen to humanity.\n\nWe do not yet know which.\"\n\n- Stephen Hawking, 2016\n\nRecent and future advances in generative Artificial Intelligence (AI) represent a shift in the capability of AI systems to solve problems previously thought unsolvable (Bucker et al., 2023).\n\nCountless techno-optimists predict a utopian future where machines can perform an ever-increasing number of tasks—but humans remain in control, the gains from prosperity are shared throughout society, and we all enjoy lives with less work and more leisure.\n\nOn the other hand, less optimistic forecasts suggest that we are headed toward a dystopian future where machines not only replace humans in the workplace, but also surpass human capability and oversight, destabilize institutions and destroy livelihoods—and perhaps even cause the downfall of humanity (Campbell, 2023; Andreessen, 2023; Bostrom, 2003).\n\nMelvin Kranzberg, a prominent scholar in the history of technology, in a presidential address to his field, defined “Kranzberg’s Laws”, the first of which states that “Technology is neither good nor bad; nor is it neutral” (Kranzberg, 1985).\n\nThis principle suggests that technologies like generative AI could have either negative or positive impacts (or both) on society, though they are not inherently predestined toward either.\n\nThis article aims to outline some of these effects, with the hope of guiding society to harness AI’s positive effects while avoiding the negative ones.\n\nGenerative AI will impact virtually every facet of society.\n\nIn this article, we speculate on the impact of AI on socioeconomic inequalities in four key areas: work, education, health, and information.\n\nFor each domain, we explore current research and suggest broad directions for future exploration.\n\nIn the workplace, generative AI could increase productivity and promote shared prosperity, especially when used to complement human efforts and create new well-paid jobs—offsetting workplace automation with new, value-adding task creation.\n\nHowever, the benefits and costs will likely be distributed unevenly across firm sizes, sectors, and worker demographics.\n\nIn education, generative AI promises personalized learning experiences, potentially bridging educational gaps.\n\nHowever, it also raises concerns about the digital divide and equal access to these advanced tools.\n\nThe health sector could greatly benefit from AI’s diagnostic and predictive capabilities, improving patient outcomes and making healthcare more accessible.\n\nYet, there is the risk of deepening existing inequalities of care and access, especially for under-resourced and marginalized communities.\n\nThe information domain, too, is set to be radically transformed.\n\nGenerative AI can democratize content creation and access but also leads to challenges such as increased misinformation and erosion of trust in digital content.\n\nWe conclude with an examination of the role of policymaking in the age of artificial intelligence.\n\nWe discuss the pros and cons of the current policy approaches in the European Union, the United States, and the United Kingdom, noting that all fall short in adequately addressing the socioeconomic risks that we identify.\n\nWe argue that policies must be designed to mitigate the potential problems posed by AI, aiming for an equitable distribution of benefits across society.\n\nWe propose several explicit policy recommendations that could be discussed in public debate and research endeavors.\n\nThis includes strategies to prevent job market inequalities, initiatives to bridge the digital divide in education and healthcare, and measures to combat AI-generated misinformation.\n\nThe ultimate goal should be to harness the potential of generative AI in ways that favor human flourishing, striking a balance between technological advancement and societal well-being.",
    "## Impact on Work\n\nPrevious waves of digital technologies have contributed to increased inequality.\n\nSome of these technologies, like personal computers, have been complementary mostly to more-educated workers (Autor et al., 1998; Goldin and Katz, 2008; Autor et al., 2003; Autor, 2019), while others, like industrial robots, have been used to automate repetitive or systematic job tasks that are often performed by less-educated workers (Acemoglu and Restrepo, 2022a, 2022b; Autor et al., 2003, Restrepo 2023).\n\nTogether, the upside for more-educated workers and downside for less-educated workers have magnified the distributional consequences of technological innovation.\n\nThe current trend in AI emphasizes automation.\n\nWhile some amount of this is unavoidable, the displacement of labor by “so-so technologies” (e.g., self-checkout kiosks or automated phone systems) that offer little or no productivity gain, along with diminished worker voice due to intensified monitoring and surveillance, can be harmful to long-run productivity and other social goals like job satisfaction (Acemoglu and Restrepo, 2022a, 2019).\n\nAlthough new technologies can boost productivity in some areas (e.g., Brynjolfsson and McAfee, 2016), the productivity gains from those technologies have often fallen well below expectations, especially when the focus has been on replacing work instead of augmenting pre-existing worker capabilities or developing new ones (e.g., Acemoglu et al., 2016; Acemoglu and Johnson, 2023).\n\nMany businesses and researchers tend to focus on automating work instead of creating new job tasks and enabling workers to build new skills.\n\nReasons for this may include hopes for cost-savings, eliminating demanding workers, reducing uncertainty, increasing control, and (to some extent) following the dominant intellectual paradigm of Silicon Valley that focuses on developing AI agents to mimic or surpass all human capabilities as quickly as possible (Acemoglu and Johnson, 2023; Acemoglu, Autor et al., 2023).\n\nNew technologies like AI should be oriented not so much toward replacing human problem-solving abilities, but rather toward enhancing them in a symbiotic relationship where machines are designed to complement human capabilities and humans can compensate for the weaknesses of machines (Licklider, 1960; Engelbart, 1995).\n\nThis “pro-worker” or “human-complementary” path could contribute more to productivity growth and could help reduce economic inequality.\n\nThe critical question we face in the new era of generative AI is whether this technology will primarily accelerate the existing trend of automation without the offsetting force of good-job creation—particularly for non-college educated workers—or whether it will instead introduce new value-adding tasks and well-paying jobs for workers with diverse skill sets and educational backgrounds.\n\nThere is cause for optimism: AI can complement workers by making them more efficient, helping them to produce higher quality work, or enabling them to take on new value-adding tasks (Acemoglu and Restrepo, 2018; Acemoglu, Ahmed et al., 2023; Korinek, 2023).\n\nRecent evidence indicates that this might be the case, especially for generative AI.\n\nBrynjolfsson and et al.\n\n(2023), for instance, consider the staggered implementation of a chat assistant by a Fortune 500 software company that provides business process software.\n\nThe chat assistant monitored customer service chats and proposed real-time response suggestions to customer service agents.\n\nAgents had the option to use or ignore these suggestions.\n\nAccess to the AI assistant increased productivity.\n\nLess-skilled or inexperienced workers were enabled to resolve around 34% more issues per hour, with average improvement across all workers measuring about 14% (the tool was less impactful for experienced and highly skilled workers).\n\nAgents using the tool with only two months of tenure performed as well as those without the tool who had more than six months of tenure.\n\nAnother study examined the impact of GPT-4 access on consultants’ abilities to perform complex knowledge-intensive tasks.\n\nAI users were generally more productive and produced higher quality work.\n\nHowever, for tasks beyond the capabilities of GPT-4—specifically, tasks that involve imperfect information or omitted data, which require cross-referencing resources and leveraging experience-gained intuition to complete successfully—AI usage resulted in fewer correct solutions.\n\nConsultants with below-average performance improved by 43% with AI, while those above average improved by 17% (Dell’Acqua et al., 2023).\n\nThis suggests that AI might reduce inequalities in performance among knowledge workers.\n\nSimilar patterns have been observed in other studies.\n\nFor instance, Peng and co-authors (2023) conducted a controlled experiment with GitHub Copilot, an AI-based programming assistant.\n\nProfessional programmers were tasked to implement an HTTP server in JavaScript.",
    "Professional programmers were tasked to implement an HTTP server in JavaScript.\n\nProgrammers with access to the AI copilot completed the task in 71 minutes on average, less than half the time of the control group’s 161 minutes.\n\nThe AI assistant provided the biggest boost to less-experienced and older programmers, as well as those coding more hours daily.\n\nIn a controlled online experiment, people with access to ChatGPT completed a writing task faster and produced higher quality work (Noy and Zhang, 2023).\n\nAs with the other studies, this reduced worker inequality by benefiting lower-ability workers more; moreover, it led to higher job satisfaction and self-efficacy.\n\nThese studies underscore the potential of generative AI to disproportionately boost productivity for workers with less experience or skill.\n\nThis fundamentally differs from other recent waves of new technology (e.g., Internet, computers), which have overwhelmingly aided highly skilled workers much more than less-skilled workers.\n\nFrom the mid-19th century through the 1970s, the worker-displacing effects of automation were generally offset by the creation of new tasks, which allowed low-skilled workers to obtain new jobs and higher wages as technology evolved (Acemoglu and Johnson, 2023).\n\nBy the 1980s, however, new task creation lagged behind automation, particularly for workers without a college education, adversely impacting wages and employment opportunities throughout the developed world (Acemoglu and Restrepo, 2019; Autor et al., 2022; Acemoglu and Johnson, 2023).\n\nThus, the seeming “inverse skill-bias” of worker-complementary generative AI, benefitting less-skilled workers much more than highly skilled workers, could radically change how technology affects labor markets.\n\nThis paradigm shift—toward a better balance of automation and augmentation—could perhaps counteract or even reverse the trends toward greater inequality observed over the last several decades.\n\nGenerative AI could also reduce inequality by reducing barriers to entry in the digital economy.\n\nFor example, its translation capabilities can help overcome language barriers.\n\nThis increased accessibility, in conjunction with trends toward diminishing geographic barriers, could have a compounding positive effect.\n\nGeographic barriers are challenging to remove—collaborating across many time zones is difficult and many firms prefer to adopt hybrid work, where employees come into the office several times per week, instead of fully virtual arrangements (Aksoy et al., 2023).\n\nNonetheless, there has been a surge of interest in remote-enabled digital economy jobs, evidenced by online job searches.\n\nNotably, a substantial part of this increase originated from rural areas (Counts et al., 2022).\n\nOne of the more notable strengths of generative AI is its ability to parse and aggregate enormous amounts of information.\n\nThis capability can equalize access to information and lower research costs by simplifying online search tasks.\n\nIf a user wants to accomplish a complex task with a traditional search engine, they have to break that task up into pieces, issue search queries for each piece, read the web pages returned by the search engine, assess the representativeness of their gathered information, and then aggregate the results to solve the original problem.\n\nGenerative search engines, on the other hand, can aggregate this information and return it to the user, requiring less bandwidth and fewer trips between the user and the system which would be helpful in lower resource environments.\n\nIn addition to the time and cost savings, these tools could compensate for expertise by identifying trustworthy resources and extracting the consensus on any topic by simultaneously considering more information than human operators can retain.\n\nThis approach could help users and small businesses in low-resource settings access information that has traditionally been available only in high-resource environments.\n\nThere are also ways in which recent advances in AI might exacerbate inequalities in the workplace.\n\nOne concern is differential access to these new tools.\n\nThe most widely available and accessible generative AI platforms still require additional technical inputs (e.g., internet access and internet-enabled devices) as well as training to optimize performance.\n\nIndustries, firms, and workers that have not yet integrated the prerequisite technologies will struggle to take advantage of the expanded capabilities and consequent productivity and earnings upsides, likely falling (further) behind well-resourced competitors or coworkers.\n\nThe role of firm behavior and social context matters, as there could be backfire effects even from well-designed tools.",
    "The role of firm behavior and social context matters, as there could be backfire effects even from well-designed tools.\n\nFor example, while the evidence discussed above suggests that the introduction of generative AI tools gives more of a boost to less-skilled workers than highly skilled workers, this equalizing force could be a way for workers to increase their earnings potential, if compensation is tied to capability.\n\nInstead, if firms exploit the higher interchangeability between workers (“why hire an expert copywriter if a less-skilled writer with an AI chatbot can do the same level of work?”) these wage gains may never be realized.\n\nSimilarly, it could be possible that a single expert is enabled to do the work of multiple experts or direct reports very quickly with generative tools—the enormous volume of search results for “how to build a website with ChatGPT in one minute” suggests that this is an anticipated use case.\n\nThis could slash the talent requirements for many business endeavors, including producing coded deliverables, marketing copy, graphic design, data analysis, etc.\n\nAI will likely have outsized impacts on U.S. workers with Bachelors’ or Associates’ degrees, compared to higher or lower levels of education (Septiandri et al., 2023).\n\nThis effect could compound over time: if generative AI tools commodify expertise and reduce the returns to specialized skills, workers may no longer spend the time or resources to acquire greater levels of expertise, leading to lower levels of worker skill and overreliance on outsourcing to generative tools.\n\nThese effects could cause greater competition at the (now larger) lower end of the skill distribution, further depressing wages.\n\nThere could be further downsides to productivity if non-automatable job tasks would benefit from workers having acquired the sort of foundational knowledge that is now disincentivized.\n\nGovernments may play an important role in mitigating the risk of increased inequality and maximizing the productivity potential of new generative AI tools.\n\nExplicit policy suggestions are postponed to the \"Policymaking in the age of artificial intelligence\" section (see also Table 5).\n\nTable 1 reports a succinct summary of the main research directions on the impacts of generative AI in the workplace.",
    "## Impact on Education\n\nVarious forms of generative AI are beginning to enter education, from chatbots that guide students’ learning to text and image generation tools for producing lesson content.\n\nThe integration of generative AI into schools, colleges and universities offers various benefits, including the potential of skill-adaptive and personalized teaching, instantaneous feedback, and on-demand student guidance and support.\n\nThese uses could be particularly effective in large class settings, with significant opportunity to scale-up implementation beyond the capabilities of traditional educational practices.\n\nConsequently, generative AI could bridge complex and persistent educational gaps.\n\nA review of AI applications in education identified several use cases that produced higher test scores when students used personalized learning systems (Akgun & Greenhow, 2022).\n\nThese systems, unlike traditional approaches like static worksheets with standardized questions, detect areas where students lack foundational understanding by adapting educational resources and tools to foster their development.\n\nFurthermore, assessment algorithms can expedite grading of written assessments, which supports students by offering timely feedback that can be applied immediately.\n\nThese systems have the potential to improve learning outcomes among students with a broad spectrum of learning styles.\n\nStudents themselves perceive AI as potentially beneficial to their education.\n\nCollege students reported that generative AI provided personalized learning, supported their writing and brainstorming, and assisted with research and analysis (Chan & Hu, 2023).\n\nHowever, students also expressed concerns about the accuracy, privacy, and ethical implications of generative AI tools—including how this technology could adversely impact their personal development and career prospects.\n\nEducational uses of generative AI pose a number of challenges.\n\nOne is the perpetuation of biases and discrimination, potentially reinforcing racial or gender-based stereotypes during personalized learning, automated scoring, and admission processes (Akgun and Greenhow, 2022; Baker and Hawn, 2022; Bender et al., 2021; Morewedge et al., 2023).\n\nThe data used to train AI models could suffer from bias, if those data are based on past human decision making (a notoriously biased process).\n\nAn example is the translation bias observed in tools like Google Translate, where gender stereotypes are inadvertently perpetuated in language translations.\n\nTranslating the phrase “she/he is a nurse” from Turkish (which is “genderless”) to English (which is “gendered”) yielded the feminine form (i.e., “she is a nurse”), while the phrase “she/he is a doctor” yielded the masculine form (i.e., “he is a doctor”; Johnson,2021).\n\nFailing to account for these biases could amplify inequalities and injustices, specifically towards historically marginalized groups.\n\nAlthough human teachers may also be prone to bias and discrimination—and AI systems can theoretically be designed to be less biased than humans—simply introducing slightly less discriminatory technologies into classrooms is not a substitute for the goal of removing discrimination from school (Pasquale, 2020).\n\nMoreover, these systems should be designed with sufficient transparency for users to monitor for and identify potential biases to ensure that these tools effectively serve their intended purposes (Stoyanovich et al., 2020).\n\nGenerative AI may place increased burdens on teachers.\n\nIn contrast to the idea that AI tools relieve teachers of repetitive and onerous work, there is growing concern that teachers have to engage in additional tasks “behind the scenes” (e.g., curating and filtering content, monitoring student-AI interactions, providing technical support) to ensure that AI tools are able to function in complex classroom settings (Selwyn et al., 2023).\n\nThis could exacerbate a generational divide among educators, as younger teachers may be more adept with new technology than older teachers.\n\nFurthermore, there could be unintended consequences of generative AI on student learning—for example, if students become overly reliant on extensive support from AI tools, this could undermine the capacity of students to work or think independently.\n\nQuestions also arise about the accuracy of AI-generated content and the new skills that students must acquire to work effectively with AI systems, such as the ability to evaluate AI-generated content.\n\nThe current debate about the role of generative AI, from primary schools to universities, revolves around whether generative AI should be banned, permitted under only some cases, or generally allowed as assistance for teachers and students.\n\nFor instance, the New York City education department and Chinese universities have banned generative AI (Elsen-Rooney, 2023; Liu et al., 2023), while the Berlin universities recommended its use in certain scenarios.",
    "A growing literature recommends the use of generative AI for teacher and student assistance within the traditional curricula (e.g., Chiu, 2023).\n\nWe argue that these approaches are limited in vision.\n\nA more forward-thinking approach would involve a curricular revolution to redefine the skills and competencies necessary to effectively utilize generative AI.\n\nCalculators did not remove the need for students to learn the properties of algebra and develop mathematical reasoning.\n\nSimilarly, the internet did not eliminate the need for careful research and fact-checking; in fact, it increased this need, as online information is frequently incorrect or incomplete (Lazer et al., 2018).\n\nIn the same vein, generative AI will not eliminate the need to learn effective thought organization, writing, and critical thinking skills.\n\nTherefore, curricula must teach how to successfully describe and share ideas, both with and without assistance from generative AI.\n\nIn addition, they need to emphasize the development of critical-thinking skills, fact-checking abilities, an understanding of how generative AI tools function, and appropriate rules of interaction—including by refraining from anthropomorphizing (and thus misunderstanding) these tools (Kasneci et al., 2023).\n\nMore specifically, the text-production abilities of generative AI present an opportunity to teach students critical thinking.\n\nThis will enable them to evaluate the argument and structure of the generated text and also to write intelligent prompts for generative AI.\n\nThis skill should be recognized and assessed by educators.\n\nThe output of generative AI is much more variable than other educational technologies (e.g., calculators); therefore, developing these critical thinking abilities and prompt-engineering skills is fundamental.\n\nAnother crucial skill is the ability to fact-check generative AI outputs.\n\nFact-checking skills are not taught sufficiently in schools.\n\nFor instance, among more than 3,000 U.S. high school students and undergraduates, 96 percent did not know how to evaluate the trustworthiness of websites (Breakstone et al., 2021).\n\nThese fact-checking abilities include smart heuristics such as lateral reading; i.e., the practice of navigating away from an unfamiliar website to verify the reliability of its information by consulting other external sources (McGrew, 2024).\n\nA toolbox of similar fact-checking heuristics needs to be developed or remediated for AI-generated content.\n\nLastly, understanding the nature of large language models, which are statistical machines that calculate correlations between words, is essential.\n\nOnly in this way can students understand the potential and limits of generative AI, rather than assuming that contemporary generative AI can “think” or “comprehend” like humans.\n\nThe adaptation of curricula in schools and other educational settings should involve a collaborative effort that includes educators, students, parents, policymakers, and AI developers to ensure that the integration of these technologies enhances learning rather than detracts from it.\n\nThe rapid development of AI tools presents challenges and opportunities for nearly every educational environment and vantage point.\n\nOne particular challenge is the digital divide: the difference in access and technology literacy between and within different countries and populations that could hinder the equitable use of AI technology in education.\n\nSteps should be taken to identify which institutions lack access to AI education tools and how to make them available to ensure development of competencies in an equitable manner.\n\nContinued research is necessary to examine the challenges and opportunities of generative AI in education.\n\nOngoing and forthcoming studies should focus on evaluating the outcomes of AI integration in educational curricula, examining potential biases embedded in AI algorithms, the degree to which AI optimizes teacher time, the effect of AI tools on learning, and strategies to harness AI in ways that surmount socioeconomic barriers.\n\nA multi-disciplinary approach can generate insights into these issues, involving experts in sociology, teaching, healthcare, psychology, AI ethics, education technology, and other fields.\n\n---\n\nThis document provides a comprehensive overview of the impact of generative artificial intelligence on socioeconomic inequalities and its interaction with policymaking.\n\nIt addresses both challenges and opportunities across several domains including work, education, health, and information dissemination, aiming to inform discussions and decisions in these areas.",
    "# Importance of Generative AI in Education\n\nIntegrating generative AI in education is challenging, but essential.\n\nWithout such changes, teachers and students may use generative AI merely as an automated assistance tool.\n\nThis would forego the opportunity to develop higher-order cognitive skills, such as critical judgment and fact-checking, that generative AI itself cannot reliably perform.\n\nThe result would be a likely decline in higher-order cognitive skills, especially in segments of the population that will use these tools in a more mechanical, less analytical manner.\n\nThe role of governments in integrating generative AI into the education sector is crucial.\n\nWe will discuss potential policy recommendations in the final section (see Table 5).\n\nTable 2 summarizes the main research directions.",
    "# Impact on Healthcare\n\nRecent advances in AI techniques can democratize healthcare by making efficacious medical care more accessible and affordable.\n\nThis is often achieved via augmenting human capacities and reducing workload: AI can support clinicians with diagnosis, screening, prognosis, and triaging, alleviating the burden on health practitioners and giving them the “gift of time” (Topol, 2019).\n\nFor instance, a review of workplace burnout among healthcare providers identified electronic health record systems as a cause of increased stress due to insufficient documentation time, a high volume of patient communications, and negative perceptions by providers (del Carmen et al., 2019).\n\nIn response, generative AI models have been suggested to aid in the completion of electronic health record-related tasks, reducing healthcare professionals’ administrative demands (Patel and Lam, 2023).\n\nAI-systems could also assist healthcare providers by analyzing and interpreting multimodal clinical data (e.g., photos, radiology images, and surgical videos) to provide relevant information to clinicians (The Lancet Regional Health – Europe, 2023).\n\nIn one study, endoscopists reviewed colonoscopy videos with and without AI assistance.\n\nThe results demonstrated that their decisions were influenced by AI, particularly when its advice was correct.\n\nThis Bayesian-like integration of human and AI judgment led to superior performance compared to either alone, highlighting effective human-AI collaboration dynamics in medical decision-making (Reverberi et al., 2022).\n\nAs a cautionary tale, other preliminary evidence finds that diagnostic performance of some expert physicians may not be improved by AI—and in fact may cause incorrect diagnoses in situations that otherwise would have been correctly assessed (Agarwal et al., 2023).\n\nAI systems can also aid in “medical visual question answering”—analyzing medical images (like X-rays or MRI scans) and providing answers to specific questions about these images, typically by leveraging advanced image recognition and AI algorithms (Ren and Zhou, 2020).\n\nThe current GPT-4 model demonstrates reasonable diagnostic accuracy in simple cases and can answer questions on standardized medical exams, though it struggles with diagnostically complex prompts (Kanjee et al., 2023).\n\nThe totality of evidence suggests that more research is needed to understand when human-AI interactions are beneficial or detrimental to clinical practice, as well as appropriate training to avoid over-reliance of human physicians on AI-generated diagnostic suggestions.\n\nGenerative AI could also enable patients to manage their health more proactively through applications that patients can access outside of clinical settings.\n\nChatGPT, for instance, has reasonable accuracy in answering common myths about cancer (Johnson et al., 2023).\n\nPeople trusted ChatGPT’s answers to low-risk medical questions, though trust reportedly varied for questions with greater medical complexity (Nov et al., 2023).\n\nFurthermore, ChatGPT’s answers to medical questions posted on Reddit’s r/AskDocs were rated as higher quality and more empathetic than those of physicians 79% of the time (Ayers et al., 2023).\n\nConversational agents based on generative AI can also provide greater access to medical advice and simplify medical jargon.\n\nThis may have positive downstream effects on inequality.\n\nBeing part of a stigmatized group affects people’s engagement and utilization of healthcare services.\n\nFor example, when contextual cues made racial stereotypes salient, Black women were more likely to feel anxious in a healthcare setting than their white counterparts (Abdou and Fingerhut, 2014).\n\nMore generally, there is evidence of considerable mistrust between health professionals and members of stigmatized groups (Cuevas et al., 2016; López-Cevallos et al., 2014).\n\nAs a result, members of stigmatized minority groups are less likely to listen to, or trust, doctors who they perceive as outgroup members (Dovidio et al., 2008).\n\nFor example, Black patients tend to be less satisfied with consultations, less likely to book an appointment, and have lower rates of medical compliance when they have consultations with white rather than Black physicians (Williams, 2005).\n\nThere are reasons to believe that AI-led healthcare will be more immune to pre-existing systemic biases or discriminatory practices than human-led healthcare.\n\nFor example, health professionals are biased in their treatment of higher-weight patients (Rathbone et al., 2020).\n\nIncorporating AI-based tools into treatment decision making may lead to less bias if it can ameliorate these stereotypes and prejudices.\n\nFurthermore, interactions between members of stigmatized groups and the healthcare system might be more positive when the system is AI-led because their stigmatized status is not made salient in the interaction.",
    "This suggests that members of stigmatized groups could become more likely to engage with AI-led healthcare because they worry less about group- or identity-based factors affecting their treatment options (Hommel et al., 2012).\n\nHowever, it is important to recognize that many societal biases are baked into training datasets—often composed of human clinicians’ decisions—and such biases are difficult to overcome.\n\nBenefits aside, patients, medical providers, and those managing healthcare systems may be hesitant to adopt AI due to several psychological barriers.\n\nIn fact, the impact of AI on clinical practice has been limited despite the growing number of AI tools (Yin et al., 2021; Aristidou et al., 2022).\n\nOne key factor is public trust in AI technologies in healthcare (Quinn et al., 2021).\n\nFor instance, patients may resist adoption because of misperceptions about AI, such as the belief that AI cannot account for a person's uniqueness as well as a human doctor (Longoni et al., 2019), or because of difficulty in holding AI accountable for mistakes (Promberger and Baron, 2006).\n\nAnother factor implicated in adoption hesitancy is the contrast between AI’s opaqueness and the illusory perception that human decision-making is more transparent than AI.\n\nIn reality, decisions made by human physicians or AI are probably equally unobservable to a patient—but because patients feel that they can understand the decision-making as explained by human providers, they ultimately penalize and resist the clinical use of AI (Cadario et al., 2021).\n\nThe most recent versions of AI tools may be less susceptible to concerns about AI’s inscrutability since the iterative nature of newer generative AI tools may allow patients to ask follow-up questions in a more familiar, conversational format.\n\nIt is possible that the back-and-forth supported by modern generative AI tools will empower patients with greater information about AI-driven decision-making, at which point patients may be better-equipped to decide whether to trust (or not trust) AI-generated medical recommendations.\n\nOther challenges to AI adoption include pushback from healthcare practitioners—who may feel more comfortable with traditional methods of patient care, or who fear being replaced by machines—and from those managing healthcare systems, who might be reluctant to initiate costly and systemic changes until the usefulness of AI-integration is fully proven.\n\nInsurance markets will also be impacted.\n\nInsurers could use AI to refine their practices, capturing a larger share of the surplus.\n\nThis could lead to welfare losses for consumers.\n\nToday, it is not possible to determine highly accurate, individualized probabilities for the future health conditions of a particular insurant—insurance as a field relies instead on population-level probabilities, with some refinement from explicit risk factors.\n\nHowever, if generative AI allows companies to more accurately estimate this probability—for example, by incorporating information from unobservable factors that are identifiable only through advanced machine learning algorithms run on text-based claims, electronic health records, or other data—they might charge higher premiums to those at greater risk without offering reductions to those at lower risk.\n\nAI could also enable insurers to reach currently uninsured groups, reducing inefficiencies and achieving market completeness.\n\nA concrete example of this is the use of Responsible Artificial Intelligence in healthcare to predict and prevent insurance claim denials, which could lead to significant cost savings and improved patient wellbeing (Johnson et al., 2021).\n\nMoreover, the application of AI by insurance companies might allow for a more accurate prediction of loss probabilities, thus reducing one of the industry’s most inherent problems, namely asymmetric information (Eling et al., 2022).\n\nGenerative AI may come to fulfill social needs for some people, which could have downstream effects on health.\n\nThere is robust evidence linking social connectedness or lack thereof to long-term health outcomes (Holt-Lunstad, 2021, 2022; Van Lange and Columbus, 2021), including increased risk for chronic illnesses such as cardiovascular disease and stroke (Cené et al., 2022; Valtorta et al., 2016), type 2 diabetes, and dementia (Penninkilampi et al., 2018), as well as mortality from all causes (Leigh-Hunt et al., 2017; Wang et al., 2023; Holt-Lunstad et al., 2010).\n\nGenerative AI can be used as a conversational companion, potentially replacing some human interaction.\n\nIndeed, digital proxies for social connection may, with increasing sophistication, mimic features of social connection, which could in turn decrease motivation to develop authentic human relationships.",
    "While digitally mediated forms of socializing (e.g., social media) have been utilized for years, there is increasing concern about the implications of these platforms for mental, social, and physical health, as highlighted by the U.S.\n\nSurgeon General and various studies (Twenge et al., 2022; U.S.\n\nSurgeon General, 2023; Valkenburg et al., 2022).\n\nOne risk is that these features may relieve some of the tensions of human connection, leading people to preferentially spend more time with AI than humans or even form pseudo-social attachments to AI systems.\n\nAI-based chatbots are insufficient stand-ins for customary human interactions (which is likely true), then many negative consequences could result.\n\nHumans are social beings, so our biological systems can become dysregulated when social needs are unmet, leading to poorer health (Beckes and Sbarra, 2022).\n\nTherefore, it is essential that some key elements of customary human interactions be retained – for example, research finds that relative to emails and other text-based interactions, those involving human voice boost social connection (Kumar and Epley, 2021).\n\nAt the same time, AI-based chatbots could be useful to add social experiences for some individuals (while not completely replacing human-to-human interaction), particularly for those facing difficulties developing relationships on their own (who need “Vitamin S,” from Social contact, see Van Lange and Columbus, 2021), but are likely to be a poor or even dangerous replacement for human interaction writ large.\n\nIn sum, generative AI presents significant opportunities to alleviate inequalities in physical and mental health, in addition to augmenting healthcare providers’ capabilities.\n\nAdditionally, it is crucial to ensure that generative AI are only designed to supplement, rather than replace, human social interactions.\n\nExcessive dependence on AI for social engagement could lead to various adverse outcomes, including social isolation and deteriorating mental and physical health.\n\nTable 3 outlines key areas for future research.\n\nIn the concluding section, we offer policy recommendations designed to effectively integrate AI systems into healthcare frameworks, aiming to diminish healthcare disparities.",
    "# Impact on (mis)information\n\nGenerative AI offers a broad spectrum of benefits in the information domain.\n\nOne key advantage is the ability for personalization, where AI can tailor content to individual preferences, enhancing and customizing user experiences in areas such as education, entertainment, and news media.\n\nLanguage translation and localization capabilities of AI extend the reach of content globally, breaking language barriers and adapting material to different cultural contexts.\n\nImportantly, AI aids in making information more accessible, particularly for individuals with disabilities, by creating text-alternative formats like audio or simplified summaries.\n\nAI is also being explored to upscale and automate the fact-checking process, aiding the spread of accurate information (Hoes et al., 2023).\n\nConcerningly, new generative AI technology and sophisticated machine learning techniques may also enable companies and platforms to collect and deploy excessive amounts of information about individuals.\n\nThis will enable exploitation of consumers’ biases or vulnerabilities in order to capture more of the consumer surplus via price discrimination or violations of consumer privacy in processing and using that data without proper consent, leading to what has been named “the age of surveillance capitalism” (Acemoglu, 2024; Zuboff, 2023).\n\nA dominant firm organization model has emerged from these data monopolies, where internet platforms earn income by aiming to optimally market digital advertisements to users (Acemoglu and Johnson, 2023).\n\nThis sort of business strategy necessarily places a premium on user attention, which has led companies to deploy AI and machine learning techniques in ways intended to prolong user engagement, often to the detriment of users’ wellbeing (Wu, 2016; Brady et al., 2017; Acemoglu, Ozdaglar, et al., 2023; Acemoglu, 2023).\n\nRelatedly, companies that have access to more data may possess an anticompetitive advantage relative to competitors, enabling them to exercise market power to extract surplus and to relax price competition in the marketplace, which would be detrimental for consumers (Acemoglu, 2024).\n\nFurther, generative AI’s capacity to activate linguistic patterns, including persuasion and rhetoric, could facilitate more tailored advertisement.\n\nThe results would be particularly worrying if generative AI’s communicative capabilities are combined with the data infrastructure of social media platforms to automate social engineering (McNealy, 2022), with potential uses in areas such as political communication.\n\nAn issue at the center of the debate is misinformation.\n\nMalicious actors can exploit generative AI to create false information in ways that convincingly copy the style and content of human-created text (Buchanan et al., 2021; Kreps et al., 2022; Lazer et al., 2018; Spitale et al., 2023), by synthetically generating text, audio, images, and videos (\"deep fakes\").\n\nNew malicious generative AI tools like WormGPT (a ChatGPT alternative for designing and refining cyber-attack strategies and malware; The Hacker News, 2023) or PoisonGPT (a modified open-source AI model designed to spread misinformation within a public data repository that is used to train other AI models; Mithril Security, 2023) show that these tools can be used to accomplish malign aims and to sabotage further technology development.\n\nThe possibility to create information that is personalized or targeted to specific individuals and groups is likely to increase as well, especially during elections (Benson, 2023).\n\nPoliticians, including Republican presidential candidate Ron DeSantis, have already started using deep fakes in their political campaigns, such as fake images of Donald Trump hugging Anthony Fauci (McCarthy, 2023).\n\nManipulated political images make up a substantial portion (~20%) of visual misinformation on social media (Yang et al., 2023), and can be especially influential during elections and intergroup conflicts such as the Russo-Ukrainian and Israel-Gaza wars (Tworney et al., 2023).\n\nThere is currently no legislation preventing the use of deep fakes in political campaigns.\n\nAI-assisted misinformation can spread rapidly on social media and research has already shown that micro-targeting people with deep fakes can influence their attitudes toward politicians (e.g., Dobber et al., 2021).\n\nCoupled with the fact that people are largely unable to tell the difference between AI- and human-generated text (Kreps et al., 2022) and that AI has been shown to generate more convincing misinformation than humans (Spitale et al., 2023), there are concerns that generative AI may also increase the quantity of misinformation.\n\nIndeed, hundreds of unreliable AI-news websites have popped up (Newsguard, 2023) and even ChatGPT can easily be prompted to generate misinformation.\n\nThis increase in misinformation may have significant social consequences.",
    "This increase in misinformation may have significant social consequences.\n\nPolitical conspiracy theories and misinformation can affect voting decisions, health-related conspiracy theories can influence people’s medical choices (e.g., vaccination), and misinformation and conspiracy theories can fuel conflict between groups (Douglas, 2021).\n\nWhile some people will simply ignore online misinformation (Acerbi et al., 2022), this content is likely to penetrate specific groups, especially since AI helps automate the micro-targeting process in which thousands of persuasive messages can now be generated easily at scale.\n\nFor example, there is evidence that conservative voters were more susceptible to misinformation during the 2016 presidential election (Grinberg et al., 2019).\n\nConspiracy theories and misinformation online can also contribute to attitude polarization (Del Vicario et al., 2016) and undermine trust (Tworney et al., 2023).\n\nTherefore, regulation and interventions to limit the diffusion of AI-generated misinformation are needed.\n\nSimply warning people of deep fakes or including a tag clarifying whether a piece of content is AI- vs human-generated might backfire, as such tags have been shown to reduce the believability of true content as well (Longoni et al., 2022; Tworney et al., 2023).\n\nIn the realm of human-generated misinformation on social media, psychological interventions based on accuracy-salience and educational interventions based on inoculation theory improve the quality of information shared.\n\nFor example, making the concept of accuracy more salient can reduce the sharing of fake news, without adversely affecting the dissemination of accurate news (Pennycook et al., 2021; Pennycook and Rand, 2022).\n\nMoreover, endorsing accuracy not only decreases the sharing of false news but also increases the sharing of true news (Capraro and Celadin, 2023).\n\nInoculation theory or “prebunking” is a preemptive approach to countering misinformation that follows the vaccination analogy (Lu et al., 2023; McGuire, 1969; van der Linden, 2023).\n\nSeveral inoculation games and videos have been developed to expose subjects to controlled (i.e., weakened) doses of misinformation along with tools on how to spot it.\n\nFor instance, in the Bad News Game, participants create fake news in a simulated social media setting with the aim to gather as many followers as possible.\n\nThis activity makes them better at detecting online manipulation (Roozenbeek and Van der Linden, 2019; Traberg et al., 2022).\n\nSimilarly, in a field study on YouTube, videos containing micro-doses of common misinformation techniques increased discernment of online manipulation tactics (Roozenbeek et al., 2022).\n\nBecause prebunking conspiracy theories often works better than debunking them (Jolley and Douglas, 2017; Mason et al., 2023), future work could adapt these techniques to AI-generated news, especially in a way that mitigates cynicism of all (visual) media (Tworney et al., 2023).\n\nOne concern, however, is that interventions based on accuracy-salience and inoculation may be most effective for easily discernible misinformation.\n\nThe risk that generative AI makes misinformation more subtle and harder to discern may necessitate a new toolbox of interventions, specifically designed to counteract (visual) AI-generated misinformation.\n\nMoreover, generative AI could lead to entirely new challenges, such as tackling misinformation disseminated via one-to-one personalized communications (e.g., through bots).\n\nThis further highlights the urgency to adapt existing or develop a new set of intervention strategies (Feuerriegel et al., 2023).\n\nEven in the absence of malicious actors, the most advanced AI-systems are known to “hallucinate” false information in a very realistic manner (Bubeck et al., 2023).\n\nThese hallucinations may induce complex social dynamics, like self-fulfilling prophecies.\n\nDating back to Merton (1948), a self-fulfilling prophecy is an initially false prediction that becomes true just because someone—e.g., a generative AI system—asserts that it will become true.\n\nIn this sense, AI may produce prophecies that could “take a life for their own” (Citron and Pasquale, 2014).\n\nFor example, automated scoring systems that predict the likelihood of default on debt repayment or of a job applicant being a bad hire may contribute to (or even cause) credit or employment risk.\n\nOnline communications are another important area of concern.\n\nIndividuals may not know whether they are interacting with a person or a machine (Natale, 2021).\n\nThis is increasingly likely to be the case when people engage online with businesses and with public services.\n\nIf they believe, rightly or wrongly, that they are interacting with a machine, evidence suggests that their behavior is likely to change (March, 2021).",
    "If they believe, rightly or wrongly, that they are interacting with a machine, evidence suggests that their behavior is likely to change (March, 2021).\n\nHuman behavior tends to become more selfish in human-machine interactions because reciprocation—a vital factor in sustaining cooperative, prosocial behavior—is not maintained as consistently as in verifiably human-human interactions (Ishowo-Oloko et al., 2019; Makovi et al., 2023).\n\nGenerosity or cooperative behavior depends on people’s beliefs in each context about the relationship between the machine and the humans behind it (von Schenk et al., 2023).\n\nWhen interacting with a machine, people respond less emotionally, feeling less guilt about being ungenerous (Chugunova and Sele, 2022).\n\nThey become more likely to be dishonest in pursuit of monetary rewards (Cohn et al., 2022).\n\nAn outstanding research question concerns whether similar slippage from ethical standards occurs not only among people interacting with a machine, but also among those on the other side of the relationship who delegate to the machine (Köbis et al., 2021).\n\nA critical but mostly overlooked implication is the impact of generative AI on the plurality of information available on the web.\n\nCompanies including Microsoft and Google have envisioned integrating large language models such as ChatGPT and Bard into their proprietary search engines (recently, ChatGPT itself incorporated Bing search capabilities for some queries), but the implications of such a move have only started to be explored (Cutler, 2023).\n\nAmong the most significant implications is users’ access to information.\n\nSearch engines that mobilize generative AI to provide tailored recommendations to users are likely to restrict the plurality of information available on the Web.\n\nWhen users input a query into the current version of Google Search, for instance, they are pointed to a plurality of sources.\n\nAlthough users tend to select among the first results returned by the search engine (Goldman, 2008), the interface enables them to browse a large number of alternative results.\n\nThe same input directed to a search engine powered by generative AI will provide an extra layer of mediation that is likely to provide a much more limited amount of source information, unless specific design features are included to counteract this.\n\nThe tool will still give users the impression of access to vast, nearly unlimited information that users customarily attributed to the Web, but it will actually reduce control over Web access to users, affecting their capacity to browse, explore and retrieve information available through the Web (Natale and Cooke, 2021).\n\nIn addition to reducing access to information, generative AI may also threaten the quality and availability of online information.\n\nThe already-pervasive issue of bot accounts and autonomously generated content on social media and online message boards may be exacerbated by new generative technologies, which can assist in coding a multiplicity of these bots as well as providing text content for the bots to post (Ferrara et al., 2016; Yang and Menczer, 2023).\n\nThese tools could also be used to generate content optimized for search engines en masse, a useful tactic for businesses to “poach” traffic from competitors’ websites (e.g., Semrush, 2023).\n\nThis is a problem because current generative AI tools essentially provide an “average” response to a particular question, and these models are trained largely on text data collected from the internet; therefore, if the practice of generating content optimized for search engines at massive scale becomes common practice, both generative AI tools and online information may crater into an average of averages, lacking true insight, creativity, or novel ideas.\n\nAt the very least, it could make useful contributions difficult to identify within a sea of machine-generated content.\n\nBecause companies often perform thought leadership or produce marketing content that doubles as an informational resource, the proliferation of unoriginal content could make it harder for novices to find the information that they need, raising new barriers to entry for those seeking to gain subject matter knowledge online.\n\nAn analogous concern is that common message boards and websites for knowledge sharing (e.g., Stack Overflow) have experienced both a reduction in questions posted—especially the sorts of basic questions that ChatGPT does well at answering—and an increase in question responses, perhaps due to writing aid from tools like ChatGPT (Burtch et al., 2023; Shan & Qiu, 2023; del Rio-Chanona et al., 2023).\n\nThough these Q&A sites require competent subject-matter experts to provide insights and suggestions, they also require neophytes to ask those questions in the first place.",
    "Though these Q&A sites require competent subject-matter experts to provide insights and suggestions, they also require neophytes to ask those questions in the first place.\n\nReduced engagement by novice users not only has effects on the continued usefulness of these websites to aggregate and share knowledge, but also for innovation and creativity that may rely on content from these platforms as input.\n\nFor example, some coding languages encourage user-written programs to expand the capabilities of the language; it is likely that entrepreneurial contributors are influenced in their decisions to create new programs based on common questions or demand from users on topic-specific Q&A sites.\n\nIn the absence of questions being posted, user-contributions may not be optimally effective, since the user-feedback loops that developers rely upon are interrupted by the information provisional capabilities of generative AI platforms.\n\nIn net, this could forestall newcomers from acquiring specialized knowledge and technical skills, in addition to preventing the development of other innovative solutions.",
    "## Regulation of AI\n\nThe rapid popularization of generative AI models has prompted many governments worldwide to begin building regulatory frameworks.\n\nThe challenges raised by generative AI are global in nature (Jobin et al., 2019).\n\nHowever, the responses to these challenges so far have been specific to individual countries or areas.\n\nIn this article, we focus on the regulatory responses of the EU, US, and UK.\n\nRegulations are also being developed in other major countries, including China and India (Haridas et al., 2023; Roberts, 2023).\n\nThe European Union’s AI Act has emerged as one of the first major attempts to provide a legal framework for the development and deployment of AI (European Parliament, 2023a).\n\nThe Act aims to address the challenges posed by AI technologies while fostering innovation and trust in AI applications (European Parliament, 2023b).\n\nThis initiative comes with several pros.\n\nFirstly, it introduces a risk-based regulatory approach, distinguishing between high-risk and low-risk AI applications.\n\nThis categorization ensures that AI systems with significant implications for individuals’ rights and safety are subject to stricter scrutiny and compliance requirements.\n\nSecondly, the Act emphasizes transparency and accountability in AI systems, requiring clear information about how AI decisions are made, particularly in high-risk scenarios.\n\nAdditionally, the Act promotes ethical AI development, focusing on fundamental rights, non-discrimination, and privacy.\n\nHowever, the Act is not without its cons (Morgan Lewis, 2023).\n\nThe broad definitions and categories within the Act pose challenges, creating potential uncertainty for AI developers and users.\n\nFurther, the strict regulations might place EU companies at a competitive disadvantage globally, particularly against firms in regions with more lenient AI laws.\n\nIn contrast, the US has historically had a more fragmented approach, with various federal and state-level initiatives rather than a single, comprehensive legislative framework (Felz et al., 2022).\n\nThis approach has its advantages.\n\nFor one, it allows for more flexibility and adaptability in regulation, catering to the diverse range of AI applications and industries in the US.\n\nIt also promotes a more innovation-friendly environment by avoiding overly prescriptive rules that could hinder technological advancement.\n\nHowever, the US approach also has notable disadvantages.\n\nThe lack of a unified regulatory framework can lead to inconsistencies and uncertainties, potentially creating a complex patchwork of regulations for AI companies to navigate.\n\nThis fragmented approach might also lag in addressing broader ethical and social concerns about AI, such as privacy, bias, and accountability.\n\nFurther, without a cohesive national strategy, the US risks falling behind in setting global standards for AI governance.\n\nOn October 30, 2023, President Biden issued an Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, which directs the development of new guidelines, reports, and governance structures relating to AI, representing an effort to establish a more cohesive federal policy on AI (White House, 2023).\n\nIn the UK, the government has published a White Paper advocating for a pro-innovation approach, particularly in commercial applications of AI (Department for Science Innovation and Technology, 2023).\n\nWhile the White Paper recognizes the risks of AI and the challenge of building public trust, it refrains from proposing a regulatory framework to encourage innovation, which contrasts with the EU’s approach.\n\nInstead, the White Paper outlines some “cross-sectoral” non-statutory soft principles: safety, security, robustness, appropriate transparency and explainability, fairness, accountability and governance, and contestability and redress.\n\nThe White Paper opts against a specialist AI regulator, preferring to support existing regulators in integrating AI considerations.\n\nFurthermore, the focus on commercial innovations has drawn criticism for overlooking the increasing use of AI in government sectors like healthcare and education.",
    "Furthermore, the focus on commercial innovations has drawn criticism for overlooking the increasing use of AI in government sectors like healthcare and education.\n\nOne leading non-governmental organization, the Public Law Project, led a civil society coalition to produce Key principles for an alternative AI white paper (Public Law Project, 2023), which argues that an alternative vision is necessary as the “white paper, misses a vital opportunity to ensure that fundamental rights and democratic values are protected… it fails to ensure that adequate safeguards and standards are in place for use of AI by public authorities.” Amongst other proposals, the alternative white paper argues that: government use of AI must be transparent, transparency requirements must be mandatory, there must be clear mechanisms for accountability, the public should be consulted about new automated decision-making tools before they are deployed by government, there must be a specialist regulator to enforce the regulatory regime and ensure people can seek redress when things go wrong, and uses of AI that threaten fundamental rights should be prohibited.\n\nIn sum, the regulations of the EU, the US, and the UK do not pay sufficient attention to socioeconomic inequalities.\n\nIn the following, we outline several key interventions currently missing from these regulatory frameworks (Acemoglu, Autor et al., 2023).\n\nSee Table 5 for a summary.",
    "## Tax System\n\nCurrent tax codes in many developed countries often place a heavier burden on firms that hire labor than on those that invest in algorithms to automate work (Abbott and Bogenschneider, 2018; Acemoglu et al., 2020).\n\nThis has resulted in a lower share of income to labor while capital investments are rewarded.\n\nWe should aim to create a more symmetric tax structure, where marginal taxes for hiring (and training) labor and for investing in equipment/software are equated.\n\nThis will help to shift incentives toward human-complementary technological choices by reducing the bias of the tax code toward physical capital over human capital.",
    "## Labor Voice and Control of Consumer Information\n\nGiven that AI will have tremendous impact across industries and throughout society, it would be prudent to ensure that workers and civil society have a voice in this change.\n\nHealth and safety rules should also be updated accordingly.\n\nIn addition, data unions could be helpful to put the power and benefits of user data back in the hands of consumers.\n\nGiven the concerns that a handful of very large companies will control the direction of generative AI, it makes sense that users be compensated for the use of their information, or enabled to support other emergent competitors to predominant market players like Microsoft and Google.",
    "## Funding for More Human-Complementary Research\n\nBecause the current path of research is biased toward automation (Acemoglu and Restrepo, 2019; Autor et al., 2022; Acemoglu and Johnson, 2023), support for research and development of human-complementary AI technologies could offer strong upsides for growth.\n\nIt is most feasible to focus on specific sectors and activities where opportunities are already abundant.\n\nThese include education, healthcare, and modern craft worker training—where the information provisional capabilities of AI systems could boost productivity and enable workers to earn higher wages by augmenting their skills.\n\nIn the US, DARPA orchestrated investments and competitions to foster development of self-driving cars and dexterous robotics—in a similar fashion, governments should encourage competition and investment that pairs AI tools with human expertise, aiming to improve work in vital social sectors.",
    "## Professional Development and Training\n\nInvestment in professional development and training is crucial for professionals such as educators and healthcare workers to effectively integrate AI tools into their work.\n\nTraining programs should focus on the capabilities and limitations of AI, include ethical considerations, and teach technical skills required to interact with AI systems.\n\nSuch training will empower professionals to use AI as a complementary tool that enhances their skills.",
    "## Combating AI-Generated Misinformation\n\nGiven the substantial impact that generative AI can have on the quality and quantity of misinformation circulated online, especially in sensitive areas such as political campaigns and news media, it is critical for governments to invest in combating AI-generated misinformation.\n\nTools and standards to identify AI-generated content, including text, images, audio, and video, should be developed.\n\nAdditionally, educational campaigns should be initiated, to reduce general susceptibility to misinformation and provide the informed public with (currently deficient) fact-checking strategies.\n\nA task force composed of policymakers, technology companies, and social scientists could help develop practical methods to effectively combat a potential infodemic.",
    "## Governmental and Consultative Expertise\n\nTo foster human-complementary AI integration, it is fundamental to have AI expertise within the government.\n\nAI will touch every area of government investment, regulation, and oversight, including transportation, energy production, labor, healthcare, education, environmental protection, public safety, and military capacity.\n\nDeveloping consultative AI bodies that can advise governments and support the many agencies and regulators tackling these challenges will support more timely and effective decision-making.",
    "## Conclusion\n\nThe future will likely be starkly different from anything we have experienced before.\n\nBut the impact of generative AI is neither inherently positive nor negative.\n\nThe effects of generative AI will ultimately depend on the choices that we make to design and deploy the technology.\n\nWe stand at a unique and historical moment; our decisions and actions today will demonstrably shape the trajectory of our future.\n\nThis responsibility extends to all sectors of society, including governance, scientific research, industry, and the general public.\n\nIn this article, we have specifically focused on the socioeconomic inequalities that are likely to be impacted by the advent of generative AI.\n\nThis technology has profound implications for critical domains such as work, education, health, and information.\n\nFor instance, in the workplace, AI could automate some job tasks, create new work, change wage distributions, and require new skill sets.\n\nIn education, AI could democratize learning and provide personalized education solutions, but also raises concerns about the digital divide.\n\nIn the healthcare sector, AI’s ability to analyze large datasets can lead to better patient outcomes, but it also raises questions about equitable access to AI-driven healthcare services and the irreplaceable value of human interactions.\n\nIn the domain of information, AI has the potential to offer more tailored, efficient, and democratic ways to process information, yet it also poses challenges related to misinformation and diversity of thought.\n\nWe have outlined several research questions that urgently require answers to address these issues effectively.\n\nThese questions aim to harness AI’s benefits while mitigating its risks.\n\nAdditionally, we have observed that current regulatory approaches in the European Union, the United States, and the United Kingdom sometimes fail to adequately address these emerging challenges.\n\nThere is a need for a dynamic regulatory framework that can keep pace with the rapid advancements in AI technology.\n\nSee Figure 1 for an infographic summarizing the main points of the article.",
    "## References\n\n- Abbott, R., & Bogenschneider, B.\n\n(2018).\n\nShould Robots Pay Taxes?\n\nTax Policy in the Age of Automation.\n\nHarvard Law & Policy Review, 12, 145-175.\n\n- Abdou, C. M., & Fingerhut, A. W. (2014).\n\nStereotype threat among black and white women in healthcare settings.\n\nCulturally Diverse Ethnic Minority Psychology, 20, 316–323.\n\n- Acemoglu, D. (2023).\n\nWritten testimony for hearing on “The Philosophy of AI: Learning from History, Shaping Our Future.” Senate Committee on Homeland Security and Governmental Affairs.\n\nRetrieved from \n- Acemoglu, D. (2024).\n\nHarms of AI.\n\nIn J. Bullock (Ed.\n\n), The Oxford Handbook of AI Governance, forthcoming.\n\nOxford University Press.\n\nRetrieved December 4, 2023, from \n- Acemoglu, D., Ahmed, F., Hart, A.J., & Johnson, S. (2023).\n\nFrom Automation to Augmentation: Redefining Engineering Design and Manufacturing in the Age of NextGen AI.\n\nIn progress.\n\n- Acemoglu, D., Autor, D., Dorn, D., Hanson, G., & Price, B.\n\n(2016).\n\nImport competition and the great U.S. employment sag of the 2000s.\n\nJournal of Labor Economics, 34, 141-198.\n\n- Acemoglu, D., Autor, D., & Johnson, S. (2023).\n\nCan we have pro-worker AI?\n\nMIT Shaping the Future of Work Initiative, policy memo.\n\nRetrieved from \n- Acemoglu, D., & Johnson, S. (2023).\n\nPower and progress: Our 1000-year struggle over technology and prosperity.\n\nPublicAffairs, Hachette.\n\n- Acemoglu, D., Manera, A., & Restrepo, P. (2020).\n\nDoes the U.S. tax code favor automation?\n\nBrookings Papers on Economic Activity.\n\nRetrieved December 7, 2023, from \n- Acemoglu, D., Ozdaglar, A., & Siderius, J.\n\n(2023).\n\nA Model of Online Misinformation.\n\nReview of Economic Studies, forthcoming.\n\n- Acemoglu, D., & Restrepo, P. (2018).\n\nThe race between man and machine: Implications of technology for growth, factor shares, and employment.\n\nAmerican Economic Review, 108, 1488-1542.\n\n- Acemoglu, D., & Restrepo, P. (2019).\n\nAutomation and new tasks: How technology displaces and reinstates labor.\n\nJournal of Economic Perspectives, 33, 3-30.\n\n- Acemoglu, D., & Restrepo, P. (2022a).\n\nTasks, automation, and the rise in U.S. wage inequality.\n\nEconometrica, 90, 1973-2016.\n\n- Acemoglu, D., & Restrepo, P. (2022b).\n\nDemographics and automation.\n\nReview of Economic Studies, 89, 1-44.\n\n- Acerbi, A., Altay, S., & Mercier, H. (2022).\n\nResearch note: Fighting misinformation or fighting for information?\n\nHarvard Kennedy School (HKS) Misinformation Review, 3.\n\n- Agarwal, N., Moehring, A., Rajpurkar, P., & Salz, T. (2023).\n\nCombining Human Expertise with Artificial Intelligence: Experimental Evidence from Radiology.\n\nNBER Working Paper No.\n\n31422.\n\n- Akgun, S. & Greenhow, C. (2022).\n\nArtificial intelligence in education: Addressing ethical challenges in K-12 settings.\n\nAI Ethics, 2, 431–440.\n\n- Aksoy, C. G., Barrero, J. M., Bloom, N., Davis, S. J., Dolls, M., & Zarate, P. (2023).\n\nWorking from home around the globe: 2023 Report (No.\n\n53).\n\nEconPol Policy Brief.\n\n- Andreessen, M. (2023).\n\nThe Techno-Optimist Manifesto.\n\nAndreessen Horowitz.\n\n- Aristidou, A., Jena, R., & Topol, E. J.\n\n(2022).\n\nBridging the chasm between AI and clinical implementation.\n\nLancet, 399, 620.\n\n- Atari, M., Xue, M. J., Park, P. S., Blasi, D. E., & Henrich, J.\n\n(2023).\n\nWhich Humans?\n\nAvailable at: \n- Autor, D. (2019).\n\nWork of the Past, Work of the Future.\n\nAEA Papers and Proceedings, 109, 1–32.\n\n- Autor, D., Chin, C., Salomons, A., & Seegmiller, B.\n\n(2022).\n\nNew frontiers: The origins and content of new work, 1940–2018.\n\nNBER Working Paper no.\n\n30389.\n\n- Autor, D., Katz, L., & Krueger, A.\n\n(1998).\n\nComputing inequality: Have computers changed the labor market?\n\nQuarterly Journal of Economics, 113, 1169–1213.\n\n- Awad, E., Dsouza, S., Kim, R., Schulz, J., Henrich, J., Shariff, A., Bonnefon, J. F., & Rahwan, I.\n\n(2018).\n\nThe moral machine experiment.\n\nNature, 563, 59-64.\n\n- Ayers, J. W. et al (2023).\n\nComparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum.\n\nJAMA Internal Medicine, 183, 589-596.\n\n- Baker, R. & Hawn, A.\n\n(2022).\n\nAlgorithmic bias in education.\n\nInternational Journal of Artificial Intelligence in Education, 32, 1052–1092.\n\n- Bakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J., ... & Summerfield, C. (2022).\n\nFine-tuning language models to find agreement among humans with diverse preferences.\n\nAdvances in Neural Information Processing Systems, 35, 38176-38189.\n\n- Beckes, L., & Sbarra, D. A.\n\n(2022).\n\nSocial baseline theory: State of the science and new directions.\n\nCurrent Opinion in Psychology, 43, 36-41.\n\n- Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021).\n\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\n\nProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–623.\n\n- Benson, T. (2023).\n\nThis disinformation is just for you.\n\nWired.\n\n- Bostrom, N. (2003).\n\nEthical Issues in Advanced Artificial Intelligence.",
    "- Benson, T. (2023).\n\nThis disinformation is just for you.\n\nWired.\n\n- Bostrom, N. (2003).\n\nEthical Issues in Advanced Artificial Intelligence.\n\nFrom Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, ed.\n\nSmit, I., et al.\n\nInstitute of Advanced Studies in Systems Research and Cybernetics.\n\n2:12–17.\n\n- Brady, W., Wills, J., Jost, J., Tucker, J., & Van Bavel, J.\n\n(2017).\n\nEmotion Shapes the Diffusion of Moralized Content in Social Networks.\n\nProceedings of the National Academy of Sciences, 114, 7313–7318.\n\n- Breakstone, J., Smith, M., Wineburg, S., Rapaport, A., Carle, J., Garland, M., & Saavedra, A.\n\n(2021).\n\nStudents’ civic online reasoning: A national portrait.\n\nEducational Researcher, 50, 505-515.\n\n- Brynjolfsson, E., Li, D., & Raymond, L. (2023).\n\nGenerative AI at work.\n\nNBER Working Paper No.\n\n31161.\n\n- Brynjolfsson, E., & McAfee, A.\n\n(2016).\n\nThe second machine age: Work, progress, and prosperity in a time of brilliant technologies.\n\nW.W. Norton.\n\n- Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., ... & Zhang, Y.\n\n(2023).\n\nSparks of artificial general intelligence: Early experiments with gpt-4.\n\nAvailable at: \n- Buchanan, B., Lohn, A., Musser, M., & Sedova, K. (2021).\n\nTruth, lies, and automation: How language models could change disinformation.\n\nCenter for Security and Emerging technology.\n\n- Burtch, G., Lee, D., & Chen, Z.\n\n(2023).\n\nThe Consequences of Generative AI for UGC and Online Community Engagement.\n\nAvailable at \n- Cadario, R., Longoni, C., & Morewedge, C. K. (2021).\n\nUnderstanding, explaining, and utilizing medical artificial intelligence.\n\nNature Human Behaviour, 5, 1636-1645.\n\n- Campbell, J.\n\n(2023).\n\nAI’s future: Utopia or dystopia?\n\nExperts weigh in on five possible outcomes.\n\nAI News Today.\n\n- Capraro, V., & Celadin, T. (2023).\n\n“I think this news is accurate”: Endorsing accuracy decreases the sharing of fake news and increases the sharing of real news.\n\nPersonality and Social Psychology Bulletin, 49, 1635-1645.\n\n- Cené, C. W., et al.\n\n(2022).\n\nEffects of Objective and Perceived Social Isolation on Cardiovascular and Brain Health: A Scientific Statement From the American Heart Association.\n\nJournal of the American Heart Association, 11, e026493.\n\n- Citron, D. K. & Pasquale, F. A.\n\n(2014).\n\nThe scored society: Due process for automated predictions.\n\nWashington Law Review, 89, 1-33.\n\n- Chan, C. K. Y.\n\n& Hu, W. (2023).\n\nStudents’ voices on generative AI: Perceptions, benefits,",
    "# References List\n\nand challenges in higher education.\n\nInternational Journal of Educational Technology in Higher Education, 20(1).\n\nChiu, T. K. F. (2023).\n\nThe impact of Generative AI (GenAI) on practices, policies and research direction in education: A case of ChatGPT and Midjourney.\n\nInteractive Learning Environments.\n\nChugunova, M., & Sele, D. (2022).\n\nWe and It: An interdisciplinary review of the experimental evidence on how humans interact with machines.\n\nJournal of Behavioral and Experimental Economics, 99, 101897.\n\nCohn, A., Gesche, T., & Maréchal, M. A.\n\n(2022).\n\nHonesty in the digital age.\n\nManagement Science, 68, 827-845.\n\nCounts, S., Suri, S., Brown, A., Xu, B., R Raghavan, S. (2022).\n\nWho gets to work in the digital economy?\n\nBusiness and Society.\n\nAvailable at: \n\nCuevas, A. G., O’Brien, K., & Saha, S. (2016).\n\nAfrican American experiences in healthcare: “I always feel like I’m getting skipped over”.\n\nHealth Psychology, 35, 987–995.\n\nCutler, K. (2023).\n\nChatGPT and search engine optimisation: The future is here.\n\nApplied Marketing Analytics, 9, 8-22.\n\nDel Carmen, M. G., Herman, J., Rao, S., Hidrue, M. K., Ting, D., Lehrhoff, S. R., ... & Ferris, T. G. (2019).\n\nTrends and factors associated with physician burnout at a multispecialty academic faculty practice organization.\n\nJAMA Network Open, 2, e190554-e190554.\n\ndel Rio-Chanona, M., Laurentsyeva, N., & Wachs, J.\n\n(2023).\n\nAre Large Language Models a Threat to Digital Public Goods?\n\nEvidence from Activity on Stack Overflow.\n\nAvailable at \n\nDell’Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., ... & Lakhani, K. R. (2023).\n\nNavigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality.\n\nHarvard Business School Technology & Operations Mgt.\n\nUnit Working Paper, (24-013).\n\nDepartment for Science, Innovation, and Technology (2023).\n\nAI-regulation: A pro-innovation approach.\n\nDel Vicario, M., Bessi, A., Zollo, F., & Quattrociocchi, W. (2016).\n\nThe spreading of misinformation online.\n\nProceedings of the National Academy of Sciences, 113.\n\n554-559.\n\nDobber, T., Metoui, N., Trilling, D., Helberger, N., & de Vreese, C. (2021).\n\nDo (microtargeted) deepfakes have real effects on political attitudes?.\n\nThe International Journal of Press/Politics, 26, 69-91.\n\nDouglas, K. M. (2021).\n\nAre conspiracy theories harmless?\n\nThe Spanish Journal of Psychology, 21, e13.\n\nJolley, D., & Douglas, K. M. (2017).\n\nPrevention is better than cure: Addressing anti‐vaccine conspiracy theories.\n\nJournal of Applied Social Psychology, 47, 459-469.\n\nDovidio, J. F., Penner, L. A., Albrecht, T. L., Norton, W. E., Gaertner, S. L., & Shelton, J. N. (2008).\n\nDisparities and distrust: The implications of psychological processes for understanding racial disparities in health and health care.\n\nSocial Sciences and Medical Journal, 67, 478–486.\n\nEling, M., Nuessle, D. & Staubli, J.\n\n(2022).\n\nThe impact of artificial intelligence along the insurance value chain and on the insurability of risks.\n\nThe Geneva Papers on Risk and Insurance: Issues and Practice, 47, 205–241.\n\nElsen-Rooney, M. (2023).\n\nNYC education department blocks ChatGPT on school devices, networks.\n\nEngelbart, D. C. (1995).\n\nToward augmenting the human intellect and boosting our collective IQ.\n\nCommunications of the ACM, 38, 30-32.\n\nEuropean Parliament (2023a).\n\nEuropean Parliament (2023b).\n\nFelz, D. J., Peretti, K. K., & Austin, A.\n\n(2022).\n\nPrivacy, cyber and data strategy advisor: AI regulation in the U.S.: What’s coming, and what companies need to do in 2023.\n\nFerrara, E., Varol, O. Davis, C., Menczer, F. & Flammini, A.\n\n(2016).\n\nThe rise of social bots.\n\nCommunications of the Association for Computing Machinery, 59(7): 96–104.\n\nFeuerriegel, S., DiResta, R., Goldstein, J.\n\nA., Kumar, S., Lorenz-Spreen, P., Tomz, M., & Pröllochs, N. (2023).\n\nResearch can help to tackle AI-generated disinformation.\n\nNature Human Behaviour, 1-4.\n\nFloridi, L., & Cowls, J.\n\n(2019).\n\nA unified framework of five principles for AI in society.\n\nHarvard Data Science Review.\n\nGoldin, C., & Katz, L. (2008).\n\n“The evolution of U.S. educational wage differentials, 1890 to 2005.” The race between education and technology, Harvard University Press.\n\nChapter 8.\n\nGoldman, E. (2008).\n\nSearch engine bias and the demise of search engine utopianism.\n\nIn: Spink A and Zimmer M (eds) Web Search: Multidisciplinary Perspectives.\n\nBerlin: Springer, pp.121–133.\n\nGreene, J. D., Sommerville, R. B., Nystrom, L. E., Darley, J. M., & Cohen, J. D. (2001).\n\nAn fMRI investigation of emotional engagement in moral judgment.\n\nScience, 293, 2105-2108.\n\nGrinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B., & Lazer, D. (2019).\n\nFake news on Twitter during the 2016 U.S. presidential election.\n\nScience, 363, 374–378.\n\nHaridas, G., Sohee, S. K., & Brahmecha, A.\n\n(2023).\n\nThe key policy frameworks governing AI in India.\n\nHoes, E., Altay, S., & Bermeo, J.\n\n(2023).",
    "Science, 363, 374–378.\n\nHaridas, G., Sohee, S. K., & Brahmecha, A.\n\n(2023).\n\nThe key policy frameworks governing AI in India.\n\nHoes, E., Altay, S., & Bermeo, J.\n\n(2023).\n\nLeveraging ChatGPT for efficient fact-checking.\n\nAvailable at \n\nHolt-Lunstad, J.\n\n(2021).\n\nThe Major Health Implications of Social Connection.\n\nCurrent Directions in Psychological Science, 30, 251-259.\n\nHolt-Lunstad, J.\n\n(2022).\n\nSocial Connection as a Public Health Issue: The Evidence and a Systemic Framework for Prioritizing the “Social” in Social Determinants of Health.\n\nAnnual Review of Public Health, 43, 193-213.\n\nHolt-Lunstad, J., Smith, T. B., Layton, J.\n\nB.\n\n(2010).\n\nSocial Relationships and Mortality Risk: A Meta-analytic Review.\n\nPLOS Medicine, 7, e1000316.\n\nHommel, K., Madsen, M., & Kamper, A. L. (2012).\n\nThe importance of early referral for the treatment of chronic kidney disease: A Danish nationwide cohort study.\n\nBMC Nephrology, 13, 108–116.\n\nIshowo-Oloko, F., Bonnefon, J. F., Soroye, Z., Crandall, J., Rahwan, I., & Rahwan, T. (2019).\n\nBehavioural evidence for a transparency–efficiency tradeoff in human–machine cooperation.\n\nNature Machine Intelligence, 1, 517-521.\n\nJobin, A., Ienca, M., & Vayena, E. (2019).\n\nThe global landscape of AI ethics guidelines.\n\nNature Machine Intelligence, 1, 389-399.\n\nJohnson, M. (2021).\n\nA scalable approach to reducing gender bias in Google translate.\n\nJohnson, M., Albizri, A.\n\n& Harfouche, A.\n\n(2021).\n\nResponsible Artificial Intelligence in Healthcare: Predicting and Preventing Insurance Claim Denials for Economic and Social Wellbeing.\n\nInformation Systems Frontiers, 25, 2179-2195.\n\nJohnson, B. S., et al.\n\n(2023).\n\nUsing ChatGPT to evaluate cancer myths and misconceptions: artificial intelligence and cancer information.\n\nJNCI Cancer Spectrum, 7, pkad015.\n\nKahane, G., Everett, J.\n\nA., Earp, B. D., Caviola, L., Faber, N. S., Crockett, M. J., & Savulescu, J.\n\n(2018).\n\nBeyond sacrificial harm: A two-dimensional model of utilitarian psychology.\n\nPsychological Review, 125, 131-164.\n\nKanjee, Z., Crowe, B., & Rodman, A.\n\n(2023).\n\nAccuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.\n\nJAMA, 330, 78–80.\n\nKasneci, E. et al.\n\n(2023).\n\nChatGPT for good?\n\nOn opportunities and challenges of large language models for education.\n\nLearning and Individual Differences, 103, 102274.\n\nKöbis, N., Bonnefon, J. F., & Rahwan, I.\n\n(2021).\n\nBad machines corrupt good morals.\n\nNature Human Behaviour, 5, 679-685.\n\nKorinek, A.\n\n(2023).\n\nGenerative AI for economic research: Use cases and implications for economists.\n\nJournal of Economic Literature, 61: 1281–1317.\n\nKranzberg, M. (1985).\n\nThe information age: Evolution or revolution.\n\nInformation technologies and social transformation, 35-54.\n\nKreps, S., McCain, R., & Brundage, M. (2022).\n\nAll the news that’s fit to fabricate: AI-generated text as a tool of media misinformation.\n\nJournal of Experimental Political Science, 9, 104-117.\n\nKumar, A., & Epley, N. (2021).\n\nIt’s surprisingly nice to hear you: Misunderstanding the impact of communication media can lead to suboptimal choices of how to connect with others.\n\nJournal of Experimental Psychology: General, 150, 595-607.\n\nLazer, D. M., Baum, M. A., Benkler, Y., Berinsky, A. J., Greenhill, K. M., Menczer, F., ... & Zittrain, J. L. (2018).\n\nThe science of fake news.\n\nScience, 359, 1094-1096.\n\nLeigh-Hunt, N., et al.\n\n(2017).\n\nAn overview of systematic reviews on the public health consequences of social isolation and loneliness.\n\nPublic Health, 152, 157-171.\n\nLicklider, J. C. (1960).\n\nMan-computer symbiosis.\n\nIRE Transactions on Human Factors in Electronics, 4-11.\n\nLiu, M., et al.\n\n(2023).\n\nFuture of education in the era of generative artificial intelligence: Consensus among Chinese scholars on applications of ChatGPT in schools.\n\nFuture Education Research, 1, 72–101.\n\nLongoni, C., Bonezzi, A., & Morewedge, C. K. (2019).\n\nResistance to medical artificial intelligence.\n\nJournal of Consumer Research, 46, 629-650.\n\nLongoni, C., Fradkin, A., Cian, L., & Pennycook, G. (2022).\n\nNews from generative artificial intelligence is believed less.\n\nIn Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (pp.\n\n97-106).\n\nLópez-Cevallos, D. F., Harvey, S. M., & Warren, J. T. (2014).\n\nMedical mistrust, perceived discrimination, and satisfaction with health care among young-adult rural Latinos.\n\nThe Journal of Rural Health, 30, 344–351.\n\nLu, C., Hu, B., Li, Q., Bi, C., & Ju, X. D. (2023).\n\nPsychological Inoculation for Credibility Assessment, Sharing Intention, and Discernment of Misinformation: Systematic Review and Meta-Analysis.\n\nJournal of Medical Internet Research, 25, e49255.\n\nMadan, R., & Ashok, M. (2023).\n\nAI adoption and diffusion in public administration: A systematic literature review and future research agenda.\n\nGovernment Information Quarterly, 40, 101774.\n\nMakovi, K., Sargsyan, A., Li, W., Bonnefon, J. F., & Rahwan, T. (2023).",
    "Government Information Quarterly, 40, 101774.\n\nMakovi, K., Sargsyan, A., Li, W., Bonnefon, J. F., & Rahwan, T. (2023).\n\nTrust within human-machine collectives depends on the perceived consensus about cooperative norms.\n\nNature Communications, 14, 3108.\n\nMarch, C. (2021).\n\nStrategic interactions between humans and artificial intelligence: Lessons from experiments with computer players.\n\nJournal of Economic Psychology, 87, 102426.\n\nMason, A. M., Compton, J., Tice, E., Peterson, B., Lewis, I., Glenn, T., & Combs, T. (2023).\n\nAnalyzing the Prophylactic and Therapeutic Role of Inoculation to Facilitate Resistance to Conspiracy Theory Beliefs.\n\nCommunication Reports, 1-15.\n\nMcCarthy, B.\n\n(June 7, 2023).\n\nRon DeSantis ad uses AI-generated photos of Trump, Fauci.\n\nAFP.\n\nMcGuire, W. J.\n\n(1964).\n\nSome contemporary approaches.\n\nIn Advances in experimental social psychology (Vol.\n\n1, pp.\n\n191-229).\n\nAcademic Press.\n\nMcGrew, S. (2024).\n\nTeaching Lateral Reading: Interventions to Help People Read like Fact Checkers.\n\nCurrent Opinion in Psychology, 55, 101737.\n\nMcNealy, J. E. (2022).\n\nPlatforms as phish farms: Deceptive social engineering at scale.\n\nNew Media & Society, 24, 1677–1694.\n\nMerton, R. K. (1948).\n\nThe Self-Fulfilling Prophecy.\n\nThe Antioch Review, 8, 193-210.\n\nMill, J. S. (1861/2016).\n\nUtilitarianism.\n\nIn Seven masterpieces of philosophy (pp.\n\n329-375).\n\nRoutledge.\n\nMithril Security (2023).\n\nPoisonGPT: How we hid a lobotomized LLM on hugging face to spread fake news.\n\nMorewedge, C. K., Mullainathan, S., Naushan, H. F., Sunstein, C. R., Kleinberg, J., Raghavan, M., & Ludwig, J. O.\n\n(2023).\n\nHuman bias in algorithm design.\n\nNature Human Behaviour, 7, 1822–1824.\n\nMorgan Lewis (2023).\n\nNatale, S. (2021).\n\nDeceitful Media: Artificial Intelligence and Social Life after the Turing Test.\n\nOxford University Press.\n\nNatale, S., & Cooke, H. (2021).\n\nBrowsing with Alexa: Interrogating the impact of voice assistants as web interfaces.\n\nMedia, Culture & Society, 43, 1000-1016.\n\nNov, O., Singh, N., & Mann, D. M. (2023).\n\nPutting ChatGPT’s medical advice to the (Turing) Test.\n\nAvailable at \n\nNoy, S., & Zhang, W. (2023).\n\nExperimental evidence on the productivity effects of generative artificial intelligence.\n\nScience, 381, 187–192.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022).\n\nTraining language models to follow instructions with human feedback.\n\nAdvances in Neural Information Processing Systems, 35, 27730-27744.\n\nPasquale, F. (2020).\n\nNew laws of robotics.\n\nHarvard University Press.\n\nPatel, S. B., & Lam, K. (2023).\n\nChatGPT: the future of discharge summaries?\n\nLancet Digit Health, 5, e107–e108.\n\nPencheva, I., Esteve, M., & Mikhaylov, S. J.\n\n(2020).\n\nBig Data and AI–A transformational shift for government: So, what next for research?.\n\nPublic Policy and Administration, 35, 24-44.\n\nPeng, S., Kalliamvakou, E., Cihon, P., & Demirer, M. (2023).\n\nThe impact of AI on developer productivity: Evidence from GitHub Copilot.\n\nAvailable at \n\nPennycook, G., Epstein, Z., Mosleh, M., Arechar, A.\n\nA., Eckles, D., & Rand, D. G. (2021).\n\nShifting attention to accuracy can reduce misinformation online.\n\nNature, 592, 590-595.\n\nPennycook, G., & Rand, D. G. (2022).\n\nAccuracy prompts are a replicable and generalizable approach for reducing the spread of misinformation.\n\nNature Communications, 13, 2333.\n\nPenninkilampi, R., Casey, A. N., Singh, M. F., & Brodaty, H. (2018).\n\nThe association between social engagement, loneliness, and risk of dementia: a systematic review and meta-analysis.\n\nJournal of Alzheimer’s Disease, 66, 1619-1633.\n\nPeters, U.\n\n(2022).\n\nAlgorithmic political bias in artificial intelligence systems.\n\nPhilosophy & Technology, 35, 25.\n\nPromberger, M., & Baron, J.\n\n(2006).\n\nDo patients trust computers?.\n\nJournal of Behavioral Decision Making, 19, 455-468.\n\nPublic Law Project (2023).\n\nKey principles for an alternative AI white paper.\n\nQuinn, T. P., Senadeera, M., Jacobs, S., Coghlan, S., & Le, V. (2021).\n\nTrust and medical AI: the challenges we face and the expertise needed to overcome them.\n\nJournal of the American Medical Informatics Association, 28, 890-894.\n\nRahwan, I.\n\n(2018).\n\nSociety-in-the-loop: programming the algorithmic social contract.\n\nEthics and Information Technology, 20, 5-14.\n\nRahwan, I., Cebrian, M., Obradovich, N., Bongard, J., Bonnefon, J. F., Breazeal, C., ... & Wellman, M. (2019).\n\nMachine behaviour.\n\nNature, 568, 477-486.\n\nRathbone, J.\n\nA., Cruwys, T., Jetten, J., & Barlow, F. K. (2020).\n\nWhen stigma is the norm: How weight and social norms influence the healthcare we receive.\n\nJournal of Applied Social Psychology, 53, 185-201.\n\nRen, F., Zhou, Y.\n\n(2020).\n\nCGMVQA: A new classification and generative model for medical visual question answering.\n\nIEEE Access, 8, 50626–50636.\n\nRestrepo, P. (2023).\n\nAutomation: Theory, Evidence, and Outlook.\n\nNBER Working Paper No.\n\n31910.\n\nReverberi, C., Rigon, T., Solari, A., Hassan, C., Cherubini, P., & Cherubini, A.\n\n(2022).",
    "Restrepo, P. (2023).\n\nAutomation: Theory, Evidence, and Outlook.\n\nNBER Working Paper No.\n\n31910.\n\nReverberi, C., Rigon, T., Solari, A., Hassan, C., Cherubini, P., & Cherubini, A.\n\n(2022).\n\nExperimental evidence of effective human–AI collaboration in medical decision-making.\n\nScientific Reports, 12, 14952.\n\nRoberts, H. (2023).\n\nThe future of AI policy in China.\n\nEast Asia Forum.\n\nRoozenbeek, J., & Van der Linden, S. (2019).\n\nFake news game confers psychological resistance against online misinformation.\n\nPalgrave Communications, 5, 1-10.\n\nRoozenbeek, J., Van Der Linden, S., Goldberg, B., Rathje, S., & Lewandowsky, S. (2022).\n\nPsychological inoculation improves resilience against misinformation on social media.\n\nScience Advances, 8, eabo6254.\n\nSanturkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., & Hashimoto, T. (2023).\n\nWhose opinions do language models reflect?\n\nAvailable at \n\nSanty, S., Liang, J. T., Bras, R. L., Reinecke, K., & Sap, M. (2023).\n\nNLPositionality: Characterizing Design Biases of Datasets and Models.\n\nAvailable at \n\nSelwyn, N., Hillman, T., Bergviken Rensfeldt, A., & Perrotta, C. (2023).\n\nDigital technologies and the automation of education.\n\nPostdigital Science and Education, 5, 15-24\n\nSemrush Team.\n\n(2023).\n\nMaximizing SEO Impact with ChatGPT: A Comprehensive Guide.\n\nSemrush Blog.\n\nSeptiandri, A.A., Constantinides, M., & Quercia, D. (2023).\n\nThe impact of AI innovations on U.S. occupations.\n\nNokia Bell Labs, Cambridge, UK, work in progress.\n\nShan, G., & Qiu, L. (2023).\n\nExamining the Impact of Generative AI on Users’ Voluntary Knowledge Contribution: Evidence from A Natural Experiment on Stack Overflow.\n\nAvailable at \n\nSpitale, G., Biller-Andorno, N., & Germani, F. (2023).\n\nAI model GPT-3 (dis)informs us better than humans.\n\nScience, 9(26), eadh1850.\n\nStoyanovich, J., Van Bavel, J. J., & West, T. V. (2020).\n\nThe imperative of interpretable machines.\n\nNature Machine Intelligence, 2, 197-199.\n\nThe Hacker News (2023).\n\nWormGPT: New AI tool allows cybercriminals to launch sophisticated cyber attacks.\n\nThe Lancet Regional Health – Europe, Embracing generative AI in health care (2023).\n\nThe Lancet Regional Health – Europe, 30.\n\nTopol, E. J.\n\n(2019).\n\nHigh-performance medicine: the convergence of human and artificial intelligence.\n\nNature Medicine, 2, 44–56.\n\nTraberg, C. S., Roozenbeek, J., & van der Linden, S. (2022).\n\nPsychological inoculation against misinformation: Current evidence and future directions.\n\nThe ANNALS of the American Academy of Political and Social Science, 700(1), 136-151.\n\nTwenge, J. M., Haidt, J., Lozano, J., & Cummins, K. M. (2022).\n\nSpecification curve analysis shows that social media use is linked to poor mental health, especially among girls.\n\nActa Psychologica, 224.\n\nTwomey, J., Ching, D., Aylett, M. P., Quayle, M., Linehan, C., & Murphy, G. (2023).\n\nDo deepfake videos undermine our epistemic trust?\n\nA thematic analysis of tweets that discuss deepfakes in the Russian invasion of Ukraine.\n\nPlos One, 18, e0291668.\n\nU.S.\n\nSurgeon General (2023).\n\nSocial media and youth mental health.\n\nAvailable at: \n\nValkenburg, P. M., Meier, A., & Beyens, I.\n\n(2022).\n\nSocial media use and its impact on adolescent mental health: An umbrella review of the evidence.\n\nCurrent Opinion in Psychology, 44, 58-68.\n\nValtorta, N. K., Kanaan, M., Gilbody, S., Ronzi, S., & Hanratty, B.\n\n(2016).\n\nLoneliness and social isolation as risk factors for coronary heart disease and stroke: Systematic review and meta-analysis of longitudinal observational studies.\n\nHeart, 102, 1009-1016.\n\nVan der Linden, S. (2023).\n\nFoolproof: Why Misinformation Infects our Minds and How to Build Immunity.\n\nNew York, NY: WW Norton.\n\nvan Lange, P. A. M., & Columbus, S. (2021).\n\nVitamin S: Why is social contact, even with strangers, so important to well-being?.\n\nCurrent Directions in Psychological Science, 30, 267-273.\n\nvon Schenk, A., Klockmann, V., & Köbis, N. (2023).\n\nSocial Preferences Toward Humans and Machines: A Systematic Experiment on the Role of Machine Payoffs.\n\nPerspectives on Psychological Science, in press.\n\nWang, F., Gao, Y., Han, Z., et al.\n\n(2023).\n\nA systematic review and meta-analysis of 90 cohort studies of social isolation, loneliness, and mortality.\n\nNature Human Behaviour, 7, 1307-1319.\n\nWhite House (2023).\n\nFACT SHEET: President Biden issues executive order on sage, secure, and trustworthy artificial intelligence.\n\nWietzke, F.B., & McLeod, C. (2013).\n\nJobs, Wellbeing, and Social Cohesion: Evidence from Value and Perception Surveys.\n\nWorld Bank Policy Research Working Papers, 6447.\n\nWilliams, D. R. (2005).\n\nThe health of U.S. racial and ethnic populations.\n\nThe Journals of Gerontology Series B: Psychological Sciences and Social Sciences, 60, 53–62.\n\nWu, T. (2016).\n\nThe Attention Merchants: The Epic Scramble to Get Inside Our Heads.\n\nPRH Knopf, New York.\n\nYang, K.C., & Menczer, F. (2023).\n\nAnatomy of an AI-powered malicious social botnet.\n\nAvailable at \n\nYang, Y., Davis, T., & Hindman, M. (2023).",
    "PRH Knopf, New York.\n\nYang, K.C., & Menczer, F. (2023).\n\nAnatomy of an AI-powered malicious social botnet.\n\nAvailable at \n\nYang, Y., Davis, T., & Hindman, M. (2023).\n\nVisual misinformation on Facebook.\n\nJournal of Communication, 73, 316-328.\n\nYin, J., Ngiam, K. Y., & Teo, H. H. (2021).\n\nRole of artificial intelligence applications in real-life clinical practice: systematic review.\n\nJournal of Medical Internet Research, 23, e25759.\n\nZheng, S., Trott, A., Srinivasa, S., Parkes, D. C., & Socher, R. (2022).\n\nThe AI Economist: Taxation policy design via two-level deep multiagent reinforcement learning.\n\nScience Advances, 8, eabk2607.\n\nZuboff, S. (2023).\n\nThe age of surveillance capitalism.\n\nIn Social Theory Re-Wired (pp.\n\n203-213).\n\nRoutledge.\n\nZuiderwijk, A., Chen, Y. C., & Salem, F. (2021).\n\nImplications of the use of artificial intelligence in public governance: A systematic literature review and a research agenda.\n\nGovernment Information Quarterly, 38, 101577.",
    "## INTRODUCTION\n\nArtiﬁcial Intelligence (AI) plays an ever-increasing role in our daily lives and has inﬂuenced ﬁelds from online advertising to sales and from the military to healthcare.\n\nWith the ongoing AI arms race in the Russia-Ukraine War, it is expected that AI-powered lethal weapon systems will become commonplace in warfare1.\n\nAlthough AI has shown promise in numerous successful applications, there remains a pressing need to address ethical concerns associated with these applications.\n\nThere are dire consequences if an AI system selects an incorrect target potentially killing non-combatants or friendly forces.\n\nSeeing the rapid emergence of AI and its applications in the military, the United States Department of Defense (DOD) disclosed ethical principles for AI in 20202.\n\nThis document emphasized ﬁve core principles, aiming for responsible, equitable, traceable, reliable, and governable AI2.\n\nIn addition, the North Atlantic Treaty Organization (NATO) also released principles for the use of AI in military, including lawfulness, responsibility and accountability, explainability and traceability, reliability, gover­nability, and bias mitigation3.\n\nThe success of these ethical principles has also been demonstrated through their ability to adopt and embed AI mindfully, taking into account AI’s potential dangers, which the Pentagon is determined to avoid4.\n\nClearly, prominent military organizations demonstrate a cautious approach toward adopting AI and are actively implementing measures to mitigate the risks associated with its potential malicious uses and applications.\n\nOn the other hand, AI has had a direct impact on the healthcare industry, with discussions ranging from the uses of AI as an assistant to medical personnel5–7 to AI replacing entire clinical departments8,9.\n\nThe use and impact of AI in clinical Natural Language Processing (NLP) in the context of Electronic Health Records (EHRs) have been profound10–13.\n\nSimilar to military organizations, the World Health Organization (WHO) has also released a document discussing the ethics and governance of AI for health14.\n\nGenerative AI, as the name suggests, refers to AI techniques that can be used to create or produce various types of new contents, including text, images, audio, and videos.\n\nThe rate of development of generative AI has been staggering, with many industries and researchers ﬁnding its use in ﬁelds such as ﬁnance15, collaborative writing16, email communication17, and cyber threat intelligence18.\n\nGenerative AI has also become an active area of research in the healthcare domain19,20, with applications such as clinical documentation21 and evidence-based medicine summarization22.\n\nDespite many successful and promising AI applications, ethics has been one of the more controversial subjects of discussion in the AI community, with diverging views and a plethora of opinions23,24.\n\nEthics deals with how one decides what is morally right or wrong and is one of the pivotal aspects that we, as the AI research community, have to consider carefully.\n\nGiven the recent emergence of generative AI models and their initial enthusiasm in healthcare, our community must seriously consider ethical principles before integrating these techniques into practical use.\n\nThe military and healthcare are notably similar in many ways, such as organizational structure, high levels of stress and risk, decision-making processes, reliance on protocols, and dominion over life and death.\n\nGiven these parallels, successful implementation of ethical principles in military applications, and the lack of speciﬁc solutions to generative AI ethics in healthcare, we propose to adopt and expand ethical principles, from military to healthcare, to govern the application of generative AI in healthcare applications.",
    "## WHAT IS GENERATIVE ARTIFICIAL INTELLIGENCE?\n\nGenerative AI refers to AI that is used primarily for generating data, often in the form of audio, text, and images.\n\nHowever, in this manuscript, we choose not to follow such a general deﬁnition and instead, focus on a particular type of generative AI.\n\nIn this section, we describe “modern” generative AI, discuss why it is important, and compare it to the term that has become so popular—“AI.”\n\nModern AI is dominated by Machine Learning (ML) methods, which leverage statistical algorithms and large amounts of data to gradually improve model performance.\n\nML methods could roughly be classiﬁed into supervised, unsupervised, and reinforcement learning (Fig.\n\n1).\n\nSupervised ML relies on labeled input (supervision), while unsupervised learning needs no human supervision.\n\nReinforcement learning takes a different approach and, instead, attempts to design intelligent agents by rewarding desired behaviors and punishing undesired ones.\n\nPopular generative AI models are typically pre-trained in an unsupervised manner.\n\nThe pre-trained generative AI models could generate novel and diverse outputs, including, but not limited to, text, images, audio, or videos.\n\nRecently, the most popular generative AI model for language generation is ChatGPT25, which was reported to have an estimated 100 million monthly active users in January 202326.\n\nThe model architectures for ChatGPT, previously known as GPT-3.527, and more recent GPT-428, are built upon the design principles of its GPT29 (Generative Pre-trained Transformer) predecessors, GPT- 230 and GPT-331.\n\nMany state-of-the-art generative AI models, also known as Large Language Models (LLMs), share a similar transformer-based architecture32.\n\nThe well-known generative AI models used for image generation from text prompts, such as Stable Diffusion33 and DALL-E 234, employ a combination of the diffusion process35 and a transformer-based architecture similar to the one used in GPT models.\n\nAll of the models are characterized by unsupervised training on very large datasets36.\n\nThe same is true of models that generate videos.\n\nMost of these generative AI models also rely on a method called prompting37, which lets users input a natural language description of a task and uses it as a context to generate useful information.\n\nThis process is also sometimes referred to as in-context learning.\n\nWhen referring to “modern” generative AI or simply generative AI, we are describing a transformer-based machine learning model trained in an unsupervised manner on extensive datasets and speciﬁcally optimized for generating valuable data through prompts.\n\nThis description also aligns harmoniously with existing research and studies38–40.\n\nWhile generative AI shows promising results, dangerous outcomes in healthcare can arise from a number of issues, including:\n- Algorithmic bias41,42\n- Hallucination43,44\n- Poor commonsense reasoning44,45\n- Lack of generally agreed model evaluation metrics46,47\n\nAll of these issues are common for generative AI in general, but more so in the healthcare domain, where algorithmic bias may result in the mistreatment of patients48, hallucination may carry misinformation49, poor commonsense reasoning can result in confusing interactions50, and lack of general and domain-speciﬁc metrics can make it difﬁcult to validate the robustness of the AI system51.\n\nFurthermore, in the context of healthcare, there are concerns about leaking Protected Health Information (PHI)52 as well as lacking empathy to patients53.\n\nSuch concerns can also be present in other forms of AI, but given the practical differences present in generative AI, the risks become elevated.\n\nFirst, due to the interactive nature of generative AI, often paired with the ability to hold human-like dialogs (e.g., ChatGPT), it can make misinformation sound convincing.\n\nSecond, since generative AI models combine various sources of large-scale data36, the risk of training on biased data sources increases.\n\nThird, the standard evaluation metrics, such as precision, recall, and F1 score, become difﬁcult to use and are less likely to reﬂect human judgment47.\n\nFinally, due to its ease of use, generative AI can be widely adopted in many ﬁelds and domains of healthcare49, which naturally increases the aforementioned risks.\n\nOverall, the importance of ethical considerations for generative AI in healthcare cannot be understated.\n\nFrom the human-centered perspective, the ultimate goal of generative AI is to enhance and augment human’s creativity, productivity, and problem-solving capabilities, which is well aligned with the goal of healthcare in improving patient care.\n\nIf the generative AI system is not used ethically and does not reﬂect our values, its role as a tool for improving the lives of people will greatly diminish.",
    "## AI APPLICATIONS IN MILITARY VS. HEALTHCARE\n\nWith the increasing prevalence of AI, it has been in the best interest of military organizations to understand and integrate AI into their operations and strategies to be at the cutting edge of security and technology in conﬂict or emergency.\n\nVarious military AI technologies for generative purposes have also been developed, including Intelligent Decision Support Systems (IDSSs) and Aided Target Recognition (AiTC), which assist in decision-making, target recognition, and casualty care in the ﬁeld54–56.\n\nEach of these uses of AI in military operations reduces the mental load of operators in the ﬁeld and helps them take action more quickly.\n\nJust as military uses of AI can save lives on the battleﬁeld, AI can help save lives by assisting clinicians in diagnosing diseases and reducing risks to patient safety57–59.\n\nUses of generative AI in healthcare help improve the efﬁciency of professionals caring for patients.\n\nApplications of generative AI in healthcare include medical chatbots, disease prediction, CT image reconstruction, and clinical decision support tools60–63.\n\nThe beneﬁts of such uses are two-fold, in that they can help healthcare professionals deliver a higher level of care to their patients, as well as improve the workload within clinics and hospitals.\n\nPeople may question that developing AI models for military and healthcare purposes hinges on distinct ideological underpinnings reﬂecting unique priorities.\n\nIn the military context, AI models are primarily designed to enhance the efﬁciency, precision, and strategic capabilities of both defensive and offensive operations.\n\nThe focus is on applications such as surveillance, target recognition, cyber defense, autonomous weaponry, and battleﬁeld analytics.\n\nPotential future uses of AI for offensive actions such as coordinating drone attacks may oppose any healthcare principle, yet is vital for the military strategy.\n\nThe fundamental ideological perspective here is the protection of national security interests, force multiplication, and minimizing human risk in conﬂict zones.\n\nOn the other hand, the use of AI in healthcare is driven by the principles of enhancing patient care, improving health outcomes, and optimizing the efﬁciency of healthcare systems.\n\nThe development of AI models in this sector aims to personalize treatments, improve diagnostic accuracy, predict disease progression, and streamline administrative tasks, among other uses.\n\nThe central ideology is the betterment of human health and well-being.\n\nWhile we acknowledge the different ideological foundations in military and healthcare due to the contrasting objectives, we argue that both military and healthcare sectors illustrate a compelling convergence of priorities for the applications of AI.\n\nSpeciﬁcally, their shared focus on application validity, attention to practical implementation, and prioritization of a human-centered approach have emerged as signiﬁcant commonalities.\n\nFirst, concerning application validity, both ﬁelds recognize the crucial importance of robust, reliable AI systems.\n\nThese systems need to function accurately and rapidly under diverse, often challenging, conditions to fulﬁll their designated tasks, whether it identiﬁes potential security threats in a complex battleﬁeld or detects subtle abnormalities in medical images.\n\nSecond, there is an evident emphasis on implementation.\n\nBeyond the theoretical development of AI models, the critical question for both sectors centers around how these models can be effectively incorporated into real-world systems, often involving multiple human and technological stakeholders.\n\nFinally, a human-centered perspective is paramount.\n\nThis means ensuring that AI technologies augment, rather than replace, human decision-making capacities and are employed in ways minimizing potential harm.\n\nIn healthcare, this involves developing AI applications that can improve patient outcomes and experience while supporting healthcare providers in their work.\n\nThus, these three factors represent key shared priorities in the utilization of AI across military and healthcare contexts.\n\nAI has been seamlessly woven into the military’s technology fabric for several decades, serving as the backbone for various advancements ranging from autonomous drone weapons to intelligent cruise missiles64,65.\n\nThe track record of robust results and reliable outcomes in complex and high-risk environments implicitly engage with foundational ethical principles.\n\nThe ethical guidelines established from military AI implementations have provided a roadmap for the incorporation of AI in healthcare scenarios.\n\nHowever, the integration of AI is relatively new to the healthcare sector, let alone generative AI, and ethical principles are neither widely implemented nor speciﬁcally designed for generative AI.",
    "However, the integration of AI is relatively new to the healthcare sector, let alone generative AI, and ethical principles are neither widely implemented nor speciﬁcally designed for generative AI.\n\nWhile healthcare has begun to adopt generative AI technologies more recently66, there are immense opportunities for this ﬁeld to glean ethical insights from the history of military application.",
    "## IDENTIFYING ETHICAL CONCERNS AND RISKS\n\nA RAND Corporation study raised various concerns about the use of AI in warfare, shown in Figure 2.3 of the research report67.\n\nThese concerns fall into the following categories: increasing risk of war, increased errors, and misplaced faith in AI.\n\nAlthough AI can allow personnel to make decisions and strategies more quickly, some experts consider this a downside, as actions taken without proper consideration could have serious repercussions, like increasing the risk of war67.\n\nInternational standards for warfare like the Law of Armed Conﬂict (LOAC) and Geneva Conventions lay out guidelines for target identiﬁcation specifying that attacks must ﬁrst distinguish between combatant and noncombatant targets before taking action to minimize harm to civilians68,69.\n\nBecause combatants are not always identiﬁable visually, some claim that reading body language to differentiate a civilian from a combatant necessitates a Human-In-The-Loop (HITL) decision-making process70.\n\nMaintaining data privacy for users of generative AI technologies is critical, as both patient data and military data are highly sensitive, and would be damaging if leaked71.\n\nIf an AI implementation collects PHI, it should be secure against breaches, and any disclosures of this protected data must comply with Health Insurance Portability and Accountability Act (HIPAA) guidelines72.\n\nThese implementations must experience few errors as healthcare is a safety-critical domain where the patient harm is unacceptable73, and errors in these systems or algorithms could cause more harm than any physician would be capable of, as many hospitals and clinics would be using the same systems and experiencing the same errors74.\n\nAdditional concerns present in the military and healthcare are trust between humans and AI and the lack of accountability.\n\nWhen there exists human-and-AI collaboration to perform a task, trust must be optimal, as shown in Fig.\n\n2.\n\nToo much trust in AI systems can lead to overuse of the AI when it is not in the best interest of patients or operators75, and too little trust can lead to underuse of the system when it would be better to use it76.\n\nIn both situations, the root cause is operators not knowing the capabilities and limitations of the systems they interact with77.\n\nMisuse can lead to non-typical errors, such as fratricide in the military or patient harm in a hospital78,79.\n\nWhile the AI must be transparent in its decision- making, the use of AI must be accompanied by sufﬁcient education on the use and limitations of AI systems so that operators are less likely to make dangerous errors.\n\nA lack of accountability can possibly arise in military or healthcare use of generative AI because military operators or clinicians do not have direct control over the actions determined by the AI.\n\nIn the same research report by RAND Corporation67, authors showed (Figures 7.8, 7.9 in the report) that the general public views autonomous systems taking military action with human authorization favorably while strongly disagreeing with combat action without human authorization.\n\nThe parallel can be drawn with healthcare, where patients express concerns over the use of AI for medical purposes without human (e.g., physician, nurse, etc.)\n\ninvolvement80.\n\nThese results could be due to the perceived lack of accountability, which is considered something that could entirely negate the value of AI, as a fully autonomous system that makes its own decisions distances military operators or clinicians from the responsibility of the system’s actions81.\n\nIn healthcare, it is critical that the systems are transparent due to their proximity to human lives and that patients understand how clinicians use these recommendations.\n\nThe burden of accountability in the healthcare sector falls to both the clinicians and the developers of the AI systems, as the decisions made are a product of the algorithm, and the use of these recommendations falls to the clinicians82.\n\nFinally, ethical concerns of equity, autonomy, and privacy regarding the use of generative AI must also be considered.\n\nIn healthcare settings, biased algorithms or biased practices can lead to certain patient groups receiving lower levels of care83.\n\nBiased outcomes could be due to biased algorithms, poor data collection, or a lack of diversity84.\n\nThere must be minimal bias in developing AI systems in healthcare, both in the algorithm and the data used for training.\n\nFurthermore, if known, the sources of bias must also be disclosed to ensure transparency and prevent inappropriate use.\n\nThe issue of human autonomy when developing generative AI is especially pertinent in healthcare, as both patient and clinician autonomy must be respected85.\n\nIt is crucial that a framework is accepted to prevent any data breaches and ensure security measures are up to date and robust.",
    "It is crucial that a framework is accepted to prevent any data breaches and ensure security measures are up to date and robust.\n\nThese risks and ethical concerns surrounding generative AI in military and healthcare applications necessitate principles for the ethical use of AI.\n\nOne of the earliest sets of principles published for responsible development and use of AI comes from Google, who did so in response to their employees petitioning their CEO as they disagreed with Google working with the DOD on Project Maven to assist in identifying objects in drone images86,87 in 2018.\n\nThese principles outline how Google will develop AI responsibly and state what technologies they will not create, like those that cause harm or injure people, provide surveillance that violates international policies, and any technologies that go against international law and human rights88.\n\nBy examining the differences and similarities between risks and ethical concerns in military and healthcare applications of generative AI, we can establish guiding principles for the responsible development and use of generative AI in healthcare.",
    "## GREAT PLEA ETHICAL PRINCIPLES FOR GENERATIVE AI IN HEALTHCARE\n\nAs AI usage has spread throughout the military and other ﬁelds, many organizations have recognized the necessity of articulating their ethical principles and outlining the responsibilities associated with applying AI to their operations.\n\nThere are several ethical principles for AI published by various organizations, including the U.S. Department of Defense2, NATO3, the American Medical Association (AMA)89, the World Health Organization (WHO)14, and the Coalition for Health AI (CHAI)90.\n\nThe AI ethical principles for DOD and NATO are similar, with NATO having an added focus on adherence to international law.\n\nFor the development of AI for healthcare, the WHO has published its own ethical principles, including protecting human autonomy, human well-being and safety, transparency and explainability, responsibility and accountability, inclusiveness, and responsive development.\n\nSimilarly, the AMA promotes AI systems that should be user-centered, transparent, reproducible, avoid exacerbating healthcare disparities, and safeguard the privacy interests of patients and other individuals.\n\nFinally, there is the Blueprint for an AI Bill of Rights published by the U.S. Ofﬁce of Science and Technology Policy (OSTP)91, which has provisions for AI systems to be safe and effective, protected against algorithmic discrimination, protect user data, have accessible documentation, and offer human alternatives.\n\nAmong the various sets of principles, we see common themes such as accountability and human presence.\n\nThe DOD and NATO both emphasize the importance of integrating human responsibility into the development and life cycle of an AI system, as well as ensuring these systems are governable to address errors that may arise during use.\n\nThe AMA and WHO policies both highlight a human-centered design philosophy protecting human autonomy and explicitly mention the need for inclusiveness and equity in the healthcare use of AI to prevent care disparity.\n\nThese principles each provide unique perspectives for developing AI for healthcare use.\n\nHowever, no set of principles encompasses all ethical concerns that healthcare providers or patients may have92.\n\nAdopting the principles of the DOD and NATO is advantageous due to each principle’s practical deﬁnition.\n\nThese principles are outlined with a focus on what actions can be taken by personnel developing AI systems, and how end-users would interact with the systems.\n\nThe existing principles establish a good foundation for the ethical development and utilization of AI in healthcare.\n\nHowever, action must be taken to tailor these principles for generative AI.\n\nBy examining the risks and concerns surrounding the use of generative AI in healthcare, comparing them to the risks and concerns of generative AI in the military, and by expanding these principles, we can have a set of principles that fulﬁll our needs93.\n\nTherefore, we use DOD and NATO guidelines as the starting point for the set of ethical principles, and expand them to meet the needs in healthcare.\n\nThe expansion is done by incorporating principles that support the betterment of mankind rather than defeating adversaries.\n\nFigure 3 shows the framework that we used for adopting and expanding ethical principles, established by various organizations, for the healthcare applications of AI.\n\nWhere similarities are present in the concerns between military and healthcare use of generative AI, it is possible to adopt principles for use, such as traceability, reliability, lawfulness, accountability, governability, and equity.\n\nIn instances when healthcare has unique circumstances or requires additional nuance, the principles related to those matters must be expanded to ﬁt into the world of medicine, such as empathy, autonomy, and privacy.\n\nThere are many concerns speciﬁc to the military that are unsuitable for forming ethical principles in healthcare, such as national security and defense, mission effectiveness, operational security, adversarial AI94, human-machine teaming, rules of engagement, rapid deployment and adaptation, and proliferation and arms race.\n\nFigure 3 also shows some of these concerns (a non-exhaustive list), which we included to highlight that the adoption or expansion of principles must be based on shared concerns.\n\nFurthermore, we want to emphasize the need for having safeguards and methods to detect and mitigate military-speciﬁc properties of AI deployed in healthcare settings, by including governability, accountability, and traceability.\n\nA detailed mapping of the proposed ethical principles to those used by DOD, NATO, and WHO guidelines is shown in Table 1.\n\nAs shown in the table, all principles, except for privacy, empathy, and autonomy, directly align with either DOD or NATO guidelines.\n\nIn cases where the principle indirectly aligns with our proposed principles, Table 1 uses a star (*) preﬁx.",
    "In cases where the principle indirectly aligns with our proposed principles, Table 1 uses a star (*) preﬁx.\n\nAs for privacy, empathy, and autonomy, despite not being related to the ethical principles in military organizations (i.e., DOD and NATO), WHO guidelines directly or indirectly align with all three.\n\nTheir inclusion was also due to the quality of betterment of mankind and mitigation of concerns speciﬁc to healthcare.\n\nIn summary, we propose the “GREAT PLEA” ethical principles for generative AI in healthcare, namely Governability, Reliability, Equity, Accountability, Traceability, Privacy, Lawfulness, Empathy, and Autonomy.\n\nThe GREAT PLEA ethical principles demonstrate our great plea for the community to prioritize these ethical principles when implementing and utilizing generative AI in practical healthcare settings.\n\nFig.\n\n4 shows the summary cards for the GREAT PLEA ethical principles.\n\nIn the following, we will delve into a comprehensive explanation of each individual principle.",
    "### Governability\n\nGovernability is the ability of a system to integrate processes and tools which promote and maintain its capability and ensure meaningful human control95.\n\nStandards for the governability of AI systems, as established by the DOD and NATO, emphasize the importance of ensuring that while AI systems fulﬁll th",
    "# Unintended Consequences\n\nIn the deployment of AI systems for their intended functions, humans must retain the ability to identify and prevent unintended consequences.\n\nIn the event of any unintended behavior, human intervention to disengage or deactivate the deployed AI system should be possible.\n\nThese standards can be adopted for the use of generative AI in healthcare.\n\nDue to the potential of widespread implementation of generative AI systems, where numerous hospitals may be using the same systems, these standards must be considered.\n\nSuppose a generative AI system, deployed across multiple clinics, poses a risk of harm to a patient.\n\nIn that case, it is crucial to recognize that numerous patients across clinics could be vulnerable to the same error.\n\nRisk to patients amplifies as healthcare expands to patient homes with remote patient monitoring or with online tools outside the clinic.\n\nIdeally, humans, whether they develop or implement the system, should possess the capability to deactivate it without disrupting the regular patient care activities in the clinics.\n\nThere must be explicit guidelines for monitoring generative AI systems for potential errors, deactivation to prevent more damage when an error occurs, remedying errors, and interaction to reduce operator errors.\n\nWith these guidelines in place, personnel in charge of the system can quickly be notified of any unintended behavior and respond quickly and appropriately.",
    "# Reliability\n\nReliability is the ability of a system or component to function under stated conditions for a specified period of time.\n\nThe proximity of generative AI to patient well-being necessitates standards for reliability to minimize potential errors that could lead to accidents.\n\nThe generative AI models should have explicit and well-defined clinical use cases.\n\nA generative AI model designed for disease prediction needs to have a clear definition of the use situation and patient criteria.\n\nIn addition, such generative AI models should be safe, secure, and effective throughout their life cycles.\n\nGenerative AI models should be demonstrated to be at least as safe as human decision-making alone and not cause undue harm.\n\nExisting generative AI models suffer from hallucination and output variations, undermining their ability to produce reliable outputs.\n\nThese shortcomings can adversely affect the trust between physicians and generative AI systems.\n\nAdopting the DOD’s principle for reliability can establish use cases for AI applications and monitor them during development and deployment to fix system failures and deterioration.\n\nHaving a thorough evaluation and testing protocol against specific use cases will ensure the development of resilient and robust AI systems, and help minimize system failures as well as the time needed to respond to these errors.",
    "# Equity\n\nEquity is the state in which everyone has a fair and just opportunity to attain their highest level of health.\n\nDue to the importance of health equity and the ramifications of algorithmic bias in healthcare, we call for adjustments to this principle.\n\nThere already exists inequity in healthcare.\n\nThe generative AI models, that naturally have elevated data bias risks due to their pre-training on massive datasets, should not exacerbate this inequity for marginalized, under-represented, socioeconomically disadvantaged, low education, or low health literacy groups, but rather incorporate their unique social situations into future AI models to ensure equity.\n\nGenerative AI must be developed with efforts to mitigate bias by accounting for existing health disparities.\n\nWithout this consideration, generative AI systems could erroneously recommend treatments for different patients.\n\nExpansion of the principle for equity must set standards for evaluation metrics of algorithmic fairness so that deployed AI systems will not reinforce healthcare disparity.",
    "# Accountability\n\nAccountability is the property of being able to trace activities on a system to individuals who may then be held responsible for their actions.\n\nTo ensure accountability and human involvement with AI in healthcare, the principle of Responsibility and Accountability outlined by NATO states that they will develop AI applications mindfully and integrate human responsibility to establish human accountability for actions taken by or with the application.\n\nA study of patient attitudes toward AI showed the importance of accountability in gaining patient trust when using AI in healthcare.\n\nThis assurance of accountability is crucial when a clinician is using generative AI to help treat a patient, as without proper measures for human accountability, the patient may feel that the clinician is not invested in the care they are delivering.\n\nWe can adopt this principle for the ethical use of generative AI in healthcare, and ensure that human involvement is maintained when more powerful generative AI systems, such as ChatGPT or generative AI-based clinical decision support systems, are used in patient care.",
    "# Traceability\n\nTraceability is tracking and documenting data, processes, and artifacts related to a system or model for transparent development.\n\nAddressing the issue of optimizing trust between healthcare professionals and the AI they interact with can be done by adopting the principle of traceability.\n\nThis way, the personnel working with AI will understand its capabilities, developmental process, methodologies, data sources, and documentation.\n\nFurthermore, providing personnel with the understanding of an AI system capabilities and the processes behind its actions, will also improve system reproducibility, allowing for seamless deployment across healthcare systems.\n\nThis is important for generative AI systems in healthcare because of their nature of being a black box system.\n\nThis high-level understanding will help optimize trust, as operators will be aware of the capabilities and limitations of the AI systems they work with and know the appropriate settings for use.\n\nWith generative AI becoming more prevalent in healthcare, proper documentation is required to ensure all end users are properly educated on the capabilities and limitations of the systems they interact with.\n\nThe generation process of generative AI models should be transparent.\n\nThe references or facts should be provided together with answers and suggestions for clinicians and patients.\n\nData sources used to train these models and the design procedures of these models should be transparent too.\n\nFurthermore, the implementation, deployment, and operation of these models need to be auditable, under the control of stakeholders in the healthcare setting.",
    "# Privacy\n\nPrivacy is an assurance that the confidentiality of, and access to, certain information about an entity is protected.\n\nPrivacy is necessary in most military and medical applications of healthcare due to their confidential nature.\n\nGenerative AI systems in healthcare must be HIPAA compliant for data disclosures, and secure to prevent breaches and developers should be advised how healthcare data should train systems for deployment.\n\nHIPAA compliance requires a regular risk assessment to determine how vulnerable patient data is, thus a clinic utilizing generative AI systems in healthcare would have to determine if these systems are weak points in their technology network.\n\nFor example, the utilization of generative AI models presents potential privacy breach risks, including prompt injection, where malicious actions could be conducted by overriding an original prompt, and jailbreak, where training data could be divulged by eliciting generated content.\n\nFurthermore, the capabilities of generative AI to process personal data and generate sensitive information make it crucial for these systems to be secure against data breaches and cyberattacks.\n\nEnsuring these systems are developed with data privacy and security in mind will assist in keeping protected patient information secure.\n\nHaving these robust measures in place to maintain the privacy of the sensitive data collected and made by AI systems is crucial for the well-being of patients and for building trust with patients.",
    "# Lawfulness\n\nLawfulness is the adherence to national and international law, including international humanitarian law and human rights law, as applicable.\n\nThis can be adopted for the use of generative AI in healthcare.\n\nThe laws that must be adhered to are not laws of conflict, but rather those related to healthcare.\n\nDifferent states in the U.S. may establish different laws for AI systems that must be heeded for deployment in those areas.\n\nGenerative AI systems in healthcare also face legal challenges surrounding safety and effectiveness, liability, data privacy, cybersecurity, and intellectual property law.\n\nA legal foundation must be established for the liability of action taken and recommended by these systems, as well as considerations for how they interact with cybersecurity and data privacy requirements of healthcare providers.\n\nGenerative AI for healthcare must be developed with these legal challenges in mind to protect patients, clinicians, and AI developers from any unintended consequences.",
    "# Empathy\n\nEmpathy is the ability to understand the personal experiences and emotions of another, without extensive bonding.\n\nA principle for empathy is not directly referenced in any guidelines by the DOD or NATO.\n\nHowever, by emphasizing the need for human involvement in the treatment of patients, it is possible to create a framework for human involvement in generative AI applications to prevent gaps in accountability and ensure patients receive care that is empathetic and helpful.\n\nThere have been notable concerns about artificial empathy of chatbots, such as ChatGPT, reinforcing the need for a principle defining empathy for generative AI in healthcare.\n\nAn empathetic relationship between provider and patient brings several benefits to both the patient and the clinic treating them, such as better patient outcomes, fewer disputes with healthcare providers, higher patient satisfaction, and higher reimbursement.",
    "# Autonomy\n\nAutonomy is the matter of control over one’s self and requires both freedom from controlling influences and the capacity for action for it to be maintained.\n\nThe more powerful AI systems become, the more concerns arise that humans do not control healthcare systems and care decisions.\n\nGenerative AI has seen staggering progress in the past several years, and hence, the protection of autonomy needs to be ensured when using generative AI in healthcare.\n\nProtecting human autonomy means that patients receive care according to their preferences and values and that clinicians can deliver treatment in the manner they want, without being encroached upon by the generative AI system.\n\nIf autonomy in decision-making is not patient-focused, the potential for adverse events and poor clinical outcomes will surely follow.\n\nBy including provisions for protecting autonomy in using generative AI in healthcare, doctor-patient relations improve, and care quality is ultimately improved.",
    "# Conclusion\n\nGenerative AI has great potential to enhance and make high-quality healthcare more accessible to all, leading to a fundamental transformation in its delivery.\n\nChallenges posed by AI in healthcare often mirror those encountered in military.\n\nWe propose the GREAT PLEA ethical principles, encompassing nine ethical principles, in the hope of addressing the ethical concerns of generative AI in healthcare, as well as the distinction between generative AI and “general” AI.\n\nThis will be achieved by addressing the elevated risks mentioned previously in the paper.\n\nGenerative AI necessitates guidelines that account for the risk of misinformation, ramifications of bias, and difficulty of using general evaluation metrics.\n\nConsidering the widespread nature of generative AI and its risks, these ethical principles can protect patients and clinicians from unforeseen consequences.\n\nFollowing these principles, generative AI can be continuously evaluated for errors, bias, and other concerns that patients or caregivers may have about their relationship with AI in their field.\n\nThe present moment urges us to embrace these principles, foster a closer collaboration between humans and technology, and effect a radical enhancement in the healthcare system.\n\nThese principles can be enforced through cooperation with lawmakers and the establishment of standards for developers and users, as well as a partnership with recognized governing bodies within the healthcare sector, such as the WHO or AMA.\n\nWe note that the enforcement of the proposed ethical principles, be it via evaluation approaches (e.g., Likert scale, prompting, or semantic similarity-based approaches for empathy) or through other means, is out of the scope of this effort.\n\nAs such, we acknowledge the lack of detailed enforcement procedures as the limitation of the work.\n\nAt the same time, implementing AI metrics or enforcement methods for GREAT PLEA ethical principles can also be the potential future avenue for exploration.\n\nReceived: 18 May 2023; Accepted: 15 November 2023; Published online: 02 December 2023",
    "# References\n\n1.\n\nRussell, S. Ai weapons: Russia’s war in Ukraine shows why the world must enact a ban.\n\nNature  (2023).\n\n2.\n\nU.S. Department of Defense.\n\nDod adopts ethical principles for artificial intelligence  (2020).\n\n3.\n\nThe North Atlantic Treaty Organization.\n\nSummary of the NATO artificial intelligence strategy  (2021).\n\n4.\n\nHicks, K. What the Pentagon thinks about artificial intelligence.\n\nPolitico   \n5.\n\nBaker, A. et al.\n\nA comparison of artificial intelligence and human doctors for the purpose of triage and diagnosis.\n\nFront Artif.\n\nIntell.\n\n3, 543405 (2020).\n\n6.\n\nChan, S. & Siegel, E. L. Will machine learning end the viability of radiology as a thriving medical specialty?\n\nBr.\n\nJ. Radiol.\n\n92, 20180416 (2019).\n\n7.\n\nMeyer, J. et al.\n\nImpact of artificial intelligence on pathologists’ decisions: an experiment.\n\nJ.\n\nAm.\n\nMed.\n\nInform.\n\nAssoc.\n\n29, 1688–1695 (2022).\n\n8.\n\nLanglotz, C. P. Will artificial intelligence replace radiologists?\n\nRadiol.\n\nArtif.\n\nIntell.\n\n1, e190058 (2019).\n\n9.\n\nCacciamani, G. E. et al.\n\nIs artificial intelligence replacing our radiology stars?\n\nnot yet!\n\nEur.\n\nUrol.\n\nOpen Sci.\n\n48, 14–16 (2023).\n\n10.\n\nYang, X. et al.\n\nA large language model for electronic health records.\n\nnpj Digit.\n\nMed.\n\n5, 194 (2022).\n\n11.\n\nLin, W.-C., Chen, J. S., Chiang, M. F. & Hribar, M. R. Applications of artificial intelligence to electronic health record data in ophthalmology.\n\nTransl.\n\nVis.\n\nSci.\n\nTechnol.\n\n9, 13–13 (2020).\n\n12.\n\nRosenthal, S., Barker, K. & Liang, Z. Leveraging medical literature for section prediction in electronic health records.\n\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 4864–4873 (Association for Computational Linguistics, Hong Kong, China, 2019).\n\n13.\n\nSinghal, K. et al.\n\nLarge language models encode clinical knowledge.\n\nNature 620, 172–180 (2023).\n\n14.\n\nOrganization, T. W. H. Ethics and governance of artificial intelligence for health  (2021).\n\n15.\n\nDowling, M. & Lucey, B. Chatgpt for (finance) research: the Bananarama conjecture.\n\nFinance Res.\n\nLett.\n\n53, 103662 (2023).\n\n16.\n\nLee, M., Liang, P. & Yang, Q. Coauthor: designing a human-ai collaborative writing dataset for exploring language model capabilities.\n\nIn Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI ’22 (Association for Computing Machinery, New York, NY, USA, 2022).\n\n17.\n\nThiergart, J., Huber, S. & Übellacker, T. Understanding emails and drafting responses—an approach using gpt-3 (2021).\n\nPreprint at   \n18.\n\nRanade, P., Piplai, A., Mittal, S., Joshi, A.\n\n& Finin, T. Generating fake cyber threat intelligence using transformer-based models.\n\nIn 2021 International Joint Conference on Neural Networks (IJCNN), 1–9 (2021).\n\n19.\n\nLiao, W. et al.\n\nDifferentiate chatgpt-generated and human-written medical texts (2023).\n\nPreprint at   \n20.\n\nChintagunta, B., Katariya, N., Amatriain, X.\n\n& Kannan, A.\n\nMedically aware GPT-3 as a data generator for medical dialogue summarization.\n\nIn Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations, (eds Shivade, C. et al.)\n\n66–76 (Association for Computational Linguistics, Online, 2021).\n\n21.\n\nSun, Z. et al.\n\nEvaluating GPT4 on impressions generation in radiology reports.\n\nRadiology 307, e231259 (2023).\n\n22.\n\nPeng, Y., Rousseau, J. F., Shortliffe, E. H. & Weng, C. AI-generated text may have a role in evidence-based medicine.\n\nNat.\n\nMed.\n\n(2023).\n\n23.\n\nGilbert, T. K., Brozek, M. W. & Brozek, A.\n\nBeyond bias and compliance: Towards individual agency and plurality of ethics in AI (2023).\n\nPreprint at   \n24.\n\nBirhane, A. et al.\n\nThe forgotten margins of ai ethics.\n\nIn 2022 ACM Conference on Fairness, Accountability, and Transparency (ACM, 2022).\n\n25.\n\nOpenAI.\n\nIntroducing chatgpt  (2022).\n\n26.\n\nHu, K. Chatgpt sets record for fastest-growing user base - analyst note.\n\nReuters   \n27.\n\nOpenAI.\n\nModel index for researchers   \n28.\n\nOpenAI.\n\nGpt-4 technical report (2023).\n\nPreprint at   \n29.\n\nRadford, A., Narasimhan, K., Salimans, T. & Sutskever, I.\n\nImproving language understanding by generative pre-training.\n\n(2018).\n\n30.\n\nRadford, A. et al.\n\nLanguage models are unsupervised multitask learners (2019).\n\n31.\n\nBrown, T. et al.\n\nLanguage models are few-shot learners.\n\nIn Advances in Neural Information Processing Systems, Vol.\n\n33 (eds Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. & Lin, H.) 1877–1901 (Curran Associates, Inc., 2020).\n\n32.\n\nVaswani, A. et al.\n\nAttention is all you need.\n\nIn Advances in Neural Information Processing Systems, Vol.\n\n30 (eds Guyon, I. et al.)\n\n(Curran Associates, Inc., 2017).\n\n33.\n\nRombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. High-resolution image synthesis with latent diffusion models.\n\nIn 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10674–10685 (IEEE Computer Society, Los Alamitos, CA, USA, 2022).\n\n34.",
    "In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10674–10685 (IEEE Computer Society, Los Alamitos, CA, USA, 2022).\n\n34.\n\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with clip latents (2022).\n\nPreprint at   \n35.\n\nLuo, C. Understanding diffusion models: A unified perspective (2022).\n\nPreprint at   \n36.\n\nZhao, W. X. et al.\n\nA survey of large language models (2023).\n\nPreprint at   \n37.\n\nLiu, P. et al.\n\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.\n\nACM Comput.\n\nSurv.\n\n55  (2023).\n\n38.\n\nKather, J. N., Ghaffari Laleh, N., Foersch, S. & Truhn, D. Medical domain knowledge in domain-agnostic generative ai.\n\nnpj Digit.\n\nMed.\n\n5, 90 (2022).\n\n39.\n\nZhang, C. et al.\n\nA complete survey on generative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you need?\n\n(2023).\n\nPreprint at   \n40.\n\nZhang, C., Zhang, C., Zhang, M. & Kweon, I. S. Text-to-image diffusion models in generative ai: A survey (2023).\n\nPreprint at   \n41.\n\nFerrara, E. Should chatgpt be biased?\n\nchallenges and risks of bias in large language models (2023).\n\nPreprint at   \n42.\n\nRutinowski, J., Franke, S., Endendyk, J., Dormuth, I.\n\n& Pauly, M. The self-perception and political biases of chatgpt (2023).\n\nPreprint at   \n43.\n\nJi, Z. et al.\n\nSurvey of hallucination in natural language generation.\n\nACM Comput.\n\nSurv.\n\n55, 1–38 (2023).\n\n44.\n\nBang, Y. et al.\n\nA multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity (2023).\n\nPreprint at   \n45.\n\nBian, N. et al.\n\nChatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problems in large language models (2023).\n\nPreprint at   \n46.\n\nChen, N. et al.\n\nMetrics for deep generative models.\n\nIn Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, Vol.\n\n84 of Proceedings of Machine Learning Research, (eds Storkey, A.\n\n& Perez-Cruz, F.) 1540–1550 (PMLR, 2018).\n\n47.\n\nThoppilan, R. et al.\n\nLamda: Language models for dialog applications (2022).\n\nPreprint at   \n48.\n\nGloria, K., Rastogi, N. & DeGroff, S. Bias impact analysis of AI in consumer mobile health technologies: Legal, technical, and policy (2022).\n\nPreprint at   \n49.\n\nPeng, C. et al.\n\nA study of generative large language models for medical research and healthcare (2023).\n\nPreprint at   \n50.\n\nWei, J. et al.\n\nChain of thought prompting elicits reasoning in large language models.\n\nIn Advances in Neural Information Processing Systems (eds Oh, A. H., Agarwal, A., Belgrave, D. & Cho, K.)  (2022).\n\n51.\n\nLeiter, C. et al.\n\nTowards explainable evaluation metrics for natural language generation (2022).\n\nPreprint at   \n52.\n\nPriyanshu, A., Vijay, S., Kumar, A., Naidu, R. & Mireshghallah, F. Are chatbots ready for privacy-sensitive applications?\n\nan investigation into input regurgitation and prompt-induced sanitization (2023).\n\nPreprint at   \n53.\n\nAyers, J. W. et al.\n\nComparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum.\n\nJAMA Intern.\n\nMed.\n\n(2023).\n\n54.\n\nDonovan - AI-powered decision-making for defense.\n\nScale  (2023).\n\n55.\n\nAdvanced targeting and lethality aided system (atlas).\n\nCoVar  (2023).\n\n56.\n\nDoctrinaire.\n\nCoVar  (2023).\n\n57.\n\nChoudhury, A.\n\n& Asan, O.\n\nRole of artificial intelligence in patient safety outcomes: systematic literature review.\n\nJMIR Med.\n\nInform.\n\n8, e18599 (2020).\n\n58.\n\nBahl, M. et al.\n\nHigh-risk breast lesions: a machine learning model to predict pathologic upgrade and reduce unnecessary surgical excision.\n\nRadiology 286, 170549 (2017).\n\n59.\n\nDalal, A. K. et al.\n\nSystems engineering and human factors support of a system of novel ehr-integrated tools to prevent harm in the hospital.\n\nJ.\n\nAm.\n\nMed.\n\nInform.\n\nAssoc.\n\n26, 553–560 (2019).\n\n60.\n\nIntercom for Healthcare   \n61.\n\nPrediction and Early Identification of Disease Through AI—Siemens Healthineers   \n62.\n\nWillemink, M. Ai for CT image reconstruction - a great opportunity.\n\nAI Blog  (2019).\n\n63.\n\nBajgain, B., Lorenzetti, D., Lee, J.\n\n& Sauro, K. Determinants of implementing artificial intelligence-based clinical decision support tools in healthcare: a scoping review protocol.\n\nBMJ Open 13, e068373 (2023).\n\n64.\n\nDavid Lat, E. M. Advanced targeting and lethality automated system archives.\n\nBreaking Defense   \n65.\n\nUtegen, A. et al.\n\nDevelopment and modeling of intelligent control system of cruise missile based on fuzzy logic.\n\nIn 2021 16th International Conference on Electronics Computer and Computation (ICECCO), 1–6 (2021).\n\n66.\n\nBohr, A.\n\n& Memarzadeh, K. Chapter 2 - the rise of artificial intelligence in healthcare applications.\n\nIn Artificial Intelligence in Healthcare, (eds Bohr, A.\n\n& Memarzadeh, K.) 25–60 (Academic Press, 2020).\n\n67.\n\nMorgan, F. E. et al.",
    "In Artificial Intelligence in Healthcare, (eds Bohr, A.\n\n& Memarzadeh, K.) 25–60 (Academic Press, 2020).\n\n67.\n\nMorgan, F. E. et al.\n\nMilitary Applications of Artificial Intelligence: Ethical Concerns in an Uncertain World (RAND Corporation, Santa Monica, CA, 2020).\n\n68.\n\nIntroduction to the law of armed conflict (loac)   \n69.\n\nRule 1.\n\nThe principle of distinction between civilians and combatants.\n\nIHL   \n70.\n\nDocherty, B.\n\nLosing humanity.\n\nHuman Rights Watch  (2012).\n\n71.\n\nGenerative Artificial Intelligence and data privacy: A Primer - CRS Reports   \n72.\n\nJournal, H. Hipaa, healthcare data, and artificial intelligence.\n\nHIPAA J.\n\n(2023).\n\n73.\n\nPatel, V. L., Kannampallil, T. G. & Kaufman, D. R. Cognitive informatics for biomedicine: human computer interaction in healthcare (Springer, 2015).\n\n74.\n\nII, W. N. P. Risks and remedies for artificial intelligence in health care.\n\nBrookings  (2022).\n\n75.\n\nLyons, J.\n\nB.\n\n& Stokes, C. K. Human-human reliance in the context of automation.\n\nHum.\n\nFactors 54, 112–121 (2012).\n\n76.\n\nAsan, O., Bayrak, E. & Choudhury, A.\n\nArtificial intelligence and human trust in healthcare: Focus on clinicians (preprint) (2019).\n\n77.\n\nLewis, M., Sycara, K. & Walker, P. The Role of Trust in Human–Robot Interaction, 135–159 (Springer International Publishing, 2018).\n\n78.\n\nHawley, J. K. Looking back at 20 years of manprint on patriot: Observations and lessons (2007).\n\n79.\n\nParikh, R. B., Obermeyer, Z.\n\n& Navathe, A. S. Regulation of predictive analytics in medicine.\n\nScience 363, 810–812 (2019).\n\n80.\n\nRichardson, J. P. et al.\n\nPatient apprehensions about the use of artificial intelligence in healthcare.\n\nnpj Digit.\n\nMed.\n\n4, 140 (2021).\n\n81.\n\nChristian, R. Mind the gap the lack of accountability for killer robots.\n\nHuman Rights Watch  (2015).\n\n82.\n\nHabli, I., Lawton, T. & Porter, Z.\n\nArtificial intelligence in health care: accountability and safety.\n\nBull.\n\nWorld Health Organ.\n\n98, 251–256 (2020).\n\n83.\n\nObermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in an algorithm used to manage the health of populations.\n\nScience 366, 447–453 (2019).\n\n84.\n\nN, O. et al.\n\nAddressing racial and ethnic inequities in data-driven health technologies 1–53 (2022).\n\n85.\n\nChar, D. S., Shah, N. H. & Magnus, D. Implementing machine learning in health care—addressing ethical challenges.\n\nN. Engl.\n\nJ. Med.\n\n378, 981–983 (2018).\n\n86.\n\nFrisk, A.\n\nWhat is Project Maven?\n\nThe Pentagon ai project Google employees want out of - -national.\n\nGlobal News (2018).\n\n87.\n\nShane, S. & Wakabayashi, D. The business of war’: Google employees protest work for the Pentagon.\n\nThe New York Times  (2018).",
    "# Acknowledgements\n\nY.W.\n\nwould like to acknowledge support from the University of Pittsburgh Momentum Funds, Clinical and Translational Science Institute Exploring Existing Data Resources Pilot Awards, the School of Health and Rehabilitation Sciences Dean’s Research and Development Award, and the National Institutes of Health through Grants UL1TR001857, U24TR004111, and R01LM014306.\n\nY.P.\n\nwould like to acknowledge support from the National Institutes of Health through Grants 4R00LM013001, R01LM014306, and the National Science Foundation through Grant 2145640.\n\nThe sponsors had no role in study design; in the collection, analysis, and interpretation of data; in the writing of the report; and in the decision to submit the paper for publication.",
    "# Author Contributions\n\nD.O.\n\nconceptualized, designed, and organized this study, analyzed the results, and wrote, reviewed, and revised the paper.\n\nJ.H.\n\nanalyzed the results, and wrote, reviewed, and revised the paper.\n\nR.K.P., J.C.P., G.L.L., and Y.P.\n\nwrote, reviewed, and revised the paper.\n\nY.W.\n\nconceptualized, designed, and directed this study, wrote, reviewed, and revised the paper.",
    "# Additional Information\n\nCorrespondence and requests for materials should be addressed to Yanshan Wang.\n\nReprints and permission information is available at \n\nPublisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nOpen Access: This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.\n\nThe images or other third-party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material.\n\nIf material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\nTo view a copy of this license, visit \n\n© The Author(s) 2023",
    "# PR Council Guidelines on Generative AI\n\nPR Council  \n1460 Broadway, Suite 6006 New York, NY 10036  \nPRCouncil.net\n\nGenerative AI is predicted to become a transformational technology and may be an increasingly useful and valuable tool for public relations professionals in the years to come.\n\nThe responsible use of AI in public relations will enhance our ability to serve our clients and advance the public interest.\n\nHowever, the responsible use of AI must always be subject to human judgment and oversight to avoid bias, misuse, and inadvertent risks of harm.\n\nThis technology is rapidly evolving, and many agencies, professionals and organizations are exploring the potential of generative AI, current issues, and longer-term implications.\n\nThe following is guidance on the use of generative AI in our work for clients and our agencies based on the PR Council Code of Ethics and Principles.\n\nThe PR Council strongly encourages agencies and their teams to actively engage in internal testing and learning relative to generative AI within a proper ethical and legal framework.\n\nFollowing these guidelines will help ensure that the use of generative AI aligns with our members’ core commitment to the highest level of professionalism, decision making, and ethical conduct.\n\nWhile these guidelines have been created with the input of a group of leaders across our Membership and reviewed by our legal partners at Davis+Gilbert, we encourage leaders to consult with their legal counsel as they tailor these guidelines and implement specific policies and training most appropriate for their clients, employees and vendors.",
    "## We protect the integrity of client information.\n\nUse caution when putting confidential client information into a generative AI tool or platform.\n\nFor example, do not use it to create the first draft of a press release about a new product or to draft internal memos for staff.\n\nOther examples of confidential information include, but are not limited to:\n\n- Client business plans,\n- Client or prospect PPTs or documents,\n- Paid or confidential analyst reports that you would like summarized,\n- Paid market insights to detail key findings,\n- Confidential research data, and\n- Text related to sensitive internal employee communications.\n\nWhy?\n\nMany different types of generative AI tools and platforms exist, and some tools may use the information you enter as prompts for written or visual content to train future iterations of their model and be incorporated into future output it generates.\n\nUnless specifically stated or identified as a closed tool or platform, anything entered as a query may enter the public sphere and release information and intellectual property into someone else’s database not under your control.\n\nDo not use generative AI images as final creative for a client campaign.\n\nThe AI-generated work may be at risk of copyright infringement and the work itself cannot be entirely protected under current U.S. copyright laws (since humans did not create it), according to current court rulings and U.S.\n\nCopyright Office policies.\n\nMoreover, certain generative AI tools may limit the user’s ownership rights in the work the platform generates.\n\nNote: The above guidelines also apply to confidential, internal agency information.",
    "## We honor our role in society.\n\nDo not use generative AI to create or spread deepfakes, nor misinformation or disinformation.\n\nWe are committed to accuracy.\n\nAlways check and source the data generative AI tools provide.\n\nAlways validate the claims with your own search of the source.\n\nJust because the tool says it came from a source does not mean it did, as some generative AI chat tools are exceptionally good at convincingly fabricating information.\n\nAlways check for inadvertent plagiarism, copyright infringement, or trademark infringement in AI-generated output.\n\nAsk vendors about how they use the AI in their tools (such as sentiment analysis) and work to eliminate biases and improve accuracy.\n\nAsk vendors to provide transparency around their prompts and inputs used to generate desired output, to help mitigate the risk of inadvertent infringement of third-party rights.\n\nWe believe that our clients and the public are best served when third-party relationships with spokespeople, bloggers, partners, and allies are open and transparent.\n\nWe recommend disclosure to clients if generative AI tools are used in any part of the creative process.\n\nFlexibility can be applied to determining how and when to disclose.\n\nWe recommend agencies include wording as part of their contract and scope of work if this is a common occurrence.\n\nIf generative AI is being used infrequently, agencies should call out its use on individual pieces of work in which it played a substantive role.\n\nWe encourage agency leaders to set policies on what requires written disclosure versus verbal.\n\nWhy?\n\nThe PR Council encourages transparency.\n\nAdditionally, most of our members have contracts that state that all materials we produce on behalf of clients are “work for hire” meaning we assign the ownership rights of those materials to our clients.\n\nHowever, that is currently impossible to do with solely AI-generated materials which could put us in violation of our contracts.\n\nWe would also be exposed to increased legal risks due to dual indemnification clauses, which are common in most agency contracts.\n\nClarify with your staff when disclosure needs to be in writing and when verbal disclosure is sufficient.\n\nAs an employee, you should disclose to your manager if you use generative AI tools as part of the drafting or creation process.\n\nInfluencers should disclose if they use generative AI tools as part of their content creation on behalf of your clients and in their posts.\n\nThe disclosure should be on every sponsored post they create.\n\nInfluencers should be transparent with the agency about how AI-generated content was developed and should be responsible to the agency and client if the content results in a legal claim.\n\nVoice/music generating AI tools should never be used to mimic the voice or style of a real person.\n\nIf there is a need to correct a voiceover and it is mutually agreed upon with the talent that AI can be used for such correction, then that will require a signed agreement before proceeding.\n\nIf the voiceover talent is a union member, or if the agency or client are signatories to a union contract, additional requirements may apply as well.\n\nRespect the rights of other creators and do not prompt generative AI to develop creative content similar to that of a specific artist.",
    "## We value diversity and inclusion in our profession.\n\nBeware of biases incorporated in AI-generated output, both in writing and in developing imagery for a campaign.\n\nSome key questions to ask when evaluating an AI platform or tool to better understand potential bias can be found below.\n\nDo not rely on generative AI tools to translate or transcreate documents into other languages.\n\nThe quality of the transcreation or translation might not be accurate.\n\nThe quality of the transcreation or translation might not be accurate.\n\nDo not use generative AI as a replacement for diverse experiences, insight, or engagement.\n\nUtilize diverse perspectives within the agency to review content created by generative AI tools to ensure no bias is accidentally overlooked or shared externally.\n\nDo not use generative AI tools to create imagery, likenesses, or avatars that create the appearance of diversity instead of working with diverse talent.",
    "## We are committed to agency practices that increase society’s confidence in the practice of public relations.\n\nAgencies should establish clear guidance and conduct regular, firm-wide training on best practices and the proper ethical and legal use of AI to protect their brands and their clients’ brands.\n\nTraining should focus on the following, to avoid ethical and legal risks:\n\n- Best practices for using AI in workflows,\n- Avoiding or mitigating potential algorithmic biases,\n- Enhancing client and stakeholder transparency,\n- Proper and full sourcing,\n- How to identify inaccurate results, and\n- Maintaining the integrity of intellectual property.\n\nAgencies should establish or expand their internal reporting frameworks to clearly define how staff can raise concerns.\n\nSince generative AI and its uses are evolving rapidly, agency leadership should update guidelines, training and discussion at least quarterly.\n\nPR Council will support Members by providing regular updates and trainings.",
    "### INTRODUCTION\n\nTo maintain this trust, financial firms design and implement policies and controls that enable employees to make good decisions and adhere to relevant regulations.\n\nOne type of policy is known as Acceptable Use, which outlines good risk management and security practices on specific systems and technologies.\n\nGenerative AI is one such technology that has the potential to revolutionize every industry.\n\nIt is a powerful driver of optimization, efficiency, and cost reduction as well as the basis for new business lines and products.\n\nIt will be integrated into our companies at all levels.\n\nBut there are many risks that come with it, and financial firms must be proactive in managing internal adoption and use of generative AI.\n\nThis framework is a guide for firms to design their own Acceptable Use policy for external generative AI.\n\nGiven the rapid development and adoption of generative AI, we hope this guide serves as a helpful tool for firms to upgrade their security and risk management policies to incorporate safe and responsible AI use into their security programs and beyond.\n\nSome argue that financial institutions (FIs) should take a stringent approach and block external generative AI systems, as these are still nascent, untested, and unvetted.\n\nOthers believe that employees may find workarounds to blocking these systems, and so it would be more productive and indeed, more secure, to educate employees on how to safely use them.\n\nThis framework offers policy guidance on both permissive and stringent approaches, allowing firms to decide the right balance for themselves.\n\nWhat follows is a short explanatory text followed by sample policy text (labeled “Policy Guidance”) that firms can adapt for their own use as they see fit.",
    "### Introduction\n\nThis policy defines requirements for the acceptable use of external generative AI services.\n\nThis policy describes management’s directive to:\n\n- Ensure protection of the company’s intellectual property\n- Ensure that use of these systems reflects the culture and ethics of the company, as well as the regulatory, privacy, and legal obligations of the company and its employees\n- Establish a baseline of proper use of these systems for all employees\n- Ensure compliance with all applicable laws, rules, and regulations, including privacy requirements such as General Data Protection Regulation (GDPR)\n- Ensure compliance with the terms of use of many generative AI systems (especially attribution and lack of copyright protection)\n\nThe policy assumes as an overarching principle that data loss risks are present in the use of generative AI systems, like any other third-party system.\n\nThis document applies in all manners of consumption, such as via API, UI, or any other interface, as well as in all manners of access, including both corporate and personal devices.",
    "#### Confidentiality\n\nCurrently, most external generative AI systems use queries for future training (although some systems allow opt-outs).\n\nThe service may preserve the queries, which hackers could breach and release.\n\nQueries with sensitive data put companies at significant risk.\n\n- References, especially if the data is proprietary or copyrighted\n- Limiting or obfuscating the data if it has Personally Identifiable Information (PII), Nonpublic Personal Information (NPI), or other data under the purview of regulators or similar entities\n- Repurposing or reducing extraneous information that if exposed publicly:\n  - Might tarnish the reputation of the company\n  - Could open the company to regulatory or legal action\n  - May allow reverse engineering\n  - May give access to IP or systems\n- A reminder to not share (in Generative AI or any form externally) any information that, if exposed publicly, could reveal the company’s strategy and/or would be in violation of safe harbor statements\n\n1.\n\nThis can be adjusted if, through contractual agreements, the firm has the keys to their data vault and can protect the data held at the service.",
    "### Responsibility\n\nAlthough generative AI has the power to increase cyber crime through convincing mimicking of existing communications, excellent translation capabilities, deep fake images, audio, and video, ease of finding code vulnerabilities, and much more, the kinds of threats still are the same as when using other third-party systems.\n\nFor instance, phishing emails may become more personalized with generative AI, but the way these enter the organization still is the same.\n\nThe responsibility to safeguard against malicious activity when using generative AI is therefore the same as general internet usage to protect IP and other concerns such as compliance.",
    "### Access\n\nThis section elucidates the range of options for controlling access to generative AI systems, including guidance for which staff and from what devices they can access, as well as whether to use corporate identities when using them, given that it is possible that such queries become public.\n\nAlso, firms should publish a list of approved generative AI systems, based on its assessment of the vendor’s compliance with laws, rules, and regulations.\n\n(FS-ISAC AI Risk Vendor Risk Subgroup is working on a questionnaire to aid this assessment.)",
    "#### Accuracy\n\nGenerative AI systems are far from perfect, especially with fact-based output.\n\nThese systems may “hallucinate” when their answers appear convincing but are completely wrong.\n\nUsers should not rely on their accuracy.\n\nIncorrect answers can cause severe issues for companies.\n\nFor example, inaccurate instructions for IT administrators may lead to data loss or other system damage.\n\nDistortion of statistics or other facts may lead to employees making public claims or key decisions based on erroneous information.\n\nOther considerations:\n\n- Since generative AI may not be current on legislation, answers on accounting, tax, or other legal considerations may be inaccurate\n- Generative AI cannot know organizational culture or intricacies of industries, leading to suggestions that may do more harm than good\n- Open users to copyright violations\n- Open users to conflicts with other firms potentially using the same or similar output\n\nAs generative AI becomes increasingly powerful, organizations will need to reevaluate what they consider “acceptable” use of such systems.\n\nNo policy document, permissive or stringent, can cover all situations.\n\nFor example, a manager short on time uses a generative AI system to write an employee appraisal.\n\nThat employee does not like their appraisal and files a complaint.\n\nSince an actual human did not write the appraisal, there is good reason to believe that the employee's grievances would be upheld.\n\nThe onus is on the user to attribute the output correctly.\n\nSpecific for programming code uses of generative AI, the input to the base of many of these systems is open-source software (OSS) code found in common repositories.\n\nSome firms limit the open-source licenses used in their products and need to verify that the output comes from an OSS license they approved.\n\nWhile there are tools that try to detect if a generative AI system developed a specific output, they are not (yet) dependable enough to find whether an employee’s work is original or not.\n\nThis document assumes you cannot use such tools, but that may change.",
    "## Executive Summary\n\nAs the use of generative AI increases, organizations are revisiting their internal policies and procedures to ensure responsible, legal, and ethical employee use of these novel tools.\n\nThe Future of Privacy Forum consulted over 30 cross-sector practitioners and experts in law, technology, and policy to understand the most pressing issues and how experts are accounting for generative AI tools in policy and training guidance.\n\nFPF’s Internal Policy Checklist is intended as a starting point for the development of organizational generative AI policies, highlighting four areas in which organizations should develop and/or assess internal policies.\n\nThe full checklist includes additional detail and guidance.\n\n- **Use in Compliance with Existing Laws and Policies for Data Protection and Security**\n  - Designated teams or individuals should revisit internal policies and procedures to ensure that they account for planned or permitted uses of generative AI.\n\nEmployees must understand that relevant current or pending legal obligations apply to the use of new tools.\n\n- **Employee Training and Education**\n  - Identified personnel should inform employees of the implications and consequences of using generative AI tools in the workplace, including providing training and resources on responsible use, risk, ethics, and bias.\n\nDesignated leads should provide employees with regular reminders of legal, regulatory, and ethical obligations.\n\n- **Employee Use Disclosure**\n  - Organizations should provide employees with clear guidance on when and whether to use organizational accounts for generative AI tools, as well as policies regarding permitted and prohibited uses of those tools in the workplace.\n\nDesignated leads should communicate norms around documenting use and disclosing when generative AI tools are used.\n\n- **Outputs of Generative AI**\n  - Systems should be implemented to remind employees to verify outputs of generative AI, including for issues regarding accuracy, timeliness, bias, or possible infringement of intellectual property rights.\n\nOrganizations should determine whether and to what extent compensation should be provided to those whose intellectual property is implicated by generative AI outputs.\n\nWhen generative AI is used for coding, appropriate personnel should check and validate outputs for security vulnerabilities.",
    "## Introduction\n\nGenerative AI is a category of artificial intelligence that “generate[s] new outputs based on the data they have been trained on.”1 Large Language Models (LLMs) are a popular type of generative AI that generates responses to natural language queries.\n\nExamples include Google Bard and Open AI’s ChatGPT (chatbots), Microsoft’s AI-powered Bing (search engine), Midjourney’s Midjourney and Open AI’s DALL-E (image generators).\n\nGenerative AI tools can draft emails or computer code, outline reports or blog posts, provide biographic information, perform customer service functions, generate images, and write scripts for popular television shows.2\n\nAs their general popularity increases, so does workplace use of generative AI and LLMs.\n\nWorkers are using generative AI tools in every field, across specialties, and at all levels of employment; there are few jobs in which LLMs are not relevant in at least one application.3 Accordingly, organizations must grapple with the legal and social risks, benefits, and long-term consequences of organizational support and use of generative AI.\n\nOrganizations are rapidly revisiting internal policies and procedures to ensure responsible, legal, and ethical use.\n\nWorkers should be properly trained on the organization’s policies and processes for acquiring and using these tools to ensure a proper understanding of how the tools work (or do not work), risks to the organization if they are not properly acquired or used, and their limitations.\n\nThe Future of Privacy Forum’s checklist provides guidance regarding:\n- Use in Compliance with Existing Laws and Policies for Data Protection & Security\n- Employee Training and Education\n- Employee Use Disclosure\n- Outputs of Generative AI\n\n1 Nick Routley.\n\n“What is generative AI?\n\nAn AI explains,” World Economic Forum (Feb. 6, 2023), link.\n\nGenerative AI can be used in a variety of contexts, to include creating images, text, videos, code, audio, etc.\n\nSee generally “The Privacy Expert’s Guide to Artificial Intelligence and Machine Learning,” FPF (October 2018), link.\n\n2 See, e.g., link.\n\n3 Annie Lowrey, “How ChatGPT Will Destabilize White-Collar Work,” The Atlantic (Jan. 20, 2023), link\n\nFPF consulted with leaders across business sectors to learn more about how organizations are using generative AI across teams and in different contexts.\n\nWe held a series of conversations that included more than 30 experts on technology, law, and policy to understand the most pressing issues and how experts are accounting for generative AI tools in policy and training guidance.\n\nThe below checklist, which provides a catalog of considerations for the use of generative AI within organizations, is a result of these conversations.\n\nThis is a living document; new issues associated with the use of generative AI or LLMs are routinely discovered and refined.\n\nWhen use of generative AI tools within an organization is imminent or already occurring, time may be of the essence, and a comprehensive training program may not be feasible.\n\nIn such cases, it is critical for key units and individuals to collaborate with all employees to understand how and why different teams may want to use these tools and, at a minimum, form a cross-functional team (e.g.\n\nprivacy and compliance, human resources, legal, etc.)\n\nto compile and clearly communicate a survey of acceptable and prohibited uses, a designated contact point for any uses that are not specifically accounted for, and a timeline for any future actions that may provide greater detail or clarity.\n\nThis full checklist should be considered as a starting point for this cross-functional team, or any other system an organization chooses, for more advanced conversations, as well as a gateway to address additional issues unique to a particular organization or field.\n\nRisk management within the context of generative AI models is also an area of ongoing exploration, as some companies have already highlighted the potential risks of their generative AI systems.4\n\nNote: We use the term “employees” as inclusive of, but not limited to: full-time staff, part-time staff, contractors, interns, or any others providing services for any form of compensation.\n\nOrganizations should adapt these recommendations to be most useful for their area or sector and different employees, and should be read in the context of those factors.",
    "## Use in Compliance with Existing Laws and Policies for Data Protection and Security\n\nDesignated teams or individuals should revisit internal policies and procedures, including privacy policies, data use policies, information classification and management policies, and terms of service, to ensure that they account for planned or permitted uses of generative AI.\n\n4 See “GPT-4 System Card,” Open AI (Mar.\n\n23, 2023), link\n\nIndividuals or teams responsible for procurement and/or enterprise risk management should collaborate to develop criteria to assess and approve new or updated third-party software and services that integrate with generative AI APIs or offer generative AI features.\n\nInternal reviewers should consider the data sets used to create the outputs, as not all tools raise the same risks.\n\nReviewers should also consider whether the organization should provide transparency to the public or impacted individuals regarding the organization’s generative AI use.5\n\nSharing data with vendors must be subject to requirements that ensure compliance with relevant US state laws regulating sharing or sale of data.\n\nReview contractual terms to ensure that any uses of data by vendors reflect mandatory state contractual language, or are subject to approved exceptions.\n\nIf use of data by a vendor is deemed sharing or sale, ensure appropriate consumer notices are in place and ensure the organization and vendors comply with relevant consumer requests such as “Do Not Sell” requests.\n\nEnsure vendors will support any required access and deletion requests.\n\nManagers should remind all employees that relevant current or pending legal obligations will continue to apply to the use of new tools, particularly in regard to internal policies as well as applicable laws and regulations related to privacy and data protection, automated-decision making, data use, bias and discrimination, intellectual property, or other legal or policy frameworks of particular interest to the organization.\n\nAs necessary, specific training may be useful as to how to mitigate legal liability in the use of generative AI.\n\nUses with heightened risks may warrant prior review, including legal review.\n\nIf an organization is part of a regulated industry, it should pay extra care to understanding and communicating any specialized legal obligations or liability.\n\nRules should be considered for employees to ensure that they are not intentionally exposing their organization to liability.\n\nThe organization should review guidance, where it exists, from relevant regulatory agencies on the use of generative AI, and incorporate that information into their internal policies and protocols.\n\nEmployees should be advised to avoid inputting sensitive or confidential information into a generative AI prompt unless data is processed locally and/or subject to appropriate controls regarding access or use.\n\nEmployees should not prompt generative AI tools to output sensitive or confidential information unless data is processed locally and/or subject to appropriate controls regarding access or use.\n\nSensitive or confidential information may include corporate trade secret information or data about users, competitors, clients, customers, employees, subscribers, or other individuals.\n\nSpecial care should be taken when handling children’s data, education data, hiring or workplace data that could lead to claims of discrimination or harassment, and other regulated forms of data.\n\nWhen using generative AI applications on work-issued devices, employees should be advised as to recommended settings or permissions associated with the LLM or generative AI to ensure that data on that device is protected against unwanted access by the application.\n\nEmployees should be reminded of prior data protection and security training to ensure that their devices and networks are secure in order to prevent unauthorized access to data.\n\n5 See “Generative AI: eight questions that developers and users need to ask,” Information Commissioner’s Office (Apr.\n\n3, 2023), link",
    "## Employee Training and Education\n\nOrganizations should inform employees about the implications and consequences of using generative AI tools in the workplace.\n\nOrganizations should review and understand the generative AI system’s terms of use and other relevant materials, including privacy policies, to understand how personal data is handled, processed, and protected.\n\nIf there are specific generative AI tools that the organization wishes to recommend, discourage use of, or issue special warnings for, be sure to communicate that clearly and affirmatively.\n\nOrganizations must identify risks of using generative AI in context, including legal, regulatory, or ethical obligations, as well as potential liabilities associated with the use of generative AI tools.\n\nOrganizations should provide employees with new or existing resources that advise about the responsible use of any automated processing tool.\n\nExisting educational resources should be updated where possible to expressly address generative AI tools.\n\nRelevant training and workshops may include, but are not limited to, training on ethics, bias, data inaccuracy, security concerns, intellectual property rights, confidential information, and data minimization.\n\nSoftware developers and data scientists accessing generative AI models through APIs or building applications that use these models should be trained on ethics, bias, data inaccuracy, security concerns, intellectual property impacts, trade secrets, and data minimization.\n\nA system should be established to regularly remind individuals of legal restrictions on profiling and automated decision-making, as well as key data protection principles such as data minimization, purpose limitation, limitations on sale of personal data, and privacy by design and by default.\n\nGiven the speed at which generative AI technologies are developing, leadership at organizations should designate personnel responsible for staying abreast of regulatory and technical developments and ensure that company policies and employee practices reflect such changes.\n\nThe contact information for these personnel should be available to all employees, and employees should be reminded of the appropriate points of contact for the organization's privacy and/or data protection policies (e.g.\n\ndata protection officers) should they have any questions or concerns about the use of generative AI tools.",
    "## Employee Use Disclosure\n\nOrganizations should establish policies for how employees should sign up to use generative AI tools that require account creation, including whether the organization requires or prohibits the use of organizational email accounts for particular AI services or uses.\n\nEmployees should only use generative AI tools or systems that have been approved by the organization.\n\nAccountability for the use of generative AI may require that employees have access to a system to document their use of these tools for business purposes.\n\nSuch tools should be easy to use, enable employees to add context around any use, and provide a method to indicate how that use fits into the organizations’ policies.\n\nFor example, organizations may require employees to download and retain chat transcripts and prompts.\n\nOrganizations should communicate when and how the organization will require employees to disclose whether internal and/or external work product was created in whole or part by generative AI tools.\n\nOrganizations should recognize the creative approaches that many employees will take to professional use of generative AI tools.\n\nTypically, organizations should not create blanket bans on use by job title (e.g.\n\nHR employees), but rather provide employees with clear guidance on how they can or cannot use generative AI tools to perform their essential job functions (e.g.\n\nrestricting or prohibiting HR employees from inputting employee names, addresses, social security numbers, etc.\n\ninto ChatGPT).\n\nOrganizations should update internal documentation, including employee handbooks and related policies, to reflect policies regarding Generative AI use.",
    "## Outputs of Generative AI\n\nEmployees should be regularly reminded that: generative AI outputs can be incorrect, out-of-date, biased, or misleading.\n\nIndividuals are responsible for the content they create, regardless of the assistance of generative AI tools, and employees are encouraged to independently verify the accuracy of any outputs.\n\nVerification is particularly important when employees use AI in situations that require legal certification of accuracy, e.g.\n\nfinancial reports, court filings, and due diligence documents.\n\nEmployees should be advised that content from generative AI tools may be subject to copyright protections or implicate holders of intellectual property.\n\nDepending on the circumstance, organizational leadership may also advise employees to refrain from using AI-generated content if there is a question about intellectual property rights.\n\nThe organization should decide whether, to what extent, and in what situations, it is determined that compensation should be provided to those whose intellectual property is implicated by the output of a generative AI, including if there is direct use, derivative use, or when it is clear that the material was a source for the output.\n\nCoding outputs by generative AI should be checked and validated for security vulnerabilities.",
    "### Regulation of AI\n\n- FTC guidance regarding generative AI.\n\nNote in particular the Commission’s warnings about representations of accuracy.\n\n- Chatbots, deepfakes, and voice clones: AI deception for sale\n- The Luring Test: AI and the engineering of consumer trust\n- Keep your AI claims in check\n- GPT, GDPR, AI Act: How (Not) To Regulate “Generative AI?” (NYU Law, April 24, 2023)",
    "### Emerging EU Guidance\n\nAlthough this document is primarily intended for a US audience, emerging guidance from EU regulators is useful for US and global audiences.\n\nGenerative AI: eight questions that developers and users need to ask (ICO, April 3, 2023)\n\nFor more information please contact FPF Policy Counsel Amber Ezzell at  or .\n\n---"
]