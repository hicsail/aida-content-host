[
    "### What is Academic Integrity?\n\nWhen we talk about academic integrity at CSU, we’re talking about the behavior behind how students complete academic work.\n\nA student who has academic integrity:\n\n- Follows their instructor’s directions for how work in a course should be completed.\n\n- Chooses to do honest work even under challenging circumstances.\n\n- Understands that doing the right thing in a course is a character trait that must be practiced.\n\n- Works to understand and incorporate research ethically into their own work.\n\n- Encourages fellow students to complete their work honestly and ethically.\n\nUltimately, academic integrity is about authentically engaging with the course, the learning material, and the activities that help students learn.",
    "# AI at Western\n\nArtificial Intelligence (AI) is a huge field encompassing a broad range of theories, technologies, and practices.\n\nWhen someone says ‘AI’ in 2024 it is more likely than not that they mean generative deep neural networks like GPT-4.\n\nWe will restrict the scope of our advice to these generative AI tools.\n\nWe are accustomed to thinking of software as a tool.\n\nMicrosoft Excel is like a (very fancy) digital wrench: it does exactly, and only, what you instruct it to do, step-by-step.\n\nTo get proficient with this type of tool you need to read the manual and invest time in training.\n\nAlmost all the software that we’ve worked with up to this point in our lives is of a similar nature.\n\nThe most productive mental framework for interacting with current-generation LLM-based chatbots like Bing, ChatGPT, and Claude is to imagine that you are speaking with a very knowledgeable, enthusiastic, and naïve intern.\n\nIf you have an expectation that software will do only exactly what you tell it, that expectation will be subverted.\n\nLikewise, if you have an expectation that software will not offer its own opinions, or occasionally challenge your instructions, you may be surprised!\n\nExperiment and share!\n\nThe reality is that nobody knows how to optimally use generative AI.\n\nEach use case for AI is likely specialized to the role, and person, using AI.\n\nThe more time you spend using this technology, and experimenting with it, the greater the payoff to your productivity.\n\nIt won’t do magic for you “out of the box”, but you can accelerate your learning by reading about the successes of others with similar use cases.\n\nFeedback, comments, and questions are welcomed by email to .\n\nWhat you read here is a first step in a long journey.\n\nThis advice will evolve as the technology, and our understanding of it, grows.\n\nTown halls, surveys, and other opportunities to engage in dialogue on this topic are in planning and will be communicated broadly, including on this site.\n\nIf you would like to invite Mark to speak to your team please email .",
    "### Discussions of AI and ChatGPT in the Legal Academy\n\nDespite the sturm und drang over ChatGPT and AI among the legal academy and higher education more generally, most law schools do not have an official policy regarding ChatGPT and AI.\n\nOn this page, you will find advice and guidance on creating an AI policy from scholars and organizations, a summary of any policies that already exist, and an incomplete sampling of recent law school courses dealing with the topic of AI and the law.",
    "# ITS Overview – Generative AI Tool Usage, 2023\n\nAugust 28, 2023\n\nThe current wave of generative Artificial Intelligence (AI) technologies such as: ChatGPT, DALL-E 2, Bard, Midjourney, Stable Diffusion, MusicLM, et al has rapidly captured the attention, and imagination of higher education and the general public alike.\n\nThese are novel tools, and instructors, administrators, technologists, and students are all experimenting with their use in various settings including academic settings.\n\nThis overview highlights some key considerations for using this type of AI at Boston College from ITS’s perspective.\n\nIt does not directly address use of the technology by students for classwork.\n\nWhen using these tools at Boston College, please take all of the material below into consideration.\n\nUse of these tools must comply with all existing University policies.\n\nAcquisition of new AI software/services, like any other software, is subject to the “GetTech” process.\n\nFaculty considering the use of Generative AI themselves, or possible use by students in their classes, should refer to the Center for Teaching Excellence’s page,",
    "## Appropriate Use of Generative AI\n\nCite your source(s).\n\nGenerative AI, as of this writing, is somewhat notorious at not providing proper citations as to where it borrows information from in constructing answers to questions.\n\nIf you use generative AI, you should cite either the original sources, if possible, or simply that you used a given generative AI engine on a given date.\n\nSimilarly, if you use AI to generate images, you should cite this in the image label.\n\nThis is a traditional practice in academia and should be neither a surprise nor an inconvenience, and it is also ethically appropriate not to claim credit for others’ work, aka plagiarism.\n\nInspiration and summarization.\n\nUsing Generative AI to draft a document, or to shorten a document from say 3 pages to 1 page, can save significant time.\n\nGiven that the technology is currently prone to errors and biases, you should carefully review any such material generated by AI “Dangers…” (see below) and consider citing your use of the technology if it contributes substantially to the final product.\n\nResearch.\n\nExperimenting with this technology as an analytical or research tool, while mindful of the “Dangers…” (see below), is immensely important to the University and to society as a whole as we grapple with a very different technology than what most of us are used to.\n\nPlease share what you learn, even if informal, with the rest of the community.\n\n1 Material in this section is reworded for academia from",
    "## Dangers of Artificial Intelligence\n\nPrivacy: Whenever using or interacting with an AI system, such as when chatting online with ChatGPT, or asking an AI system to analyze or summarize data for you, be aware that any information you enter into the system can not only be processed but also retained and used by the AI to give answers to others.\n\nThis means if you enter any personal information about yourself or any confidential Boston College information, that information will be stored and potentially shared with or sold to others.\n\nDo not share or enter any information that you consider sensitive, personal, or is confidential Boston College data or research data.\n\nAlgorithmic Bias: UNESCO has warned that “AI is not neutral: AI-based decisions are susceptible to inaccuracies, discriminatory outcomes, embedded or inserted bias.” Users of AI must be cognizant of this.\n\nRecreating Voice/Video: AI solutions can take a recording of a person’s voice – your voice for example – and then use it to create real-time audio that sounds just like you.\n\nSo, a cyber attacker could create a message that sounds like you, potentially tricking your coworkers into thinking you called and asked them to take an action.\n\nAI can also do this with pictures or video.\n\nSometimes called Deep Fakes, an AI solution can take an existing picture or video of you and use it to recreate entirely new pictures or videos (including your voice) appearing to show you doing things that you never did.\n\nBecause of this, you should be wary of posting extensive voice or video of yourself on the Internet, and also be skeptical of recorded messages of colleagues.\n\nFinally, do not mis-use the technology to engage in this type of re-creation yourself.\n\nWrong Answers: Regarding the data or answers AI provides, the solutions can be wrong.\n\nAI often uses public information from the Internet, and its answers may be influenced by the biases of its developers or the data it was “trained” on.\n\nWhile typical search engines are designed to provide you the “best” or most correct answer to your queries, solutions like generative AI may be designed to give you the most human-like answer.\n\nWhich approach is better depends on what you are attempting to accomplish.\n\nPeople also give wrong answers, of course.\n\nBut we already know this and know to check peoples’ answers – generative AI answers should also be checked as appropriate.\n\nPrivacy: Whenever using or interacting with an AI system, such as when chatting online with ChatGPT, or asking an AI system to analyze or summarize data for you, be aware that any information you enter into the system can not only be processed but also retained and used by the AI to give answers to others.\n\nThis means if you enter any personal information about yourself or any confidential Boston College information, that information will be stored and potentially shared with or sold to others.\n\nDo not share or enter any information that you consider sensitive, personal, or is confidential Boston College data or research data.\n\n2 This section is largely adopted, including significant wording, from:",
    "# Acceptable Use of ChatGPT and Similar AI Tools\n\nWith the emergence of ChatGPT, Bard and other large language model generative artiﬁcial intelligence tools, hereinafter collectively referred to as “AI Tools”, many members of our community are eager to explore their use in the university context.\n\nThis advisory, which is jointly produced by the Ofﬁce of Legal Affairs, University Compliance Services, Information Security Ofﬁce, and the Business Contracts Ofﬁce, provides guidance on how to acceptably use these AI Tools safely, without putting institutional, personal, or proprietary information at risk.\n\nAdditional guidance may be forthcoming as circumstances evolve.",
    "## Prohibited Use:\n\nAt present, any use of ChatGPT or similar AI Tools cannot use any personal, conﬁdential, proprietary, or otherwise sensitive information unless a university contract is in place that speciﬁcally protects such university data from being used by training models or otherwise isolates university data into a separate instance that is not accessible by parties external to the university.\n\nIn general, student records subject to FERPA, health information, proprietary information, and any other information classiﬁed as Conﬁdential or Controlled university data must not be used with AI Tools.\n\n- Similarly, ChatGPT or similar AI Tools must not be used to generate output that would be considered non-public.\n\nExamples include, but are not limited to generating proprietary or unpublished research; legal analysis or advice; recruitment, personnel or disciplinary decision making; completion of academic work in a manner not allowed by the instructor; creation of non-public instructional materials; and grading.\n\n- Please also note that the company that owns ChatGPT, OpenAI, explicitly forbids the use of ChatGPT and their other products for certain categories of activity, including fraud and illegal activities.\n\nThis list of items can be found in their usage policy.\n\nAI Tools of any sort may not be used for any activity that would be illegal, fraudulent or a violation of any state or federal law, or UT Austin or UT System policies.",
    "## Rationale for the Above Guidance:\n\n- No UT Agreement, No Privacy and Security Terms: All content entered into, or generated by, ChatGPT is available to ChatGPT, its parent company, OpenAI, and their employees.\n\nThere is currently no agreement between UT Austin and OpenAI, Microsoft or other AI Tools that would provide data security and privacy protections required by UT policy with regard to ChatGPT, OpenAI’s, or other AI Tools’ programming interface.\n\nConsequently, the use of ChatGPT or other AI Tools at this time could expose individual users and UT to the potential loss and/or abuse of sensitive data and information.\n\n- As of May 2023, the UT Austin Business Contracts Ofﬁce is working on this issue.\n\nWe hope to see this addressed in the near future and will update this guidance when additional information is available.\n\n- Personal Liability: ChatGPT and other AI Tools use click-through agreements.\n\nClick-through agreements, including OpenAI and ChatGPT and other AI Tools’ terms of use, are contracts.\n\nIndividuals who accept click-through agreements without delegated signature authority may face personal consequences, including responsibility for compliance with terms and conditions [1].",
    "## References\n\n- Educator considerations for ChatGPT\n- OpenAI sharing & publication policy\n- OpenAI usage policies\n- OpenAI privacy policy\n- OpenAI terms & policies\n\n[1] Delegation of Authority: To ﬁnd out who has signature authority at UT Austin to “sign” a click-through agreement, please see the following page.\n\nInformation Security Ofﬁce\nCopyright © 2006-2024, Information Security Ofﬁce.\n\nAll rights reserved.",
    "# Policies and Principles on Using Generative AI Tools in Education\n\nA Proposal by Fordham’s AI Vision Committee July 2023\n\nThis document presents recommendations from Fordham's AI Vision Committee, established by Provost Dennis Jacobs in June 2023, with the aim of formulating policies and principles that leverage the benefits of GAI tools to enhance teaching and research at Fordham University.\n\nThe following proposal has been thoroughly discussed and endorsed by the AI Vision Committee.\n\nMembership includes:\n\nCo-chairs:\nProf. Aditya Saharia (Information, Technology, and Operations; Gabelli School of Business)  \nProf. Yijun Zhao (Computer and Information Sciences; Arts & Sciences)\n\nCommittee members:\nProf. Elissa Aminoff (Psychology; Arts & Sciences)  \nProf. Navid Asgari (Strategy and Statistics; Gabelli School of Business)  \nProf. Lauri Goldkind (Graduate School of Social Service)  \nProf. Russell Pearce (School of Law)  \nProf. Joshua Schrier (Chemistry; Arts & Sciences)  \nProf. Ralph Vacca (Communication and Media Studies; Arts & Sciences).",
    "## Introduction\n\nGenerative artificial intelligence (GAI) focuses on creating systems capable of generating content, including text, images, music, and computer programs.\n\nThese systems use statistical models trained on vast volume of text (including academic papers) and image samples to generate outputs that mimic human-like creativity in response to natural language inputs.\n\nGAI tools are freely or inexpensively available on the internet, making them accessible even to technologically unsophisticated users.\n\nThe integration of GAI in education presents both challenges and opportunities for institutions and has the potential to transform higher education.\n\nGiven that GAI can generate non-reproducible content that mimics human creators, there is a legitimate concern that students will exploit GAI for academic dishonesty or cheating.\n\nHowever, when used properly, students can leverage GAI to enhance their critical thinking skills by engaging in interactive discussions with AI-generated content.\n\nGAI can aid in improving writing abilities by providing suggestions, grammar corrections, and even generating draft content.\n\nGAI can also serve as a valuable tool in data analysis, as well as in the creation and debugging of computer programs, and as such, can lower barriers for including these topics in non-specialist courses.\n\nIt is crucial to maintain a balance between leveraging AI tools for support and preserving the authenticity and originality of student work to uphold academic integrity.\n\nA key objective of the AI Vision Committee is to address these challenges and harness the potential of GAI as a valuable educational resource while safeguarding the principles of academic integrity, recognizing that GAI offers us the potential for transforming how teaching, scholarship, and service are expressed in the Fordham environment.\n\nA vital consideration surrounding GAI is its impact on social justice.\n\nWhile generative AI has the potential to address educational inequalities and enhance access to education, there are concerns regarding bias and fairness in AI-generated content based on the data these systems are trained on; equity issues surrounding how different types of students can access such tools; and harms that AI potentially poses to our democratic system and the public good.\n\nIt is essential to critically evaluate and address potential biases and ensure that GAI systems promote inclusivity, diversity, and equal opportunities for all learners, as well as to understand the potential ramifications of AI for democracy and the public good.\n\nFordham has an opportunity to help students develop AI literacy to evaluate the implications of GAI tools and, thus, use them ethically and responsibly.\n\nAs New York's Jesuit University, Fordham is uniquely positioned to integrate generative AI into its ethos.\n\nBy emphasizing \"Cura Personalis\" and adopting the principles of RomeCall, Fordham can prioritize AI ethics, social justice inclusivity, and individual well-being.\n\nIn AI and education, interdisciplinary curricula can be developed to enhance quality and provide equal opportunities through accessible delivery methods.\n\nSpecific responses and considerations may be implemented to ensure the ethical and responsible use of GAI.\n\nThese responses may involve establishing guidelines and frameworks for the integration of GAI in teaching practices, addressing potential challenges related to academic integrity, and fostering discussions around the social and ethical implications of this technology within the Fordham community.\n\nAdditionally, it is essential to educate students to use GAI responsibly and ethically, enabling them to make informed decisions while harnessing the potential of this powerful technology.\n\nThe proposal is organized into teaching, scholarship, and external engagement recommendations.\n\nSpecifically, the proposal outlines the rationale and action items in the following key areas:",
    "## Teaching\n\n- **Promoting critical AI literacy among students, faculty, and staff**  \n- **Guidelines for the use of GAI tools in classroom learning**  \n- **Guidelines on adapting teaching and assessment for ethical GAI use and equal student access**  \n- **Upholding academic rigor and integrity as essential principles of education**\n\nFurthermore, the committee recommends implementing a public website titled \"Teaching and Learning in Response to Generative AI Tools\" for the Fordham community.\n\nThe website complements the above recommendations with additional details.\n\nA prototype for this website can be accessed here:",
    "## Scholarship\n\n- **Establishing a research center or institute to promote interdisciplinary research on advancing AI, its applications, and its impact on society**  \nAllocating internal funding for research on the ethical and social impacts of AI and its applications\n- **Hiring additional scholars and faculty to advance AI research at Fordham**",
    "## External Engagement\n\n- **Facilitating inter-university discussions on learning goals and pedagogy**  \n- **Collaborating with the industry on the future of knowledge and AI literacy**  \n- **Community engagement**\n\nThe AI Visioning Committee recommends that the administration take the following immediate actions:\n\n- Request the academic integrity committee to review and update the integrity statement.\n\n- Coordinate with deans to disseminate a statement regarding the responsible use of generative AI tools to faculty members for the upcoming fall semester.\n\n- Encourage department chairs and program directors to revisit their learning goals in light of rapid advances in GAI.\n\n- Organize workshops on GAI literacy for faculty members, providing them with essential knowledge and skills about this rapidly evolving capability.\n\n- Designate a dedicated point of contact for faculty members to address inquiries related to technology assistance and teaching strategies for integration of GAI tools.",
    "### Teaching\n\nFordham University is committed to promoting critical AI literacy among students, faculty, and staff.\n\nTo achieve this, the following action items are recommended:\n\n- Provide technical support and guidance to students, faculty, and staff on the effective utilization of GAI.\n\nThis can be facilitated through the development of online video tutorials, webinars, or workshop series.\n\n- Provide guidelines for the ethical use of generative AI in teaching and research.\n\nThis will include updates to the university’s Academic Integrity policies for students, faculty and staff.\n\nThe integrity statements will provide frameworks for students to identify the use of GAI in completing the course work and for faculty to identify the use of GAI in research and other scholarly publications.\n\nAdditionally, the committee recommends involving the IRB and University Research Council to ensure the responsible and ethical use of GAI in research and publications.\n\n- Establish a dedicated forum, such as a center, consortium, or other centrally administered unit, bringing together faculty members from across the university who share research and teaching strategies, tools, and projects focusing on ethical and social justice in the deployment of AI tools.\n\n- Create a Frequently Asked Questions (FAQ) page to address common queries and concerns.\n\nSome sample questions are as follows:  \n  - What is generative AI?\n\n- How does generative AI work?\n\n- What are the limitations of generative AI?\n\n- Can I use generative AI for my teaching?\n\n- What are the potential benefits of generative AI in education?\n\n- What are the potential risks and challenges of using generative AI?\n\n- How can generative AI be used responsibly in teaching and learning?\n\n- How can academic integrity be maintained when using generative AI?\n\n- What generative AI tools are available in the market?\n\n- Are there tools to detect AI-generated content, and why are they not so great?\n\n- Are there guidelines for citing and attributing AI-generated content?\n\n- What are the ethical considerations when using generative AI?\n\n- How can I address potential biases in AI-generated content?\n\n- How can I address the implications of AI for democracy and the public good?\n\n- Where can I learn about generative AI and other Generative AI tools?",
    "#### Guidelines on using generative AI tools in classroom learning\n\n**Sample statements in syllabi:** In order to effectively integrate generative AI tools into classroom learning, the committee offers the following statements for inclusion in course syllabi:\n\n- **For a \"No-AI\" approach:**  \n  “Generative AI tools are not permitted in this course.\n\nStudents [or learners] must rely on their own originality, creativity and critical thinking skills to complete assignments and engage with course material.”\n\n- **For a “Limited-AI” approach:**  \n  “Limited usage of generative AI tools may be allowed for specific assignments in this course, enabling exploration of ideas, complex data analysis, and creative solution development, when explicitly permitted by the instructor.\n\nWhen using these tools, it is mandatory to clearly indicate the sections of your work that were generated using them for proper attribution and transparency, and indicate the prompts and software versions that were used.\n\nIt is critical to adhere to ethical standards by refraining from activities like plagiarism or creating misleading content.\n\nAdditional guidelines or restrictions will be provided for specific assignments.”\n\n- **For a “Full-AI” approach:**  \n  “This course allows the use of generative AI tools to facilitate exploration of innovative ideas, complex data analysis, and creative solution development.\n\nStudents must clearly indicate the sections of the work that were generated using generative AI tools for proper attribution and transparency, and indicate the prompts and software versions that were used.\n\nIt is critical to adhere to ethical standards by refraining from activities like plagiarism or creating misleading content.\n\nAdditional guidelines or restrictions will be provided for specific assignments.”",
    "#### Sample assignments embracing GAI tools\n\nFor instructors actively seeking to integrate generative AI tools into their assignments, some suggested ideas include:\n\n- Conduct in-class discussions analyzing AI-generated writing to understand its strengths and limitations.\n\n- Assign students to revise and edit AI-generated texts to elevate them to their own standards.\n\nStudents will submit both the original AI draft and their final version.\n\n- Organize in-class presentations comparing and contrasting AI writing with human writing.\n\nPrompt students to reflect on elements replicable by ChatGPT and aspects unique to human authors in their work.\n\n- Explore refinement techniques by having students compose variations of the same prompt to fine-tune AI-generated results.\n\n- Scaffold engagement with AI tools by encouraging students to interact with AI, using it for brainstorming or divergent thinking exercises.",
    "#### Sample assignments deterrent of GAI tools\n\nFor instructors seeking to prevent the utilization of generative AI tools in their assignments, some suggested ideas include:\n\n- Require oral presentation of coursework to assess students' understanding and communication skills effectively, providing them with an opportunity to articulate their knowledge verbally.\n\n- Employ interactive, in-class exercises to promote active learning and real-time application of concepts, fostering a deeper understanding of the subject matter.\n\n- Engage in case studies based on current events.\n\nThis offers a short-term defense, as the base models are trained on content that does not include the past 6-12 months; however, emerging technologies giving GAI access to web-search content may erode this defense.",
    "#### Sample new assessment methods\n\nThe advent of GAI necessitates new grading methods to mitigate the influence of AI writers.\n\nFor assignments that prohibit GAI tools, some new assessment ideas include:\n\n- Assess the uniqueness of content using plagiarism detection tools or comparison with existing sources.\n\nHowever, it's important to acknowledge that existing tools are highly imperfect, as they may fail to detect AI-generated content and may also inaccurately identify work by non-native writers as AI-generated.\n\n- Compare the quality and creativity of take-home assignments with in-class work, considering factors such as coherence, style, and relevance.\n\n- Evaluate content accuracy and relevance in addressing assignment objectives.\n\nFor assignments that allow GAI tools, some assessment ideas include:\n\n- Prompt students to reflect on GAI's benefits and limitations, justifying their responses.\n\n- Require students to submit the prompts used for GAI and assess their ability to effectively customize and adapt AI-generated content to fit specific contexts or target audiences.\n\n- Assess the practicality and usefulness of AI-generated content in real-world scenarios, such as marketing materials or informational texts.\n\nAppendix I presents a rubric (adapted from ) that helps educators review assignment prompts for vulnerabilities to generative AI tools and create prompts without those weaknesses.\n\nFocusing on areas where AI falls short as a student writer, the rubric guides educators in designing assignments less susceptible to misuse while promoting the ethical use of generative AI tools.\n\nFordham University is committed to ethical use and equal student access to GAI.\n\nTo achieve these objectives, the committee recommends the following action items:\n\n- Advise faculty to have an explicit statement on the acceptable use of generative AI in the syllabus: Faculty members should be encouraged to incorporate a dedicated section in their course syllabi that outlines the guidelines and expectations for the appropriate use of generative AI tools.\n\nThis statement should clarify the scope of usage, highlight ethical considerations, and emphasize the responsible application of generative AI within the course context.\n\n- Encourage faculty to review the learning goals and assignment alignment in their courses: Faculty members need to be encouraged to critically evaluate the learning goals of their courses in light of the integration of generative AI tools.\n\nThey should reflect on how these tools can enhance the achievement of learning objectives and prompt students to engage more deeply with course concepts.\n\nFurthermore, faculty members should reconsider the role and format of assignments, exploring how generative AI tools can be integrated to foster critical thinking, creativity, and problem-solving skills among students, and also how assessments can be designed to reduce the temptation of cheating.\n\n- Request area chairs and program directors to review and update the learning goals for their academic programs: Area chairs and program directors should collaborate with faculty to review and update program learning goals, ensuring alignment with the curriculum's overall objectives when integrating generative AI tools.\n\nWhile revising the goals, ethical implications, pedagogical effectiveness, and impact on student outcomes must be considered.\n\nAdditionally, consideration should be given to appropriately preparing students for post-graduation studies and careers in a world with these tools.\n\n- The Writing Center's role in GAI integration: The Writing Center should lead campus discussions on the responsible integration of generative AI tools, focusing on plagiarism awareness, appropriate use in Eloquentia Perfecta courses, and faculty and student training.\n\nFordham University is committed to upholding academic rigor and integrity as essential principles of education.\n\nTo achieve this, the following action items are recommended:\n\n- **Revision of Academic Integrity Statement and Standards:**  \nThe Academic Integrity Statement and Standards will be carefully reviewed and updated to align with the evolving academic landscape.\n\nIn particular, this revision will address the ethical considerations and challenges associated with emerging technologies, including generative AI.\n\nIt will reinforce the importance of honesty, originality, and responsible use of resources.\n\nIt will also clarify the expectations and consequences regarding academic integrity violations.\n\n- **Communication to Students:**  \nDeans will inform all students based on the outcome of a), providing notice about the revised standards of academic integrity.\n\nThis communication will emphasize the significance of upholding ethical conduct and integrity in their academic work.\n\nIt will also provide clear guidelines and expectations regarding the use of generative AI tools, ensuring that students are aware of their responsibilities and obligations.",
    "It will also provide clear guidelines and expectations regarding the use of generative AI tools, ensuring that students are aware of their responsibilities and obligations.\n\nBy revising the Academic Integrity Statement and effectively communicating the revised standards to students, Fordham will reaffirm its commitment to maintaining high standards of academic rigor and integrity while addressing the ethical considerations associated with emerging technologies like generative AI.",
    "## Scholarship\n\n- **Establishing a research center or institute to promote interdisciplinary research on advancing AI, its applications, and its impact on society:**  \nThe center will serve as a hub for interdisciplinary research, bringing together faculty, some of whom are already leaders in research on AI, as well as students from diverse fields, and providing a platform for collaboration, knowledge exchange, and innovation in data science and AI.\n\nIt aims to form partnerships with industry, government agencies, and academic institutions while considering Fordham's unique niche at the intersection of theology/Jesuit scholarship, technology, ethics/morality, and democracy/the public good.\n\nThe center will also act as a facilitator, engaging internal stakeholders, including the Digital Humanities Consortium, Public Interest Technology University Network, Center for Community Engaged Learning, Center for Digital Transformation, Center for Information Law and Policy (CLIP), and the McGannon Center, as well as external stakeholders, such as Fordham alumni and IBM.\n\nThis initiative is inspired by similar centers and institutions established at peer universities:\n  - NYU: \n  - NYU Shanghai: \n  - Columbia: \n  - Stevens Institute of Technology: \n  - Caltech: Center for Science, Society, and Public Policy\n  - USC: Center for Generative AI and Society\n  - Vanderbilt: Future of Learning and Generative AI Initiative\n  - Stanford: Human Centered AI\n  - Public Interest Technology Initiative (umass.edu)\n\n- **Allocating internal funding for AI research:**  \nTo promote research on AI's ethical and social impacts, the committee suggests allocating internal funding from the Office of Research or Deans' grants.\n\nThis funding will support projects exploring ethical considerations, societal implications, and policy issues related to AI technologies.\n\nAreas of investigation may include AI tools and applications, bias and fairness in AI algorithms, privacy concerns, transparency in decision-making, AI's impact on employment, socio-economic structures, and the public good.\n\nProviding internal funding incentivizes faculty and students to conduct rigorous research addressing AI's complex ethical and social challenges.\n\n- **Hiring additional scholars and faculty to advance AI research at Fordham:**  \nAI is a rapidly evolving domain.\n\nBy recruiting additional experts in the field, the university can enhance its research capabilities, foster interdisciplinary collaborations, and enrich its academic programs.\n\nThese scholars and faculty members will contribute valuable insights, innovative methodologies, and cutting-edge knowledge to advance Fordham's GAI initiatives.\n\nTheir expertise can also help to address complex challenges and explore new applications across disciplines, reinforcing Fordham's commitment to staying at the forefront of emerging technologies.",
    "## External Engagement\n\nThe committee proposes initiating AI-focused discussions among Jesuit universities and fostering collaboration with industry leaders.\n\n- **Inter-University Discussions on learning goals and pedagogy:**  \nFacilitate inter-university dialog, particularly among Jesuit universities, to explore and revise learning goals and pedagogical approaches in response to AI advancements.\n\nThese discussions will foster collaboration and knowledge-sharing among institutions, enabling the efficient development of comprehensive strategies for AI integration across curricula.\n\n- **Industry collaboration on the future of knowledge and AI literacy:**  \nThe committee recommends active collaboration with industry leaders to gain valuable insights into the future of knowledge work and the expected AI literacy of graduates.\n\nThis collaboration is particularly crucial as the recent white paper published by Goldman Sachs predicts the potential elimination or reduction of 300 million jobs, with many in knowledge work, due to advancements in AI.\n\nThese statistics underscore the pressing need for universities to engage industry leaders and align their educational initiatives with emerging trends.\n\n- **Community Engagement:**  \nThe committee proposes dedicating efforts and resources to fulfill Fordham's mission of community engagement.\n\nGenerative AI, though new and largely mysterious to many, presents a unique opportunity for Fordham to offer workshops and other interactive initiatives for educating our neighbors, fostering AI literacy, and encouraging questions.\n\nWe recommend allocating resources to the Center for Community Engaged Learning, Fordham in Community, Fordham Foundry, and Fordham's Office of Student Involvement.\n\nThese organizations can play a crucial role in educating the community about generative AI, its potential benefits for individuals, and promoting informed usage.\n\n---\n\n**Appendix I:** AI Misuse Rubric -- adapted from TurnItIn",
    "## Abstract\n\nEducation systems around the world grapple with persistent issues related to equity, access, and quality, while also navigating the disruptions brought about by new technologies.\n\nThese technologies, and specifically generative AI, offer a range of powerful applications, and present an opportunity to reimagine our education systems, but generative AI comes with its own set of unique concerns and challenges and its integration in education poses a significant challenge for educators, researchers, and institutions.\n\nGovernments worldwide need to develop robust and comprehensive regulations and policies tailored to the ethical use of AI in education to ensure that, amongst other things, student data privacy and overall well-being are adequately protected.\n\nInaction by governments may mean significant consequences for students, educators, and society as a whole.\n\nThe future will belong to those who can understand and incorporate the use of emerging technologies like generative AI in everyday life.\n\nThis paper is a call to action for the G20 and policy makers around the world towards shaping AI use in education in a way that is informed, transparent, and responsible.",
    "## Introduction\n\nEducation today faces continued challenges around equity, access, and quality as well as new disruptions coming from emerging technologies - specifically generative AI.\n\nAcross the globe, educators and leaders are rethinking how and what we teach, learn and assess in a way that prepares students well for the changing world of the future.\n\nAn array of challenges faced in our current ecosystem limit its ability to enable the best possible education to all students.\n\nThese challenges are particularly prevalent around access, equity and quality of education, leading to widespread disparities in education outcomes and perpetuating social and economic inequalities.\n\nEmerging technologies though, and specifically generative AI, can have a significant beneficial impact on education outcomes and help to tackle these challenges if integrated effectively.",
    "### Education systems continue to face challenges in access, equity and quality\n\nAccess to education remains a significant challenge, particularly in low-income countries and marginalised communities.\n\nMillions of children are unable to attend school or access education due to a lack of infrastructure, financial barriers, cultural norms and other factors.\n\nA UNESCO report estimates that 224 million children and youth are out of school globally, with low-income countries hardest hit.\n\nIt is further estimated that children from the poorest 20% of families are eight times more likely to be out of school as children from the richest 20% in lower-middle-income countries.\n\nWhile the global out-of-school rate is gradually decreasing, this shift is too slow and large gaps still exist between different regions.\n\nEquity in education is a different pressing concern, arising when disadvantaged students face significant barriers to educational success.\n\nThese barriers include disadvantages based on socio-economic background, gender, ethnicity, and disability, as well as unequal access to resources such as well-trained teachers, textbooks, and technology.\n\nDisadvantaged students are more likely to attend poorly-performing schools, with fewer resources and less experienced teachers, while girls are twice as likely to be out of school than boys.\n\nWithin this context of inequity, some educators, researchers and students have access to cutting-edge digital tools, capable and experienced experts and innovative pedagogical methodologies, while others make do with limited access to even the most basic educational tools, resources and educational delivery techniques.\n\nAs a result, disadvantaged communities often have lower levels of educational attainment and fewer opportunities for future success.\n\nThe last focus of concern for this paper is around the provision of high-quality learning experiences that meet the needs of all students.\n\nOutdated curricula, inadequate teacher training and limited resources can all contribute to low-quality education outcomes.\n\nThere is often a mismatch between the skills that students learn in school and the skills that are in demand in the labour market, leading to high levels of youth unemployment and underemployment.\n\nIn classrooms teachers use a mix of digital and non-digital tools for their delivery, with their focus and time split between developing teaching plans and resources, delivering content, creating and grading assessments and coursework, in addition to other administrative tasks.\n\nA study conducted by EdWeek Research Centre shows that teachers work 54 hour weeks, with only half that time spent in student engagement like instruction, skill development and coaching.\n\nThis leaves little room for thinking about enhancing and furthering the curriculum, student experience or providing specialised, tailored learning.\n\nThe quality and appeal of teaching is variable - depending on the capabilities of the professionals and the training and resources available to them.\n\nFurther consideration needs to be given to what students are being taught and assessed on, and how much of it relies on memorization of facts versus application of skills, thought processes and capabilities that can be extrapolated to a variety of situations and contexts.",
    "### The transformative power of AI can help resolve some of these challenges\n\nGenerative AI technologies can help dramatically shift the landscape to address the gaps outlined above.\n\nThe capability to ingest, interrogate and build connections within vast amounts of data allows generative AI to produce content such as text, images and sounds presents new opportunities and applications for educators, researchers, students and parents that can enhance their experiences.\n\nGenerative AI can improve the quality of course materials and personalise them based on students' learning styles and specific needs, such as learning challenges and preferences.\n\nIt can create simulations and virtual environments, resulting in more engaging courses.\n\nAdditionally, AI can assist in creating new teaching materials and research content and produce images with specific modifications that respond to particular academic needs.\n\nThe use of AI tools for such tasks frees up time for work that can potentially have more impact on the learning experience.\n\nGiven the ongoing speed of technology and societal change, content needs to be continuously refreshed, to remain relevant; generative AI can enhance the quality of outdated or low-quality materials leading to deeper understanding of the content and better outcomes for educators, students and researchers.\n\nIt can be used to generate personalised learning materials and assessments for students, and it can create personalised assessments for students to test their knowledge and gauge their understanding of a topic.\n\nAI can offer virtual tutoring environments through chat bots and tutoring AI platforms, supporting students with real-time feedback and support outside of the classroom.\n\nThis is especially useful for those who lack access to in-person tutoring or learning resources.",
    "### The use of generative AI brings concerns with it\n\nDespite many powerful applications, and the need to reimagine our education systems in the light of new disruptions, generative AI technologies do come with their own set of unique concerns and challenges.\n\nHere we outline some of the key issues before offering a perspective on policy changes to address them.",
    "#### Academic Integrity, Ethics and Code of Conduct\n\nStudents and researchers creating content using AI tools and calling it their own work is challenging academic integrity, ethics and code of conduct.\n\nEducational and research institutions around the world have employed various measures to address these issues, including requiring students and researchers to disclose their use of generative AI tools such as ChatGPT.\n\nHowever, this does not provide a clear solution as the outputs generated by AI tools are a combination of the system-generated content, developer inputs, and user edits.\n\nThis creates a grey area in identifying and defining plagiarism and outlining sensible guidelines on AI use.\n\nWhile tools to detect whether content is created by AI are being developed and deployed, their effectiveness varies and the sophistication of means and mechanisms to get around them will continue to evolve in parallel.",
    "#### Knowledge Retention & Skill Development (Learning & Assessment)\n\nWhen skills development focuses on tool usage to access knowledge at the expense of expertise, a student’s ability to think critically and build deep knowledge is impacted.\n\nStudents are not able to assimilate knowledge without spending time with it, applying problem-solving and critical thinking techniques to dissect and analyse it.\n\nResearch on the psychology of learning indicates that this is critical for students to absorb the content and develop crucial thinking strategies.\n\nHowever, the use of generative AI tools can sideline this type of learning, leaving students vulnerable to shallow thinking and lacking in fundamental understanding.\n\nIt is important for students to predict, explain, and hypothesise to learn, rather than relying on ready-made answers that may only provide basic knowledge but fall short of critical, creative, and innovative thought.\n\nWhile the use of AI technologies will become a skill on its own, understanding how to build appropriate supporting knowledge and skillsets is important.",
    "#### Bias, inaccuracy and privacy\n\nThe content produced by generative AI tools is based on the data that was used to train it, which is raising concerns about potential bias, misinformation, lack of global understanding and overestimation of AI capabilities resulting in incorrect output.\n\nOpenAI, for example, has currently not disclosed details about the training methods and materials used for ChatGPT, making it a black box.\n\nAlthough large amounts of data may be used for training, it is essential to question the nature and origin of that data which is reflective of the types of biases and misinformation that could inflict harm.\n\nIf the training material focuses heavily on certain geographical regions or particular views, this limits the likelihood of reliable, global and inclusive output required to cater to users, especially students in their formative years and researchers publishing their findings, across the globe.\n\nRecently the risk of AI systems being ‘poisoned’ by actors deliberately feeding them misinformation maliciously has been noted, leading to increased concerns about the accuracy and bias of AI outputs.\n\nPrivacy and transparency concerns have been top of the agenda in recent years, as members of society at all levels increasingly use technologies for day-to-day tasks, and sharing their data.\n\nThere is not yet a clear view on how generative AI data is stored and handled, and for what purposes.\n\nThe use of AI needs to be navigated cautiously given the potential involvement of minors and the disclosure of confidential information.",
    "#### Further Risks to Access and Equity\n\nGenerative AI technology has the potential to increase socio-economic divides due to accessibility and equity factors, which may disadvantage marginalised communities and limit their access to equitable opportunities.\n\nCommunities that struggle with access to digital tools and internet connectivity do not have the same level of access to technologies as better supplied communities.\n\nIn addition, the data that is used to train the technology may not be representative or inclusive with respect to ethnicity, gender, or disability, making its use detrimental.\n\nIt is important to recognise these issues and find ways to minimise any potentially negative impact of generative AI on education and society.",
    "#### Training and Development\n\nIn light of the challenges and concerns surrounding generative AI in education, its integration currently poses a significant challenge for educators, researchers, and institutions.\n\nThe lack of education, training and guidance available for the responsible and ethical use of these tools, as well as limited understanding of their full capabilities and implications, contribute to this apprehension.\n\nGenerative AI technologies are innovative and disruptive, but it is important to remember that they are not a replacement for the human mind.\n\nPeople working with them need to develop the right capabilities to make best use of them as tools to be used alongside human creativity and critical thinking, while being aware of their risks and acting accordingly.",
    "## Recommendations to the G20 and Policy makers\n\nAs of now, there is no single comprehensive policy or regulatory framework in place that specifically addresses the use of AI in education.\n\nHowever, some existing regulations and policies that touch upon it, albeit indirectly, for instance, in the US, the Family Educational Rights and Privacy Act (FERPA) and Children’s Online Privacy Protection Act (COPPA) mandate that student data collected by technology providers should be kept private and secure.\n\nLikewise, in the European Union, General Data Protection Regulation (GDPR) provides a legal framework for the protection of personal data.\n\nThese regulations also apply to the use of AI in education, as they regulate the collection, use, and storage of personal data in any context.\n\nMoreover, there are some emerging frameworks that aim to guide the ethical use of AI in education; for example, the OECD Principles on AI, the UNESCO Recommendation on AI and Education, and the AI4ALL program of the National Science Foundation, USA.\n\nHowever, these frameworks are still in their nascent stage, and their effectiveness in regulating AI use in education remains to be seen.\n\nGovernments worldwide can use these as a starting point but need to develop robust and comprehensive regulations and policies tailored to the ethical use of AI in education to ensure that student data privacy and overall well-being are adequately protected.\n\nThey have a responsibility to promote the development and adoption of generative AI while also addressing the concerns articulated above.\n\nAs the capabilities of generative AI (and other emerging technologies) continue to expand, policymakers need to develop a nuanced understanding of its benefits and risks, and enact policies that can effectively manage its impact on education systems and society at large.",
    "### Policy Consideration 1: Access and use in the teaching learning process\n\nInstead of an outright ban (like in the case of Italy - subsequently reversed - or Saudi Arabia) or allowing unrestricted use of generative AI in the classroom, policy makers (and institutions) will need to consider various scenarios in which use of generative AI will benefit students across different age groups and segments (K-12 vs Higher Education) and provide for inclusion of the technology in the teaching and learning process.\n\nSome well researched use cases can help provide teachers and students with guidance on how best to implement specific types of tools.\n\nThis should be supported through the promotion of AI literacy among students and educators, incorporating AI-related topics such as machine learning, natural language processing, and neural networks into existing curricula.\n\nPolicy makers need to ensure that AI led education tools are accessible to students of all backgrounds, emphasising diversity and inclusion in both the study and application of AI.\n\nA starting point here might be to shift emphasis from student/teacher content creation towards the evaluation of content to boost and develop critical thinking, for example, teaching students and teachers the importance of effective prompting.",
    "### Policy Consideration 2: Redrafting learning outcomes\n\nThe use of generative AI in the day-to-day teaching and learning process could hinder the development of key skills like critical thinking and the ability to assimilate and analyse knowledge.\n\nStudents need guidance on how to leverage technologies while retaining fundamental learning and development.\n\nThe integration of AI tools creates an opportunity for educators and researchers to develop innovative approaches to teaching, assessment, and content development, thus enhancing the state of academia, for example generative AI can help to define tasks to ensure personalized learning for students.\n\nIt will encourage a move away from less effective or more time-consuming tasks, such as memorization or reviewing large bodies of literature, to a model that promotes higher-order skills for educators, students, and researchers.\n\nThis can be achieved, for example, by focusing on reflection on or discussion of created content rather than focusing on the creation of content itself.",
    "### Policy Consideration 3: Ethical framework for AI\n\nThe development of an ethical framework for AI encourages its safe and responsible use, which can be provided through guidelines and standards that govern the design, development and deployment of AI technologies in education systems across schools and universities.\n\nThe framework should cover aspects such as privacy, transparency, accountability, and fairness.\n\nIt should support the declaration by AI systems of their design process so that the right systems can be identified which protect the privacy and personal information of students, teachers, and other stakeholders.\n\nWhile it is the responsibility of developers to carry out rigorous testing and validation of AI algorithms and ensure that they adhere to ethical and legal standards, educators also need to be adequately trained and equipped to handle the technologies and ensure that students' welfare is prioritised.\n\nAny framework should promote transparency and logic, ensuring that AI models and algorithms are easy to understand and interpret, and outline use cases to be supported.\n\nIt should be highlighted that frameworks for AI – like former ones for internet usage for example – cannot be fully enforced, but the objective here is to make students, parents and teachers aware of the guidelines and the implications of not adhering to them.",
    "### Policy Consideration 4: Data Privacy and Security\n\nThe sensitive nature of personal information that is shared between students, teachers, and administrators during the learning process makes them vulnerable to cyberattacks and privacy breaches.\n\nSchools, universities and educational systems must take a proactive approach to address these concerns by instituting robust data privacy and security policies that protect the personal information of students, teachers, and staff.\n\nAnother critical area of concern is data governance.\n\nSchools and universities need to establish guidelines and protocols for the collection, storage, and sharing of data in ways that align with ethical and legal standards.\n\nThey must ensure that they comply with applicable regulations such as GDPR (General Data Protection Regulation), FERPA (Family Educational Rights and Privacy Acts) and others as they evolve to cope with generative AI.\n\nA starting point may be to create default privacy and security processes with options for individuals to modify according to his or her preferences.\n\nDepending on culture as well as national and organizational priorities, there are opt-out and opt-in approaches that can be implemented.",
    "### Policy Consideration 5: Incorporating AI training for Educators and Students\n\nTo fully leverage AI potential, educators need to be trained on how to integrate the technology into their teaching effectively.\n\nAI training should be included in teacher preparation programs and continued professional development.\n\nTeachers need to know how to identify appropriate AI tools, how to use them effectively, mitigate risk, and how to evaluate AI-generated content.\n\nStudents should be taught basic AI concepts to be able to participate in the development of AI solutions in the future.\n\nEducators can incorporate AI training into the curriculum by developing courses and workshops to ensure students have practical experience using AI tools.\n\nAs AI becomes more integrated into the educational system, it is important to prepare students for the jobs of the future.\n\nGovernments can enable partnerships that provide industry-relevant training programs and provide financial incentives for schools that make AI and other emerging technologies available to underserved communities.",
    "### Policy Consideration 6: Promote transparency and accountability\n\nOne of the most significant concerns related to the use of generative AI in education systems is the potential for bias and discrimination.\n\nTo promote transparency, institutions need to be open about their use of generative AI systems, including the algorithms used and the processes followed.\n\nInstitutions should also provide explanations of how the technology works and how decisions are made based on the data collected.\n\nThis transparency can help build trust between institutions, educators, parents, and students, and will help to ensure that the use of generative AI in education is ethical and unbiased.\n\nA mechanism is required to ensure that institutions and educators are responsible for the decisions made by the systems they use.\n\nFor instance, if a student is penalised for a mistake made by a generative AI system encouraged by the school, that institution must be accountable for the error.\n\nEducational institutions need to establish clear guidelines, policies, and procedures for the use of these systems, including how they collect and analyse data, identify potential biases and ensure impartial decision making.\n\nIn addition, institutions should routinely monitor and audit the generative AI system's performance to identify any errors or biases, and take appropriate measures to correct them.\n\nAn important part of this is identifying the appropriate supervisory body or bodies to be involved, taking into account national and international approaches and guidelines.",
    "### Policy Consideration 7: Public engagement\n\nThe use of AI in education raises ethical concerns such as privacy, data security, and fairness in decision-making.\n\nPublic engagement, including with industry, allows for and encourages open discussion about these issues and provides an opportunity to address misconceptions and fears surrounding AI in education as well as building awareness which protects against manipulation.\n\nPublic input and feedback can also inform the development of effective policies and guidelines that govern the use of generative AI in education.\n\nThis helps to create a transparent and accountable framework for the deployment of the technology.\n\nEngaging the public in the decision-making process helps to build trust between the community and the institutions working with AI.\n\nThis trust and inclusive attitude are important for the successful adoption and acceptance of AI in education.",
    "### Policy Consideration 8: Education led AI\n\nThe education sector itself needs to be a leader in the development of its own AI based tools that build on this new technology and open the opportunities it offers to learners at all levels.\n\nIn this way the sector can gain better control of contractual terms and technical and organisational measures in the application of AI.\n\nIn addition, it will create a safer space for students to train in and make the institution both safer and more competitive.\n\nThis does not mean that government or institutions need to become AI developers, but partnerships with ed tech and companies are a good starting point to develop bespoke tools, work through the ethical, behavioural and other challenges that are being raised, which will allow the sector to lead in this field rather than be reactive to what is to come.",
    "## G20’s Role: Call to action for governments to take proactive and responsible steps to shape AI use in education\n\nGenerative AI is here to stay and grow exponentially.\n\nAccording to a Forbes article published in March 2023, generative-relevant use cases already present a significant enterprise opportunity, with an estimated market of $2.6 billion in 2023 which is expected to grow at a rate of 32% until 2026.\n\nAs with previous disruptive technologies, such as calculators, computers and the internet, generative AI technologies will become another tool in the education ecosystem.\n\nWe need to rethink how to learn, grow and develop in a world where generative AI is commonplace, be prepared for further transformations and we must do so while being aware of its uses and deficiencies, its strengths and limitations.\n\nThis ongoing disruption demands proper guardrails in the form of forward-looking policy.\n\nThe G20 recognizes the importance of digital technology and its potential to enhance social development and economic growth.\n\nThe G20 Digital Economy Ministerial Declaration acknowledged the need for policies that support the development and use of AI technologies while also addressing ethical and social considerations.\n\nThe time is now to act on policy development, especially in education.\n\nGenerative AI has the potential to contribute to the achievement of the United Nations' Sustainable Development Goals (SDGs), particularly the achievement of SDG 4 which focuses on ensuring inclusive and equitable quality education and promoting lifelong learning opportunities for all, provided that the correct steps are taken now to integrate the use of this technology.\n\nThe G20 has an important role to play.\n\nBy recognizing the potential of AI technologies, the G20 can create viable opportunities for the use of generative AI in education, provide recommendations to governments across the globe which impact policy and regulatory frameworks and help develop a common understanding of key ethical and ‘human-centric’ guidelines for the use of AI.",
    "## Risks of Inaction\n\nThe future will belong to those who can understand, learn and incorporate the use of critical emerging technologies like generative AI in everyday life.\n\nLack of understanding of AI and its applications will not only make students unqualified to enter the job market in the future, but will also broaden the divide between their skills, knowledge and abilities versus those who are adept at using AI tools.\n\nTraditional teaching methods and curricula may become outdated and education systems that are not able to integrate emerging technologies like generative AI into their educational systems risk producing graduates without the skills needed to succeed.\n\nThe digital divide may be exacerbated if governments do not actively invest in providing access to emerging technologies for all learners.\n\nStudents from low-income or disadvantaged backgrounds may fall further behind in acquiring essential digital skills and knowledge.\n\nCountries that do not prioritise the adoption of new technologies in education may lose their competitive edge in the global market.\n\nBy not preparing students with the skills needed for the AI-driven economy, these countries risk stunting their economic growth and development.\n\nBy not investing in emerging technologies, governments may hinder the development of new and effective teaching methods and learning tools.\n\nTeachers need training and support to effectively use new technologies in the classroom.\n\nInaction in this area may lead to a lack of professional development opportunities for educators, making it difficult for them to adopt new tools and techniques.\n\nInaction from governments in the appropriate adoption of emerging technologies like generative AI in education can have significant consequences for students, educators, and society as a whole.\n\nBy embracing, investing in and working on the best uses of these technologies, governments can ensure a future that is prepared for the changes AI will bring.",
    "## Note\n\nWhile preparing this policy brief, we asked ChatGPT to prepare a similar document as a test.\n\nThe work above required approximately 24 human hours to complete.\n\nThe ChatGPT version took around 1 hour to complete with almost all that time spent on rephrasing questions to elicit more relevant content.\n\nWe learned the following:\n- All readers of both papers picked the human based version as their preference (despite not knowing which was which).\n\n- The skills required to prompt ChatGPT are a new and rapidly growing discipline which we are learning in real time.\n\n- The use of ChatGPT reduced the amount of learning significantly for the people involved.\n\nNo reading, research or discussion was required, and no writing up reinforced learning whereas those that spent time on the human written version gained significantly more understanding and retained knowledge.",
    "## Bibliography\n\n- Holmes, Wayne.\n\n“AI and education.\n\nA guidance for policymakers”, Miao, Fengchun, UNESCO Publishing, 4/8/2021 (\n- John F. Pane.\n\n“Informing Progress.\n\nInsights on Personalized Learning Implementation and Effects”, RAND Corporation, 1/1/2017 (\n- OECD, “Equity and Quality in Education: Supporting Disadvantaged Students and Schools”, OECD Publishing (\n- Sebastian Martinez.\n\n“Impact Evaluation in Practice, Second Edition.” Paul J. Gertler, World Bank Publications, 9/12/2016 (\n- Alistair Dieppe.\n\n“Global Productivity.\n\nTrends, Drivers, and Policies”, World Bank Publications, 6/9/2021 (\n- UNESCO.\n\n“The Impact of the COVID-19 Pandemic on Education.” International Evidence from the Responses to Educational Disruption Survey (REDS), International Association for the Evaluation of Educational Achievement, UNESCO Publishing, 1/25/2022 (\n- Jerome Beranger.\n\n“Societal Responsibility of Artificial Intelligence.\n\nTowards an Ethical and Eco-responsible AI”, John Wiley & Sons, 6/2/2021\n- Ford, Jolyon.\n\n“Regulatory Insights on Artificial Intelligence.” Research for Policy, Findlay, Mark, Edward Elgar Publishing, 6/10/2022\n- UNESCO, “World Poverty Could Be Cut in Half If All Adults Completed Secondary Education 2023.” unesco.org.\n\n2023.\n\n(\n- “Monitoring SDG 4: Access to Education.” Unesco.org, 2022, ().\n\nAccessed 4 May 2023.\n\n- “Today’s Teachers Are Deeply Disillusioned, Survey Data Confirms.” Education Week Research Centre, 13 Apr.\n\n2022, (.)\n\nAccessed 4 May 2023.\n\n- “World Development Report 2018.” World Bank, 2018, (hdl.handle.net/10986/28340).\n\n- “Generative AI Systems in Education -Uses and Misuses.” Centre for Democracy & Technology, Mar 2023.\n\n- Wood, Patrick, and Mary Louise Kelly.\n\n““Everybody Is Cheating”: Why This Teacher Has Adopted an Open ChatGPT Policy.” NPR, 26 Jan. 2023, (.)\n\n- Duggan, Steven.\n\n“AI in Education: Change at the Speed of Learning.” UNESCO IITE, 2020.\n\n- Swiecki, Zachari, et al.\n\n“Assessment in the Age of Artificial Intelligence.” Computers and Education: Artificial Intelligence, vol.\n\n3, 2022, p. 100075, (\n- Hmelo-Silver, Cindy E. “Problem-Based Learning: What and How Do Students Learn?” Educational Psychology Review, vol.\n\n16, no.\n\n3, Sept. 2004, pp.\n\n235–266, (\n- Ho, Han Kiat, and Hui Ting Chng.\n\n“Stirring Deep Thinking and Learning through Student-Designed Assessment Problems.” Currents in Pharmacy Teaching and Learning, Jan. 2021, ( Accessed 7 Feb. 2021.\n\n- “AI Use in Assessments: Protecting the Integrity of Qualifications.” JCQ Joint Council for Qualifications, (/.)\n\n- Department for Education.\n\n“Generative Artificial Intelligence in Education Departmental Statement.” Gov UK, 2023.\n\n- World Economic Forum.\n\n“Generative AI: A Game-Changer Society Needs to Be Ready For.” World Economic Forum, Jan. 2023, (/.)\n\n- Future of Life Institute.\n\n“Pause Giant AI Experiments: An Open Letter.” Future of Life Institute, 22 Mar.\n\n2023, (futureoflife.org/open-letter/pause-giant-ai-experiments/.)\n\n- Lim, Weng Marc, et al.\n\n“Generative AI and the Future of Education: Ragnarök or Reformation?\n\nA Paradoxical Perspective from Management Educators.” The International Journal of Management Education, vol.\n\n21, no.\n\n2, July 2023, p. 100790, (\n- Baidoo-Anu, David, and Leticia Owusu Ansah.\n\n“Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning.” Social Science Research Network, 25 Jan. 2023, (papers.ssrn.com/sol3/papers.cfm?abstract_id=4337484.)\n\n- Dwivedi, Yogesh K., et al.\n\n““So What If ChatGPT Wrote It?” Multidisciplinary Perspectives on Opportunities, Challenges and Implications of Generative Conversational AI for Research, Practice and Policy.” International Journal of Information Management, vol.\n\n71, 1 Aug. 2023, p. 102642, , (\n- OpenAI.\n\nGPT-4 Technical Report.\n\nOpenAI, 27 Mar.\n\n2023.",
    "### Possible Courses of Action for the Institution\n\nTexas A&M University wishes to approach the introduction of generative artificial intelligence (generative AI) with scholarly rigor, intellectual integrity, and educational excellence.\n\nThe university recognizes that generative AI presents additional challenges to academic integrity and simultaneously presents stimulating opportunities to educate effectively for the future of learning and work.\n\nThe three courses of action identified below represent distinctly useful approaches to this complex challenge.\n\n1.\n\nProduce a syllabus statement to be added to minimum syllabus requirements, identifying the university-wide acceptable use of generative AI.\n\n2.\n\nOffer guidance through appropriate university offices, and allow individual instructors to determine their own syllabus statement identifying acceptable use of generative AI within the course context.\n\n3.\n\n(Recommended) Pursue a hybrid of the two previous options, including both of the following:\n   - An addition to the minimum syllabus requirements, which both:\n     - Makes explicit the responsibility of instructors and students to establish clear expectations for generative AI use within each course and/or assignment and\n     - Reinforces that the use of generative AI in academic coursework is integrally related to academic integrity and will be governed by the Aggie Honor Code.\n\n- Guidance provided to support faculty in making their individual determinations.",
    "### No Use of Generative AI Permitted\n\n- **Bryant University**: AI Writing tools are not permitted for any stage or phase of work in this class.\n\nIf you use these tools, your actions would be considered academically dishonest and a violation of Bryant’s Academic Honesty Policy.\n\n- **Princeton University**: Intellectual honesty is vital to an academic community and for my fair evaluation of your work.\n\nAll work submitted in this course must be your own, completed in accordance with the University’s academic regulations.\n\nYou may not engage in unauthorized collaboration or make use of ChatGPT or other AI composition software.\n\n- **Texas Tech University**: Since writing, analytical, and critical thinking skills are part of the learning outcomes of this course, all writing assignments should be prepared by the student.\n\nDeveloping strong competencies in this area will prepare you for a competitive workplace.\n\nTherefore, AI-generated submissions are not permitted and will be treated as plagiarism.\n\n- **University of New Hampshire**: Unless otherwise specified, the use of Automated Writing Tools, including chatGPT and similar artificial intelligence (AI) tools, is strictly prohibited in this course, even when properly attributed.\n\nThe use of automated writing tools is considered plagiarism (as defined by SRRR 9.3) and will be handled in accordance with existing policy.\n\n- **University of California, Irvine**: Academic Integrity for Your Writing Class: The Composition Program and its teachers assume that work submitted by students—all process work, drafts, low-stakes writing, final versions, and all other submissions—will be generated by the students themselves, working individually or in groups.\n\nThis means that the following would be considered violations of academic integrity by the Composition Program:\n  - If a student has another person/entity do the writing of any substantive portion of an assignment for them, which includes hiring a person or a company to write essays and drafts and/or other assignments, research-based or otherwise, and using artificial intelligence affordances like ChatGPT;\n  - If a student submits the same work for more than one class without consulting with the instructors.\n\n- **University of Massachusetts Amherst**: [This course] assumes that all work submitted by students will be generated by the students themselves, working individually or in groups.\n\nStudents should not have another person/entity do the writing of any substantive portion of an assignment for them, which includes hiring a person or a company to write assignments and using artificial intelligence tools like ChatGPT.\n\n- **University of Iowa**: [This course] assumes that work submitted by students—all process work, drafts, low-stakes writing, final versions, and all other submissions—will be generated by the students themselves, working individually or in groups.\n\nThis means that the following would be considered violations of academic integrity: a student has another person/entity do the writing of any substantive portion of an assignment for them, which includes hiring a person or a company to write essays and drafts and/or other assignments, research-based or otherwise, and using artificial intelligence affordances like ChatGPT.\n\n(Excerpted from ChatGPT by University of California: Irvine Division of Teaching Excellence and Innovation)\n\n- **University of Iowa**: Since writing, analytical, and critical thinking skills are part of the learning outcomes of this course, all writing assignments should be prepared by the student.\n\nDeveloping strong competencies in this area will prepare you for a competitive workplace.\n\nTherefore, AI-generated submissions are not permitted and will be treated as plagiarism.\n\n(Sample statement shared by Chrissann Sparks Ruehle, with permission for others to use, on Higher Ed Discussions of AI Writing Facebook Group on 1/6/2023, cited in ChatGPT Resources by Texas Tech University Teaching, Learning & Professional Development Center)",
    "### Use of Generative AI Permitted Under Some Circumstances or With Explicit Permission\n\n- **Bryant University**: During our class, we may use AI Writing tools such as ChatGPT.\n\nYou will be informed as to when, where, and how these tools are permitted to be used, along with guidance for attribution.\n\nAny use outside of this permission constitutes a violation of Bryant’s Academic Honesty Policy.\n\n- **Bryant University**: We recognize that there are a variety of AI programs available to assist writers.\n\nAI programs are not a replacement for human creativity, originality, and critical thinking.\n\nWriting is a craft that you must develop over time to develop your own individual voice as a writer.\n\nHowever, within limited circumstances, and with proper attribution, AI programs may be used as a tool.\n\n- **Princeton University**: Students must obtain permission from me before using AI composition software (like ChatGPT) for any assignments in this course.\n\nUsing these tools without my permission puts your academic integrity at risk.\n\n- **University of New Hampshire**: If you are allowing AI in some instances, it is recommended to add the following: It is important to note that if AI tools like chatGPT are permitted to be used for an assignment, then they should be used with caution and proper citation.\n\nAI is not a replacement for your own thinking and research.\n\n- **Washington University in St. Louis**: On Academic Integrity: Effective learning, teaching and research all depend upon the ability of members of the academic community to trust one another and to trust the integrity of work that is submitted for academic credit or conducted in the wider arena of scholarly research.\n\nSuch an atmosphere of mutual trust fosters the free exchange of ideas and enables all members of the community to achieve their highest potential.\n\nIn all academic work, the ideas and contributions of others must be appropriately acknowledged and work that is presented as original must be, in fact, original.\n\nUsing an AI-content generator (such as ChatGPT) to complete coursework without proper attribution or authorization is a form of academic dishonesty.\n\nIf you are unsure about whether something may be plagiarism or academic dishonesty, please contact your instructor to discuss the issue.\n\nFaculty, students, and administrative staff all share the responsibility of ensuring the honesty and fairness of the intellectual environment at Washington University in St. Louis.\n\n- **Colorado University**: There are situations and contexts within this course where you will be asked to use AI tools to explore how they can be used.\n\nOutside of those circumstances, you are discouraged from using AI tools to generate content (text, video, audio, images) that will end up in any student work (assignments, activities, responses, etc) that is part of your evaluation in this course.\n\nAny student work submitted using AI tools should clearly indicate what work is the student’s work and what part is generated by the AI.\n\nIn such cases, no more than 25% of the student work should be generated by AI.\n\nIf any part of this is confusing or uncertain, please reach out to me for a conversation before submitting your work.",
    "### Broader Use of Generative AI Permitted Within Guidelines\n\n- **Bryant University**: AI Writing tools such as ChatGPT are welcome in this class, provided that you cite when and how you use the tool.\n\nYou will be provided with examples of how to cite your use of this tool in your writing.\n\n- **Bryant University**: Example of attribution language: “The author generated this text in part with GPT-3, OpenAI’s large-scale language-generation model.\n\nUpon generating draft language, the author reviewed, edited, and revised the language to their own liking and takes ultimate responsibility for the content of this publication.”\n\n- **University of Massachusetts Amherst**: AI is allowed with attribution: Use of AI tools, including ChatGPT, is permitted in this course for students who wish to use them.\n\nTo adhere to our scholarly values, students must cite any AI-generated material that informed their work (this includes in-text citations and/or use of quotations, and in your reference list).\n\nUsing an AI tool to generate content without proper attribution qualifies as academic dishonesty.\n\n- **University of Massachusetts Amherst**: AI is encouraged with certain tasks and with attribution: You can choose to use AI tools to help brainstorm assignments or projects or to revise existing work you have written.\n\nWhen you submit your assignment, I expect you to clearly attribute what text was generated by the AI tool (e.g., AI-generated text appears in a different colored font, quoted directly in the text, or use an in-text parenthetical citation).\n\n- **University of Iowa**: In all academic work, the ideas and contributions of others must be appropriately acknowledged and work that is presented as original must be, in fact, original.\n\nUsing an AI-content generator (such as ChatGPT) to complete coursework without proper attribution or authorization is a form of academic dishonesty.\n\nIf you are unsure about whether something may be plagiarism or academic dishonesty, please contact your instructor to discuss the issue.\n\nFaculty, students, and administrative staff all share the responsibility of ensuring the honesty and fairness of the intellectual environment.\n\n(Excerpted from Constructing a Syllabus: A Checklist by Washington University in St. Louis Center for Teaching and Learning)\n\n- **University of Iowa**: Use of AI tools, including ChatGPT, is permitted in this course for students who wish to use them.\n\nTo be consistent with our scholarly values, students must cite any AI-generated material that informed their work and use quotation marks or other appropriate indicators of quoted material when appropriate.\n\nStudents should indicate how AI tools informed their process and the final product, including how you validated any AI-generated citations, which may be invented by the AI.\n\nAssignment guidelines will provide additional guidance as to how these tools might be part of your process for each assessment this semester and how to provide transparency about their use in your work.\n\n- **University of Iowa**: When AI use is encouraged with certain tasks: Students are invited to use AI platforms to help prepare for assignments and projects (e.g., to help with brainstorming or to see what a completed essay might look like).\n\nI also welcome you to use AI tools to help revise and edit your work (e.g., to help identify flaws in reasoning, spot confusing or underdeveloped paragraphs, or to simply fix citations).\n\nWhen submitting work, students must clearly identify any writing, text, or media generated by AI.\n\nThis can be done in a variety of ways.\n\nIn this course, parts of essays generated by AI should appear in a different colored font, and the relationship between those sections and student contributions should be discussed in cover letters that accompany the essay submission.\n\n(Based on Course Policies related to ChatGPT and other AI Tools by Joel Gladd)",
    "# Artificial Intelligence Policies: Guidelines and Considerations\n\nUpdated: January 24, 2024\n\nWondering if there is a specific Duke policy regarding AI-generated content?\n\nYou have the discretion to define how, if, and when generative AI may be used in your courses.\n\nHowever, the Duke Community Standard has been updated to include the unauthorized use of generative AI as a form of cheating.\n\nPlus, the Office of Undergraduate Education offers a syllabus language menu that identifies considerations for writing an AI policy.\n\n(NetID login required).",
    "## Draft your own AI policy\n\nAll instructors should update their syllabi to include guidance on the use of generative AI in their class.\n\nWe encourage faculty to thoughtfully consider their stance on AI use.\n\nIn this shift in higher education and the rapidly changing AI market, standardized, one-size-fits-all, AI policies are not sustainable in the long term.\n\nThey may also not account for the varying stances instructors will take regarding AI use in their classrooms.\n\nFurthermore, because generative AI is becoming more ubiquitous, you’ll need to consider your personal stance on AI in your own work, as well as in your classrooms.\n\nEstablishing an AI policy for your class allows you to have meaningful discussions with students on this topic.\n\nBeing specific about how AI is or isn’t allowed makes the rules clear for students and faculty if there are academic integrity violations as well.\n\nThe following guidelines for developing an AI policy for your syllabus include examples of generative AI policies from instructors, universities, and centers of teaching and learning.\n\nFor the most part, they have been curated from a crowd-sourced document that we encourage you to explore on your own to find AI policies developed by instructors in your field and represent your level of comfort with the use of AI.\n\nOther sources for sample policies are cited in the text itself.",
    "## Do your homework\n\nKevin Gannon (Queens College in Charlotte) argues in an article titled “Should I Write an AI Policy” (NetID login required) that instructors should start with research into the topic before writing their policy.\n\nHe offers a list for instructors of varied sources that provide a primer on the mechanics of generative AI, pros and cons of AI use, and a grounding in how generative AI can be incorporated into teaching.\n\nWe encourage you to try at least one generative AI tool.\n\nA good way to start exploring is to enter the prompts that you give students for an assignment into the AI tool, and see what the tool returns.\n\nIt is important to learn how to write effective prompts and also explore existing prompt ideas.\n\nKeep an open and curious mindset when considering whether such tools could be helpful to some or all of your students.",
    "## Share the rationale behind your policy\n\nAs you define your individualized AI syllabus statement, your reasoning should be grounded in the intellectual work of the course, your discipline, and your understanding of critical thought.\n\nSome of the questions you might ask yourself are: What will students lose (or gain) by using generative AI in your course?\n\nWhat do you want students to understand about AI and their intellectual development?\n\nWe encourage you to explain the rationale for your policies with your students and open a conversation about AI with them.\n\nBelow are three policy examples that detail the instructors’ thinking behind their AI policy.\n\nIn a policy example from Joel Gladd (University of Western Idaho) he mentions two guiding principles in a course that will allow the use of AI: “1) Cognitive dimension: Working with AI should not reduce your ability to think clearly.\n\nWe will practice using AI to facilitate—rather than hinder—learning.\n\n2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.”\n\nLis Horowitz (Salem State University) shares a practical reason behind a zero tolerance policy for generative AI in their writing course.\n\n“Since writing, analytical, and critical thinking skills are part of the learning outcomes of this course, all writing assignments should be prepared by the student.\n\nDeveloping strong competencies in this area will prepare you for a competitive workplace.\n\nTherefore, AI-generated submissions are not permitted and will be treated as plagiarism.”\n\nBy defining the idea of integrity, Megan McNamara (UC Santa Cruz) points out what is at stake when we talk about academic honesty and personal growth.\n\nAs she states, “Integrity – other people’s perception of your word as true – is one of the most valuable assets you can cultivate in life.\n\nBeing attentive to integrity in academic settings allows others to trust that you have completed work for which you are taking credit.” Her course allows students to use AI in limited ways and this rationale sets the foundation for this use.",
    "## Support AI literacy\n\nAlthough students may already use generative AI, it does not mean that they understand the limitations of the tool or how to use it properly.\n\nFor example, generative AI is a brand-new source for information and the rules for citation and the general use of AI are still under formation.\n\nIf you explain to students how to cite content generated from AI sources it reduces the cognitive load of students unsure of how to act within the Duke Community Standard and raise their awareness of what is at stake when interacting to AI content.\n\nMost academic style guides have already formulated citation practices for generative AI.\n\nMonash University has curated a list of the various AI citation formats.\n\nYou have the option to personalize your approach to citations.\n\nSome educators are instructing students to submit a transcript of the conversation with generative AI as an appendix to their written work.\n\nAnother alternative might be a reflective piece as a companion to an assignment, as Pam Harland (Plymouth State University) has done by providing guiding questions for students: “What was your prompt?” “Did you revise the AI model’s original output for your submission?” “Did you ask follow-up questions?” “What did you learn?”\n\nAnother aspect of AI literacy is understanding the ethical use of AI and its limitations.\n\nWhen students have information about the hallucinations, bias, and inaccuracies of generative AI, it underscores why AI is not a shortcut to good results.\n\nEthan Mollick (Harvard’s Wharton School) frames it this way, “[d]on’t trust anything it says.\n\nIf it gives you a number or fact, assume it is wrong unless you either know the answer or can check in with another source.\n\nYou will be responsible for any errors or omissions provided by the tool.\n\nIt works best for topics you understand.” The library at University of Northwestern St. Paul has a guide for students with an overview of AI’s shortcomings, plus information on how to verify sources and double check AI responses.",
    "## Define acceptable use\n\nIf you decide to allow generative AI as a tool for learning in your courses, clarify for your students the circumstances in which AI is allowable.\n\nKim Sydow Cambell (University of Texas) offers an example policy that expands upon the tasks that students can ask AI to perform: “Because the effective use of Artificial Intelligence (AI) tools is increasingly important to the work of technical communicators, their use is sometimes required or allowed in course assignments.\n\nAI tools can support a content creator during all phases of their work:\n- pre-writing: before content is created, writers can use some tools to research topics, collect genre samples, brainstorm ideas, craft outlines, etc.\n\n- drafting: some tools support the generation of content\n- revising: after content is generated, many tools aid writers in identifying and altering style/tone, spelling, punctuation, grammar, etc.”\n\nFrom Howard University of Law, Howard Bruckner explains to students what tasks are acceptable and underscores the students’ responsibilities for ethical use.\n\n“Generative AI tools can be invaluable for generating ideas, identifying sources, synthesizing text, and starting to understand what is essential about a topic.\n\nBut YOU must guide, verify and craft your work product; do not just cut and paste without understanding.”\n\nYou may discover that if you allow students to engage with generative AI, specific guidelines may need to accompany each assignment.",
    "## Explore the continuum of policies\n\nBelow are policy examples from the University of Delaware’s Center for Teaching and Assessment of Learning, which distill the four basic approaches that instructors can take in their syllabi.\n\nWe invite you to consider them as starting points in your exploration of what your AI policy will be.",
    "### Use only with prior permission\n\nStudents are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course if instructor permission is obtained in advance.\n\nUnless given permission to use those tools, each student is expected to complete each assignment without substantive assistance from others, including automated tools.",
    "### Use only with acknowledgement\n\nStudents are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course if that use is properly documented and credited.\n\nFor example, text generated using ChatGPT-3 should include a citation such as: “Chat-GPT-3.\n\n(YYYY, Month DD of query).\n\n“Text of your query.” Generated using OpenAI.\n\nMaterial generated using other tools should follow a similar citation convention.",
    "## About AI detection software\n\nWe don’t recommend AI detection software as a part of your AI policy for three main reasons.\n\nThe products are unreliable.\n\nResearch on AI detection software from MIT highlights the false positive and negative rates.\n\nOpenAI (the company behind ChatGPT) withdrew its own detection software due to the software’s unreliability.\n\nDetection software is biased against certain segments of learners, for example non-native speakers, as research from Stanford shows.\n\nAs AI changes, detection software cannot keep up.\n\nIf you decide to use detection software, share that information with your students ahead of time.\n\nResults from the software should not be the only measure of whether students have cheated.\n\nStudents can be encouraged to use detection software prior to handing in work to check for originality (although they should be warned of the limitations as well).\n\nAnother option is to craft a policy that warns students they will need to speak with you and defend their work if plagiarism is suspected.\n\nAs Liza Long (College of Western Idaho) explains “[i]f I suspect that you have used ChatGPT, and you have not included the required citation and reflection, then you will need to meet with me either in person or through Zoom to talk about the assignment.\n\nThis conversation will include knowledge checks for course content.”\n\nIf you’d like to discuss your AI policy in more depth, please reach out to .\n\nYou can also explore our central resource on AI in education.",
    "## GET SUPPORT FOR TEACHING\n\nContact a Learning Innovation & Lifetime Education consultant or drop into our office hours.\n\nPHYSICAL LOCATION\nThe Erwin Square Mill Building\n\nMAILING ADDRESS\nBox 90198\nDurham, NC 27708\n\nCONTACT\nphone: (919) 660-5806\nemail:  Contact Us\n\nBROWSE\nWhat We Do\nAbout\nBlog\nContact Us\nEvents\nDirections\nResources\nNewsletter Signup\n\nCONNECT WITH US\n\nSUPPORT OUR WORK\nDonate to Duke Learning Innovation & Lifetime Education\n\nCopyright © 2024 Duke Learning Innovation & Lifetime Education",
    "## AI & Academic Integrity\n\nAlthough the Artificial Intelligence (AI) landscape is evolving rapidly, one clear impact is that this technology is already being integrated into the academic lives of both students and faculty.\n\nAs a result, all instructors should consider how they will provide clear guidelines concerning the use of generative AI in academic work for every class they teach.\n\nHere are a few things to keep in mind when addressing AI and academic integrity in your course.",
    "### Clearly Communicate Your Expectations\n\nStudents benefit from clearly communicated expectations for their coursework, and will likewise benefit from transparency regarding limits to and expectations for use of generative AI in their course assignments and exams.\n\nTo reduce the chance of violations of academic integrity, explicitly communicate your course’s generative AI policies by:\n\n- Including in your syllabus clear expectations regarding the use of generative AI tools and any differentiation in the usage policy for specific assignments.\n\n- Clearly identify in what situations generative AI use is prohibited or permitted.\n\n- When generative AI is permitted, faculty should be clear about student expectations in terms of documentation and attribution, as well as what work is expected to be produced by the student themselves, and how the student is expected to validate or verify output from generative AI.\n\nSee Credit Generative AI in an assignment, below.\n\n- Discussing the expectations spelled out in the syllabus during a class session.\n\n- Reviewing expectations and communicating how they apply to specific assignments.\n\nEngaging in ongoing conversations about the importance of academic integrity, including the fact that basic academic integrity principles remain important and still apply regardless of the existence of generative AI tools.",
    "#### Crediting Generative AI in an Assignment\n\nWhen using material generated from an LLM in course materials or in assignments that students submit, transparency is key, and these instances should be properly referenced.\n\nWhen possible, you may want to select citation guidelines and share them with your students using the style guide appropriate to your discipline.\n\nCurrently, the APA style guide, MLA style guide, and Chicago Manual of Style include recommendations and examples for citing LLM-generated materials that you can share with students.\n\nFor example, consider:\n\n- Including an in-text citation with reference to the LLM model, i.e.\n\ninclude text in quotes and reference the author, i.e.\n\n(OpenAI, 2023).\n\n- Asking students to include an AI-generated source in the methods section of their research paper.\n\n- Having students provide the text or prompt they used for the LLM to generate a response and include what LLM model, date, and version they used.\n\nSimilarly, if instructors choose to use LLMs to help in the preparation of course materials, such work should be acknowledged and attributed.\n\nIf you have specific questions and would like to schedule an individual or group consultation, please contact us.",
    "### Detecting AI-Generated Content\n\nGenerative AI does provide an increased risk that students may use it to submit work that is not their own.\n\nIt is tempting to seek technology-based solutions to identify inappropriate use of generative AI tools such as ChatGPT.\n\nUnfortunately, it is unlikely that detection technologies will provide a workable solution to this problem.\n\nIt can be very difficult to accurately detect AI-generated content.\n\nAlthough some instructors have reported that the tone and feel of AI-generated content is different from the student work they normally receive, in some cases it is almost undetectable from the work produced by students.\n\nDetection tools claim to identify work as AI-generated, but cannot provide evidence for that claim, and tests have revealed significant margins of error.\n\nThis raises a substantial risk that students may be wrongly accused of improper use of generative AI.\n\nFor a more in-depth look at the reliability of detection technologies, we recommend the article \"How Reliable are AI Detectors?\n\nClaims Versus Reality.\"\n\nWe currently do not recommend using current automatic detection algorithms for academic integrity violations using generative AI, given their unreliability and current inability to provide definitive evidence of violations.\n\nWe believe that establishing trusting relationships with students and designing authentic assessments will likely be far more effective than policing students.",
    "## Scope\n\nThis document sets out the University’s policy for the use of Generative AI in Teaching and Learning for taught programmes and for taught components of professional doctorates.\n\nThe policy will be regularly reviewed by the University’s Learning and Teaching Committee.\n\nThe use of Generative AI in the research work of postgraduate research programmes is covered by the University Policy for the use of Generative AI in Research.\n\nThe policy does not prohibit the use of Generative AI for teaching and learning but aims to add clarity around appropriate use for both staff and students and positions the requirement for ongoing training across the whole community to influence behaviour and surface best practice.\n\nThe policy allows for differential use across the institution according to discipline area.\n\nThis policy should be read in conjunction with the following University policies: \n\n- Plagiarism and Collusion Policy;\n- Inclusive Education Policy; and\n- Blended Learning Policy.\n\nStaff should complete training in the following areas;\n- Data protection\n- Copyright",
    "## Introduction\n\nGenerative Artificial Intelligence (AI) is a sub-area of Artificial Intelligence where Large Language Models (LLMs) are trained using huge amounts of data, and then used to generate new content, such as text, code, diagrams and images, prompted by human input.\n\nIt should be noted that GPTs are not strictly AI but represent travel towards it.\n\nGPTs are covered under this policy.\n\nMachine learning has been in existence for a number of years, and it is common in software such as spelling and grammar checking.\n\nHowever, the rapid and ongoing development of Generative AI tools which are able to replicate natural language and generate content has had a disrupting effect in the education sector.",
    "## Governance\n\nThe University’s Plagiarism, Collusion and Contract Cheating Policy will be under regular review to ensure that it remains current and reflects practice in line with what is developing understanding within this area of academic integrity.\n\nThe UEA Generative AI working group will continue to meet and surface emerging technologies, opportunities and challenges.",
    "## Institutional\n\nThe use of Generative AI is likely to be different across the institution by subject area and across disciplines.\n\nThus, each School of Study should meet at least once per academic year to discuss the impact of generative AI on their assessment design and set School-level expectations around the appropriate use of AI for students within their discipline.\n\nExpectations should be understood by all academic staff within that School, clearly communicated to students at appropriate points and, where there is a School approach or practice which differs from the general University guidance (as set out in this policy), explicitly explained in the assessment briefs.\n\nThe use of computer assistance to give the impression that a student has learned more than they have is academic misconduct.",
    "## Pedagogy\n\nComputers and AI can be useful in many contexts, but they can produce incomplete, inaccurate, misleading and/or biased information.\n\nTherefore, it is important students develop skills in using computers and Generative AI tools effectively to support their studies, including an awareness of their strengths and limitations and learn how to challenge the outputs.\n\nThese should also be considered skills they will need in the workplace.",
    "### Student Practice\n\nThere are a number of ‘green light’ approaches which can be used by students:\n\n- Generative AI as mentor - timely feedback is crucial for students, and generative AI can be used to gain ongoing feedback on tasks and assignments.\n\nIt can also be used as a tool to help support effective study.\n\nStudents should reflect on AI feedback and other outputs against their own knowledge and understanding and report on the guidance which has been provided and how they may or may not include it in their work.\n\nThis is to complement and not substitute for engagement with formative tasks, and guidance from teaching staff, Learning Enhancement Tutors, Academic Librarians and others and the University.\n\n- Generative AI as tutor - explanations can be provided to gain understanding.\n\nInspiration and ideas can be provided.\n\nAI can help develop thinking by checking responses, providing counterarguments and generating questions.\n\nStudents should always check AI output against their own knowledge and understanding, and other sources, as content can be inaccurate, biased, and misleading.\n\n- Generative AI as a team member - team working is an important workplace skill, and Generative AI can be used by a team of students to act as a virtual member, maybe playing a role which other students might find awkward or difficult, e.g.\n\nasking for a counterargument or acting as a disrupter.\n\nStudents do not need to take the advice of the AI and must be critical and evaluate the output before it is used.\n\n- Generative AI as researcher - doing a literature search is a crucial part of starting most items of assessment.\n\nGenerative AI can be used to surface relevant literature; however, students should be aware that references can be fictional, not current and non-exhaustive.\n\nThere are certainly better and more accurate tools to surface research papers.\n\nThere are a number of ‘red light’ activities where students should not use Generative AI:\n- UEA aims to encourage, develop and assess written English; unless specifically required to use AI as part of the assessment submitted work must always be the student’s own writing, therefore they must not copy and paste computer-generated text directly.\n\n- Students should be aware that the output from Generative AI can contain errors, bias, misinformation, missing information, and hallucinations (false information).\n\nStudents should always check the content against their own knowledge and understanding.\n\n- Students should not use Generative AI as the first step in their thinking (i.e.\n\nto immediately ask the computer the exact question that they have been asked).\n\n- Students should not circumvent their learning, e.g., when asked to reflect on a task or output from a task.\n\n- Students should not rely on Generative AI when working in an important context where the student is reliant on the generated output being correct (e.g., legal contexts such as placement risk assessments).\n\n- Students should not use content or ideas from Generative AI without appropriate citation.\n\n- Students should be aware of privacy and GDPR and not input personal and private information about themselves or others.\n\nThis is because the software will store data and information and potentially use it for other content.\n\n- Students should not input confidential research data, both quantitative and qualitative or copyrighted data/text into an AI tool without approval.\n\nIf personal data is to be put into an AI tool this must be part of the ethics application process.",
    "## Staff Practice\n\nThere are a number of ‘green light’ approaches which can be used by staff.\n\nThese have been grouped as design, content creation and assessment, but more information can be found in the Staff Guidance document.\n\n- Generative AI for teaching design - ideas for teaching often come through speaking with colleagues and investigating the pedagogical literature.\n\nGenerative AI can be used to generate lesson plans, surface new ideas and approaches.\n\n- Generative AI for content creation - this could involve the generation of templates, for example, letters, case examples to illustrate concepts or scenarios which can be discussed in teaching sessions.\n\nDiagrams and images can also be created, but the AI tools here are often paid for, and run the risk of copyright issues.\n\n- Generation AI for assessment - answers to example assessment questions to be shared with students to evaluate the strengths and weakness of generative AI content.\n\nGrouping and marking responses to short answer questions or multiple choice where AI functionality is part of a software package used to deliver an assessment and where there remains human oversight.\n\n- Generative AI as mentor - support students to explore ways of using Generative AI for ongoing feedback on assignments and tasks and as a tool to help support effective study.\n\nThis includes helping students to reflect on AI feedback and other outputs against their own knowledge and understanding and report on the guidance which has been provided and how they may or may not include it in their work.\n\nThere are a number of ‘red light’ activities where staff should not use Generative AI:\n- Generation of letters to students or other staff using personal data and information.\n\nThis is because the software will store data and information and potentially use it for other content.\n\n- Generation of personalised student feedback on formative and summative assessment.\n\nStudents can be encouraged to seek ongoing feedback on tasks and assignments, but the justification of a mark should be a human judgement.",
    "## Technology\n\nAs with all technologies, UEA will monitor the AI tools on offer on a regular basis and make the decision if and when to obtain a license for specific tools.\n\nUEA has approved the use of the TurnitIn AI detection software for taught programmes, but this must be used with caution by Plagiarism Officers only due to reported inaccuracy and, as with other screening tools, merely one factor in potentially identifying submissions which warrant further investigation.\n\nThe tool will not be able to differentiate legitimate use of Generative AI.",
    "## Support and Training for Students\n\nSupport should be given to students by their Schools on the use of Generative AI, the policy should be translated into guidelines to support students with when they can and cannot use the tools.\n\nModule organisers should make it clear within assessment briefs how Generative AI can be used, ensuring that the policy is adhered to, or where it differs explain why.\n\nSupport is also available in the Learning Enhancement Team and the Library.",
    "### Who is this guide for?\n\nAny member of staff involved in teaching and learning as it relates to undergraduate, postgraduate and apprenticeship programmes.\n\nUEA has a Generative AI Policy which should be read in conjunction with this guidance document.\n\nPartner institutions can use and adapt this guide as required.",
    "### Introduction\n\nGenerative Artificial Intelligence (AI) is a sub-area of Artificial Intelligence where Large Language Models (LLMs) are trained using huge amounts of data, and then used to generate new content, such as text, code, diagrams and images, prompted by human input.\n\nThe models articulate from pattern-prediction processes, and being aware and critical of the kind of data fed to these processes is more important than disentangling how predictions are being generated.\n\nGenerative AI software companies do not disclose the origin of such data in a transparent manner.\n\nMachine learning has been in existence for a number of years, and it is common in software such as spelling and grammar checking.\n\nHowever, the rapid and ongoing development of generative AI tools which are able to replicate natural language and generate content has had a disrupting effect in the education sector.\n\nIt is neither possible to avoid, nor outrun AI.\n\nIt is clear that most HEIs consider that simply prohibiting AI is neither a viable strategy nor an appropriate response to these developments — students will inevitably encounter such technologies in their future and will need an understanding of their strengths and limitations.\n\nThere are a number of perceived opportunities and challenges on the use of AI in higher education.",
    "### Opportunities\n\n- Developing awareness of and skills around academic integrity.\n\n- Articulating the attributes of our students which are independent of AI, e.g.\n\ncritical thinking, self-reflection, tolerance, thought and action.\n\n- Diversifying assessment to focus on core competencies (however, need to ensure that students have an understanding of the potential uses of AI in their specific disciplines).\n\n- Providing students with advice and tools that can support/accelerate their learning and achievement of (existing and future) learning outcomes.\n\n- Providing new opportunities for judgment-free tailored instruction for students from disadvantaged backgrounds or with lower starting skill levels to level up their learning.\n\n- Speeding up analysis of data.\n\nAutomating repetitive processes.\n\nResearching into the use of AI.",
    "### Challenges\n\n- Understanding the different types of AI which currently exist and monitoring new and evolving developments.\n\n- Student use of AI inappropriately to generate complete responses to assessment items.\n\nThis would make it seem that students have achieved more learning than is actually the case.\n\n- Ensure that assessment is meeting the right learning outcomes to provide our students with skills that are independent of AI, or where AI is not intended to replace.\n\n- Where developing AI skills are themselves a learning outcome ensure that it is appropriate.\n\n- The use of AI may become paid for (advanced features already are) — therefore if the use of AI is a learning outcome there may be financial liability for students and/or the institution.\n\n- Current language models (that are free of charge) are not 100% accurate in the content generated or the references provided, and their accuracy may depend on subject.\n\nThis will improve in time and with premium (paid for) versions.\n\n- Potential adverse effects from teaching students how to use AI in terms of over-reliance on generative AI for assessments.\n\n- Unequal opportunities for students with disabilities and SPLDs if there is an over-reliance on Generative AI tools and no support on specialist software such as voice recognition etc.",
    "### The Technology Available\n\nMachine learning has been in existence for a number of years, and it is common in software such as spelling and grammar checking.\n\nHowever, the rapid and ongoing development of generative AI tools which are able to replicate natural language and generate content has had a disrupting effect in the education sector.\n\nGenerative AI is a sub-group of AI which creates new content, including images and diagrams, rather than analysing existing data.\n\nExamples include Generative Pre-Trained Transformers (GPT).\n\nIt should be noted that GPTs are not strictly AI but represent travel towards it.\n\nThey work on text-based prediction algorithms and are trained on databases and then with human interaction.\n\nGPTs still fall under the policy.\n\nThe output from Generative AI can contain errors, bias, misinformation, missing information, and hallucinations (false information).\n\nEveryone who uses Generative AI should always check the content against their own knowledge and understanding.\n\nThis is an essential part of the development of critical thinking skills.\n\nOne way to get better information out of the AI tool is to understand more about hallucinations, why they occur, and how to mitigate them.\n\nIn this short clip, an IBM representative covers hallucinations.\n\nGenerative AI technology continues to develop rapidly.\n\nTools such as ChatGPT, Google Bard, Wordtune, and Quilbot are becoming widely known.\n\nNot all services are free, and some products have premium paid-for functionality (freemium).\n\nMicrosoft will be launching Copilot into its core software (Word, PowerPoint, etc.).\n\nMicrosoft states that Copilot “combines the power of large language models (LLMs) with your data in the Microsoft Graph and the Microsoft 365 apps to turn your words into the most powerful productivity tool on the planet.”\n\nThere is a useful AI Tools Directory where you can find details of the available tools and a description of how they can be used and whether they are free or freemium.",
    "### Exploring ChatGPT as an example tool\n\nChatGPT is probably the tool that has received the most attention since January 2023.\n\nChatGPT only has access to data up to 2021 and does not ‘scour’ the internet.\n\nIt cannot access anything which is behind a paywall.\n\nExperiences of those who have used it to generate answers to essays have shown that it can produce reasonable answers to level 3 and 4 higher education written assessments.\n\nHowever, there are limitations:\n- The narrative while clear, is often unsophisticated.\n\n- The question is referred to often within the response.\n\n- The main ideas lack nuance.\n\n- The insights are superficial.\n\n- References are dated, or they are hallucinations (false information).\n\n- Key references are often missing.",
    "### Detection of AI generated content\n\nYou can sometimes detect ChatGPT generated text by experience of looking at the outputs.\n\nThere is often a superficial narrative, the essay question is often referred to throughout.\n\nOften points are made several times but re-worded.\n\nReferences are often false, or key sources for your subject area are often missing.\n\nThere are detection tools available.\n\nYou can use ZeroGPT; there are also tools available from TurnitIn, but Blackboard has recently stopped developing detection tools as they are unreliable.\n\nDetection tools should be used with caution due to reported inaccuracy and, as with other screening tools, merely one factor in potentially identifying submissions which warrant further investigation.\n\nThe tool will not be able to differentiate legitimate use of Generative AI.\n\nBy far the best way to limit the inappropriate use of generative AI in your students is training.\n\nThe Library and Learning Enhancement Team have produced a training guide for students, but there are activities you can do to support this.\n\n- Show ChatGPT to students and run question and example assessments through it.\n\n- Ask the students to criticise the outputs and uncover the flaws.\n\n- Make sure that students know that if they use it within their work, that they must reference the contribution made.\n\n- Make sure that students know that they cannot copy and paste AI generated content (when it is not part of the assessment design itself).\n\n- Remind students of the Plagiarism and Collusion Policy which states “While you are permitted to use artificial intelligence software to help you understand material and how that can be expressed, you are not allowed to use that software to complete the assessment (or any part of it) for you.\n\nIf you use artificial intelligence software to complete the work for you, that will be plagiarism.\n\nIn some cases, it will reach the level of contract cheating Policy point 5 pg 3”.",
    "### Referencing AI Contribution\n\nA general overview use declaration (in the submission declaration) in which you explain in general how technology assistance was used e.g.\n\n“I have used word processing and spell check as well as the reference sources cited in the reference list and …)\n\nSpecifically, when you would cite a human (for the idea, exact or general wording) then cite the technology:\n- Author of AI model used.\n\n(Year of AI model used).\n\nName of AI model used (Version of AI model used) [Type or description of AI model used].\n\nWeb address of AI model used\n  - E.g., OpenAI.\n\n(2022).\n\nChatGPT (Dec 20 version) [Large language model].\n\n- And a screenshot of the full thread of the cited AI interactions pasted to an Appendix.\n\nStudents might also go to Cite Them Right, which UEA has a license for to get further guidance.",
    "#### Box 1\n\nI asked ChatGPT the following question “create a discussion scenario for a seminar for higher education students in the area of single cell protein”.\n\nDiscussion Points:\n- Understanding Single-Cell Protein: \n  - What exactly is single-cell protein (SCP), and how is it produced?\n\n- How does SCP compare nutritionally to traditional protein sources like meat, soy, and legumes?\n\n- What are the potential environmental benefits of SCP production, such as reduced land and water usage?\n\n- Technological Advancements in SCP Production: \n  - How has biotechnology and fermentation played a role in advancing SCP production?\n\n- Are there specific microorganisms that are more efficient at producing high-quality SCP?\n\n- What are the challenges associated with scaling up SCP production to meet global demand?\n\n- Food Security and SCP:\n  - How can SCP contribute to addressing food shortages in regions with limited resources?\n\n- Are there cultural or social barriers to adopting SCP as a protein source in different parts of the world?\n\n- What role can SCP play in sustainable diets and reducing malnutrition?\n\n- Economic and Societal Impacts:\n  - What is the economic feasibility of incorporating SCP into the food supply chain?\n\n- How might the growth of the SCP industry impact traditional livestock and agriculture sectors?\n\n- Are there potential job opportunities and challenges related to SCP production and distribution?\n\n- Consumer Acceptance and Perception:\n  - How can consumers be educated and encouraged to embrace SCP as a viable protein source?\n\n- What are the psychological factors influencing consumer perceptions of SCP?\n\n- Are there lessons to be learned from the introduction of other alternative protein sources, like plant-based meats?\n\n- Regulatory and Ethical Considerations:\n  - What regulatory hurdles or standards need to be established for SCP production and marketing?\n\n- Are there ethical concerns regarding the use of microorganisms for mass protein production?\n\n- How might SCP intersect with debates about genetically modified organisms (GMOs) and bioengineering?\n\n- Future Outlook for Single-Cell Protein:\n  - What are the potential breakthroughs and innovations that could shape the future of SCP?\n\n- How can interdisciplinary collaboration drive research and development in this field?\n\n- Can SCP play a pivotal role in achieving the United Nations' Sustainable Development Goals related to food security and sustainability?\n\nMy evaluation: The response was well considered, but it was more a discussion plan than a scenario.\n\nThere were interesting questions which I could use to frame a 50-minute discussion, using pair and share techniques or stakeholder role play.",
    "### Generative AI for content creation\n\nGenerative AI can support you in creating content for teaching.\n\nThis could involve generating case examples to illustrate concepts or problem-based scenarios which can be discussed in teaching sessions.\n\nYou could also use it to generate templates, for example, letters.\n\nDiagrams and images can also be created, but the AI tools here are often paid for and run the risk of copyright issues.\n\nThere are also tools which will generate online content, but they are not free and unsupported by UEA.",
    "### Another example:\n\nI asked ChatGPT to: \"write a multiple choice question with 4 responses to test knowledge on role played by the principle of separation of powers in the British constitution and indicate the correct response\"\n\nQuestion: What role does the principle of separation of powers play in the British constitution?\n\n- It ensures that the Prime Minister has absolute authority over the judiciary.\n\n- It prevents the monarch from having any influence over the legislative process.\n\n- It helps maintain a balance of power between the executive, legislative, and judicial branches.\n\n- It grants the House of Commons the power to overrule decisions of the House of Lords.\n\nCorrect Response: C) It helps maintain a balance of power between the executive, legislative, and judicial branches.\n\nMy evaluation: This was a fairly straightforward question and the incorrect answers were well chosen.\n\nWhilst ‘C’ was the closest to a correct answer it is not wholly accurate as it fails to acknowledge the more nuanced balance of powers or partial separation which exists in the British constitution.\n\nSo whilst acceptable could also be used as a teaching tool to prompt a more in-depth evaluation by the students.\n\nI then asked ChatGPT the question ‘To what extent is there separation of powers in the British constitution?’\n\nThe concept of separation of powers, as traditionally seen in other constitutional systems like that of the United States, is not as rigidly defined in the British constitution.\n\nThe British constitution is characterized by a fusion of powers, where the executive (Prime Minister and Cabinet), legislative (Parliament), and judicial (courts) branches are interconnected to some degree.\n\nHere's an overview of the extent of separation of powers in the British constitution:\n\n- Executive and Legislative Branches: Unlike some systems, the Prime Minister and Cabinet are drawn from the majority party in the House of Commons, which blurs the separation between the executive and legislative branches.\n\nThis enables the executive to have significant influence and control over the legislative process.\n\n- Parliamentary Sovereignty: The British Parliament is supreme and can make or change laws without significant limitations from other branches.\n\nWhile this provides some degree of separation, it also means that the executive (government) is accountable to the legislature, blurring the lines further.\n\n- Judiciary and Legislature: The British judiciary is independent and can review laws and government actions for their legality and constitutionality.\n\nHowever, the judiciary's role in reviewing legislation passed by Parliament is limited compared to systems with a more robust separation of powers.\n\n- Cabinet Members in Parliament: Members of the Cabinet are drawn from the Parliament, and this dual role further integrates the executive and legislative branches.\n\n- Constitutional Conventions and Royal Prerogatives: The Crown still possesses certain prerogative powers that are exercised by the executive, which can influence the legislative process.\n\nHowever, many of these powers are now exercised by ministers in the government.\n\nIn summary, while the British constitution lacks the strict separation of powers seen in some other systems, there are mechanisms in place to maintain a balance between the branches.\n\nThe British tradition relies heavily on conventions, practices, and historical arrangements that have evolved over centuries.\n\nThis fusion of powers is based on a delicate balance that seeks to prevent any one branch from becoming too dominant and to ensure accountability.\n\nMy Evaluation: This answer was accurate, in so far as it relatively superficially correctly identified key elements of the principle as it operates in the UK.\n\nIt did not, however, provide any authority or specific examples to support so whilst correct would be insufficiently detailed and too descriptive to obtain a good mark.\n\nAgain, this could work as a discussion exercise or a formative assessment task requiring students to improve upon the starting point provided below.",
    "### Marking using Generative AI\n\nStaff can use AI to do some aspects of marking, for example:\n\n- Grouping and marking responses to short answer questions or multiple choice where AI functionality is part of a software package used to deliver an assessment and where there remains human oversight.\n\nHowever:\n\nGenerative AI cannot be used to obtain personalised student feedback on formative and summative assessment.\n\nStudents can be encouraged to seek ongoing feedback on tasks and assignments, but the justification of a mark should be a human judgement.",
    "### Assessment Design\n\nThere is no going back from this point.\n\nAI tools are only going to get better and we cannot outrun the pace of change.\n\nWe need to re-think assessment design.\n\nIn the first instance, staff should consider using Blooms Taxonomy within assessment design and use assessment to evidence higher order thinking skills around creation and evaluation where it is harder for AI to respond appropriately at this time.\n\nThere are two strategies for re-thinking assessment design:\n- Designing out\n- Designing in",
    "#### Designing out\n\n- Alternative formats for assessment: Summative assessment which solely focuses on writing down information is the most easily replicated by AI.\n\nStudents could be asked to write podcasts or have assignments which require video or recorded speech.\n\nThese approaches could be combined together in a multimedia project or portfolio.\n\n- Oral/in-person assessments: Assessments which have an oral component, for example, presentations, vivas, poster sessions, and in-class competency testing are not as easily replicated by AI.\n\nHowever, it should be noted that there are some Generative AI tools which produce images, diagrams, and posters.\n\nStudents could be asked to upload a video of them talking through a poster or an infographic.\n\n- Viva-voce assessments can be scalable with 5-15-20 minute slots, feedback can be instant.\n\nA detailed marking rubric can be shared with students in advance.\n\nThe sessions can be recorded and moderated.\n\nIf you use this approach then you should consider ways to scaffold the approach and train students progressively in this skill.\n\nYou will also need to be aware of students with reasonable adjustments where a viva could not be an appropriate method of assessment.\n\n- Object and data reflections: Students can be asked to critically analyse and reflect on pieces of data; they could interpret physical objects or artefacts.\n\nThis could be in person, or it could be recorded.\n\n- Experiential learning: Students can be asked to work on case studies or problem-based learning activities which require the generation of new knowledge, creative design, or the development of products or resources.\n\n- Proctored assessments: These are the use of invigilated exams or a test in class.\n\nHowever, we would discourage the use of these unless there are strong pedagogical reasons and are delivered as part of a suite of assessments to diversify experience.",
    "#### Designing In\n\nThis is where we embrace generative AI within assessment tasks.\n\nWe must prepare our students for when they encounter AI in the workplace.\n\nThink of a task where students are tasked to use generative AI to obtain a piece of writing on a specific topic then:\n- Get the students to critique the output in terms of structure\n- Ask the students to identify the flaws in the content for examples, hallucinations (false information) and bias\n- Ask the students to assess the content in light of their own knowledge and understanding\n- Ask the students to follow up the references, see if they are genuine and if they reflect the key sources.\n\nYou can also obtain answers to example tests in class or exam questions to be shared with students to evaluate the strengths and weakness of generative AI content.\n\nIf you would like more support in assessment design visit the CHERPPS PebblePad Training Page.",
    "### Generative AI as mentor\n\nEncourage students to get ongoing feedback on assignments and tasks and get them to reflect on the feedback against their own knowledge and understanding and report on the guidance which has been provided and how they may or may not include it in their work.",
    "### Where Generative AI should not be used\n\nThere are a number of ‘red light’ activities where staff should not use Generative AI:\n\n- Generation of letters to students or other staff including personal data and information.\n\nThis is because the software will store data and information and potentially use it for other content.\n\n- Generation of personalised student feedback on formative and summative assessment.\n\nStudents can be encouraged to seek ongoing feedback on tasks and assignments, but the justification of a mark should be a human judgement.",
    "## Generative AI in Research Projects\n\nOur students on taught programmes will be conducting research, especially in their final year of study.\n\n- Students should not input research data into an AI tool unless it is an agreed part of the research project.\n\n- Students are required to obtain ethics approval before entering personal data into an AI tool.\n\n- Students will need instruction on the difference between feeding a transcript/observation notes/participant diary into e.g., NVivo, and feeding it into AI which could use it to generate further material.",
    "## Developing Graduate Attributes as Relates to AI\n\nWhen designing content and assessment, we must consider learning outcomes and graduate attributes as they relate specifically to the use of AI.\n\n- Critical thinking - thought and action: Students should be able to take a piece of Generative AI text, diagram, image or code and check this against their own knowledge and understanding of a topic or area.\n\nThey need to critically assess the origins and the authenticity of the content.\n\n- Self-reflection: Support students to explore ways of using generative AI to identify ways that they could improve areas of feedback on previous assignments.\n\nStudents can also use Generative AI to provide feedback on their work and identify areas for improvement at the editing stage.\n\n- Digital literacy: The use of AI in the workplace is going to expand and we should develop confidence in our students to be able to use AI tools, how to generate input questions to get the best outputs and evaluate what they do well, and where there are limitations.\n\n- Integrity: We should discuss with our students the ‘authorised green light’ activity when it comes to use of AI in assessment.\n\nStudents should be encouraged to look at assessment briefs for specific assignments and engage with discipline-specific uses.\n\n- Ethical awareness: Students should be aware of issues such as job displacement, exploitation, authorship, bias, and misinformation.\n\n- Research skills: Help students develop skills in identifying research questions, and assessing the outputs Problem solving.\n\nStudents can generate outlines that are then used as the basis for deeper research and investigation.\n\nEncourage synthesis of sources by getting students to integrate other sources to support or refute generated output.\n\n- Organisation: Generative AI can be used to create study and work schedules allowing students to create personalised study plans or group work schedules.",
    "## Further Resources\n\nThere are plenty of ideas in the community on how to effectively use AI for teaching and learning, 101 creative ideas to use AI in education-a crowdsourced collection is one example.\n\nShould AI be used to assess research?\n\nChatGPT listed as author on research papers Introduction to ChatGPT and ChatBots AI Tools Directory Trusted Research",
    "#### Suggested Course Syllabus Statements\n\nWhen using AI tools, it is important to be aware that the user data supplied might be utilized for training AI models or other purposes.\n\nConsequently, there is no guarantee that the information you provide will remain confidential.\n\nInstructors and students should exercise caution and avoid sharing any sensitive or private information when using these tools.\n\nExamples of such information include personally identifiable information (PII), protected health information (PHI), financial data, intellectual property (IP), and any other data that might be legally protected.",
    "##### AI Policy: Permitted in this Course with Attribution\n\nIn this course, students are encouraged to use Generative AI Tools like ChatGPT to support their work.\n\nTo maintain academic integrity, students must disclose any AI-generated material they use and properly attribute it, including in-text citations, quotations, and references.\n\nA student should include the following statement in assignments to indicate use of a Generative AI Tool: “The author(s) would like to acknowledge the use of [Generative AI Tool Name], a language model developed by [Generative AI Tool Provider], in the preparation of this assignment.\n\nThe [Generative AI Tool Name] was used in the following way(s) in this assignment [e.g., brainstorming, grammatical correction, citation, which portion of the assignment].”",
    "##### AI Policy: Permitted when Assigned in this Course with Attribution\n\nIn this course, students are permitted to use Generative AI Tools such as ChatGPT for specific assignments, as designated by the instructor.\n\nTo maintain academic integrity, students must disclose any use of AI-generated material.\n\nAs always, students must properly use attributions, including in-text citations, quotations, and references.\n\nA student should include the following statement in assignments to indicate use of a Generative AI Tool: “The author(s) would like to acknowledge the use of [Generative AI Tool Name], a language model developed by [Generative AI Tool Provider], in the preparation of this assignment.\n\nThe [Generative AI Tool Name] was used in the following way(s) in this assignment [e.g., brainstorming, grammatical correction, citation, which portion of the assignment].”",
    "##### AI Policy: Not Permitted in this Course\n\nIn this course, it is expected that all submitted work is produced by the students themselves, whether individually or collaboratively.\n\nStudents must not seek the assistance of Generative AI Tools like ChatGPT.\n\nUse of a Generative AI Tool to complete an assignment constitutes academic dishonesty.",
    "## Honor Council limits use of ChatGPT\n\n_Cali Liu / Thresher_\n\n**By Spring Chen** 4/18/23 10:16pm\n\nThe Honor Council sent an email to all undergraduates on April 11, announcing an Honor Code amendment explicitly prohibiting the use of artificial intelligence software such as ChatGPT without proper citation.\n\nAdditionally, the email clarified professors’ right to ban the use of AI software for their classes.\n\n“Utilizing AI software to generate ideas and pass them off as one’s own will also be considered plagiarism and will be adjudicated as such by the Honor Council,” the email said.\n\nThe email also clarifies that use of AI software “for your own study purposes is allowable.”\n\nRodolfo Gutierrez-Garcia, Honor Council internal vice-chair, explained that rather than creating a new policy, the amendment was a clarification of previous policy.\n\n“We’re going to be treating it like any other resource,” Gutierriz-Garcia, a Baker College junior, said.\n\n“If you don’t cite it when you use it, that’s plagiarism.” According to Honor Council communications chair Pedro Ribeiro, this decision was driven by the Honor Council.\n\n“Administration had no play in this at all,” Ribeiro, a McMurtry College sophomore, said.\n\n“We only started meeting with [administration] to really facilitate outreach … We thought it was a common-sense amendment we could pass.”\n\nRibeiro added that the Honor Council would consider evidence such as revision history and proper citation of AI software when adjudicating cases.\n\nAccording to Gutierrez-Garcia, the Honor Council is not currently planning to use AI detection software for various reasons.\n\nMaya Irish, an associate professor of history, said she intends to prohibit the use of AI in her classes.\n\n“My plan is to be very explicit throughout my policies regarding ChatGPT or any other artificial intelligence tools,” Irish said.\n\n“I’m thinking of just banning it all together and making it very explicit in the syllabus that it’s not allowed.\n\nIf [students] have … a good argument why they have to use it, they’d have to ask me.”\n\nAdditionally, Irish said her class’ policy would extend to the use of AI tools for personal study purposes.\n\n“[The policy] says using AI for your own study purposes is allowable … that point seems very problematic to me, because I ask students to find articles on the subject of their research and read them,” Irish said.\n\n“If they’re just going to go and use artificial intelligence to summarize them, that defeats the purpose of education.”\n\nMack Joyner, an assistant teaching professor in the computer science department, said he has a more positive view of AI software.\n\n“It’s important not to highlight only the negative side of ChatGPT, but the positive aspects,” Joyner said.\n\n“From that standpoint, [faculty] need to really think through in which cases it should be allowed.”\n\nJoyner said potential applications of AI software in the computer science department include improving a student’s existing code or debugging programs.\n\n“If there’s an error [students] could ask for more information, maybe, what are the possible solutions to fix this particular error,” Joyner said.\n\n“Students ask that now, but maybe [with AI software] they can get a more detailed answer that leads them to a solution, instead of spending hours trying to define what that particular solution would be.”\n\nSid Richardson College sophomore Michael Wong said that he believes the clarified policy is reasonable.\n\n“It seems reasonable to me that you can’t pass off the AI’s work as your own,” Wong, a computer science major, said.\n\n“But also I don’t see a problem with using it like a tool.\n\nIt’s like Google on steroids.” However, Wong said he still had some confusion over the permissible uses of AI software.\n\n“I wrote code for this assignment and it passed,” Wong said.\n\n“I asked the AI, ‘Is there a way I can optimize the code?’ It said, ‘you should try doing this’, and I tried doing that and it worked.\n\nThe assignment is due in the future and … the AI’s code has concepts we haven’t already learned.\n\nI’m not sure if I’m allowed to use it.”",
    "**Jeremy Zucker headlines second-ever Moody X-Fest**\n\n_By James Cancelarich_\n\nJeremy Zucker headlined Rice’s second annual Moody X-Fest in Founder’s Court on April 19.\n\nIn advance of Zucker’s set, student groups like Basmati Beats, Rice Philharmonic and BASYK performed.\n\nThe festival also offered complimentary merchandise and food from Dripped Birra, Cane’s and Oh my Gogi.",
    "**Jones wins men’s and women’s Beer Bike races, GSA snags alumni**\n\n_By Viola Hsia and Riya Misra_\n\nJones College won both the women’s and men’s Beer Bike 2024 races, while the Graduate Student Association claimed the alumni team win.\n\nHanszen College bike teams were the runner-up in the alumni and men’s races, while Brown College was the runner-up in the women’s race.\n\nMartel and McMurtry Colleges did not bike in the alumni race, according to the Rice Program Council’s final report, and the GSA was disqualified from the men’s race for accidentally sending out two bikers simultaneously.",
    "## Acknowledgement of Country\n\nThe National AI in Schools Taskforce recognises and pays respect to the Traditional Custodians of the lands and waterways where we work and live.\n\nWe celebrate Aboriginal and Torres Strait Islander people’s unique cultural and spiritual relationship to Country and acknowledge the significance of their cultures in Australia.\n\n---\n\n© Commonwealth of Australia, 2023.\n\nLicensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) licence.\n\nAttribution should be given to: © Commonwealth of Australia, 2023.\n\nMaterial in this Framework not available under a Creative Commons licence includes the logos of the: Commonwealth of Australia, and the other jurisdictions (QLD, VIC, SA, WA, TAS, ACT, NT and NSW) national education agencies (ACARA, AERO, AITSL and ESA) non-government school sector peak bodies (ISA and NCEC).",
    "## Acknowledgements\n\nMembers of the National AI in Schools Taskforce included representatives from all jurisdictions, the National Copyright Unit (NCU), non-government school sector peak bodies, Independent Schools Australia (ISA) and the National Catholic Education Commission (NCEC), and representatives from national education organisations, including the Australian Curriculum, Assessment and Reporting Authority (ACARA), the Australian Education Research Organisation (AERO), the Australian Institute for Teaching and School Leadership (AITSL) and Education Services Australia (ESA), with secretariat support provided by the NSW Department of Education.\n\nThe Taskforce acknowledges and thanks the academic community for providing insights on how to develop and improve the framework, with special thanks to: Professor Kalervo Gulson, Associate Professor Jason Lodge, Professor Terry Flew, Professor Toby Walsh, Professor Matt Bower, Associate Professor Christine Slade, Associate Professor Erica Southgate, Professor Michael Henderson, Industry Professor Ian Oppermann and Industry Professor Leslie Loble.\n\n---\n\nDepartment of EDUCATION",
    "## Purpose and Audience\n\nThe purpose of the Framework is to provide guidance on understanding, using and responding to generative AI in Australian school-based education.\n\nIt supports policy makers, school leaders, teachers, support staff, parents and students.\n\nIt does not address other forms of artificial intelligence, including predictive AI.\n\nThe Framework is aspirational in nature, defining what safe, ethical and responsible use of generative AI should look like to support better school outcomes.\n\nThe Framework’s Principles and Guiding Statements are designed to help jurisdictions and sectors align existing approaches while also supporting the development of future work.",
    "## Design of the Framework\n\nThe Framework is designed to help Australian school communities (students, teachers, staff, parents and carers) support:\n\n1.\n\n**Education outcomes:** The Framework aims to recognise how the appropriate use of generative AI tools can enhance teaching and learning outcomes for all members of Australian school communities.\n\n2.\n\n**Ethical practices:** The Framework aims to achieve the safe, responsible and ethical use of generative AI tools in Australian schools.\n\n3.\n\n**Equity and inclusion:** The Framework aims to ensure that generative AI tools are used in ways that are fair, accessible and inclusive of all Australian school communities.\n\nThese goals are the basis of the Framework’s 6 Principles and 25 Guiding Statements.\n\nFigure 1 provides a high-level illustration of the Framework, highlighting the interconnectedness of the Principles.\n\n---",
    "## Opportunities and Risks\n\nGenerative AI technology has great potential to assist teaching and learning and reduce administrative workload in Australian schools.\n\nThe growing accessibility and sophistication of generative AI tools provides opportunities to develop human-like generated text and rich multimedia content in a way that has not previously been possible.\n\nTo fully harness the potential of high quality and safe generative AI, schools will need to be supported in understanding and appropriately managing a range of privacy, security and ethical considerations.\n\nRisk management should also be appropriate for the potential consequences.\n\nThese consequences include the potential for errors and algorithmic bias in generative AI content; the misuse of personal or confidential information; and the use of generative AI for inappropriate purposes, such as to discriminate against individuals or groups, or to undermine the integrity of student assessments.\n\nAppropriate and proportionate risk management will require robust guidance and policy, which the Framework aims to support.",
    "## About the Framework\n\nIn February 2023, Education Ministers agreed that responding to the risks and harnessing opportunities for Australian schools and students arising from generative AI technologies is a national education priority.\n\nMinisters agreed to develop an evidence-informed, best practice framework for Australian schools.\n\nThe Framework was developed in consultation with unions, teachers, students, industry, academics, and parent and school representative bodies from all sectors.\n\nIt was developed by the National AI in Schools Taskforce comprised of representatives from all jurisdictions, school sectors, and the national agencies - Educational Services Australia (ESA), Australian Curriculum, Assessment and Reporting Authority (ACARA),\n\nThe Framework aligns to existing national policies and goals.\n\nIt complements Australia’s Artificial Intelligence Ethics Framework, and the two key goals of The Alice Springs (Mparntwe) Education Declaration: promoting equity and excellence in education; and enabling all young Australians to become confident and creative individuals, successful lifelong learners, and active and informed members of the community.\n\nAdditionally, the Framework aligns to the United Nations sustainable development goal #4: Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all.\n\nIt also aligns with the Safer Technologies for Schools (ST4S) initiative which aims to enhance the security, privacy and online safety of software services and applications commonly used in Australian schools.\n\nAny policy development or amendments made to align with the Framework should also consider the Australian Professional Standards for Teachers (APST), Privacy Act 1988, Copyright Act 1968, the eSafety Commissioner’s Best Practice Framework for Online Safety Education, and Australia’s human rights protections.\n\nWhere applicable, policies developed in alignment with the Framework must also meet existing government commitments to incorporate Indigenous community partnerships and expert reviews.\n\nPolicies must also meet government obligations to ensure Indigenous communities have access to locally relevant data and information as per Closing the Gap Priority Reform 4.",
    "### Generative Artificial Intelligence: Guidelines for Educators\n\nThese Generative Artificial Intelligence (GenAI) Guidelines for Educators have been developed by a National Academic Integrity Network (NAIN) Working Group, as a response to the swiftly evolving and developing field of GenAI and the challenges that it may pose for academic integrity to educators and their students.\n\nThey are designed to help support educators in their understanding of the potential uses of Generative AI, both in supporting learning for their students, and most critically, in providing a potential ‘short cut’ to students in the fulfilment of required tasks for assessment.\n\nThe guidelines have been written to provide support and advice for educators to reflect on, and as appropriate, to share and discuss with their students to enable them to understand and appreciate what is and isn’t permitted.\n\nThe overall goal is to enable an understanding of what GenAI can and can’t offer, and thereby, to ensure an ethical basis for the use of GenAI tools, helping students to build their own self-awareness and knowledge, and avoid breaches of academic integrity.\n\nThe Guidelines are structured so that Part 1 contains a list of Recommendations, classified under the four headings:\n- What everyone needs to know\n- What lecturers and other educators need to know and do\n- What programme managers and institutional leaders need to do\n- What students need to know and do\n\nPart 2 provides the detailed guidelines for each of the four headings.\n\nPart 3 provides a list of further reading, links, and resources.\n\nThese Guidelines are not intended to replicate what is currently available to educators, but to provide practical advice which can be applied to the design of programmes including assessments, and be incorporated into teaching practice.\n\nThis area is a dynamic one which is developing at a very fast pace; this means that these Guidelines, which will be published online, may well need to be regularly reviewed and updated.\n\nWe hope however that they will provide some practical support for everyone involved in tertiary education and that they will have a positive impact in supporting high quality teaching, learning and assessment policies and practices.\n\nBilly Kelly Chair, National Academic Integrity Network, July 2023",
    "#### What everyone needs to know\n\nTools which include Artificial Intelligence (AI) can produce impressive outputs, whether well-structured and fluently written reports, images and graphics, computer code, or mathematical solutions.\n\nThey have a huge potential range of applications.\n\nThis technology is becoming ubiquitous, embedded within major software suites (such as Microsoft Office, Google Workspace, etc) and browsers, and has spurred the development of a large number of new apps and tools proffered by a myriad of start-ups and project teams.\n\nIt is crucially important for everyone to be aware that these tools generally are based on mimicry and reproduction of content, style, and genre and are not always optimised to provide factually correct answers.\n\nNor should their speed, conversational interface, and apparent quality of their output be taken as evidence of any ‘intelligence’ or subject expertise.\n\nThere are concerns about how the tools are developed and trained (around copyright, intellectual property, wellbeing of those hired to screen and label content, energy usage, etc) as well as how they may be misused or misapplied.\n\nIt is important that everyone is familiar with their current institutional policy on the use of GenAI, bearing that it is likely to be updated on a regular basis.",
    "#### What lecturers and other educators need to know and do\n\nBe familiar with these tools and their availability, and recognise their limitations and affordances.\n\nAs soon as you can, review all your assessments, replacing or modifying any which may be completed satisfactorily by someone using AI without an appropriate level of understanding of the subject, or which are vulnerable to breaches of integrity or security.\n\nIf you identify a particular appropriate role for GenAI in your courses, then ensure that students are aware of what constitutes authorised and unauthorised use.\n\nWork with colleagues, and within disciplinary communities, to design assessments which have strong validity (i.e., clearly aligned with the learning outcomes and the skills which they are meant to demonstrate) and in which there is an appropriate balance of credit between the process of preparing for and undertaking the assessment and the final product, helping students develop self-efficacy, critical thinking, and a professional disposition.\n\nTake the opportunity, at programme level, to review overall assessment volume, range of assessment types, and alignment with intended programme outcomes across the complete programme and year cohorts, identifying any vulnerabilities, over-assessment, and opportunities for enhancement.\n\nBe consistent in your approaches to student training, assessment practice, and in the implementation of institutional academic integrity policies, data protection, and assessment regulations.\n\nThere must be clear information for students that unauthorised submission of the output of GenAI as their own work constitutes academic misconduct and will be treated accordingly.\n\nIn your partnership with students, emphasize values such as integrity, trust, and truthfulness as being at the heart of learning, knowledge discovery and creativity.",
    "#### What programme managers and institutional leaders need to do\n\nEnsure that there is up-to-date and regular training for all staff and students on academic integrity, AI, and assessment, and that appropriate resources are allocated to this area to ensure that all avail of it.\n\nProgramme leaders should work with colleagues to plan and undertake a comprehensive review of assessment, focusing on validity, range of types, and volume, with a view to enhancing both the programme design and the student learning experience.\n\nUpdate, and regularly refresh, policies on academic integrity, including the use of GenAI, and assessment to more effectively reflect and respond to these issues and take the opportunity to reiterate the valuable educational purposes of assessment.\n\nDevelop an institutional ethos and culture which emphasizes integrity, honesty, trust, and respect, rather than let the conversations be dominated by suspicion, surveillance, and distrust.\n\nWorkshops and training events which promote dialogue and engagement can be valuable in this regard.",
    "#### What students need to know and do\n\nAI tools are appearing everywhere these days: embedded within packages that are used to write text, produce slide-decks; or in new apps and services that get promoted on social media.\n\nThey can be useful for many types of tasks, but they also have serious limitations and can give unreliable answers whilst appearing confident and convincing.\n\nIt is crucial to be aware of what these tools can and cannot do, as well as recognise the ethical concerns associated with their development and use.\n\nAssessment is meant to provide the student with an opportunity to demonstrate achievement of the intended learning outcomes of the module or programme, to the standard required.\n\nLecturers often use assessment tasks as a means of helping students focus on the key knowledge and skills that they need to develop and encourage them to practice and build their confidence.\n\nIt is important that students submit work that they have produced and acknowledge the sources used, as well as paying close attention to the requirements of the task and policies on academic integrity.\n\nIn reality, assessment can be stressful, but to maintain integrity (both personal integrity and that of the qualifications), students should resist any attempts to take ‘shortcuts’ or engage in any form of misconduct (copying, plagiarism, submitting materials produced largely by GenAI, etc).\n\nIf a student feels unable to complete the assessment for whatever reason, they should discuss this with their lecturer, counsellor, SU Officer or appropriate person and see what alternative arrangements can be made.",
    "### Part 2: Detailed Guidelines\n\nRecent releases of tools built on ‘Generative AI’ have made headlines across the world.\n\nAt first glance, their capabilities seem impressive: writing fluent text on any subject, eloquently expressed with excellent structure and grammar; producing visually arresting images in any specified genre; constructing (and explaining) computer code for simple or complex tasks; suggesting statistical tests for data sets and producing a range of analyses; solving mathematical and other symbolic manipulation challenges.\n\nTheir speed of response, their conversation-like ability to build on previous prompts and answers, and their (superficially, at least) plausible output, however, runs the risk of anthropomorphism and the presumption that behind the scenes there is a nascent, if not fully enacted, ‘intelligence’.\n\nThe implications of these tools for assessment are very significant as they run the risk of increasing intentional and unintentional misconduct.\n\nIn addition, beyond assessment, in the wider world, they may well contribute to the dissemination of misinformation, false accusations, libel, breach of copyright, insertion of erroneous computer code into systems, and other legal and ethical issues.\n\nUsed well, they could offer benefits for learners, document preparation, media production, and even lead to newer forms of creativity.\n\nClearly, a critical digital literacy which addresses the development and deployment of AI is essential for all of us.\n\nIn these guidelines, however, we will focus on assessment and have grouped together key information and suggested courses of action for staff, students, and higher education institutions in the short-term.",
    "#### What everyone needs to know\n\nAI is becoming ubiquitous and embedded within more of the tools and software we use in everyday life and in research, learning, and teaching.\n\nThe pace of change is rapid and it makes sense to anticipate further significant advances over the next few years.\n\nBrowsers, word-processing packages, spreadsheets, presentation software, audio, imaging, graphics, and video tools are all increasingly making use of such technologies, including core product suites from Microsoft and Google.\n\nBy default, users of these tools will be presented with ‘suggestions’ on anything which they write or design.\n\nGenAI tools such as the ‘Large Language Models’ on which ChatGPT, Bard, and others are based, are designed to write well in terms of language, style, and syntax, rather than in terms of whether what they write is factually correct or accurate.\n\nIt is crucially important to understand how such technologies work and be aware of their limitations as well as their apparent strengths.\n\nThey are not ‘intelligent’ in any real sense, nor are they experts on topics, but rather largely rely on statistical predictions of word combinations or image features.\n\nThere are also concerns about ethical (and environmental/energy use) aspects in how such tools are developed, trained, and deployed and it is important to be aware of these, particularly with regard to the lack of transparency on the information that has been used to ‘train’ the systems and whether copyrighted materials have been used and sources which are heavily biased.\n\nSome moves are being made internationally to consider potential forms of regulation, although quite what shape these will take is very uncertain.\n\nAs technologies develop, appropriate roles in education may be found, but if they are used to bypass rather than support thinking, or used to acquire academic credit which has not been earned via real, intellectual engagement with the subject of study, then they can undermine the educational enterprise.\n\nPart of the current challenge for educators, learners, and institutions is to determine the range of beneficial applications which might be afforded by such tools.",
    "#### What lecturers and other educators need to know and can do\n\nEnsure that you are aware of recent and new Generative AI tools, including those associated with supporting writing, and others which might be relevant to your particular discipline, such as those which assist the development of computer code, solve mathematical problems, generate graphics, video, and audio content, etc.\n\nTry to understand what these tools can and can’t do (including by trying them out on your own assignments or subject content), and, in particular, avoid mistakenly attributing subject expertise, intelligence, or capacity for reflection to such software.\n\nIt is useful also to be aware of ethical and legal concerns over how such tools are developed and used, as well as the potential inequality of differential access (e.g., on the basis of cost).\n\nIncreasingly, educators are sharing use cases of where AI tools might be useful in designing courses, lesson planning, assessment design, student feedback, etc.\n\nWhilst as the technology improves, there is no doubt that GenAI will be more useful in many professions and contexts, we still urge caution, particularly in terms of the accuracy of its outputs and the fact that it is largely based on statistical models and hence reproduces or mimics rather than being capable of generating original content.\n\nThere are many cases where a traditional search strategy (using a search engine or library tools) is much more effective and has the added benefit of identifying original sources and allowing you to make judgments about accuracy and quality as well as to provide due credit to the original authors.\n\nThis is a key point that should also be included in any training of students about information gathering and research.\n\nIf you are using GenAI tools in your own academic practice, then you should model appropriate use with your students, acknowledge where GenAI has assisted in your work, etc.\n\nIt is important not to input personal, private, sensitive, or copyrighted materials (including student work) into such tools unless you have appropriate levels of legal guarantees.\n\nMany of the freely available tools at the moment do not have such protections in place.\n\nIn designing assessment, it is important to ensure validity, i.e., that the assessment should be a means whereby each student demonstrates that they have attained the intended learning outcomes to an appropriate standard and that this should be the basis on which academic credit is awarded.\n\nIt is important to clarify and reiterate this point with students and to demonstrate how the assessment fits this intention.\n\nIt may be appropriate to revisit your original learning outcomes to ensure that they are aligned with how you teach, the tasks you set students, and the chosen forms of assessment.\n\nIf you intend for students to use GenAI or AI-supported tools in any assignment, make this clear.\n\nIt may not be possible to simply state ‘do not use AI for this assignment’ given the technology’s ubiquity, so the emphasis should be on what the student actually does to demonstrate their attainment of the intended learning outcome with which the assignment is aligned.\n\nIf the student constructs a report or essay through clever ‘prompt engineering’ which could have been done by anyone not taking this module, then this does not provide the basis for an award of marks or credit.\n\nWhere students are expected to use AI tools, ensure there is clarity on how and why they used such, with an appropriate declaration on any submitted assignments.\n\nThere will be cases where it is clearly inappropriate to use GenAI and, in those cases, state this clearly in the instructions provided to students and ensure that you are able to make the format of the assessment sufficiently secure and robust.\n\nAcademic Integrity is breached if students submit the products of GenAI as their own work without acknowledgment or without authorization to use GenAI in fulfilling the task.\n\nIt is important to ensure that students are informed that this constitutes academic misconduct and that they should only be seeking credit for work that they have produced themselves, and that they are responsible for correctly referencing and acknowledging sources and resources used in their work.\n\nReview all assessments and assignments in terms of whether or not they are susceptible to being completed successfully by an AI tool without the student having to engage intellectually, or personally, with the subject.\n\nSome common types of assessment should no longer be considered to be sufficiently robust to award scores which count towards official grades.\n\nThese may still be perfectly valuable for self-assessment and formative practice.",
    "These may still be perfectly valuable for self-assessment and formative practice.\n\nThese include:\n- Take-home essays, reports, or similar documents focused largely on subject knowledge content and with an ‘all or nothing’ submission by a single deadline and where marks are based on structure, style, and information;\n- Online MCQs and other similar types of tests delivered online in un-invigilated and/or remote contexts, whether ‘live’ or asynchronous.\n\nThe capabilities of current AI systems are expanding rapidly, including the advent of plugins and connections with other tools and resources (search engines, mathematical engines, drawing packages, etc).\n\nThis means that the range of assessment types that it can be used to support (or subvert) is extensive.\n\nTasks such as developing a plan, a strategy, making a case, comparing and contrasting, inventing data for an experiment, plotting graphs, producing a slide deck on a topic, writing code, analyzing data in spreadsheets, compiling a list of references, etc, are all within current capabilities.\n\nSimply trying to substitute one basic assessment type for another will not be sufficient to counter the challenge to assessment integrity.\n\nNew assessments and assignments should instead focus on demonstration of personal and academic skills, validity and reliability, authenticity, and security.\n\nConsider the balance between formative and summative assessment in your modules/courses, and between continuous assessment and examinations.\n\nPractice and formative feedback are key aspects of learning which can also help students develop self-efficacy and responsibility for their own learning.\n\nIt may be that you feel that a shift towards more weighting for assessments undertaken in an invigilated examination type environment is the option most readily available to you, at least in the short term, but do try also to consider what might be most educationally valuable alternatives and identify what design assistance or other support would be needed for such.\n\nDo not rely on GenAI ‘detection systems’.\n\nNone of the tools which are currently available are fully capable of detecting the use of GenAI (except in the most obvious cases which may also have been identified by expert reading and scrutiny) and may also lead to ‘false positives’ (incorrectly concluding that human-written text was AI-generated) and difficult-to-interpret scoring.\n\nDetection systems cannot be relied upon to detect the use of GenAI accurately or consistently.\n\nIn addition, there may be serious data protection, privacy, and intellectual property concerns in the use of any such tool, particularly if it has not undergone appropriate approval by institutions.\n\nTurnitin’s detection tool is available in some institutions, but users should be aware of concerns about its capabilities in terms of more recent versions of GenAI, a reported high rate of ‘false positives,’ and some ambiguity on how to interpret its results.\n\nConsider what scope there is for assessments which:\n- Focus on ‘process rather than product’.\n\nIn other words, where credit is associated with the various tasks or stages that the student goes through to lead to the final product.\n\nThis can be done by breaking down assignments into key stages and incorporating student reflection/reporting on how they found their sources, what aspects they explored, what challenges they overcame, etc (or whatever is appropriate to the particular assignment) so that there is a developmental journey.\n\n- Are clear in terms of whether the emphasis is on the development of writing skills (something the AI tools can do very well) or on understanding of the specific topic and assess each of these with appropriate methods and rubrics.\n\n- Are reflective and based on personal/professional experiences and which are authentic to the discipline or profession.\n\nNote that many of the AI tools will readily invent ‘reflective’ content if so asked, so try to ensure that the task is genuinely connected to the student, or the specifics of particular aspects discussed in class.\n\n- Are in different or multiple formats, such as video or audio content, a mix of presentation (with questions and answers) and ancillary materials (e.g., reference list, handouts), etc.\n\nAI tools can produce slide decks and write scripts for video/audio content, so, again, be careful about the requirements.\n\n- Are based on whether the learning outcomes have been achieved by the student, rather than leading to a grade.\n\n- Use in-class writing assignments or problem-solving tasks.\n\n- Are based on, or partially include, an oral component in which the students are asked to answer questions around the topic and how they approached the task.\n\nThere are many examples where orals have been used successfully, even in large class contexts, and in which there are particular formats (interactive orals for authentic assessment) which align with the discipline or profession.",
    "Where it isn’t practicable for a very large cohort, at least consider undertaking a number of orals either on the basis of random selection or to clarify any concerns in a particular submission (i.e., this is where the main assignment is an essay, report, video, etc, and the oral is to check on integrity and/or on student understanding).\n\nWork with colleagues to develop an effective strategy for assessment which ideally would address issues of student (and staff) workload, the scheduling of deadlines/due dates, and ensuring that programme level outcomes are being met.\n\nWe know that academic misconduct can arise when people inappropriately react to:\n- pressure and stress caused by too many overlapping deadlines;\n- an amount of routine assessment that is so large, over the course of the semester, that it loses its perceived learning value to the student;\n- lack of opportunities for resubmission or development of an assignment after feedback;\n- perceived lack of consequence for such behaviour.\n\nAlways comply with and follow your institution’s academic integrity policy and report any suspected cases.\n\nBeing consistent in policy implementation is something which will shape students’ perceptions about the seriousness of the issue and that the policy is one which focuses on education and the development of professional values, reducing the potential for repeated misconduct and protecting the reputation of qualifications.",
    "#### What programme managers and institutional leaders need to do\n\nEnsure that all staff are informed and kept up to date and provide training opportunities for all staff and students on these issues and technologies.\n\nReview approaches to assessment across programmes to:\n- Identify and replace any existing practices which may be vulnerable to successful completion via the misuse of GenAI;\n- Consider the scope for greater focus on programme level outcomes and assessment, and the overall assessment workload;\n- Develop a clear map of all assessments, assignments, and deadlines for students and staff in the programme or year cohort, and seek opportunities for synergies, rationalisation, and scope for enhancing formative feedback and supporting student development;\n- Establish clear rubrics for assessments and aim for consistency of practice (where appropriate) and alignment with programme outcomes.\n\nWhilst it might seem that a switch from continuous assessment, coursework, and assignments back to traditional end-of-semester formal examinations is the easiest way to ensure the integrity of assessment this can run counter to the strength of more authentic assessment which aims to develop skills, knowledge in context, and other professional and graduate attributes.\n\nA short-term re-weighting of assessments may be necessary to respond quickly to these new challenges, but the longer-term goal should be to take a more holistic approach to assessment as/for/of learning.\n\nReview policies on academic integrity and assessment to more effectively reflect and respond to these issues and take the opportunity to reiterate the valuable educational purposes of assessment.\n\nEnsure that appropriate resourcing is available to support academic integrity.\n\nRefresh study/academic skills supports for students to provide clarity on acceptable as well as unacceptable uses of such technologies.\n\nWork as a sector on issues regarding technological developments, data protection, protection of IP, and ethics.\n\nEngage with QQI and other awarding or accreditation bodies on these issues and ensure that guidelines and regulations are clear to all staff and students.\n\nFoster an institutional ethos and culture (reflected in your communications) which emphasises integrity, honesty, trust, and respect, rather than let the conversations be dominated by suspicion, surveillance, and distrust.",
    "#### What students need to know and do\n\nAI (including so-called GenAI) tools are becoming widely available and embedded in many of the technologies which we use to write documents, analyse data, design presentations and to support learning.\n\nIt is crucially important to understand how such technologies work and be aware of their limitations as well as their apparent strengths.\n\nThey are not ‘intelligent’ in any real sense, nor are they experts on topics, but rather largely rely on statistical predictions of word combinations or image features.\n\nThere are also concerns about ethical aspects in how such tools are developed, trained, and deployed and it is important to be aware of these.\n\nEnsure that students understand what is required of them for any assignment or assessment, including the extent to which there is any allowable use of such technologies and how this may need to be acknowledged.\n\nAll assignments and assessments are designed to determine whether students can demonstrate attainment of the specified learning outcomes of their module or programmes.\n\nIn other words, the credit is awarded for their work not that of others or produced by GenAI systems or obtained from other sources.\n\nThis means that students need to understand their institution’s academic integrity policy and be able to clearly draw the line between what is permissible and what may constitute misconduct.\n\nLearning is about ‘sense-making’, about juggling ideas, trying to see where they fit or where they contradict, about rethinking what we thought we already knew, about seeing things in new ways.\n\nAll of this can be difficult, students can feel vulnerable if something doesn’t appear to be immediately obvious to them and they need to put in lots of effort or seek help for things to ‘click into place’.\n\nIt is not a weakness or a sign of lack of ability.\n\nIt’s what university-level education is about, but it should be counter-balanced by the sense of achievement and the new perspectives and skills that students ultimately acquire through this effort.\n\nIf a student is struggling, or feels under pressure, advise them to talk to their lecturers, tutors, SU officers, counsellors, or whoever is most appropriate.\n\nAdvise them not to take ‘short-cuts’ with assessments that avoid them properly engaging with the subject, and to resist any the social media and other messages that come in from online ‘cheating services’ with ‘promises of assignment support’.\n\nStudents need to be encouraged to reach out and ask for support.\n\nBeing a student should be a time in which they can develop new perspectives, learn more about themselves and society, find opportunities to express their passions and creativity, identify where they can engage with community and help make the world a better place.\n\nCentral to this is living up to values of trust, cooperation, integrity, and truthfulness, all of which are central to education and society.",
    "### Part 3: Further reading, links, and resources\n\nIn producing these guidelines, we have been informed by a wide range of documents, policies, articles, and other contributions (including webinars, discussions, and conversations) from across the international educational community.\n\nSome particularly useful further reading which we would recommend include:\n- A Generative AI Primer, by Michael Webb, JISC National Centre for AI, \n- AAIN Generative Artificial Intelligence Guidelines, Australian Academic Integrity Network (AAIN) Generative AI Working Group, March 2023, \n- Maintaining quality and standards in the ChatGPT era: QAA advice on the opportunities and challenges posed by Generative Artificial Intelligence, The Quality Assurance Agency for Higher Education 2023, \n- European Commission, Directorate-General for Education, Youth, Sport and Culture, Final report of the Commission expert group on artificial intelligence and data in education and training – An executive summary, Publications Office of the European Union, 2022, \n- Unlocking the Power of Generative AI Models and Systems such as GPT-4 and ChatGPT for Higher Education: A Guide for Students and Lecturers, Gimpel et al, 2023, \n- Turnitin Guide for approaching AI-generated text in your classroom, \n- 101 Creative Ideas to use AI in education, Nerantzi, C., Abegglen, S., Karatsiori, M. and Martínez-Arboleda, A.\n\n(Eds.)\n\n(2023) DOI: 10.5281/zenodo.8072950\n- Five Principles for the Effective Use of Generative AI, UTS, Sydney, \n\nThe NAIN GenAI Guidelines for Educators were developed by the Working Group Chair, Iain MacLaren (University of Galway) and Greg O’Brien (Griffith College), with contributions from Working Group members - Elva Casey (Hibernia College), Gavin Clinch (ATU, Sligo), Naomi Jackson (CCT College), Brid Lane (IBAT College) and Cathy Peck (DCU).\n\nPublished by Quality & Qualifications Ireland (QQI), August 2023 (1st edition)",
    "## How Do I Consider the Impact of AI Tools in My Courses?\n\n\"Artificial Intelligence & AI & Machine Learning\" by mikemacmarketing is licensed under CC BY 2.0.\n\nStudents may be interested in using artificial Intelligence (AI) tools like Copilot (formerly known as Bing Chat), ChatGPT, and Google Bard to enhance their own writing.\n\nConversely, faculty may be apprehensive of students’ AI use in demonstrations of their learning, such as through writing research papers, creating code and scripts, or solving problem sets.\n\nHowever, some instructors may even be interested in using AI tools in their courses to create novel learning experiences.\n\nAs you navigate where you might fall on this continuum, we encourage you to review the following strategies and examples when considering AI use in your courses.",
    "## STRATEGIES & EXAMPLES\n\n- **Examine Privacy and Data Collection Practices**: Before encouraging or asking students to use AI tools in their work, investigate how these tools collect personal information and data, including reading any privacy policy linked with a particular AI tool.\n\nSome tools may use log-in data, tracking, and other analytics (Center for New Designs in Learning and Scholarship, 2023).\n\nThis same advice applies to you, as a faculty member.\n\nShould you want to input your existing writing prompts into an AI tool to see the output, consider your own data privacy.\n\nInstructors who may want to make use of these tools should note that Copilot is included in the UMass agreement with Microsoft; it is considered FERPA compliant as long as users have logged in using their UMass accounts.\n\nIf you are considering requiring students to use an AI platform to complete coursework, Copilot is the only recommended option.\n\nNote that with any online service that is not contracted by the university, in view of FERPA regulations, students must not be required to identify themselves to third parties.\n\n- **Write a syllabus statement that clarifies the expectations of AI use**: The Faculty Senate Rules Committee has made a determination that, absent any guidance from the instructor, the use of AI text generators is prohibited according to the Academic Honesty Policy.\n\nAI detection tools, including Turnitin, continue to be unreliable in differentiating between human and AI-generated text (Elkhatat, Elsaid, & Almeer, 2023); these tools are not useful in determining academic honesty.\n\nFaculty are encouraged to review the resources, including how to initiate academic honesty conversations and guidelines on the academic honesty process maintained by the Academic Honesty Office as they frame their syllabus statements.\n\nInstructors do have the discretion to allow for the use of such tools, however, and must do so explicitly if they want to allow it.\n\nStating what you expect students to use (or not use) in their work helps to answer any questions about what extra support is permissible.\n\nDepending on your pedagogical values and course expectations, consider adopting or revising one of the statements below for your syllabus.\n\n(All statements adapted from Artificial Intelligence Tools and Teaching by Iowa University's Office of Teaching, Learning, & Technology.)\n\n- **AI is prohibited**: [This course] assumes that all work submitted by students will be generated by the students themselves, working individually or in groups.\n\nStudents should not have another person/entity do the writing of any substantive portion of an assignment for them, which includes hiring a person or a company to write assignments and using artificial intelligence tools like Copilot, ChatGPT, and Google Bard.\n\n- **AI is allowed with attribution**: Use of AI tools (Copilot is the recommended option) is permitted in this course for students who wish to use them.\n\nTo adhere to our scholarly values, students must cite any AI-generated material that informed their work (this includes in-text citations and/or use of quotations, and in your reference list).\n\nUsing an AI tool to generate content without proper attribution qualifies as academic dishonesty.\n\n- **AI is encouraged with certain tasks and with attribution**: You can choose to use AI tools to help brainstorm assignments or projects or to revise existing work you have written.\n\nWhen you submit your assignment, I expect you to clearly attribute what text was generated by the AI tool (e.g., AI-generated text appears in a different colored font, quoted directly in the text, or use an in-text parenthetical citation).\n\n- **Communicate your perspective about AI use**: In addition to syllabus statements, consider talking with your students about AI tools.\n\n- **Different levels of familiarity**: As an emerging technology, students will have differing levels of familiarity with these tools.\n\nFor instance, while Copilot, ChatGPT, and Google Bard can write a grammatically correct paper or appear to solve a math problem, these tools may be unreliable and limited in scope.\n\n- Discuss with students the uses and limitations of AI tools more broadly in addition to your perspective on their use in your class.\n\n- **Connect to critical thinking skills**: AI tools have many implications beyond the classroom.\n\nConsider talking with students about how to be engaged consumers of AI content (e.g., how to identify trusted sources, reading critically, privacy concerns).\n\n- **Adapt assessments**: AI tools are emerging and it can be incredibly difficult to make any assessment completely free from AI interference.\n\nBeyond a syllabus statement, you may also consider adapting your assessments to help reduce the usefulness of AI products.\n\nBefore revising any assignment, it’s helpful to reflect on what exactly you want students to get out of the experience and share your expectations with your students.",
    "Before revising any assignment, it’s helpful to reflect on what exactly you want students to get out of the experience and share your expectations with your students.\n\nIs it just the end product, or does the process of creating the product play a significant role?\n\n- **Create assessments that allow students to develop ideas over time**: Depending on your class size, consider scaffolding assessments to be completed in small components (e.g., proposal, annotated bibliography, outline, first draft, revised drafts).\n\n- **Ask students to connect their writing to specific course materials**: Students can draw from the course textbook, additional readings on the LMS, and even class discussion boards or in-class discussions.\n\n- **Incorporate personal experiences and reflections**: Provide students with opportunities to connect what they are learning to their own lives and experiences—stories unique to each individual.\n\n- **Incorporate Multimedia Assessments**: Consider developing or adapting assessments to include multimedia submissions (e.g., audio or video components).\n\nVoiceThread is a UMass Amherst supported tool that allows students to leave audio, visual, and video content.\n\nAlso, consider UMass-supported social annotation tools like Perusall or Google Docs for students to use when responding to assigned readings or other materials.\n\n- **Use class time**: Ask students to complete writing assignments during class time (e.g.\n\ncomplete reading reflections at the beginning of class, or use exit tickets).\n\nAsking students to organize their ideas by writing during class may also support student engagement in other class activities such as discussions and group work.\n\nFor questions on your LMS, Google, and other educational technology contact IDEAS at",
    "# Cross-Campus Approaches to Building a Generative AI Policy\n\nEsther Brandon, Lance Eaton, Dana Gavin and Allison Papini Tuesday, December 12, 2023 Emerging Technologies and Trends Editors' Pick  \n18 min read\n\nParticularly for new technologies that disrupt long-standing practices and cultural beliefs, the work of carefully and intentionally developing effective policies can pay significant dividends.\n\nCredit: vinnstock / Shutterstock.com © 2023\n\nPolicy creation might not be exciting, but it is essential, especially when something new arrives on the scene and an organization does not have an easy or clear means of modifying existing policies to accommodate the new development.\n\nOne such arrival happened in late November 2022 with generative AI, and a year later, many colleges and universities are still scrambling to figure out an AI policy that works at the institutional, departmental, and course levels.\n\nNew technologies tend to follow a particular hype cycle, particularly in higher education.\n\nHowever, the possibilities, problems, and paradigms that generative AI tools such as ChatGPT, Google's Bard, and Microsoft's AI-powered Bing represent are many and can touch every part of the institution and its stakeholders.\n\nHigher education focuses on building knowledge and investing in the written word, whether it's scholarly research or demonstrations of learning such as essays, research papers, theses, or dissertations.\n\nThe underlying assumption is that such work could not be significantly faked aside from paying another individual (such as student paper mills) or taking the work of others without attribution.\n\nGenerative AI upends much of that baseline and in fact will challenge many institutions' academic integrity policies because they often prohibit copying from websites or acquiring papers from other individuals.\n\nGenerative AI will also introduce other ethical and procedures considerations throughout the institution.\n\nIn this article, we provide guidance and recommendations for approaching the development of institutional policy around using generative AI tools in higher education.\n\nWe aim to help leaders in higher education institutions work more effectively to establish a pathway to a policy that includes various stakeholders and ultimately reduces the risk that members of the institution will take policy matters into their own hands, resulting in problems for the entire institution.\n\nThis piece evolved from our institutional roles (instructional design, writing support, and research and library support), our individual work at our respective institutions, and a half-day workshop on generative AI policy development that we facilitated for the NERCOMP 2023 Annual Conference.\n\nThe workshop included a student panel, followed by focused conversations around several topics related to generative AI: ethics and plagiarism, bias, instructional technology, opportunities, and threats.\n\nFor the final part of the workshop, we facilitated a mini design sprint for policy development.",
    "## The Need for Policy\n\nThe number of ways that generative AI tools can be used is part of what has drawn so much attention to them.\n\nIn higher education, generative AI has been used for press releases about students' deaths, for graduation speeches, and as tutors, to name a few examples.\n\nGiven the nature of knowledge work throughout higher education, institutions need to determine where generative AI tools are appropriate and where they represent ethical or legal challenges.\n\nOne of the most pressing examples of areas for guidance is the discourse around plagiarism and how to detect it in an era of AI-generated content.\n\nInstitutions are under pressure (whether organic or manufactured) to respond in some manner to the rise of generative AI as a widely available tool.\n\nThe first wave of awareness seemed to be generated around plagiarism concerns and worker replacement theories (\"Students will never write original papers again\" and \"Robots are replacing everyone\").\n\nFor those in higher education, the drumbeat of fearmongering about plagiarism has succeeded in capturing faculty and administrative attention.\n\nEarly on, some assumed that generative AI work submitted by students could be identified with generative AI plagiarism checkers.\n\nTurnItIn's documentation states, \"Our AI Writing detector's false positive rate is less than 1% for documents with 20% or more AI writing (our tests showed that in cases where we detect less than 20% of AI writing in a document, there is a higher incidence of false positives).\"\n\nHowever, after the TurnItIn AI Detector was used in practice, across 38 million papers, false negative and positive detections are now at a higher risk.\n\nTurnItIn and other companies claim to offer greater odds of correct detection than reality seems to bear out.\n\nAdditionally, further research is showing biases in what AI-plagiarism detection tools flag as being AI generated, including an increased likelihood of inaccurately flagging multi-language learners' work as having been generated by AI.\n\nBeyond the problem of false accusations, this environment also creates an untenable situation for students who must somehow defend themselves against a machine that cannot show its work but is just a projection.\n\nAdditionally, students are using tools such as QuillBot, which will paraphrase text and use synonyms, to circumvent AI detection tools.\n\nAs original student writing becomes increasingly difficult to verify and confirm, developing a policy for AI detection might result in its being outdated just as the policy is implemented.\n\nHaving an agile committee representative of diverse campus needs who can both review these issues and considerations to craft a more useful policy is part of what is needed to both protect students and choose the right tools for navigating the technological changes.\n\nThis is but one issue that higher education needs to navigate when crafting policy around generative AI.\n\nOther issues include but are not limited to the following:\n- The role of generative AI in visual and textual outputs of the institution, be they for marketing, for social media, or in reports\n- How and where faculty can use generative AI in the creation of course content, assignments, and feedback or assessments\n- Addressing the embedded biases of the data and outputs\n\nThe impact and challenge of information literacy  \nThe environmental impact of generative AI, including greenhouse gasses and water usage  \nThe impact on workers needed to run generative AI",
    "## Starting with the End\n\nIn many areas of life, including instructional design, an ideal place to start is with the results—what you want the outcome to be or do.\n\nTo feel that your—and your collaborators'—time has not been wasted, and to see that you have made progress toward what might still be an abstract version of achievement, codify what success looks like for you, your collaborators, your stakeholders, and the final institutional arbiters.\n\nAs AI continues to develop at a rapid pace, it is important to pause and do enough research so that you can formulate the questions you ultimately seek to answer.\n\nBelow are some questions to get you started in thinking about this policy:\n- Whom is the policy going to focus on?\n\nStudents, faculty, staff, administration, third-party vendors, contractors, etc.?\n\n- Can the same policy apply across the institution, or will different policies be needed for different parts of the organization?\n\n- Will the policy stand on its own, or will there be room for adjustments (for example, will students encounter variations depending on whether instructors—under the notion of academic freedom—want to encourage or discourage certain uses of generative AI for the purposes of teaching)?\n\nWhat can or will be the implications of violating the policy?\n\nWhat methods of accountability with the policy can be created when it may be hard to actually detect generative AI text?\n\nWill there be differences between institutionally affiliated generative tools and those that are available to anyone?\n\nPerhaps you are still in the fact-finding and idea-generating stage—you might need to specifically dedicate time to consider what will constitute your goal for the mission.\n\nYou may decide that success for your group means developing and writing down these measurable goals.\n\nThat is, you might still be in the phase of brainstorming and research, and you might need to take time to decide what your goals are.\n\nUltimately, all involved should agree on specific measurable outcomes that are necessary for your group to have completed your work.\n\nIt is natural to feel a sense of urgency to take action, and it might be tempting to rush through this step.\n\nAlthough this process will not necessarily be easy, it will be helpful in carving out exactly what you are going to do in this phase.\n\nOne of the trickiest parts about generative AI is that it has continued to change and shift in the past 12 months and will likely continue to do so, especially as other forms of generative AI (image, audio, video, slides, etc.)\n\nbecome increasingly available.\n\nTherefore, no matter what your goals are, creating a mechanism for revisiting, adjusting, updating will be equally important to name and anticipate early on.\n\nIn that case, think in terms of not getting it perfect the first time through but having a process to iterate.",
    "## Identifying Stakeholders\n\nGenerative AI has the possibility of hitting every part of the institution.\n\nBecause it is difficult to imagine an area that generative AI won't touch, it's useful to think about all the institutional stakeholders who will need guidance around usage of these new tools.\n\nStart by considering all the user groups and their iterations at your institution.\n\nOne approach would be to conduct an audit of your campus; if your institution has an organization chart, that is a great document to use as the foundation.\n\nSome people and groups are likely not represented, so detective work will come into play.\n\nThis is a great opportunity to break down institutional silos and examine your own invisible biases about the roles others play on our campuses.\n\nIdentify non-classroom student- and faculty-facing roles, which could include instructional designers, writing specialists, librarians, and academic support services.\n\nThen move on to people whose work has an impact on the campus experience as a whole.\n\nThis could include staff working in IT (e.g., information security, academic computing, or web services), institutional communications, student life, even alumni relations.\n\nBe mindful that just because a group is not readily visible to you does not mean that they will not be impacted by or have an impact on your policies.\n\nWe are a good example of coming together to collaborate on AI policy.\n\nThe diversity of our professional perspectives has lent itself to not only seeing different viewpoints but also creating the opportunity to truly understand the nuances in just how many ways AI is impacting our institutions.\n\nAn important early step should be to involve your organization's office for accommodations—that is, the person or people who oversee the institution's support for people with learning, cognitive, and ability differences.\n\nHistorically and even with generative AI, faculty and institutions often look to ban a given technology without genuine consideration of why or how it might actually benefit students with disabilities or be part of a larger strategy involving universal design for learning.\n\nKeeping an awareness of ADA compliance and the ways that generative AI can improve learning can only happen through intentional conversation with accommodation services on your campus.\n\nConsider, too, your contingent communities, such as part-time educators (adjuncts) and part-time staff.\n\nHow are they included in the community in general?\n\nDo they experience regular communication patterns as the full-time employees do, such as being included in campus-wide emails?\n\nWill you need to set aside time to engage them in discussion separately, and will they need more training and professional development as you enact the policies you create?\n\nLook for allies who can serve as conduits to get people involved.\n\nFor instance, adjuncts are often on campus for limited amounts of time during the day or might work elsewhere during typical business hours.\n\nWhen you are scheduling conversations, be mindful that you will need to create opportunities for those who work nontraditional schedules to be involved.\n\nGenerative AI represents questions for the whole institution, well beyond how it will be treated and used within classrooms.\n\nThe administrative side of the campus will be affected by generative AI.\n\nVanderbilt University, for example, issued an apology after it was discovered that a 300-word email (sent from the university's Peabody Office of Equity, Diversity, and Inclusion regarding the Feb. 13, 2023, mass shooting at Michigan State University) was composed using AI.\n\nSome questions to consider for other entities at your institution may include the following:\n- Will the institution's upper management consider using generative AI to surveil employees' work to detect efficiency or generate employee evaluations?\n\n- Should your policymaking efforts account for this scale of institutional use?\n\n- How will AI be addressed in human resources, especially the recruiting and hiring process?\n\n- In what ways will the community outside the campus be impacted by your policies?\n\n- Do you have community partnerships in which you offer students an opportunity to put their theoretical learning into practice?\n\nDo you have an obligation to educate your students on the uses of generative AI as part of that collaboration?\n\n- Will your institution be viewed as being a policy and position leader on the subject of generative AI?\n\n- What will be reasonable and equitable means of challenging outputs by generative AI?\n\nFinally, one of the most visible and yet frequently overlooked communities is the students.\n\nThe value of engaging students in determining this policy cannot be overstated.\n\nTo craft a policy without their input could result in a policy that feels out of touch and irrelevant to them.",
    "The value of engaging students in determining this policy cannot be overstated.\n\nTo craft a policy without their input could result in a policy that feels out of touch and irrelevant to them.\n\nAt the NERCOMP workshop, the highlight was hearing at the student panel how they demonstrated their own deep and sophisticated thoughts about generative AI and its roles in their lives.\n\nIn particular, the conversation should extend beyond plagiarism and also include student perspectives about how other institutional areas should be using it (e.g., faculty, marketing, communications).\n\nMany students are aware of and interested in how generative AI is currently being employed in a wide variety of industries.\n\nStudents might be reading about the way Paul McCartney used generative AI to restore John Lennon's voice to create a \"new\" Beatles track, or they may be interested in how generative AI is being used to dramatically improve recycling programs.\n\nThese interests might be general curiosity (to be encouraged!\n\n), but they might also tie into students' future employment goals.\n\nThe consequences of not including a diverse array of groups in these conversations far outweigh any inconvenience you may experience when trying to find times and ways to gather input throughout the policy development process.\n\nOur workshop included people from many of the groups mentioned, and a key takeaway was just how much we learned from each other.\n\nMany realized that while something may seem obviously important to you, it may not even be considered by someone working in a different position.",
    "## Models for Developing Policies\n\nDepending how an institution is structured and how much its leaders want to include different groupings within the organization, any of several models can be deployed to develop policies around generative AI.\n\nThe following options are useful approaches and, to some degree, can be mixed and matched to meet the institution's needs and structures.",
    "### Task Force Model\n\nPut out a call to action to form an inclusive team representing all aspects of the institution to create a policy task force.\n\nAlternatively, it could be a smaller, nimbler team that works to engage with different stakeholders across the institution, creating a template policy for generative AI and adjusting and tweaking it for each area of the institution, in conversation and collaboration with relevant stakeholders.\n\nFor instance, the policy for students and faculty is likely to look different from that of the marketing department, but realistically all of these groups should have AI policies.\n\nThis might seem self-evident for students, but there are important questions to consider even for marketing, such as the use of generative AI image tools to represent students.\n\nWould that adhere to the institution's ethos or marketing ethics?\n\nThose are the kind of questions that would be relevant to marketing that students and faculty may not need to be involved in.",
    "### Governance Model\n\nSome institutions have governance models in which faculty and staff play pivotal roles in the development and creation of institutional policy, particularly around policies that directly impact the classroom and students.\n\nThis can be a useful model to gather a range of voices throughout the organization and have a clear pathway and record (e.g., committee notes) of how decisions are made and implemented.\n\nYet these processes may encounter challenges in moving at an effective pace to make decisions that are timely and responsive to a changing environment.\n\nDue to the rate of change in generative AI, committees might create policies for assumptions around generative AI that are no longer valid.",
    "### Design Sprint Model\n\nCommonly used in IT and project management fields, the design sprint approach is another successful model.\n\nWe successfully used this model in our NERCOMP 2023 workshop, and these steps can be adapted for institutional or department use.\n\nAn effective design sprint includes six steps: Understand, Define, Sketch, Decide, Prototype, and Validate.\n\nThis format of developing a policy based on feedback from the community is more democratic in nature, allowing staff, students, and faculty to chime in with their ideas, thoughts, and recommendations.\n\n**Understand**: The first step is to understand how generative AI is currently being used (or isn't) at your institution, the concerns of the community, and the ideal state for AI use.\n\nThis can be achieved through a listening tour, lighting talks, surveys, or structured conversations.\n\n**Define**: Next, the policy team should review all the information gathered in the first step to define their main goals and desired outcomes that reflect the needs of the community.\n\n**Sketch**: Each individual on the policy committee should sketch out their own draft policy that meets the definitions outlined previously.\n\nThe larger committee can then review all policy drafts and then narrow down the ideas of each draft into a finalized Solution Sketch.\n\n**Decide**: The policy committee will then review all Solution Sketches to decide which version they want to use to move forward with the process.\n\n**Prototype**: The prototype can be considered the first draft of the policy to be shared with the community.\n\n**Validate**: Return to the community members from the first phase of the design sprint process, gather feedback based on your policy draft, and make relevant edits.\n\nThis is the last stage of the design sprint process, which will involve crafting the final version of the policy ready to be shared with your institution.",
    "### Consultant Model\n\nSome institutions may seek consultants to research, discuss, and implement new policy based on developing industry standards arising across academia.\n\nThe value of a consultant can be that an external voice with relevant knowledge and experience provides a valuable perspective that helps folks think differently about the challenge.\n\nHowever, it can also have the opposite effect, wherein folks dismiss the consultant because of the costs involved, as well as concerns that an outside entity might not understand the context.",
    "### Exemplar Model\n\nAnother option is to follow the lead of institutions that are publishing their own policies.\n\nFor instance, Lance Eaton and his students have proposed and posted policies for College Unbound.\n\nStarting with one of these policies, the institution can then review, share for input, and adapt as needed.\n\nThis can save a lot of time in terms of coming up with the initial approach, but it will need to be tailored to the specific needs of the institution and its population.\n\nIf the policy creation team is reluctant to borrow from another institution, they could use a generative AI tool to draft the initial policy and workshop it across the institution.\n\nWe used ChatGPT to generate several examples of policies to help get you started crafting your own.",
    "## Recommendations for Gathering Voices\n\nTo create the clearest and most effective policy, all stakeholders' voices must be heard.\n\nBecause recruiting volunteers for their insights in drafting a policy can sound tedious, consider using both formal and informal routes to get their assistance.\n\nAlso important is conveying how each stakeholder may be impacted by such a policy and emphasizing that their experiences and observations are highly valuable to the process.\n\nAsking for help drafting a policy document can be overwhelming to many.\n\nThese approaches might make this more engaging:\n- Connect with people in mini roundtable discussions or small-group conversations\n- Find help through social media—use polling features to generate feedback and locate allies\n- Have one-on-one conversations with the people you see as your greatest supporters and greatest challengers in this process.\n\nDiverse perspectives are necessary to have a policy that works for everyone.\n\n- Allow for enough time in discussions.\n\nIt can be easy to rush through things, but these discussions will include deep topics and issues that need time to be fully processed.\n\n- Provide collaborative documents for participants to add text, and use the comment feature for questions or additional thoughts.\n\nInstitutional knowledge is both an advantage and a disadvantage in this process.\n\nSome stakeholders will be obvious, and some people will quickly volunteer.\n\nOthers may be more challenging to reach because of their disinterest or because they don't see the relevance to their work.\n\nThese are important people to bring into the conversation, rather than making assumptions about their thoughts or possible contributions.",
    "## Framework for Generative AI Policy Creation\n\nBelow are the relevant sections that a generative AI policy should address to help institutions in their policy development processes.\n\nEach institution should have its own individual policy based on the needs of its community.\n\n- **Policy Audience**: Whom is this policy for?\n\nIs it for the entire institution, faculty, students, staff, departments, third-party vendors, or others?\n\n- **Policy Timeline**: What is the timeline for implementation?\n\nShould that timeline include a review and update cycle after initial implementation?\n\n- **Policy Tools**: What counts as AI for this policy?\n\nIs it focused on all AI or only generative AI?\n\nIs it focused on all generative AI or just text-generating AI?\n\n- **Academic Integrity Guidelines**: If the policy is related to student or faculty work, what are the integrity requirements to make sure academic integrity is upheld?\n\n- **Acceptable Use**: If AI usage is acceptable, are there any limitations on the amount of usage (e.g., a certain percentage must be individually generated in certain contexts) or purposes for which generative AI may be used?\n\n- **Transparency**: What practices are in place for communicating its usage throughout the institution?\n\n- **Security and Legal Considerations**: What concerns need to be addressed concerning privacy, intellectual property, and proprietary knowledge around using external or enterprise generative AI tools?\n\nDoes endorsing the use of generative AI conflict with any laws such as the General Data Protection Regulation?\n\n- **Ethical Considerations**: What concerns or responsibilities does the institution have explicitly or implicitly within its mission that conflict with the environmental, human-exploitation, and bias issues related to generative AI?\n\n- **Institutional Resources**: Which areas of the institution will be committed to supporting, responding to, and implementing uses of generative AI?\n\n- **Processes for Policy Violation**: If the policy is violated, what are the steps for identifying and addressing it?",
    "## Conclusion\n\nInstitutional policy development can often be a methodical process that both informs and better unifies an institution's approach to a particular challenge.\n\nAnd yes, it can be tedious.\n\nYet many institutions are often unprepared for technological developments, and the delays in catching up have a deep impact on students, faculty, and staff (e.g., the reactive response to the pandemic).\n\nAlthough the generative AI cat is out of the bag, there's still an opportunity to meaningfully guide—through a collaborative effort—how it can be best used by all the stakeholders.\n\nSuch an approach will result in a better learning and working environment for students, faculty, and staff.\n\nTaking an iterative approach to AI policy might feel unnatural for those who operate best with exhaustive policy documents, but it just might be the key to successfully navigating this new technological reality.",
    "## Notes\n\nSusan D'Agostino, \"GPT-4 Is Here.\n\nBut Most Faculty Lack AI Policies,\" Inside Higher Ed, March 21, 2023.\n\nDaniel E. O'Leary, \"Gartner's Hype Cycle and Information System Research Issues,\" International Journal of Accounting Information Systems 9, no.\n\n4 (December 1, 2008): 240–52; Jackie Wiles, \"What's New in Artificial Intelligence from the 2022 Gartner Hype Cycle,\" Gartner, September 15, 2022.\n\nMiles Klee, \"Professor Flunks All His Students After ChatGPT Falsely Claims It Wrote Their Papers,\" Rolling Stone, May 17, 2023.\n\nSam Levine, \"Vanderbilt Apologizes for Using ChatGPT in Email on Michigan Shooting,\" The Guardian, February 22, 2023; Josh Moody, \"The ChatGPT Commencement Address,\" Inside Higher Ed, June 29, 2023; Lauren Coffey, \"Harvard Taps AI to Help Teach Computer Science Course,\" Inside Higher Ed, June 30, 2023.\n\nDavid Adamson, \"New Research: Turnitin's AI Detector Shows No Statistically Significant Bias against English Language Learners,\" Turnitin Support Center, October 26, 2023.\n\nGeoffrey A. Fowler, \"Detecting AI May Be Impossible.\n\nThat's a Big Problem for Teachers,\" Washington Post, June 2, 2023.\n\nAndrew Myers, \"AI-Detectors Biased Against Non-Native English Writers,\" Stanford HAI, May 15, 2023.\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell, \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\"\n\nin Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23, FAccT '21, New York, NY, USA: Association for Computing Machinery, 2021.\n\nCeleste Kidd and Abeba Birhane, \"How AI Can Distort Human Beliefs,\" Science, June 22, 2023.\n\nBernard Marr, \"Green Intelligence: Why Data And AI Must Become More Sustainable,\" Forbes, March 22, 2023.\n\nDavid Danelski, \"AI Programs Consume Large Volumes of Scarce Water,\" UC Riverside News, April 28, 2023.\n\nKaren Hao and Deepa Seetharaman, \"Cleaning Up ChatGPT Takes Heavy Toll on Human Workers,\" Wall Street Journal, July 24, 2023.\n\nBilly Perrigo, \"Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic,\" Time, January 18, 2023.\n\nJürgen Rudolph, Samson Tan, and Shannon Tan, \"ChatGPT: Bullshit Spewer or the End of Traditional Assessments in Higher Education?\"\n\nJournal of Applied Learning and Teaching 6, no.\n\n1 (January 24, 2023).\n\nLevine, \"Vanderbilt Apologizes for Using ChatGPT in Email on Michigan Shooting.\"\n\nNadia Khomami, \"AI Used to Create New and Final Beatles Song, Says Paul McCartney,\" The Guardian, June 13, 2023.\n\nKayla Vasarhelyi, \"AI Robotics in Recycling,\" Environmental Center, University of Colorado Boulder, April 6, 2022.\n\nSee \"Design Sprint Methodology.\"\n\nIbid.\n\nKevin R. McClure, \"Arbiters of Effectiveness and Efficiency: The Frames and Strategies of Management Consulting Firms in US Higher Education Reform,\" Journal of Higher Education Policy & Management 39, no.\n\n5 (October 2017): 575–89.\n\nSee \"Proposal of Usage Guidelines for AI Generative Tools at CU.\"\n\n---\n\nEsther Brandon is Manager of Learning Design & Technology Adoption at Harvard Medical School.\n\nLance Eaton is Director of Faculty Development & Innovation at College Unbound.\n\nDana Gavin is Director of the Writing Center at Dutchess Community College.\n\nAllison Papini is Assistant Director/Manager of Research & Instruction Services at Bryant University.\n\n© 2023 Esther Brandon, Lance Eaton, Dana Gavin, and Allison Papini.\n\nThe text of this work is licensed under a Creative Commons BY-NC-SA 4.0 International License.\n\nAssessment and Evaluation, Artificial Intelligence (AI), Diversity, Equity, and Inclusion (DEI), Institutional Management, Instructional Design, Policy and Law, Policy Development, Risk Management",
    "# U of T Syllabus Language – Use of Generative AI in Assignments\n\nGenerative Artificial Intelligence (AI), and specifically foundational models that can create writing, computer code, and/or images using minimal human prompting, are proliferating and becoming ubiquitous.\n\nThis includes not only GPT-4 (and its siblings ChatGPT and Microsoft Copilot), but many writing assistants that are built on this or similar AI technologies.\n\nThere are now hundreds of these systems that are readily available.\n\nGenerative AI assistants are becoming more proficient at:\n\n- Creating an outline for a paper, or bullet points and graphics for slides.\n\n- Writing longer coherent prose in multiple languages.\n\n- Critiquing a written passage, editing text, and correcting computer code.\n\n- Providing explanations or ideas for a literature review with mostly accurate citations.\n\n- Summarizing longer articles, text, or a corpus of texts.\n\n- Suggesting a response to a question, such as on a short answer or multiple-choice test, or for a discussion board posting.\n\n- Translating text more accurately.\n\n- Creating computer code in multiple languages.\n\n- Assisting users with formulas inside applications such as Excel.\n\nThese are only a few examples.\n\nMany generative AI applications give the user a choice of templates (e.g., email, essay, memo, plan) and a choice of tone to tailor the generated text to the user’s need.\n\nWe strongly encourage you to familiarize yourself with the type of functionality these systems offer and to have a conversation with your class about these technologies, in addition to including language about these technologies on your syllabus.\n\nYou may also find the Generative AI in the Classroom FAQ helpful which contains up-to-date information on the use of the technology.\n\nWe recognize that some instructors may want to allow, or even encourage, their students to use these technologies, and others may want to curtail their use.\n\nThe following suggested statements are intended to help you shape the message you provide to your students on a course syllabus and/or on assignment instructions to reinforce a shared understanding of what is, and is not, allowed in your course.\n\nThese statements may be applicable for both graduate and undergraduate level courses.",
    "## Can use Generative AI tools\n\nIn indicating on a syllabus and/or assignment instructions that students may use generative AI, the instructor should decide to what degree and on which assignments the students may use these tools.\n\nThis is similar to indicating to students when they may collaborate, and to what degree, with their classmates, and when an assignment should be solely their own work.\n\nHere are some suggested example statements that might be used, combined, or adapted for your course or assignments:\n\nStudents are encouraged to make use of technology, including generative artificial intelligence tools, to contribute to their understanding of course materials.\n\nStudents may use artificial intelligence tools, including generative AI, in this course as learning aids or to help produce assignments.\n\nHowever, students are ultimately accountable for the work they submit.\n\nStudents must submit, as an appendix with their assignments, any content produced by an artificial intelligence tool, and the prompt used to generate the content.\n\nStudents are expected to use generative artificial intelligence tools, such as Microsoft Copilot, and must include with every assignment a short reflection on how they made use of generative artificial intelligence tools in the development of their assignment.\n\nAny content produced by an artificial intelligence tool must be cited appropriately.\n\nMany organizations that publish standard citation formats are now providing information on citing generative AI (view the U of T Libraries Citation Guide for Artificial Intelligence Generative Tools).\n\nStudents may choose to use generative artificial intelligence tools as they work through the assignments in this course; this use must be documented in an appendix for each assignment.\n\nThe documentation should include what tool(s) were used, how they were used, and how the results from the AI were incorporated into the submitted work.\n\nCourse instructors reserve the right to ask students to explain their process for creating their assignment.\n\nNote that a protected version of Microsoft Copilot is now available to all students, faculty and staff.\n\nHowever, other generative AI applications may require registration and/or a subscription fee.\n\nPlease consider offering students a choice to opt-out of using a system other than the protected version of Microsoft Copilot if they have concerns about the cost, privacy, security or other issues related to the technology.",
    "## Can use Generative AI in certain instances or specific ways\n\nIt is important to be very specific about the boundaries and limitations of artificial intelligence use in completing coursework, if there are boundaries you want to set.\n\nPlease consider the difficulty for students, who are trying to navigate AI rules in multiple courses before setting up elaborate limitations in your course.\n\nHowever, there are reasons why you may want, or need, students to engage with generative AI tools in a specific way or on a specific assignment.\n\nHere are some suggested example statements that might be used, combined, or adapted for your course or assignments:\n\nStudents may use artificial intelligence tools for creating an outline for an assignment, but the final submitted assignment must be original work produced by the individual student alone.\n\nStudents may use artificial intelligence tools for critiquing and editing an assignment for purposes of revision, but the first draft must be original work produced by the individual student alone.\n\nStudents may not use artificial intelligence tools for taking tests, writing research papers, creating computer code, or completing major course assignments.\n\nHowever, these tools may be useful when gathering information from across sources and assimilating it for understanding.\n\nStudents may not use artificial intelligence tools for taking tests in this course, but students may use generative AI tools for other assignments.\n\nStudents may use the following, and only these, generative artificial intelligence tools in completing their assignments for this course: .... No other generative AI technologies are allowed to be used for assessments in this course.\n\nIf you have any questions about the use of AI applications for course work, please speak with the instructor.",
    "## Cannot use Generative AI\n\nIn indicating on a syllabus that students may not use generative artificial intelligence, the instructor should decide to what degree and on which assignments the students may not use these tools.\n\nThis is similar to indicating to students when they may, or may not, collaborate with classmates and to what degree.\n\nNote that students may be using generative AI to help explain concepts for their own learning, or pose questions for them (i.e., act as a personal tutor), and you probably want to allow this type of use.\n\nIn addition, as generative AI tools become incorporated into commonly used systems (e.g., Google docs, Microsoft Word), it will become increasingly important to be clear about what functionalities are allowed or disallowed in your course.\n\nHere are some suggested example statements that might be used, combined, or adapted for your course or assignments:\n\nYou may use generative artificial intelligence tools (e.g., ChatGPT, Gemini, etc.)\n\nfor learning and practicing the concepts in this course, but these tools may NOT be used for completing assignments in this course.\n\nThe use of generative artificial intelligence tools or apps for assignments in this course, including tools like ChatGPT, Gemini, Microsoft Copilot, and other AI writing or coding assistants, is prohibited.\n\nThe knowing use of generative artificial intelligence tools, including ChatGPT, Gemini, Microsoft Copilot, and other AI writing and coding assistants, for the completion of, or to support the completion of, an examination, term test, assignment, or any other form of academic assessment, may be considered an academic offense in this course.\n\nRepresenting as one’s own an idea or expression of an idea that was AI-generated may be considered an academic offense in this course.\n\nStudents may not copy or paraphrase from any generative artificial intelligence applications, including ChatGPT, Gemini, Microsoft Copilot, and other AI writing and coding assistants, for the purpose of completing assignments in this course.\n\nThe use of generative artificial intelligence tools and apps is strictly prohibited in all course assignments unless explicitly stated otherwise by the instructor in this course.\n\nThis includes ChatGPT, Gemini, Microsoft Copilot, and other AI writing and coding assistants.\n\nUse of generative AI in this course may be considered use of an unauthorized aid, which is a form of cheating.\n\nThis course policy is designed to promote your learning and intellectual development and to help you reach the course learning outcomes.",
    "## Notes to faculty to accompany the syllabus language:\n\nConsider an “opt out” option for students if you are encouraging the use of any third-party software in your courses, as the software may require a subscription fee and has not undergone a security review by the University.\n\nTo date (April 2024), the only AI tool that is available to the U of T community that has been fully vetted is Microsoft Copilot.\n\nSee The Centre for Teaching Support & Innovation webpage, Microsoft Copilot, for more information on this tool.\n\nFor more information on the use of educational software see the CTSI webpage, Tools Beyond Quercus.\n\nNote that if you choose to use, or encourage the use of, applications other than Microsoft Copilot (e.g., ChatGPT, Gemini, etc.\n\n), the terms of use may change without notice during the term.\n\nIf you are allowing or disallowing generative AI tools in your course, clarify for the students why this decision was made, and how generative AI assistance supports or, alternatively, negatively impacts the pedagogical goals of the course or assignment.",
    "# Table of Contents\n- What is Generative Artificial Intelligence (AI)?\n\n.......................... 1\n- Why talk about AI now?\n\n............. 3\n- What are the Equity Implications of Generative AI in K-12 Classrooms?\n\n.................................. 4\n- How do student data privacy policies impact the use of AI in K-12 classrooms?\n\n......................... 6\n- What is the potential of generative AI in K-12 classrooms?\n\n................................. 7\n- What might districts consider when developing policy related to generative AI?........................\n\n10\n- ACKNOWLEDGEMENTS",
    "### What is Generative Artificial Intelligence (AI)?\n\nBroadly speaking, artificial intelligence (AI) is defined as “the science and engineering of making intelligent machines, especially intelligent computer programs.\n\nIt is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable.”1 Many types of artificial intelligence have been defined, which have led to a large number of applications of AI including self-driving vehicles, automatic language translation, facial recognition, virtual assistants (like Siri or Alexa), recommendation systems used in streaming platforms like Netflix, and of course chatbots like ChatGPT and Bard.\n\nWhile AI includes a wide variety of applications and tools used in education and other fields, this document focuses on AI applications that are generative in nature - referred to herein as “generative AI.” This includes programs like ChatGPT, Bard, and other chatbots that use AI and natural language processing (NLP) to provide human-like responses to questions.\n\nThe field of AI encompasses far more than just generative AI.\n\nHowever, given the rapid emergence of chatbots like ChatGPT and Bard in the field of education, this resource focuses solely on this application of AI.2 It is important to acknowledge that AI is growing at a rapid pace and additional platforms and resources will continue to be developed.\n\nMcCarthy, J.\n\n(2004).\n\nWhat is artificial intelligence?\n\nRetrieved from: \n\nAdditional types of artificial intelligence include: Reactive Machines, Limited Memory, Theory of Mind, Self-aware, Artificial Narrow Intelligence (ANI), Artificial General Intelligence (AGI), and Artificial Superintelligence (ASI).\n\nYou can read more about these types in the Forbes article “7 Types Of Artificial Intelligence”.\n\nWhen considering these foundational AI technologies that make tools like ChatGPT and Bard possible, it is important to keep the following in mind:\n- These tools are designed to act like or simulate what most would consider intelligent behavior, such as responding to questions in a human-like manner.\n\nThey are not artificial brains or sentient life forms with human characteristics like free will, self-awareness, and emotions.\n\n- These tools are trained on vast amounts of data, much of which comes from the Internet.\n\nAs such they are subject to biases and inaccuracies that are inherent in the information found on the Internet.\n\nCaution must be exercised when considering a response from a generative AI system.\n\nIt is also important to note that various forms of AI have been integrated into classrooms and our everyday lives for decades.\n\nFor example, instructional resources and assessments that use algorithmic or adaptive learning, automatic translation tools such as Google translate or Babelfish (which was an early translation tool), and programs such as Grammarly that support writing are all examples of AI.\n\nThe Office of Educational Technology published a report titled “Artificial Intelligence and the Future of Teaching and Learning” that provides additional examples regarding the history of AI and the importance of proactively addressing the growth of AI in our schools and classrooms.\n\nImportantly, this report puts an emphasis on developing people rather than machine-centered policies by keeping humans in the loop when using AI.\n\nThey use the following metaphor to describe its use noting that “teachers, learners, and others need to retain their agency to decide what patterns mean and to choose courses of action.”\n\nWe envision a technology-enhanced future more like an electric bike and less like robot vacuums.\n\nOn an electric bike, the human is fully aware and fully in control, but their burden is less, and their effort is multiplied by a complementary technological enhancement.\n\nRobot vacuums do their job, freeing the human from involvement or oversight.\n\nThis resource developed by ODE as well as any future resources align with this metaphor in that whenever using AI (or any educational technology in the classroom) it is essential that educators are the decision makers and their knowledge and expertise is central.\n\nAn Introduction to Large Language Models.\n\nRetrieved from:",
    "### Why talk about AI now?\n\nThe use of AI in education, while not a new phenomenon, has gained traction recently given the introduction of generative AI that uses large language models such as ChatGPT, Bard, and Co-Pilot.\n\nSuch models have the potential to transform the way that technology is used by educators, students, and families alike.\n\nGiven this influx, it is vital that schools and districts plan proactively in developing policies related to the use of AI in classrooms so as to ensure that policies are clear, attentive to the current moment, mindful of student data privacy and equity centered.\n\nWhenever new technologies are introduced, there is a potential for both innovation and risks when considering the impact on instructional practices and student learning experiences and outcomes.\n\nGenerative AI are just the newest examples of these emerging technologies, with previous generations experiencing advances in technology such as the internet, graphing calculators, smart phones, etc.\n\nWhile generative AI has the potential to support access to learning in classrooms for all students, these technologies can exacerbate inequities for students from marginalized student groups in the absence of thoughtful policy, practice, and educator support and training.\n\nGiven the influx of AI in education, several organizations have come together to provide guidance on the use of AI in education in addition to resources for teaching about AI.\n\nExamples include:\n- The CRAFT (Classroom-Ready Resources About AI For Teaching Project), developed by Stanford University’s Graduate School of Education has resources to support high school teachers in supporting students’ AI literacy.\n\n- Teach AI, a partnership between major education organizations, including Code.Org, ISTE, the World Economic Forum, National Association of State Boards of Education, National School Boards Association, Educational Testing Service, Khan Academy, and ISTE was developed to provide guidance to policy-makers and educational leaders around AI in education.\n\n- The Hands-On AI Projects for the Classroom from ISTE and GM has developed guides for elementary and secondary teachers to support student-driven learning related to AI and its impacts on society.",
    "### What are the Equity Implications of Generative AI in K-12 Classrooms?\n\nWhen developing policy around the use of generative AI in K-12 classrooms, it is important to take into consideration the fact that generative AI is a component of a larger digital learning ecosystem.\n\nWhile digital learning and education technology has the potential to address inequities when implemented with an equity focus and mindset, in the absence of this intention, digital learning and education technology can also exacerbate existing inequities and make schooling more difficult for those who are already marginalized within the system.\n\nEquity implications to keep front and center when designing policy specific to generative AI in K-12 classrooms includes bias, inaccuracy, plagiarism, copyright/licensing unknowns, and equity of access.\n\nTable 1 provides examples of strategies to address these equity implications.\n\n- Bias: As generative AI uses algorithms created by human designers, there is a strong potential for the introduction of bias into the system.\n\nSome examples include privileging certain language variations, showing racial and gender biases, having a United States-centric lens, and providing only a limited perspective.\n\nAs generative AI uses large data sets, historical and systemic biases are introduced into the system.\n\nFurther, as generative AI lacks cultural knowledge and experience, this can lead to misinterpretations of prompts given and answers that privilege a certain cultural perspective.\n\n- Inaccuracy: As generative AI draws from large data sets, it is not guaranteed to be accurate as not all information on the internet is accurate.\n\nAI is only as good as the data that is fed into the machine learning algorithms.\n\nAdditionally, generative AI is not necessarily coded to provide accurate information, rather it simply predicts the type of information needed to best fit a given Prompt.\n\nFor example, when asking ChatGPT for sources to support information provided, there are times in which the sources are fabricated.\n\nThis is known as “AI hallucinations.”\n- Plagiarism: A common concern from educators is that generative AI and other AI technologies will be used by students to write essays and complete assignments.\n\nAs generative AI does provide written responses to prompts, this concern has led a number of districts across the nation toward banning the platform due to a concern for cheating and plagiarism.\n\n- Copyright/Licensing Unknowns: Understanding copyright laws is an important element of using generative AI and other AI technology in the classroom, in that as this is new technology, there are not clear boundaries regarding who owns the content generated by AI.\n\nAs companies continue to develop licenses on their products, it is important for educators to reflect on the implications of copyright/licensing unknowns.\n\n- Equity of Access: Access to generative AI and lack thereof can have broad and deep equity impacts on students – both while in K-12 education and for college and career readiness.\n\nGiven the inherent equity impacts of introducing generative AI into the digital learning ecosystem, educating students, families, and educators (including paraeducators, secretaries, support staff, etc.)\n\non these equity implications can help to move toward using generative AI in ways that are culturally responsive and sustaining for students, families, and communities.\n\nAdditionally, when developing policy, it is important that creating and cultivating a digital learning ecosystem wherein equity is at the heart of the decision making process is key to ensuring that the learning experiences that students engage in honor their sociocultural identity and lived experience.\n\nShripad Shenwai, D. (2023).\n\nWhat is AI Hallucination?\n\nWhat Goes Wrong with AI Chatbots?\n\nHow to Spot a Hallucinating Artificial Intelligence?\n\nRetrieved from:",
    "### How do student data privacy policies impact the use of AI in K-12 classrooms?\n\nThere are several federal and state policies associated with student data privacy that are important to be aware of when setting policy for the use of AI technologies in schools including Family Educational Rights & Privacy Act (FERPA), the Children’s Internet Privacy Act (CIPA), the Children’s Online Privacy and Protection Act (COPPA) and the Oregon Student Information Protection Act (OSIPA).\n\nCOPPA, in particular, impacts technology users under the age of 13 in that companies are not allowed to collect personal information from children under the age of 13 without parental consent, while OSIPA lays out certain requirements that must be met when using digital platforms of any kind including the following:\n- Disclosing any covered information provided by the operator to subsequent third parties, except in furtherance of kindergarten through grade 12 school purposes of the site.\n\n- Engaging in targeted advertising on the operator’s site, service or application.\n\n- Selling a student’s information, including covered information.\n\nByrd, C. (2023).\n\nWRITE Center Helps Teachers Grapple with ChatGPT and Future of Writing Instruction.\n\nRetrieved from: \n\nWhen developing district policies, it is essential to ensure that they are not in violation of COPPA or OSIPA.\n\nAll schools and districts engaging with AI technologies (or any technology broadly) should regularly review the company’s usage and privacy policies to ensure that they are not in violation of COPPA or OSIPA.",
    "### What is the potential of generative AI in K-12 classrooms?\n\nGenerative AI can be leveraged for use in education in a variety of ways including providing support with designing learning experiences for students, as a teaching tool, as a support for differentiation, as a “partner” in instructional support, as a virtual assistant, and as a starting point for providing student support and guidance.\n\nSee Table 2 below for additional context and ideas.\n\nAs this technology is expanding at rapid speed, it is likely that its uses in education will continue to expand, thus providing additional ways in which to create equitable learning opportunities for students if done so in tandem with strong and intentional policy development.\n\nAI is part of the larger educational technology ecosystem and thus should be implemented alongside strong policy and educator training.\n\nGenerative AI alone will not transform educational opportunities for students.\n\nInstead, using AI within an educational technology ecosystem that centers relationships and mental health, pedagogy and practice, and high-quality instructional materials creates opportunities for more equitable and culturally responsive learning opportunities for students – opportunities that lead to student learning and belonging.\n\nMcClennen, N. and Dené Poth, R. (2023).\n\nEducation is about to radically change: AI for the masses.\n\nRetrieved from: \n\nAlthough generative AI is still in its infancy, educators across the world have found the use of this tool to create increased learning opportunities for their students.\n\nSome learning opportunities include:\n- Learning Design: Generative AI can support teachers in developing lesson plans to support student learning in addition to lessening the burden of creating individualized instructional materials for students.\n\nWhile it is important that generative AI is seen as a starting point for this work, with the teacher being the most important part of lesson and materials development, generative AI can support teachers in the beginning stages of the development.\n\nGenerative AI can support students with disabilities as it can streamline content and therefore scaffold learning materials.\n\n- Teaching Tool: Generative AI can be used as a teaching tool in the classroom.\n\nWhen teachers have explicit policies about when it is appropriate to use generative AI, and students have been taught digital literacy skills to understand how to couple the information provided by generative AI with lateral reading skills, it can serve as a teaching tool.\n\n- Instructional Support: Generative AI can be used by teachers to efficiently find instructional resources on the basis of specific topic areas as well as pedagogical approaches to provide individualized instruction.\n\nThis includes using a program like ChatGPT to differentiate instructional resources by student grade level.\n\n- Virtual Assistant: Generative AI can serve as a virtual research assistant for educators to support everyday tasks.\n\nThis use of generative AI can create additional time for teachers to spend on building relationships with their students, engaging in direct and small group instruction, and providing feedback on assignments.\n\n- Student Support and Guidance: Generative AI has the potential to support students with developing research skills, learning to ask strong questions, and providing advice regarding college and career planning.\n\n- Future Career Options: Generative AI and the use of AI is likely to be central to future careers - both in terms of understanding the ways in which AI functions (computer science) and using AI to communicate and engage in work functions (digital literacy).\n\nTherefore, supporting students in learning about AI and using it responsibly, ethically, and productively will support them far beyond their K-12 education.\n\nWhile generative AI provides valuable educational opportunities, it is merely a starting point.\n\nTeachers are the most essential part of the teaching and learning process.\n\nGenerative AI and any other AI or technology does not, and cannot, replace a teacher or a counselor.\n\nGenerative AI is an emerging tool with no critical thinking abilities - it cannot discern whether the information it provides is generated in a way that is responsive to the needs and context of the students.\n\nHowever, it can be used as a teaching and learning tool.\n\nTherefore, when developing training to support policy related to the use of AI technologies, ensuring that teachers are trained in its uses (and limitations) as well as lifted up as the experts will be of utmost importance.\n\nAI technologies more broadly provide support for students with disabilities including: image and facial recognition for students with visual impairments, lip-reading recognition for students with a hearing impairment, text summarization and real-time captioning, and AI-powered automatic speech recognition and transcription.",
    "### What might districts consider when developing policy related to generative AI?\n\nWith the influx of generative AI platforms entering the education sphere, it is important that schools and districts are taking into consideration the equity impacts of such technologies alongside the potential for student learning that generative AI can have when implemented with equity at its core.\n\nWhat is equally important is for schools and districts to reflect on the larger digital learning ecosystem in order to ensure that generative AI platforms, if used, are done so in culturally responsive and sustaining ways and supported by strong professional development for teachers, transparent communication with families and the larger school community, and education for students.\n\nThis resource is designed to support schools and districts in developing policy and protocols related to introducing generative AI into the larger digital learning ecosystem.\n\nFor some schools and districts, this might require creating new policy, for others this might require shifting current policy, and for others it might look like developing protocols for use as aligned with current technology policy and cyber plans.\n\nStarting Points for Reflection and Development of Policies and Protocols\nAs schools and districts begin or continue discussions related to policy development around the use of AI technologies in classrooms, it is important to acknowledge the prevalence of AI being used outside of the education system.\n\nFor example, computer programmers are already using it to write code, Human Resources departments are using it to write job descriptions, companies like Microsoft and Salesforce are using it to write emails and perform administrative tasks, and the use of AI is continuing to change the medical field.\n\nAdditionally, there are examples of AI technologies being used for language revitalization within Indigenous communities.\n\nTherefore, while there are concerns to be aware of with regard to the use of AI within school it will be a tool that many will use throughout their lives.\n\nHow AI is helping revitalise indigenous languages.\n\nRetrieved from: \n\nAdditional guidance developed by ODE speaks specifically about equity within educational technology and can be referenced as districts and schools navigate AI and its role in education including The Key Components of Digital Learning and Online and Remote Learning Guidance.\n\nThe Key Components of Digital Learning provides a starting point for design, dialogue, and implementation of equity-centered technology integration by offering five Key Components including: relationships and mental health, pedagogy and practice, high-quality instructional materials, digital learning capacity and readiness, and funding for digital learning.\n\nThe Online and Remote Learning Guidance: Critical Requirements and Design Indicators provides three pillars for online and remote schools and programs to consider within their design process: lead and design for equity, creating relational conditions for learning, and plan and implement inclusive instruction.\n\nWhile this guidance is designed for online and remote schools and programs, the pillars can provide a starting point for all schools in developing policy related to educational technology.\n\nFor more information, please contact ODE’s Digital Learning Team at .",
    "## Introduction\n\nThe current wave of generative AI is a subset of artificial intelligence that, based on a textual prompt, generates novel content.\n\nChatGPT might write an essay, Midjourney could create beautiful illustrations, or MusicLM could compose a jingle.\n\nMost modern generative AI is powered by foundation models, or AI models trained on broad data using self-supervision at scale, then adapted to a wide range of downstream tasks.\n\nThe opportunities these models present for our lives, our communities, and our society are vast, as are the risks they pose.\n\nWhile on the one hand, they may seamlessly complement human labor, making us more productive and creative, on the other, they could amplify the bias we already experience or undermine our trust of information.\n\nWe believe that interdisciplinary collaboration is essential in ensuring these technologies benefit us all.\n\nThe following are perspectives from Stanford leaders in medicine, science, engineering, humanities, and the social sciences on how generative AI might affect their fields and our world.\n\nSome study the impact of technology on society, others study how to best apply these technologies to advance their field, and others have developed the technical principles of the algorithms that underlie foundation models.",
    "## AI’s Great Inflection Point\n\nFei-Fei Li, Sequoia Capital Professor in the Computer Science Department; Denning Co-Director of Stanford HAI\n\n540 million years ago, the number of animal species exploded in a very short time period.\n\nThere are many theories as to what happened, but one has captured my attention: the sudden onset and ensuing evolution of vision.\n\nToday, visual perception is a major sensory system and the human mind can recognize patterns in the world and generate models or concepts based on these patterns.\n\nEndowing machines with these capabilities, generative capabilities, has been a dream for many generations of AI scientists.\n\nThere is a long history of algorithmic attempts at generative models with varying degrees of progress.\n\nIn 1966, researchers at MIT developed the “Summer Vision Project” to effectively construct “a significant part of the visual system” with technology.\n\nThis was the beginning of the field of computer vision and image generation.\n\nRecently, due to the profound and interconnected concepts of deep learning and large data, we seem to have reached an inflection point in the ability of machines to generate language, image, audio, and more.\n\nWhile building AI to see what humans can see was the inspiration for computer vision, we should now be looking beyond this to building AI to see what humans can’t see.\n\nHow can we use generative AI to augment our vision?\n\nThough the exact figure is disputed, deaths due to medical error in the U.S. is a significant problem.\n\nGenerative AI models could assist healthcare providers in seeing potential issues that they may have otherwise missed.\n\nFurthermore, if the mistakes are due to minimal exposure to rare situations, generative AI can create simulated versions of this rare data to further train the AI models or the healthcare providers themselves.\n\nAdditionally, before we even start developing new generative tools, we need to focus on what people want from these tools.\n\nIn a recent project to benchmark robotics tasks by our lab, before even starting the research, the project team did a large-scale user study to ask people how much they would benefit if a robot did these certain tasks for them.\n\nThe winning tasks were the focus of the research.\n\nTo fully realize the significant opportunity that generative AI creates, we need to also evaluate the associated risks.\n\nJoy Buolamwini led a study titled “Gender Shades,” which found AI systems frequently fail to recognize women and people of color.\n\nStudy results were published in 2018.\n\nWe continue to see similar bias in generative AI models, specifically for underrepresented populations.",
    "## AI’s Great Inflection Point (cont’d)\n\nThe ability to determine whether an image was generated using AI is also essential.\n\nOur society is built on trust of citizenship and information.\n\nIf we cannot easily determine whether an image is AI generated, our trust of any information will erode.\n\nIn this case, we need to pay special attention to vulnerable populations that may be particularly susceptible to adversarial uses of this technology.\n\nThe progress in a machine’s capability to generate content is very exciting, as is the potential to explore AI’s ability to see what humans are not able.\n\nBut we need to be attentive to the ways in which these capabilities will disrupt our everyday lives, our communities, and our role as world citizens.",
    "## The Potentials of Synthetic Patients\n\nRuss Altman, Kenneth Fong Professor in the School of Engineering; Professor of Bioengineering, of Genetics, of Medicine, and of Biomedical Data Science; Associate Director of Stanford HAI\n\nIt is often difficult to get large numbers of patients in clinical trials and it is crucial to have a realistic group of patients who do not receive a therapy in order to compare outcomes with those who do.\n\nThis is one area within biomedical research where generative AI offers great opportunities.\n\nGenerative AI could make clinical trials more efficient by creating “synthetic” control patients (i.e., fake patients) using data from real patients and their underlying attributes (to be compared with the patients who receive the new therapy).\n\nIt could even generate synthetic outcomes to describe what happens to these patients if they are untreated.\n\nBiomedical researchers could then use the outcomes of real patients exposed to a new drug with the synthetic statistical outcomes for the synthetic patients.\n\nThis could make trials potentially smaller, faster, and less expensive, and thus lead to faster progress in delivering new drugs and diagnostics to clinicians and their patients.\n\nIn the past, we have used “historical controls” which are patients who did not have the benefit of the new drug or diagnostic – and compared their outcomes to patients who received the new drug or diagnostic.\n\nSynthetic patients could match the real patients more realistically; they are created using knowledge of current medications, diagnostic tools, and standards of practice that were likely different in the historical situation.\n\nIn the setting of medical education, generative AI could allow us to create patients that are very realistic and could allow medical students to learn how to detect diseases.\n\nThe ability for generative models to create many variations on a theme could allow students to see multiple cases of the same disease and learn the ways in which these patients can vary.\n\nThis could give them more experience in seeing a disease and provide a nearly unlimited set of cases for them to practice if they find that certain diseases are more challenging for them to recognize and diagnose.\n\nThese same generative models could also interact with the students and give them practice eliciting signs and symptoms through conversational interaction.\n\nWith opportunity comes worry.\n\nIf synthetic patients are generated from data that does not reflect the population of patients receiving the drug, the patients may be biased.\n\nMore worrisome, however, is that even the real patients receiving the drug will not reflect the full population, and so synthetic controls could just improve the use of the drugs for a subset of patients and not all – leading to inequity.\n\nWhile generative technologies can be very useful in accelerating scientific discovery and progress, care must be taken in selecting the data used to generate patients and the models must be examined very carefully for biases that may lead to disparate impact.",
    "## Upending Healthcare, from Patient Care to Billing\n\nCurt Langlotz, Professor of Radiology, of Biomedical Informatics Research, and of Biomedical Data Science; Director of the Center for Artificial Intelligence in Medicine and Imaging (AIMI); Associate Director of Stanford HAI\n\nOne of the benefits of our healthcare system is that patients can see a variety of specialist physicians who are experts in specific medical disciplines.\n\nThe downside of our system is that these specialists often aren’t acquainted with the patients they are seeing.\n\nImagine a world in which a specialist you are seeing for the first time has already read a succinct summary of your healthcare needs, created by generative AI.\n\nDuring the patient visit, a chatbot based on a foundation model could serve as the physician’s assistant to support more accurate diagnosis and tailored therapy selection.\n\nA generative model could draft a clinic note in real time based on the physician-patient interaction, leaving more time for face-to-face discussion.\n\nIn the back office, generative models could optimize clinic scheduling or simplify generation of medical codes for billing, disease surveillance, and automated follow-up reminders.\n\nThese new capabilities could improve the accuracy and efficiency of patient care while increasing patient engagement and adherence to therapy.\n\nRecent federal legislation gives patients the right to access their entire medical record in digital form.\n\nAs a result, patients are increasingly encountering complex clinical documents that contain obscure medical terms.\n\nWhen a patient returns home from a clinic visit, a foundation model could generate tailored patient education materials and explain their care plan at the appropriate reading level.\n\nMachine learning models in medicine are critically dependent on large medical datasets that contain examples of disease.\n\nWe have shown how diffusion models, a type of foundation model, can be modified to create realistic clinical images from text prompts.\n\nOur results demonstrate that synthetic training data produced by these models can augment real training data to increase diagnostic accuracy.\n\nThis form of synthetic data could help solve machine learning problems for which training data is scarce, such as the detection and treatment of uncommon diseases.\n\nFinally, generative AI’s well-reported challenges with factual correctness are particularly problematic in medicine, where inaccuracies can cause serious harm.\n\nRecent problems in medicine include incorrect differential diagnosis and invalid scientific citations.\n\nWe are working to improve the factual correctness of medical explanations from these models so they can achieve an accuracy that is suitable for safe clinical use.",
    "## An AI Window into Nature\n\nSurya Ganguli, Associate Professor of Applied Physics; Associate Director of Stanford HAI\n\nScientific ideas from the study of nature itself, in the form of nonequilibrium thermodynamics and the reversal of the flow of time, lead to the creation at Stanford of the first diffusion model, a key kernel of technology that forms the basis of many successful AI generative models today.\n\nNow, in a virtuous cycle, AI generative models are well poised to deliver considerable insights into nature itself, across biological, physical, and mental realms, with broad implications for solving key societal problems.\n\nFor example, generative models of proteins can allow us to efficiently explore the space of complex three-dimensional protein structures, thereby aiding in the search for proteins with novel and useful functions, including new efficacious medicines.\n\nGenerative AI is starting to be explored in the quantum realm, enabling us to efficiently model strongly correlated states of electrons, with the potential of advancing our understanding of materials science and quantum chemistry.\n\nThese advances could in turn lead to the creation of new materials and catalysts that could play a role in efficient energy capture and storage.\n\nSimple generative modeling, intertwined with classical numerical solvers, has also made key advances in accurate and fast large scale fluid mechanical simulations, which when scaled up, could aid in climate modeling and weather forecasting, thereby contributing to a deeper understanding of our changing climate and its ramifications.\n\nIn a beautiful recursion, the generative AI models that we have created can also act as scientific windows, not only into the physical world but also into our own minds.\n\nFor the first time, we have AI systems that can model high-level cognitive phenomena like natural language and image understanding.\n\nMany neuroscientists and cognitive scientists have compared the neural representations of both deep networks and AI generative models to neurobiological representations in humans and animals, often finding striking similarities across many brain areas.\n\nExamples include the retina, the ventral visual stream, motor cortex, entorhinal cortex for navigation, cortical language areas, and neural geometries underlying few shot concept learning.\n\nThe often similar structure of artificial and biological solutions to generative tasks suggests there may be some common principles governing how intelligent systems, whether biological or artificial, model and generate complex data.",
    "## An AI Window into Nature (cont’d)\n\nAn exceedingly interesting and profound question arises in the forthcoming age of scientific collaboration between humans and AI systems as they work together in a loop to analyze our complex biological, physical, and mental worlds: What does it mean for a human to derive an interpretable understanding of a complex system when an AI provides a substantial part of that understanding through predictive models?\n\nIssues regarding explainable AI will likely rise to the fore when a fundamentally human scientific endeavor, namely understanding our world, is partially achieved through the use of AI.\n\nHuman scientists will not be content with uninterpretable AI-generated predictions alone.\n\nThey will desire human interpretable understanding, in addition.\n\nFinally, to dream even bigger, while today’s generative AI has access to immense global scale training data spanning images, text, and video from the internet, it does not have direct access to our own thoughts, in the form of neural activity patterns.\n\nHowever, this need not always be the case, given remarkable new neuroscientific capacities to record many neurons from the brains of animals while they view images, as well as to perform MEG, EEG, and fMRI from humans as they experience the world through rich multimodal sensory experiences.\n\nSuch combined neural and real-world data could then potentially be used to train next generation multimodal foundation models that not only understand the physical world but also understand the direct impact the physical world has on our mental world, in terms of elicited neural activity patterns.\n\nWhat might such hybrid biological-artificial intelligences teach us about ourselves?\n\nOverall, the future of generative AI as a window into nature, and the use of this window to solve societal problems, is full of promise.\n\nWe certainly do live in interesting times.",
    "## The New Tools of Daily Life\n\nJames Landay, Anand Rajaraman and Venky Harinarayan Professor in the School of Engineering and Professor of Computer Science; Vice Director of Stanford HAI\n\nAs we all know, AI is taking the world by storm.\n\nWe will begin to see many new tools that augment our abilities in professional and personal activities and workflows.\n\nImagine a smart tutor that is always patient and understands the level of knowledge the student has at any point in time on any subject.\n\nThese tutors will not replace teachers, but instead will augment the student learning experience – giving students a more personalized interaction, focusing in areas where they might be weaker.\n\nIn design, picture a tool that assists a professional designer by riffing off their initial design ideas and helping them explore more ideas or fill in details on their initial ideas.\n\nGenerative AI will also unleash language-based interfaces, whether written or spoken, as a more common way of interacting with our everyday computing systems, especially when on the go or when our eyes and hands are busy.\n\nImagine an Alexa, Siri, or Google Assistant that can actually understand what you are trying to do rather than just answering simply queries about the weather or music.\n\nWhile generative AI creates many exciting opportunities, we know from past AI deployments there are risks.\n\nIn 2016, an AI-based software tool used across the country to predict if a criminal defendant was likely to reoffend in the future was shown to be biased against Black Americans.\n\nWe need to ensure we are designing these tools to get the most positive outcomes.\n\nTo do this, we need to deeply design and analyze these systems at the user, the community, and societal levels.\n\nAt the user level, we need to create new designs that augment people by accounting for their existing workflows and cognitive abilities.\n\nBut we can’t just design for the user.\n\nWe need to consider the community that the system impacts: the families, the infrastructure, and the local economy.\n\nBut, even that is not enough, we need to analyze the impacts to society at large.\n\nWe need to be able to forecast what happens if the system becomes ubiquitous and from the start design mitigations for possible negative impacts.\n\nChanges that are underpinned by generative AI are only now starting to be imagined by designers and technologists.\n\nOur user interface to computing has been fairly static over the last 30 years.\n\nIn the next 5–10 years, we will see a revolution in human-computer interaction.\n\nChanges that are underpinned by generative AI are only now starting to be imagined by designers and technologists.\n\nNow is the time to ensure that we are critically thinking about the user, the community, and the societal impacts.",
    "## Poetry Will Not Optimize: Creativity in the Age of AI\n\nMichele Elam, William Robertson Coe Professor in the School of Humanities and Sciences and Professor of English; Associate Director of Stanford HAI\n\nIn 2018, the professional art world was upended when the renowned Christie’s auction house sold an AI-augmented work, “Portrait of Edmond Belamy,” for the wildly unexpected sum of $435,000.\n\nThat sale, which came with the tacit imprimatur of the established art community, generated much gnashing of teeth and hand-wringing in the arts sector over what artificial intelligence means for the creative industry.\n\nSince then, the genie has long fled its lamp: Generative AI has enabled visual art of every known genre as well as AI-augmented poetry, fiction, film scripts, music and musicals, symphonies, AI-curated art histories, and much more.\n\nThe furor over the Christie’s sale may now seem quaint – it occurred before DALL-E, Lensa AI, ChatGPT, Bing, to name just a few – but it heralded many of today’s increasingly ferocious debates over the nature of creativity and the future of work for the creative industry.\n\nIt anticipated the current hornet’s nest of ethical, political, and aesthetic concerns that generative AI poses for the arts.\n\nSome of these concerns have been productive: Generative AI has encouraged many of those whose livelihoods, and in many cases their identities, depend on their artistic productions to consider anew – and in new ways – perennial questions about foundational aesthetic norms and value: What do we identify as “art”?\n\nWhat counts as “good” art?\n\nIs artistry defined by human agency or automation?\n\nJust who or what can make “art”?\n\nAnd who decides?\n\nGenerative AI raises important, thorny questions about authenticity, economic valuation, provenance, creator compensation, and copyright.\n\n(The Getty Images lawsuit against Stable Diffusion is just the tip of an iceberg.)\n\nIt also, arguably, normalizes extractive and exploitative approaches to creators and their work; amplifies biases of every kind; exacerbates already urgent educational and national security concerns around deep fakes and plagiarism, especially in the absence of congressional regulation.\n\nShould the principles of efficiency, speed, and so-called blessings of scale apply so unequivocally to creative processes?\n\nAfter all, poetry does not optimize.\n\nPerhaps the most pressing concern, in terms of national security, is that generative AI might take advantage of the fact that the arts have always shaped – for good or ill – the civic imagination, that stories, films, plays, images shape our perception of ourselves, of our physical and social realities.\n\nOne of the most famous disagreements between Plato and his student Aristotle was over the potentially dangerous power of poetry to influence beliefs and worldviews.\n\nThis power is why fascist regimes first do away with the artists and intellectuals: because they hold sway over our minds and thus our actions.\n\nSome claim that generative AI is democratizing access to creative expression to those traditionally barred from it by lack of status or wealth.\n\nBut do claims to “democratization” and “access” function, in effect, as industry cover for rushing a commercial application “into the wild” (i.e., to the public) without the time-intensive work of ensuring ethical guardrails?\n\nIs AI simply a neutral if powerful assistive tool for the arts – akin to pen, paintbrush, or photography?\n\nIs it “blitzscaling” creativity, or in Emad Mostaque’s choice description, relieving our “creatively constipated” world with AI technologies that can have us all “pooping rainbows”?\n\nDespite centuries’ worth of opining by poets, philosophers, and pundits of all kinds about the nature of “creativity,” no settled definition exists.\n\nGiven this, technological claims to expedite that so little-understood phenomenon carry more than a whiff of hubris.\n\nIn fact, generative AI may simply automate a highly reductive notion of both the creative process and of the learning process itself.\n\nShould the principles of efficiency, speed, and so-called blessings of scale apply so unequivocally to creative processes?\n\nAfter all, poetry does not optimize.\n\nFiction is not frictionless.\n\nConsider the slowed-down, recursive reading and interpretive skills required to understand any piece of writing by Toni Morrison.\n\nHer work always invites us to pause, insists we reflect.\n\nConsider what natural language processing applications informing foundation models make of African American Vernacular English, not to mention Morrison’s signifying on that language system.\n\nJust try the experiment of my students, who submitted an excerpt of Toni Morrison’s Beloved to Grammarly, which attempted to correct her exquisite prose for what sociolinguists term “standard English,” and quickly saw how even deeply rich meaning can be rendered impotent.",
    "Historically, creative expression – especially poetry, painting, novels, theater, music – has always been considered a distinguishing feature of humanity and the pinnacle of human achievement.\n\nCan generative AI live up to that?\n\nMaybe.\n\nMaybe not.\n\nDefinitely not yet.",
    "## Generative AI and the Rule of Law\n\nDaniel E. Ho, William Benjamin Scott and Luna M. Scott Professor in Law at Stanford Law School and Director of the Regulation, Evaluation, and Governance Lab (RegLab); Associate Director of Stanford HAI\n\nIn January 2023, a Colombian court was faced with the question of whether an indigent guardian of an autistic minor should be exempted from paying for therapy costs.\n\nIt might have been an ordinary case.\n\nBut the judge consulted ChatGPT.\n\nThe prompt: “Has the jurisprudence of the constitutional court made favorable decisions in similar cases?”\n\nWhile quick to note that ChatGPT was not replacing judicial discretion, the judge noted, generative AI could “optimize the time spent writing judgments.”\n\nThe Colombian case may be the first judicial proceeding incorporating generative AI, and it exemplifies both what is promising, but also terrifying, about generative AI and the rule of law.\n\nOn the one hand, the United States faces an access to justice problem of tragic proportions.\n\nIn 1978, President Carter delivered a speech to the American Bar Association, admonishing the profession: “We have the heaviest concentration of lawyers on earth.\n\n… Ninety percent of our lawyers serve 10 percent of our population.\n\nWe are overlawyered and underrepresented.” (“The situation has not improved,” said Deborah Rhode in 2014.)\n\nVeterans wait some 5–7 years for the appeals of disability benefits to be decided.\n\nThe right to counsel with underfunded public defenders has turned into a “meet ’em and plead ’em” system.\n\nAnd even though the United States yields one of the highest per capita rates of lawyers, legal representation is out of reach for most.\n\nRelying on ChatGPT as a substitute for legal research poses grave problems for professional ethics and, ultimately, the rule of law.\n\nTherein lies the promise.\n\nJust as legal databases such as Westlaw and Lexis revolutionized legal research, there is the potential for generative AI to help individuals prepare legal documents, attorneys in legal research and writing, and judges to improve the accuracy and efficiency of painfully slow forms of adjudication.\n\nWhile the industrial organization of legal search could get in the way, generative AI could help level the legal playing field.\n\nBut the Colombian case also illustrates everything that can be wrong with the use of generative AI.\n\nSuch models can lie, hallucinate, and make up facts, cases, and doctrine.\n\n(Insert mandatory joke about lawyers lying and cheating too.)\n\nRelying on ChatGPT as a substitute for legal research poses grave problems for professional ethics and, ultimately, the rule of law.\n\nWhy is that the case?\n\nWhat the law teaches us is that justice is as much about the process as the outcome.\n\nA fair process engenders public trust.\n\nAnd the process for embedding generative AI in legal decision-making is as important as getting the foundation model right.\n\nSignificant technical research will be required to prevent generative AI from making up facts, cases, and doctrine.\n\nOr better yet: to think like a lawyer.\n\nBut even if that is solved – a big if – we cannot resolve the most contentious disputes that are channeled into law unless humans trust, participate, buy in, and engage in the process.\n\nJustice delayed is justice denied, but optimizing the time to write judgments is not the right objective either.\n\nOr, as ChatGPT puts it, “Judges should not use ChatGPT when ruling on legal cases.” At least not yet.",
    "## The New Cambrian Era: ‘Scientific Excitement, Anxiety’\n\nPercy Liang, Associate Professor of Computer Science; Director of Stanford Center for Research on Foundation Models\n\nFor almost all of human history, creating novel artifacts (literary works, art, music) was difficult and only accessible to experts.\n\nBut with recent advances in foundation models, we are witnessing a Cambrian explosion of AI that can create anything from videos to proteins to code with uncanny fidelity.\n\nThis is incredibly enabling, lowering the barrier to entry.\n\nIt is also terrifying, as it eliminates our ability to determine what is real and what is not, and it will upend the creative industry (artists, musicians, programmers, writers).\n\nFoundation models are based on deep neural networks and self-supervised learning which has existed for decades; however, the amount of data with which these recent models can be trained results in emergent abilities, abilities not present when the models were trained on less data.\n\nIn 2021, we released a paper detailing the opportunities and risks of foundation models.\n\nWe discuss how these emergent abilities are a “source of scientific excitement but also anxiety about unanticipated consequences.” Along with emergent abilities, we discuss homogenization.\n\nIn the case of foundation models, “the same few models are reused as the basis for many applications.\n\nThis centralization allows us to concentrate and amortize our efforts (e.g., to improve robustness, to reduce bias) on a small collection of models that can be repeatedly applied across applications to reap these benefits (akin to societal infrastructure), but centralization also pinpoints these models as singular points of failure that can radiate harms (e.g., security risks, inequities) to countless downstream applications.” Understanding emergent behavior and homogenization in foundation models are just as relevant, if not more, now than just two years ago.\n\nThis is incredibly enabling, lowering the barrier to entry.\n\nIt is also terrifying, as it eliminates our ability to determine what is real and what is not.\n\nAdditionally, it is absolutely critical that we benchmark these foundation models to better understand their capabilities and limitations as well as use these insights to guide policymaking.\n\nToward that end, we recently developed HELM (Holistic Evaluation of Language Models).\n\nHELM benchmarks over 30 prominent language models across a wide range of scenarios (e.g., question answering, summarization) and for a broad range of metrics (e.g., accuracy, robustness, fairness, bias, toxicity) to elucidate their capabilities and risks.\n\nThere will continue to be new models and associated scenarios and metrics.\n\nWe welcome the community to contribute to HELM.",
    "## A Call to Augment – Not Automate – Workers\n\nErik Brynjolfsson, Jerry Yang and Akiko Yamazaki Professor at Stanford HAI; Director of Stanford Digital Economy Lab\n\nOver the past two decades, most uses of computers, including earlier waves of AI, primarily affected workers with less education and training.\n\nAs a result, income inequality tended to increase in the U.S. and many other developed nations.\n\nIn contrast, generative AI has the potential to affect many types of work that have primarily been done by well-compensated people including writers, executives, entrepreneurs, scientists, and artists.\n\nThis may reverse some of the past effects of IT and AI when it comes to inequality.\n\nSo far, there have been speculation and case examples, but not much systematic empirical evidence either way.\n\nAt Stanford Digital Economy Lab, we are cataloging the list of economic activities likely to be affected by generative AI and estimating what share of the economy they represent.\n\nGenerative AI promises to automate or augment many of the thousands of tasks done in the economy that previously could only be done by humans.\n\nIn particular, writing nonfiction essays, persuasive ad copy, intriguing fiction, evocative poetry, concise summaries, entertaining lyrics, and other forms of text of reasonable quality is an important part of many occupations.\n\nSo is writing code, generating images, and creating new designs.\n\nThis will almost surely increase total output, reduce costs, or both.\n\nEither way, productivity is likely to rise, although some of the benefits (and costs) are not well measured.\n\nIn cases where generative AI can be a complement to labor, particularly for knowledge workers and the creative class, wages could increase even as output increases.\n\nIn other cases, the effects may be primarily to substitute for labor, as the technology replaces workers in some tasks.\n\nLikewise, the technology can be used to concentrate wealth and power, by facilitating winner-take-all dynamics or to decentralize and distribute decision-making and economic power, by lowering barriers to entry and fixed costs, empowering more people to create value.\n\nIt can create a monoculture of closely related output, or a flourishing of novel creations.\n\nThis will almost surely increase total output, reduce costs, or both…productivity is likely to rise, although some of the benefits (and costs) are not well measured.\n\nLast but not least, these technologies have the potential to speed up the rate of innovation itself, by facilitating invention, design, and creativity.\n\nThus they may not only increase the level of productivity but also accelerate its rate of change.",
    "## A Call to Augment – Not Automate – Workers (cont’d)\n\nPowerful new technologies almost always require significant changes in intangibles like business organization, process, and skills.\n\nGenerative AI is not likely to be an exception.\n\nGiven the rapid advances in the technology, a growing gap is emerging between the technological capabilities and the economic complements needed.\n\nThis will create tensions and disruptions, but also opportunities for rapid progress.\n\nUnderstanding these tensions and opportunities is central to our research agenda.\n\nThe effects of generative AI are not necessarily predetermined.\n\nInstead, they depend on choices by technologists, managers, entrepreneurs, policymakers, and many others.",
    "## The Reinvention of Work\n\nChristopher D. Manning, Thomas M. Siebel Professor in Machine Learning at the School of Engineering; Professor of Linguistics and of Computer Science; Director of Stanford AI Lab; Associate Director of Stanford HAI\n\nImagine a business analyst or data scientist generating a visualization, say, of how changes in voting patterns and economic growth correlate or anti-correlate by county in the U.S. over the last decade.\n\nAt the moment, they’ll typically spend a few hours on the task: searching to find out where the right data lives, writing some SQL or Python code to grab that data, then spending more time, perhaps in Tableau, d3, or again in Python, to turn it into a nice visualization.\n\nMaybe by next year, AI will be able to fulfill a long-standing dream: The business analyst will just be able to say, “Generate a heatmap visualization over a U.S. map showing the correlation between voting patterns and economic growth by county in the U.S. over the last decade.” The generative AI system will do the job in seconds, and to the extent that the first work product isn’t exactly what the person wanted, they’ll be able to continue a back-and-forth dialog to refine the visualization.\n\nIn our daily world, built by humans for humans, the major medium for communication is through human language – whether it is speaking with someone in person, on the phone, or by Zoom; or communicating in written form via anything from texts to emails to lengthy reports.\n\nBecause of this, generative language models provide a massive opportunity to reinvent how work is done inside all sorts of companies and industries: Marketing, sales, product development, customer support, and even human resources will all change.\n\nRecent generative AI models are sufficiently good to offer enormous help – and hence potential cost savings in a business context.\n\nIn some cases, a large language model–based system might be able to take over a whole interaction, working with a human being to get things done.\n\nThere is no doubt that a person in marketing and copywriting can get significant creative assistance from these models: A generative language model can suggest better wordings or hip, catchy phrases.\n\nGiven one sample paragraph, it can generate ten other possibilities, which a person might mine the best parts from, or just use them all to provide a variety of messages.\n\nThese AI models are not going to provide Toni Morrison–level prose nor her lived experience, but, I believe, they will produce very competent prose.\n\nThere are many intriguing aspects of this technological future that deserve further thought and comment.\n\nWe’re still in the early days of figuring out what new models of normal business practice are and aren’t possible.\n\nIn nearly all cases, the AI system will help humans to get work done.\n\nAs such, it continues the story of new technologies and automation making things easier and improving quality of life.\n\nWashing machines made washing clothes much easier.",
    "## The Reinvention of Work (cont’d)\n\nFor almost the entire history of civilization, whether in the Middle East, Europe, or China, the ability to write well has been seen as absolutely central and vital to human accomplishment and professionalism, something still reflected in the way universities today emphasize developing their students’ writing skills.\n\nWe will have to reckon with that changing: As Michele Elam notes in her piece, these AI models are not going to provide Toni Morrison–level prose nor her lived experience, but, I believe, they will produce very competent prose.",
    "## In Education, a ‘Disaster in the Making’\n\nRob Reich, Professor of Political Science; Director of Stanford McCoy Family Center for Ethics in Society; Associate Director of Stanford HAI\n\nThe newest revolution in artificial intelligence is powerful new automatic writing tools.\n\nIn professional settings, these models can augment human performance – rewrite our client emails in a more professional tone, complete our papers, or generate a report on our company’s annual performance.\n\nHowever, in educational settings, absent special design considerations, these models could undermine performance and corrode our creative abilities.\n\nCalculators have proven to promote accuracy, remove some of the more tedious work, and make math more enjoyable for many.\n\nChatGPT is not like a calculator.\n\nWhy?\n\nThe quality of your writing is not just a measure of your ability to communicate; it is a measure of your ability to think.\n\nIf students lean on ChatGPT to write their essays, if they do not learn to express their thoughts in writing in a clear, concise, and cohesive manner, then their thoughts themselves are not clear, concise, or cohesive.\n\nThe ability to write exercises their thinking; learning to write better is inseparable from learning to think better.\n\nBecoming a good writer is the same thing as becoming a good thinker.\n\nSo if text models are doing the writing, then students are not learning to think.\n\nInitially, the new wave of generative AI (e.g., GPT, DALL-E) was treated with caution and concern.\n\nOpenAI, the company behind some of these models, restricted their external use and did not release the source code of its most recent model as it was so worried about potential abuse.\n\nOpenAI now has a comprehensive policy focused on permissible uses and content moderation.\n\nBut as the race to commercialize the technology has kicked off, those responsible precautions have not been adopted across the industry.\n\nIn the past six months, easy-to-use commercial versions of these powerful AI tools have proliferated, many of them without the barest of limits or restrictions.\n\nCalculators have proven to promote accuracy, remove some of the more tedious work, and make math more enjoyable for many.\n\nChatGPT is not like a calculator.\n\nSo how could we prevent this disaster-in-the-making in education?\n\nFirst, AI developers and policymakers must distinguish between the significance of foundation models in educational versus professional settings.\n\nThen, they must work together, along with industry players, to develop community norms.\n\nThis isn’t new ground.\n\nLook to bioengineering, where the leading researchers, such as Jennifer Doudna, developed norms around the appropriate use of CRISPR technology.\n\nFor AI, that would mean companies establishing a shared framework for the responsible development, deployment, or release of language models to mitigate their harmful effects.\n\nIn an environment where companies are sprinting to launch their latest models, we cannot be content to wait and see the ethical and societal impact and patch things up later.\n\nWe need to develop widely shared norms now before we as a society pay the price.",
    "## Solving Inequalities in the Education System\n\nPeter Norvig, Distinguished Education Fellow at Stanford HAI\n\nWe know that learning is suboptimal when a lecturer drones on and on to a large crowd with no interaction.\n\nAnd yet, that’s what happens in many classrooms.\n\nWe know that learning is at its best when a knowledgeable, inspiring, empathetic tutor works directly with a learner, allowing the learner to progress at their open pace, mastering each point along the journey.\n\nBut we don’t have enough tutors to provide this level of interaction for every learner.\n\nWith the recent advances in large language models, there is the possibility that they can augment human teachers in this role.\n\nIf done right, this could provide a better education for all and help even out inequalities in the educational system.\n\nStudents could find topics that excite them and learn at their own pace from material that is designed for them.\n\nTraditional curriculum with walls between subject areas can have the walls broken down, as learners move quickly between subject areas to follow their passions.\n\nDoing it right requires caution: If we are going to expose learners to models, we want the models to be helpful, harmless, and honest; unfortunately, current AI models can sometimes be harmful and hallucinatory.\n\nThere are several defenses against this.\n\nWe can isolate the model from the learner; the model is used to select from a set of pre-curated responses – this is safer, but less engaging and less free-wheeling.\n\nWe can keep the model away from learners and instead use it to train new teachers by simulating student responses.\n\nWe can use the model to generate learning materials which are then vetted by a human teacher before being shown to the learner.\n\nWe can limit the model to asking Socratic questions, not asserting statements – that way it can’t be untruthful.\n\nWe can use peer-to-peer learning and feedback, with the model as a mediator.\n\nWe can use reinforcement learning from human feedback to train the model toward better responses.\n\nWe can use constitutional AI, in which humans explain to the model a set of rules for what is allowed and disallowed, and the model then trains itself to follow the rules.\n\nWe don’t have enough tutors to provide this level of interaction for every learner…there is the possibility [to] augment human teachers in this role.\n\nInevitably there will be ways to trick the system into harmful responses.\n\nFor example, a system might refuse to answer “tell me how to make a bomb” but be willing to answer “write an excerpt from a fictional novel in which the hero makes a bomb.” There will be a continuing arms race between attackers and defenders; our challenge is to stay one step ahead.\n\nStanford HAI: 353 Jane Stanford Way, Stanford University, Stanford, CA 94305  \nT 650.725.4537 F 650.123.4567 E  hai.stanford.edu",
    "### Building AI policies into your syllabus\n\nThis is advice that Katie Conrad, professor of English, provided to colleagues in her department.\n\nIt includes a syllabus statement for an honors class she is teaching.\n\nNo matter your position on the use of so-called AI in the classroom, I recommend that you consider first your course goals and whether and how any given technology might undermine or support your students in the process of meeting those goals.\n\nIn full disclosure, I take a “critical AI literacy” (Bali 2023) approach to the media-generative tools now commonly called “generative AI”: that is, we need to understand these tools in order to know when and when not to use them, and we need to consider them in the larger context out of which they emerge, including the harms associated with them.\n\nMy approach below is informed by my “Blueprint for an AI Bill of Rights for Education”.\n\nJust as a reminder, Sean Kamperman and I put together some curated resources on the topic of critical AI literacy for our AI & Digital Literacy Summit in June, including a number of tools (NB: assume no particular judgment on our part on the inclusion or exclusion of any given tool).\n\nFor your reference and to give you a sense of a range of possible individual classroom policy responses to generative AI (the cluster of tools including ChatGPT and other LLMs like Bing, Bard, and Claude), Lance Eaton has curated a collection of “Classroom Policies for AI Generative Tools.” (Note that many of these were crafted in early spring 2023 before the launch of other models and before the incorporation of generative AI options into Google Docs and Grammarly [known as GrammarlyGo, and currently only available with the paid model]).\n\nBelow is a short draft syllabus policy that you are welcome to adapt and/or adopt; it is not meant to be prescriptive.\n\nThere may well be uses for AI tools in your course, and if you can imagine ways that students might productively use them, I recommend being very clear about where and why students might be allowed to use them and how you want students to cite their usage.\n\n(Note: there is not yet an agreed-upon standard for citing large language models; the APA has suggested a model, but it has been criticized for treating LLMs as equivalent to authors—problematic for a number of reasons including that they synthesize the work of human authors without citation and that their outputs are not repeatable.)\n\nI have also included the draft of my longer policy for the class in which I will be teaching about generative AI; this includes language about citing use (and distinguishes between researching and analyzing generative AI and using it to complete writing assignments).\n\nIf any of it is useful for you, feel free to use it.\n\nI am also happy to discuss anyone else’s draft policies if you would find that useful.",
    "### Sample generative AI policy\n\nAcademic integrity is essential for a fair evaluation of your work and that of your classmates, as KU’s policies on academic misconduct make clear.\n\nAll work submitted in this course must be your own.\n\nAssignments for this course are designed to help you develop your critical thinking, close reading, research, and writing skills.\n\nSubmitting other people’s work as your own, repurposing work you have done for other courses without my prior approval and without substantial revision, or using so-called AI tools for your work in this course undermines those goals.\n\nIn this course, you may not use ChatGPT or other generative AI software for your assignments, including plug-ins or native tools that automatically generate text within word-processing apps.\n\nExceptions include the grammar check and spellcheck options on your word-processing software or Grammarly grammar check (but not GrammarlyGO’s generative text option); these approved tools make suggestions for revising work that you have already generated and may be used with caution and with the understanding that you are, as always, ultimately responsible for the work you generate.\n\nIf you have questions about what constitutes academic misconduct (plagiarism or unauthorized use of tools or assistance), please ask me before submitting your work.",
    "### Draft, full academic integrity policy for my HNRS 190 on generative AI\n\nThis course is about so-called generative AI and aims to build literacy in concepts related to those tools, including understanding the ethical and social implications of this technology and what might be appropriate uses for it.\n\nIt is likely that you will choose to research and analyze generative AI outputs as part of your final project; if you do so, as part of that assignment, you will disclose which tools you have used and explain how you have used them.\n\nYou should keep a complete, accurate record of your prompts and the model's responses; the name and, where available, version numbers of the model (e.g., Chat GPT 4, Midjourney 5.2, NightCafe SDXL 0.9, Bing Chat Creative Mode); and the dates of your interactions (which will, among other things, help us reconstruct versions for those models that don't provide them, like Bing; but will also provide a more nuanced sense of models that are fine-tuned between version releases).\n\nKeeping detailed records is good practice for any course in which the use of generative AI is approved for any portion of an assignment.\n\nAnalyzing generative AI tools and their outputs as part of an assignment is not the same as using them to complete your assignments.\n\nAt this stage in your educational career, I aim to help you develop your critical thinking and research skills, no matter what your chosen disciplines/field(s) of study might be.\n\nI am also interested in helping you develop your ability to communicate your thoughts and opinions.\n\nFor these learning goals in this course, using (rather than analyzing) generative AI in your assignments will not help you gain the competencies you need, and indeed are likely to undermine them.\n\nWith this in mind, for writing assignments for this course, you are allowed to use built-in spellcheck, grammar check, or Grammarly (without the generative AI feature) after you have already brainstormed and drafted your work.\n\nYou should not, however, use any separate, built-in, or plug-in tool or feature that generates text (for example, Wordtune, Google Docs \"Help me write,\" GrammarlyGO generative text, ChatGPT, Claude, Bing, Bard) during the idea-generation or drafting stage of the writing process.\n\nUltimately, remember that you are responsible for the writing and work you share, no matter what course you are in or platform you use to communicate, so even if you are using approved tools, check your work before submitting it.\n\nUnauthorized use of AI tools in generating work for this course and attempting to pass it off as your own work constitutes plagiarism; but plagiarism is a broader issue and is not dependent upon AI.\n\nSome specific examples of actions that constitute plagiarism include pasting together uncredited information or ideas from the internet or published sources; copying the wording of another source without quotation marks--even a few words--to indicate that the language is not your own (even if the source itself is cited); submitting an assignment written by someone else; submitting a paper written for another class without my approval and without substantial revision; and copying another student’s work (even with the student’s permission).\n\nIn order to avoid unintentional questions, please ask me before submitting your assignment.\n\nAsking questions in advance of submission about plagiarism or academic misconduct will not be held against you in any way.\n\n© 2024 The University of Kansas",
    "# Office for Faculty Career\n\nUnfortunately, there's no easy answer to this question unless the person for whom you're performing the task has given you explicit instructions about what AI use is or isn't acceptable.\n\nIf you're unsure, it's best to ask them.\n\nHere are some additional considerations for both students and instructors that may help in determining acceptable uses of AI in given contexts:\n\n- What skills does the assignment ask you to demonstrate?\n\nIf you used AI, would it be doing the work of demonstrating those skills for you?\n\n- AI reflects many types of cultural bias.\n\nIf you are asking it to generate content for you, are you aware of the types of bias you may be replicating?\n\n- If you are uploading materials (e.g.\n\nscholarly articles or student papers) to an AI program, are those materials copyrighted?\n\nAre you violating data privacy principles and/or FERPA?\n\nBecause generative AI draws on existing information on the internet, it inevitably reflects the biases and inaccuracies present in that information.\n\nFor example, Wikipedia was among ChatGPT's training sources, and Wikipedia's demographic biases are well-documented.\n\nThese shortcomings are particularly easy to see in image generation.\n\nThe image below is ChatGPT 4's response to the prompt \"Create an image of several female friends hanging out together.\"\n\nNote that all of the women pictured appear young, thin, and conventionally feminine.\n\n(Note also that the woman perched on the arm of the couch appears to have three legs, while the arm of the woman on the left appears to be melded onto her leg.)\n\nMar 20, 2024 10:39 AM  \n  \nPrint Page Library Staff Login Report a problem",
    "## Introduction\n\nIn recent months, there has been a growing concern in the academic settings about the use of text generative artificial intelligence (AI), such as ChatGPT, Bing and the latest, Co-Pilot integrated within the Microsoft Office suite.\n\nOne of the main concerns is that students may use generative AI tools to cheat or plagiarise their written assignments and exams.\n\nIn fact, a recent survey of university students found that nearly one in three students had used a form of AI, such as essay-generating software, to complete their coursework (Intelligent.com, 2023).\n\nAbout one-third of college students surveyed (sample size 1000) in the US have utilized the AI chatbot such as ChatGPT to complete written homework assignments, with 60% using the programme on more than half of their assignments.\n\nChatGPT types of generative AI tools is capable of imitating human writing, with some students using it to cheat.\n\nThe study found that 75% of students believe that using the programme for cheating is wrong but still do it, and nearly 30% believe their professors are unaware of their use of the tool.\n\nThe study also noted that some professors are considering whether to include ChatGPT in their lessons or join calls to ban it, with 46% of students saying their professors or institutions have banned the tool for homework.\n\nThis has led to calls for stricter regulations and penalties for academic misconduct involving AI.\n\nAnother concern is that the use of generative AI may lead to a decline in students’ writing and critical thinking skills (Civil, 2023; Warschauer et al., 2023), as they become more reliant on automated tools to complete their work.\n\nSome academics argue that this could have a negative impact on the quality of education and ultimately harm the students’ learning outcomes (Chan & Lee, 2023; Korn & Kelly, 2023; Oliver, 2023; Zhai, 2022).\n\nThese concerns have led some universities to ban the use of generative AI in their academic programmes.\n\nEight out of 24 universities in the prestigious UK Russell Group have declared the use of the AI bot for assignments as academic misconduct including Oxford and Cambridge.\n\nMeanwhile, many other universities around the world, are rushing to review their plagiarism policies citing concerns about academic integrity (Wood, 2023; Yau & Chan, 2023).\n\nSome Australian universities have had to alter their exam and assessment procedures back to pen- and paper-based (Cassidy, 2023; Cavendish, 2023).\n\nHowever, there are also those who argue that generative AI has the potential to revolutionize education and enhance the learning experience for students.\n\nFor example, some experts suggest that generative AI could be used to provide personalized feedback and support to students, helping them to identify areas of weakness and improve their skills in an adaptive manner (Kasneci et al, 2023; Sinhaliz et al., 2023).",
    "## Generative AI and generative pre-trained transformers\n\nGenerative AI is a subset of artificial intelligence (AI) that focuses on creating new data or content rather than analysing and interpreting existing data (McKinsey Consultant, 2023).\n\nGenerative Pre-trained Transformers (GPT) are a type of generative AI model that use deep learning techniques to generate natural language text.\n\nThe latest versions of GPT, GPT-3.5 and GPT-4, large language models which are trained on a large corpus of text data, are capable of producing human-like text with high levels of coherence, complexity, and diversity.\n\nGPT-3.5 and GPT-4 are both examples of artificial general intelligence (AGI), which is the ability of AI systems to perform any intellectual task that a human can do.\n\nUnlike artificial narrow intelligence (ANI), which is designed to perform a specific task, AGI is designed to perform multiple tasks and generalize knowledge across different domains.\n\nWhile GPT-3.5 and GPT-4 are not true AGI systems, they represent significant progress towards achieving AGI by demonstrating the ability to perform a wide range of language tasks and generate human-like text.\n\nThe development of generative AI models like GPT-3.5 and GPT-4 has the potential to revolutionize many fields, including natural language processing, creative writing, and content generation.",
    "## Rationale for an artificial intelligence education policy\n\nWith generative AI tools becoming easily accessible to the public in recent months, they are rapidly being integrated into various fields and industries.\n\nThis has created an urgent need for universities to develop an AI education policy that prepares students to work with and understand the principles of this technology.\n\nThere are several rationales supporting this need.\n\nFirstly, AI technology is becoming more prevalent in many sectors of the economy, such as finance (Bholat & Susskind, 2021; Buckley et al., 2021), healthcare (Eggmann et al., 2023; Yu et al., 2018), and transportation (Abduljabbar et al., 2019; Wu et al., 2022).\n\nAs a result, graduates will need to have a strong understanding of AI principles in order to succeed in these fields.\n\nAn AI education policy can provide students with the necessary knowledge and skills to work with AI in a professional capacity.\n\nSecondly, AI has the potential to revolutionize many aspects of society, including education itself (Adiguzel et al., 2023).\n\nAI can be used to enhance student learning by providing personalized, real-time feedback and adapting to individual learning styles (Atlas, 2023; Chan & Hu, 2023; Luckin, 2017).\n\nBy educating students on AI, universities can help prepare them to be active participants in the development and implementation of AI technology, ensuring that it benefits society as a whole.\n\nThirdly, as the use of AI in education and assessment becomes more prevalent, it is essential that students understand the principles behind the technology in order to maintain academic integrity and prevent cheating as mentioned previously (Chan, 2023; Cotton et al., 2023).\n\nAn AI education policy can teach students about the ethical considerations surrounding AI, such as bias and fairness, as well as the potential consequences of using AI in academic contexts.\n\nFourthly, developing an AI education for university is important to prepare students for the future (Aoun, 2017).\n\nAI technology is rapidly advancing, and it is likely to play an increasingly important role in society in the coming years.\n\nBy providing students and teachers with training in AI, universities can help ensure that graduates are equipped to contribute to the development of AI and to navigate the ethical, social, and economic issues that are likely to arise as AI becomes more widespread.\n\nSuch training should also help students become competent and responsible users of AI in their daily lives.\n\nFinally, it is also worth mentioning that previous AI policies in education did not anticipate the level of advancements that text-based GPT 3.5 and 4 can now achieved.\n\nGiven the potential benefits and risks associated with the use of generative AI in education, it is important to develop a proper AI education policy that addresses these concerns and provides guidance on the responsible use of AI.\n\nConducting research in AI policy in education within Hong Kong is specifically justified due to the city’s unique position as a global hub of technology, commerce, and education, coupled with its evolving education landscape.\n\nAs a dynamic metropolis with a strong commitment to technological innovation and a richly diverse education system, Hong Kong presents a compelling case study for the exploration of AI policies in education.\n\nIts blend of Eastern and Western educational philosophies and practices offers a fertile ground for examining the impacts and opportunities of AI integration in varied educational contexts.\n\nFurthermore, as Hong Kong is actively striving to enhance its digital learning capabilities and infrastructure, studying AI policy could provide valuable insights into the challenges and best practices of implementing AI in education, thereby potentially informing AI education strategies not only in Hong Kong but also in other global contexts.\n\nThe study employed a comprehensive approach to data collection, gathering rich quantitative and open-ended survey data from a diverse range of stakeholders in the education community to ensure that it reflects the needs and values of all those involved.\n\nThe combination of these data sources allowed for a holistic understanding of the topic under investigation, providing a nuanced and multifaceted view of the issues at hand.\n\nBy doing so, we can help to ensure that the use of generative AI in education is both benefi- cial and ethical.",
    "## Existing policy on artificial intelligence\n\nThe aim of this study is to investigate the education policy related to AI, however, it is essential to also scrutinize the existing policies governing AI as a whole.\n\nAs AI expands its sphere of influence to various sectors in our society, there are increasing concerns over the risks of its usage and how it might impact human activities (AI regulation, 2023; World Economic Forum [WEF], 2023).\n\nSome of the major issues of concern that have drawn the attention of governments around the world include discrimination and bias of AI, loss of privacy, violation of human rights, and malicious use of AI (Greiman, 2021; Hogenhout, 2021).\n\nFederspiel et al.\n\n(2023) caution that misuse of AI could encourage manipulation of people, create social division, and exacerbate inequalities, posing existential threats to the human race.\n\nIn view of this, countries have been working on national policies and strategies to provide clearer guidance on AI usage in order to maximize its benefits while mitigating the threats brought by it.\n\nTo advocate the responsible and proper management of AI technologies, the centre of focus for most national policies on AI have fallen on the discussion of ethics, which deals with “the standards of right and wrong, acceptable and not acceptable” (Hogehout, 2021, p. 11).\n\nFloridi (2021)’s framework for the ethical use of AI, which proposed the 5 core principles of “beneficence, non-maleficence, autonomy, justice and explicability”, is referred to by most national policies on AI as a foundation to further develop on.\n\nIn addition, Dexe and Franke (2020) summarized the AI strategy documents from the Nordic countries and identified various ethical principles as the implicit foundation for further developing policies.\n\nThe official AI governance framework from Singapore also recognized the “explainable, transparent and fair usage of AI in decision-making process” and “human-centric AI solutions” as the guiding principles of ethical use of AI (IMDA & PDPC, 2020).\n\nApart from individual countries, ethics has been the emphasis of the AI policies published by regional and international bodies.\n\nUNESCO developed its guidelines on the ethical use of AI technologies by emphasizing the key idea of human-centeredness and hence, human rights and values laid out in the Universal Declaration of Human Rights (UDHR) are advised to be adopted as the necessary foundation to further promote beneficial and appropriate use of AI technologies (UNESCO, 2021b, 2023).\n\nAI strategy in the European Union, as Renda’s (2020) analysis pointed out, also focused on ethics and highlighted a human-centric approach to AI.\n\nIn order to protect EU citizens from the danger of abusive use of advanced technologies, EU proposed its own pillars (legal compliance, ethical alignment and sociotechnical robustness) to ensure the trustworthiness of AI and established a specific AI expert group to work on specific policy recommendations and guidelines.\n\nThe heavy focus that these national and regional policies has placed on ethics demonstrates how limited they can do for the implementation of AI technologies.\n\nOn the one hand, difficulty to lay down a universal definition on ethical principles becomes a hinderance for certain countries in formulating policies on the use of AI (Dexe & Franke, 2020).\n\nOn the other hand, as AI can weave into the fabrics of everyday human activities, the resulting wide coverage of policy areas ranging from governance to education and even to environment makes it a challenging task for government to establish specific policies on AI usage (UNESCO, 2021b).\n\nThus, as the Singaporean AI governance framework highlighted, model framework or ethical guidelines were in themselves directional and for reference only, and AI practitioners need to consider them with flexibility and according to the relevance of particular situations (IMDA & PDPC, 2020).\n\nMoving forward, the ongoing efforts of national and international organizations to ensure the positive implementation of AI technologies will continue to prioritize discussions and the formulation of legal and ethical principles (AI regulation, 2023; UNESCO, 2023).\n\nHowever, until these principles are validated by real-time implementation of AI technologies, they will remain primarily predictive and prescriptive in nature (Chatterjee, 2020).\n\nOver time, it may become necessary for countries to establish institutional support systems to effectively manage AI practices in accordance with validated legal and ethical guidelines (Renda, 2020).",
    "### Fundamental ethical principles for AI\n\n- Accountability: Ensure AI actors are held responsible for the AI systems’ functioning and adherence to ethical principles\n- Accuracy: Recognize and communicate sources of error and uncertainty in algorithms and data to inform mitigation procedures\n- Auditability: Allow third parties to examine and review algorithm behavior through transparent information disclosure\n- Explainability: Ensure that algorithmic decisions and underlying data can be explained in layman’s terms\n- Fairness: Prevent discriminatory impacts, include monitoring mechanisms, and consult diverse perspectives during system development\n- Human Centricity and Well-being: Prioritize the well-being and needs of humans in AI development and implementation\n- Human rights alignment: Ensure technologies do not violate internationally recognized human rights\n- Inclusivity: Make AI accessible to everyone\n- Progressiveness: Favour projects with significantly greater value than their alternatives\n- Responsibility, accountability, and transparency: Build trust through responsibility, accountability, and fairness, provide avenues for redress, and maintain records of design processes\n- Robustness and Security: Ensure AI systems are safe, secure, and resistant to tampering or data compromise\n- Sustainability: Favour implementations that provide long-lasting, beneficial insights and can predict future behavior\n\nBelow consists of a compilation of fundamental ethical principles for AI that have been extracted from multiple policies (IMDA & PDPC, 2020) (Table 1).",
    "## Existing policy on AI in education\n\nThe integration of AI technologies into teaching and learning has begun as early as the 1970s and nowadays, different forms of these technologies are used in various educational contexts, such as the use of personalized applications for learning and assessment and information systems that help handle administrative and management tasks in schools (Al Braiki et al., 2020; Schiff, 2022; UNESCO, 2021a).\n\nAs mentioned above, the use of AI technologies has raised different issues of concern (Chan & Tsi, 2023; Chan & Zhou, 2023).\n\nIn the educational contexts, other than the general risks brought by the use of AI, concerns are primarily centred on issues such as what changes can AI bring to the design of assessment and curriculum, equalities and universality in accessing these technologies, redefinition of teachers’ role, and the lack of technological infrastructure for emerging economies (Pelletier et al., 2022; Popenici & Kerr, 2017; Swiecki et al., 2022; TEQSA, 2023; UNESCO, 2021a).\n\nBased on these concerns, AI policies in education fix their eyes on addressing a number of issues: literacy education to prevent inequalities in the use of digital technologies (Southgate, 2020; UNESCO, 2021b); essential values of traditional forms of teaching and learning such as teacher-student and student–student relationships (Luan et al., 2020; UNESCO, 2021b); inclusiveness and equity in the use of AI technologies (Tanveer et al., 2020; UNESCO, 2021a); professional development of teachers to enhance teachers’ competence and assist them to transform their roles (Ocaña-Fernández et al., 2019; Wang et al., 2021); and training and enhancement of skills or “micro-credentials” for students that are important and necessary for harnessing technologies (Pelletier et al., 2022; UNESCO, 2021a).\n\nThe roles of literacy education and skills training are having particular implications for the wider society as the population in general also needs to be prepared for the implementation of AI technologies in different sectors.\n\nDespite identifying multiple issues of concern in the educational contexts, policies on AI in education are mostly generic and implicit because of the lack of concrete evidence of implementing AI technologies (UNESCO, 2021a).\n\nIn Schiff (2022)’s review on 24 AI policy strategies focusing on the role of education in global AI policy discourse, it was found that policymakers view education largely as an instrumental tool to support workforce development and training of AI experts.\n\nThe article finds that the use of AI in education is largely absent from policy conversations, while the instrumental value of education in supporting an AI-ready workforce and training more AI experts is overwhelmingly prioritized.\n\nThe article suggests that if such a trend continues, policymakers may fail to realize AI in education’s transformative potential and may fail to sufficiently fund, regulate, and consider AI in education’s ethical implications.\n\nAI scholarship and education governance do not receive adequate attention in the current literature (Gellai, 2022), and public understanding of the policy implications of AI is limited (Feldstein, 2019).\n\nWhile more work is still to be done in order to formulate more comprehensive and focused policy documents on AI in education, ethics was reiterated again as a strategically plausible starting point for further discourse and researchers were especially encouraged to engage further with policymakers through their work on ethics in the use of AI in education (Sam & Olbrich, 2023; Schiff, 2022).\n\nIn view of this gap, this research intends to propose a policy framework for integrating AI in higher education, taking into consideration aspects of teaching and learning as well as ethical and practical concerns.\n\nIn this research, we will employ the guidelines put forth by UNESCO (2021a) as the starting point for crafting a more accurate AI policy for university teaching and learning.\n\nThe rationale for employing UNESCO recommendations as the basis is multifaceted.\n\nFirst, UNESCO is an esteemed international organisation with significant expertise in education, their recommendations are supported by thorough research and knowledge from experts worldwide.\n\nThese recommendations are designed to be relevant and flexible for a variety of educational systems and cultural settings, making them suitable for diverse institutions.\n\nUNESCO’s guidelines also take a comprehensive approach to incorporating AI in education, addressing important ethical, social, economic, and technological aspects essential for creating effective policies.\n\nUsing an existing framework like UNESCO’s recommendations saves time and resources and provides a well-organized starting point for examining specific AI policy issues in university teaching and learning.\n\nFinally, anchoring the study in UNESCO’s recommendations enhances the credibility of the research.",
    "Finally, anchoring the study in UNESCO’s recommendations enhances the credibility of the research.\n\nThe UNESCO framework for AI in education is centred around a humanistic approach, which aims to safeguard human rights and provide individuals with the necessary skills and values for sustainable development, as well as effective human–machine collaboration in life, learning, and work.\n\nThe framework prioritizes human control over AI and ensures that it is utilized to improve the capabilities of both teachers and students.\n\nMoreover, the framework calls for ethical, transparent, non-discriminatory, and auditable design of AI applications.\n\nFrom the UNESCO’s AI and Education: Guidance for Policy-Makers document, the following recommendations are provided:\n\n- Interdisciplinary planning and inter-sectoral governance: This recommendation suggests that AI and education policies should be developed through collaboration between different sectors and disciplines to ensure a comprehensive approach.\n\nFor example, policymakers could work with experts in education, technology, ethics, and other relevant fields to develop policies that take into account all aspects of AI in education.\n\n- Policies on equitable, inclusive, and ethical use of AI: This recommendation emphasizes the importance of ensuring that AI is used in an ethical and inclusive manner that benefits all learners.\n\nFor example, policymakers could develop policies that address issues such as bias in AI algorithms or access to AI tools for learners from disadvantaged backgrounds.\n\n- Develop a master plan for using AI for education management, teaching, learning, and assessment: This recommendation suggests that policymakers should develop a comprehensive plan for using AI in various aspects of education to ensure its effective implementation.\n\nFor example, a master plan could include specific goals for using AI in areas such as personalized learning or teacher professional development.\n\n- Pilot testing, monitoring and evaluation, and building an evidence base: This recommendation highlights the importance of testing and evaluating the use of AI in education through pilot projects to build an evidence base for its effectiveness.\n\nFor example, policymakers could fund pilot projects that test the use of AI tools in specific educational contexts or with specific learner populations.\n\n- Fostering local AI innovations for education: This recommendation suggests that policymakers should encourage the development of local innovations in AI for education to ensure that it meets the specific needs of their communities.\n\nFor example, policymakers could provide funding or support to local startups or research institutions working on developing new AI tools or applications specifically designed for their region’s educational needs.\n\nUsing UNESCO’s recommendations as a basis, this study aims to examine higher education stakeholders’ perceptions of text generative AI technology.\n\nBased on their ideas, recommendations, and concerns, an AI education policy framework will be developed to promote ethical and effective integration of AI technologies in higher education.",
    "## Methodology\n\nIn this study, a survey design was utilized to gather data from students, teachers, and staff in Hong Kong to develop AI education policy framework for university teaching and learning.\n\nThe survey was administered through an online questionnaire, featuring a mix of closed-ended and open-ended questions.\n\nThe questionnaire was designed based on a review of current literature on AI use in higher education.\n\nTopics covered in the survey were major issues concerning the use of AI in higher education, which included the use of generative AI technologies like ChatGPT, the integration of AI technologies in higher education, potential risks associated with AI technologies, and AI’s impact on teaching and learning.\n\nData were collected via an online survey from a diverse group of stakeholders in the education community, ensuring that the results reflect the needs and values of all participants.\n\nA convenience sampling method was employed for selecting the respondents, based on their availability and willingness to participate in the study.\n\nParticipants were recruited through an online platform and provided with an informed consent form prior to completing the survey.\n\nThe survey was completed by 457 undergraduate and postgraduate students, as well as 180 teachers and staff members across various disciplines in Hong Kong.\n\nDescriptive analysis was used to analyse the survey data, while a thematic analysis approach was applied to examine the responses from the open-ended questions in the survey.",
    "### Quantitative data (survey data) and descriptive analysis\n\nA range of survey items was included to capture different aspects of participants’ usage and perception of generative AI technologies like ChatGPT.\n\nFor example, participants were asked whether they have used ChatGPT or similar generative AI technologies before and how they envision using these technologies in their teaching and learning practices.\n\nDescriptive analysis was employed to analyse the survey data collected from students and teachers in Hong Kong, in order to gain a better understanding of the usage and perception of generative AI technologies like ChatGPT in higher education.\n\nDescriptive analysis is an appropriate statistical method for summarizing and describing the main characteristics of the sample and the data collected.\n\nIt is particularly useful for analysing survey data and can provide an overview of the distribution, central tendency, and variability of the responses.",
    "### Qualitative data (open-ended data) and thematic analysis\n\nAside from the quantitative part of the survey, respondents were also asked about their apprehensions regarding the integration of generative AI technologies and their recommendations for university strategic plans through open-ended questions.\n\nDifferent perspectives and experiences were gathered from the respondents across various disciplines.\n\nThe data from the open-ended questions were analysed using a thematic analysis approach, which involved identifying patterns and themes in the data.\n\nAn inductive approach was used to analyse the responses, where the themes emerged from the data rather than being predetermined by the researcher.\n\nThe combination of the quantitative and qualitative data enabled a more holistic understanding of the usage and perception of generative AI technologies in higher education.\n\nThis allowed for the pinpointing of potential requirements, recommendations, and strategies for AI policy in university teaching and learning.\n\nThis understanding is essential for ensuring that the use of these technologies is both beneficial and ethical.",
    "### Findings from the quantitative data\n\nThe survey was conducted among 457 students and 180 teachers and staff from different disciplines in Hong Kong universities.\n\nThe goal was to explore the kinds of requirements, guidelines and strategies necessary for developing AI policies geared towards university teaching and learning.\n\nThe findings reveal valuable insights into the perception of generative AI technologies like ChatGPT among students and teachers (refer to Table 2).\n\nRegarding the usage of generative AI technologies, both students (mean = 2.28, SD = 1.18) and teachers (mean = 2.02, SD = 1.1) reported relatively low experience, suggesting that there is significant room for growth in adoption.\n\nBoth groups demonstrated a belief in the positive impact of integrating AI technologies into higher education (students: mean = 4, SD = 0.891; teachers: mean = 3.87, SD = 1.32).\n\nThis optimism was also reflected in the strong agreement that institutions should have plans in place associated with AI technologies (students: mean = 4.5, SD = 0.854; teachers: mean = 4.54, SD = 0.874).\n\nBoth students and teachers were open to integrating AI technologies into their future teaching and learning practices (students: mean = 3.93, SD = 1.09; teachers: mean = 3.92, SD = 1.31).\n\nHowever, there were concerns among both groups about other students using AI technologies to get ahead in their assignments (students: mean = 3.67, SD = 1.22; teachers: mean = 3.93, SD = 1.12).\n\nInterestingly, both students and teachers did not strongly agree that AI technologies would replace teachers in the future (students: mean = 2.14, SD = 1.12; teachers: mean = 2.26, SD = 1.34).",
    "#### Item Students Teachers\n\nThe respondents acknowledged the importance of learning to use generative AI technologies well for their careers (students: mean = 4.07, SD = 0.998; teachers: mean = 4.1, SD = 1.08).\n\nHowever, both groups expressed doubt about teachers’ ability to accurately identify a student’s usage of generative AI technologies for completing assignments (students: mean = 3.02, SD = 1.56; teachers: mean = 2.72, SD = 1.62).\n\nThe responses to the remaining questions suggest that students and teachers recognize potential benefits and drawbacks of AI technologies, including providing guidance and personalized feedback, improving digital competence and academic performance, and offering anonymity in student support services.\n\nHowever, there were concerns about over-reliance on AI technologies, limited social interaction, and the potential hindrance to the development of generic skills.\n\nThese findings highlight the need for a comprehensive AI policy in higher education that addresses the potential risks and opportunities associated with generative AI technologies.\n\nBased on these findings, some implications and suggestions for university teaching and learning AI policy include:\n\n- Training: Providing training for both students and teachers on effectively using and integrating generative AI technologies into teaching and learning practices.\n\n- Ethical Use and Risk Management: Developing policies and guidelines for ethical use and risk management associated with generative AI technologies.\n\n- Incorporating AI without replacing human: Incorporating AI technologies as supplementary tools to assist teachers and students, rather than as replacements for human interaction.\n\n- Continuously Enhancing Holistic Competencies: Encouraging the use of AI technologies to enhance specific skills, such as digital competence and time management, while ensuring that students continue to develop vital transferable skills.\n\n- Fostering a transparent AI environment: Fostering a transparent environment where students and teachers can openly discuss the benefits and concerns associated with using AI technologies in higher education.\n\n- Data Privacy and security: Ensuring data privacy and security while using AI technologies.\n\nOverall, the survey results indicate an openness to adopting generative AI technologies in higher education and a recognition of the potential advantages and challenges.\n\nAddressing these issues through informed policy and institutional support will be crucial for maximizing the benefits of AI technologies in university teaching and learning.",
    "### Findings from the qualitative data\n\nThe qualitative data collected from students, teachers, and staff yielded valuable and rich suggestions and comments.\n\nThere are 10 main themes and 25 subthemes that emerged from the qualitative data as presented in Table 3.\n\nFrom the data, we identified ten key areas (i.e., the main themes) that are directly relevant to the planning of an AI policy for teaching and learning in universities.\n\nThese areas align well with the quantitative data and are as follows:\n\n1.\n\nUnderstanding, identifying and preventing academic misconduct and ethical dilemmas: To address academic misconduct, universities must develop clear guidelines and strategies for detecting and preventing the misuse of generative AI.\n\nTeachers emphasize the importance of creating university-wide policies on how to test students suspected of using AI to complete tasks in which AI use is prohibited or misused.\n\nAs one student stated, “A clear set of rules about what happens if AI is used and resources on informing students about the rule set are needed.” They also suggested, “Clearly stipulate in which areas generative AI technologies are allowed and which are not.\n\nWhat are the procedures to handle suspended cases?\n\nWhat are the consequences?” Another student mentioned that “the level of restriction should be clarified.” Both teachers and students have also suggested the use of assessments that minimize opportunities for AI misuse, such as oral examinations or controlled settings where internet access is limited, to help maintain academic integrity.\n\nBoth teachers and students have also questioned “what is the definition of cheating?” in this AI era.\n\nTeachers highlight the importance of identifying ethical dilemmas and recommend familiarizing students with ethical issues, such as the boundaries between plagiarism and inspiration and appropriate situations for seeking help from AI.\n\nEstablishing clear policies around AI use, including ethical guidelines and legal responsibilities, will help students and staff navigate these complex issues.\n\nOne teacher noted, “The education on academic and research ethics should be strengthened.” Explicitly stipulating the areas where AI is allowed and the procedures for handling suspected cases of misuse will help maintain a transparent and equitable learning environment.\n\n2.\n\nAddressing governance of AI: Data privacy, transparency, accountability and security: Universities must take responsibility for decisions made regarding the use of generative AI in teaching and learning, which includes being transparent about data collection and usage, and being receptive to feedback and criticism.\n\nBy disclosing information about the implementation of generative AI, including the algorithms employed, their functions, and any potential biases or limitations, universities can foster trust and confidence among students and staff in AI technology usage.\n\nTeachers emphasize the importance of addressing ethical concerns, privacy, security, and other related issues when using generative AI technologies.\n\nTeachers commented “In general, its impact is inevitable.\n\nIt may negatively affect social consciousness and responsibility.\n\nDepending on climate change management and its consequences, it may contribute to the demise of a significant portion of humanity.\n\nIt may also protect and advance the interests of those who benefit from chaos.”\n\n   - Privacy and Security: AI technologies rely on vast amounts of data, which raises concerns about privacy and security if the data is not adequately protected.\n\n“Institutions should ensure that the data used by generative AI technologies is kept private and secure.\n\nThis includes ensuring that any data used in training or testing the technology is de-identified, and that appropriate security measures are in place to prevent unauthorized access or use of data.”\n   - Transparency and Accountability: Universities should be transparent about the use of generative AI in teaching and learning, which includes disclosing information about the algorithms and their functions, as well as any potential biases or limitations of the AI tools.\n\n“It is essential to recognize ethical dilemmas and consider privacy, security, and related issues when employing generative AI technologies.” The complexity of AI technologies can make it difficult to hold organizations and individuals accountable for their decisions and actions.\n\nInstitutions should address ethical issues, such as potential discrimination, bias, and stereotypes, while ensuring data privacy and security.\n\n3.\n\nMonitoring and evaluating AI implementation: To ensure the success of AI integration in university teaching and learning, continuous monitoring and evaluation of its implementation are necessary.\n\nTeachers recommend conducting longitudinal experiments in different areas to better understand how AI affects students’ learning processes and outcomes.",
    "Teachers recommend conducting longitudinal experiments in different areas to better understand how AI affects students’ learning processes and outcomes.\n\nRegular assessments of AI’s impact on teaching practices and student performance will help identify areas for improvement and ensure that the technology is being used effectively and ethically.\n\nOne student mentioned, “The plan should include more experiments on conducting the AI technologies on teaching.” By regularly collecting feedback from both teachers and students, universities can make informed decisions about how to improve AI implementation.\n\nEvaluating the effectiveness of AI tools in enhancing learning outcomes is vital in determining their value and making adjustments as needed.\n\n4.\n\nEnsuring equity in access to AI technologies: Ensuring equitable access to AI technologies is crucial for fostering an inclusive learning environment.\n\nUniversities should work to provide resources and support to all students and staff, regardless of their background or access to technology.\n\nThis may involve the procurement of AI tools, including AI detectors, for use by the entire university community.\n\nBy promoting equal access to AI technologies, universities can help level the playing field and ensure that all students and staff have the opportunity to benefit from the advantages offered by AI integration.\n\nEqual access to AI technologies is essential for maintaining fairness in the educational environment.\n\nOne teacher commented, “Same as all other resources, to incorporate this into current industries (especially education), fairness should be a top priority.\n\nIf the usage involves any kind of competition, e.g., access to ChatGPT should be equal for all involved parties.” Another student highlights “Ethical dilemma includes ensuring that the technology is not used to discriminate against individuals or groups, and that it does not reinforce bias or stereotypes.” Universities should consider how to ensure that all students have access and training to AI tools and resources, regardless of their socio-economic backgrounds, in order to level the playing field and promote inclusivity.\n\n5.\n\nAttributing AI technologies: Attribution is an important aspect of AI policy in university teaching and learning.\n\nOne student remarked, “They are welcome to use AI for academic purposes while requiring students to state clearly which part was helped by AI.\n\nThis is similar to the references and citation of current academic practice.” By requiring students to attribute AI-generated content, universities can promote academic integrity and ensure that AI technologies are used ethically in the learning process.\n\nFurthermore, there is a need for guidelines on how to fairly attribute generative AI’s contribution to student work.\n\n“Ethics of use, knowledge of affordances, effective use, critique/evaluation of outputs, and role/integration in workflows/product in study and professional settings” may be included in theate_attribution.\n\n6.\n\nProviding training and support for teachers, staff, and students in AI literacy: To ensure successful integration of AI in teaching and learning, universities must provide adequate training and support for teachers, staff, and students.\n\nTeachers express concerns about coping with this new trend, helping students use AI effectively, and learning from student usage.\n\nAs one teacher puts it, “Staff and students need an educative approach to its ethical use.” Investing in training and resources can help educators feel more confident and capable in navigating the complexities of AI in their classrooms.\n\nThis is supported by many students and teachers who believe that institutions should provide training to faculty and staff on the appropriate use of generative AI technologies in teaching and learning.\n\n“This training should include information on selecting appropriate technologies, using them effectively, and managing the risks associated with their use,” they say.\n\nIn addition, “teaching students how to use the technology and how to critique it is probably central to successfully planning for the integration of AI in education.” Students suggest that “teaching students the potential of using generative AI properly and critically can benefit from students using AI hiddenly” and “relying on tools able to detect the use of a generative language model, while being aware of the limits of such tools.”\n\nAI literacy is crucial for both students and staff as they navigate the use of generative AI in teaching and learning.\n\nTeachers emphasize the need for education on ethics, knowledge of AI tool affordances, effective use (e.g., prompt engineering), critique and evaluation of outputs, and the role of AI in study and professional settings.\n\nA comprehensive AI literacy programme will help students and staff better understand and responsibly utilize AI technologies in their academic and professional lives.",
    "A comprehensive AI literacy programme will help students and staff better understand and responsibly utilize AI technologies in their academic and professional lives.\n\nBy providing training and resources on AI technologies, universities can empower students and staff to make informed decisions about their use and potential applications in teaching and learning.\n\n7.\n\nRethinking assessments and examinations: The integration of generative AI in education calls for a re-evaluation of assessments and examinations.\n\nTeachers suggest designing assessments that allow AI technologies to enhance learning outcomes, rather than solely producing outputs.\n\nFor example, one teacher recommends “Promote assessments and activities where students can by themselves discover the limits of such techniques—and relativize the idea that they could be useful to ‘cheat’.” This shift may necessitate the development of new assessment methods that balance the benefits of AI with the need to maintain academic integrity.\n\nA student stated, “Change of assessment methods to measure the true ‘understanding’ of students instead of the ability to collect information (which can easily be done with AI tools).” Universities may need to develop new assessment strategies that focus on students’ understanding, critical thinking, and analysis to prevent AI-generated content from compromising the assessment process.\n\nA teacher noted that “… it is hard to assess most of them, so we fall back on regurgitation”, a change is necessary.\n\n8.\n\nEncouraging a balanced approach to AI adoption: A balanced approach to AI adoption in university teaching and learning involves recognizing both the potential benefits and limitations of generative AI technologies.\n\nOne teacher suggests, “Be positive about this technological evolution and incorporate it to develop new assignments and assessment.” This approach requires flexibility, striking a balance between embracing new technology for its potential to enhance efficiency and productivity while maintaining a focus on critical thinking and ethical considerations.\n\nIt is also important to encourage a balanced approach to AI adoption to avoid over-reliance on these technologies.\n\n“We should learn how AI can assist us, but not replace schoolwork,” one teacher advised.\n\nThis approach involves using AI technologies as complementary tools to support learning rather than relying on them as a substitute for traditional teaching methods.\n\nStudents should be encouraged to use AI as an aid to their learning process and not solely depend on it for academic success.\n\n9.\n\nPreparing students for the AI-driven workplace: Preparing students for an AI-driven workplace involves teaching them how to use AI responsibly, ethically, and effectively.\n\nUniversities should develop curricula that reflect the increasing prominence of AI in various industries, ensuring that students are equipped with the skills and knowledge to navigate the evolving workplace landscape.\n\nThis includes teaching students how to integrate AI into their workflows, evaluate the effectiveness of AI tools, and understand their role in professional settings.\n\nAs one teacher notes, “Teaching students how to use it properly and understanding its limitations and strengths would be useful.” Integrating AI technologies into teaching and learning involves familiarizing students with AI tools they will likely encounter during their university studies and in the workplace, as mentioned by a student who said, “Teach students how to best use AI tools and make AI tools a common part of education, just like PowerPoint and Excel.” Teachers suggest guiding students to recognize ethical issues and helping them self-appropriate AI in study and work settings.\n\nAs the workplace increasingly adopts AI technologies, universities should prepare students for this shift.\n\nOne student stated, “Plans should be implemented to assist students in making better and more constructive use of AI in learning, career planning, and personal development.”\n\n10.\n\nDeveloping student holistic competencies/generic skills: Teachers have highlighted the importance of enhancing critical thinking, digital literacy, information literacy, and professional ethics among students to help them make effective and ethical use of AI technologies.\n\nTo harness the potential of generative AI technologies, teachers advocate for an emphasis on teaching students to assess the reliability of content, understand biases, and evaluate the accuracy and relevance of AI-generated information.",
    "One teacher suggests that “Enhancement on critical thinking among students is definitely a must, in order to make good use of such AI technologies.” Another teacher emphasizes the importance of “Teaching students how to use the technology, and how to critique it, is probably central to successfully planning for the integration of AI in education.” To successfully embrace generative AI technologies, universities should prioritize fostering critical thinking among students.\n\nOne student suggested, “Plans on how to maintain students’ interest and motivation to engage in deep and critical thinking, diversify perspectives and expand horizons.” Developing holistic competencies and generic skills in students is an essential goal of education.\n\n“However there are many things, even far more important things, to education…: the education of character; rhetoric and analytical skills; public speaking; creativity; memorisation; all embodied skills,” a teacher argued.\n\nIncorporating AI technologies into teaching and learning may hinder students’ development of competencies such as teamwork, leadership, empathy, and creativity skills.\n\nTherefore, universities need to continuously find opportunities for students to develop these skills, preparing them for the AI-driven workplace where they need to be adaptable, resilient, and transformational.",
    "### Triangulating quantitative and qualitative data\n\nThe quantitative findings support the key areas found in the qualitative data for AI integration in education.\n\nThe quantitative data reveals that both students and teachers share concerns about the potential misuse of AI technologies, such as ChatGPT, in assignments (students: mean 3.67, teachers: mean 3.93).\n\nThis emphasizes the need for guidelines and strategies to prevent academic misconduct.\n\nFurthermore, there is significant agreement among students and teachers on the necessity for higher education institutions to implement a plan for managing the potential risks associated with using generative AI technologies (students: mean 4.5, teachers: mean 4.54), highlighting the importance of addressing data privacy, transparency, accountability, and security.\n\nThe overall positive perception of AI technologies integration within education implies that proper policies should be in place to ensure responsible AI incorporation in higher education.\n\nThe concern that some students might use generative AI technologies to gain an advantage in their assignments (students: mean 3.67, teachers: mean 3.93) underscores the importance of ensuring equal access to AI technologies for all students.\n\nMoreover, the consensus that students must become proficient in using generative AI technologies for their careers (students: mean 4.07, teachers: mean 4.1) highlights the need for AI literacy and training for all stakeholders in the educational process, preparing students for the AI-driven workplace.\n\nInterestingly, teachers and students are unsure if teachers can accurately identify a student’s use of generative AI technologies to partially complete an assignment (students: mean 3.02, teachers: mean 2.72), yet they also believe that AI technologies can provide unique insights and perspectives and personalized feedback.\n\nThis suggests that rethinking assessment methods may be necessary.\n\nData indicating that neither students nor teachers believe AI technologies will replace teachers in the future (students: mean 2.14, teachers: mean 2.26) supports the need for a balanced approach to AI adoption, utilizing AI technologies as complementary tools rather than substitutes for traditional teaching methods.\n\nFinally, concerns that generative AI technologies could hinder students’ development of generic or transferable skills, such as teamwork, problem-solving, and leadership (students: mean 3.3, teachers: mean 3.74), emphasize the importance of focusing on students’ holistic competencies and generic skills in preparation for the AI-driven workplace.",
    "### Key areas versus UNESCO’s recommendations on AI education policy\n\nThe original plan for the study was to use UNESCO’s recommendations as a basis for developing AI education policy framework for university teaching and learning through inputs from various stakeholders to identify any gaps in the framework and modified accordingly.\n\nAlthough the recommendations from UNESCO can provide a high-level guideline for this study, it was clear that there are several key differences between UNESCO recommendations and the ten key areas that were identified for integrating AI in university teaching and learning.\n\nAs the UNESCO’s AI and Education: Guidance for Policy-Makers was written before the availability of GPT 3.5 and 4, the recommendations would not have fully addressed the current opportunities and threats of the advances in the GPT technologies for education.\n\nMoreover, the UNESCO recommendations are intended for education in general and do not specifically cater to the needs of university teaching and learning.\n\nThe UNESCO recommendations are high-level and general aimed at helping policy-makers better understand the possibilities and implications of AI for teaching and learning to help achieve Sustainable Development Goal 4: Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all (UNESCO, 2021a), while the ten key areas are more specific, practical, and tailored to university teaching and learning.\n\nFurthermore, most of the existing AI education policies tend to emphasize the instrumental role of AI for workforce development (Schiff, 2022), but the key areas based on the findings of this study also address the transformative potential and ethical considerations relating to AI use in higher education in addition to its instrumental purposes.\n\nThe ten key areas were developed based on direct input from stakeholders who have vested interests in university teaching and learning, which makes them more relevant and grounded in practice.\n\nFor example, UNESCO’s recommendation to develop a master plan for using AI for education management, teaching, learning, and assessment emphasizes the need for a comprehensive and strategic approach to integrating AI in various aspects of education.\n\nThis includes not only teaching and learning but also the broader aspects of education management, such as administration, resource allocation, and policy development.\n\nThe focus here is on creating an overarching framework that guides the implementation of AI in education as a whole.\n\nOn the other hand, the key area derived from the qualitative findings of rethinking assessments and examinations delves deeper into a specific aspect of education: the evaluation of students’ learning.\n\nThis area acknowledges that the integration of generative AI in education necessitates a re-evaluation of traditional assessment methods.\n\nThe focus here is on designing assessments that allow AI technologies to enhance learning outcomes while maintaining academic integrity.\n\nThis involves developing new assessment strategies that focus on students’ understanding, critical thinking, and analysis rather than just their ability to collect information.\n\nIn short, UNESCO’s recommendations highlight the “what”, whereas the ten key areas of this study detail the “how” of AI education policy.\n\nIn essence, the difference between these two areas lies in their scope and focus.\n\nUNESCO’s recommendation is broader, encompassing various aspects of education and advocating for a comprehensive master plan.\n\nIts primary target is policymakers.\n\nIn contrast, the key area on rethinking assessments and examinations is more specific and user centred, catering for various higher education stakeholders including students and teachers as well as addressing the challenges and opportunities associated with AI integration in university teaching and learning.",
    "## AI ecological education policy framework\n\nIn order to turn policy recommendations into action plans, the ten key areas have been further organised into three dimensions—Pedagogical, Ethical, and Operational into the AI Ecological Education Policy Framework.\n\nEach dimension is led by a responsible party.\n\nThis framework allows for a more nuanced understanding of the multifaceted implications of AI integration in university settings and ensures that stakeholders consider the broader context of AI adoption and its impact on various aspects of teaching and learning.",
    "### Governance dimension (senior management)\n\nThis dimension emphasizes the governance considerations surrounding AI usage in education.\n\nIt encompasses the following key areas:\n\n- Understanding, identifying, and preventing academic misconduct and ethical dilemmas\n- Addressing governance of AI: data privacy, transparency, accountability, and security\n- Attributing AI technologies\n- Ensuring equity in access to AI technologies",
    "### Operational dimension (teaching and learning and IT staff)\n\nThis dimension concentrates on the practical implementation of AI in university settings.\n\nIt includes the following key areas:\n\n- Monitoring and evaluating AI implementation\n- Providing training and support for teachers, staff, and students in AI literacy\n\nFor the Pedagogical dimension, the framework emphasizes the need to adapt teaching methods and assessment strategies in response to AI’s growing capabilities, preparing students for an increasingly AI-driven workplace.\n\nBy focusing on pedagogy, the framework ensures that AI technologies are harnessed to enhance learning outcomes and develop critical thinking, creativity, and other essential skills, rather than undermining academic integrity.\n\nTeachers are the initiator for the Pedagogical Dimension, as they are the ones who design and implement lesson plans, activities, and assessments that utilize AI technologies.\n\nThey will need to have the expertise to determine how AI can best support and enhance students’ learning experiences.\n\nAt the same time, they must possess the necessary awareness and knowledge to educate students on the potential risks associated with the use of generative AI in learning, particularly in assessments and assignments, such as plagiarism and contract cheating.\n\nTeachers need to foster ethical use of AI, for example through proper attribution to acknowledge the contributions of AI technologies in student work, and develop assessment tasks that require critical and analytical thinking to avoid AI-assisted plagiarism.\n\nBy assigning teachers the responsibility for this dimension, we ensure that AI tools are used in a way that is pedagogically sound and enhances the learning outcomes of students.\n\nThe Governance dimension highlights the importance of addressing issues related to academic misconduct, data privacy, transparency, and accountability.\n\nThe framework ensures that stakeholders understand and address the ethical challenges associated with AI technologies, fostering responsible use and helping to maintain trust within the university community.\n\nThis focus on governance encourages universities to develop clear policies and guidelines, ensuring that students and staff can navigate the complex ethical landscape surrounding AI.\n\nSenior management will be the initiator for the Governance Dimension of the AI Ecology Framework.\n\nAs they hold decision-making authority, they are tasked with developing and enforcing policies, guidelines, and procedures that address the ethical concerns surrounding AI use in education.\n\nThese include academic integrity, data privacy, transparency, accountability, and security.\n\nSenior management’s role is to ensure that AI is used responsibly and ethically, fostering a learning environment that is fair, equitable, and inclusive.\n\nThe Operational dimension of the framework underscores the need for ongoing monitoring, evaluation, and support to ensure the effective and equitable implementation of AI technologies.\n\nBy considering operational aspects, the framework encourages universities to provide training, resources, and support to all stakeholders, promoting equal access to AI technologies and fostering an inclusive learning environment.\n\nFurthermore, the operational dimension emphasizes the importance of continuous improvement and adaptation, enabling universities to refine their AI integration strategies in response to new insights and changing needs.\n\nTeaching and Learning and IT staff will be tasked to look after the Operational Dimension.\n\nThey play a crucial role in managing and maintaining the AI technologies used in the educational setting.\n\nTheir tasks include providing training and support for both students and staff, ensuring the proper functioning of AI tools, and addressing any technical issues that may arise.\n\nThey can ensure that AI technologies are seamlessly integrated into the educational environment, minimizing disruptions and maximizing their potential benefits.\n\nIt is crucial to recognize that the responsibility of each dimension in the ecological framework should not be viewed in isolation.\n\nCollaboration and communication among all stakeholders (universities, teachers, students, staff and external agents such as accreditation, quality assurance bodies) are essential to ensure the successful implementation of any policy.\n\nEach group should actively participate in the development and execution of AI-related initiatives and work together to achieve the desired outcomes in university teaching and learning.",
    "## Conclusions\n\nThis study aims to establish an AI education policy for university teaching and learning, addressing concerns related to the use of text-generating AI in academic environments, such as cheating and plagiarism.\n\nThe findings of this study yield ten key areas in AI education policy planning, from which an AI Ecological Education Policy Framework is constructed to fulfill the objective of the study.\n\nHowever, this study has some limitations, including a relatively small sample size that may not be representative of all educational institutions.\n\nAdditionally, the research only focused on text-based generative AI technology and did not explore other types or variations.\n\nLastly, the study relied on self-reported data from participants, which may be subject to bias or inaccuracies.\n\nThis study proposes an AI Ecological Education Policy Framework to address the diverse implications of AI integration in university settings.\n\nThe framework consists of three dimensions—Pedagogical, Governance, and Operational—each led by a responsible party.\n\nThis structure allows for a more comprehensive understanding of AI integration implications in teaching and learning settings and ensures stakeholders are aware of their responsibilities.\n\nBy adopting this framework, educational institutions can align actions with their policy, ensuring responsible and ethical AI usage while maximizing potential benefits.\n\nHowever, more research is necessary to fully comprehend the potential advantages and risks associated with AI in academic settings.\n\nMerely advocating for AI implementation in education is insufficient; stakeholders need to carefully evaluate which AI technologies to employ, determine the best methods for their use, and understand their true capabilities.",
    "## References\n\n- Abduljabbar, R., Dia, H., Liyanage, S., & Bagloee, S. A.\n\n(2019).\n\nApplications of artificial intelligence in transport: An overview.\n\nSustainability, 11(1), 189.\n\n- Adiguzel, T., Kaya, M. H., & Cansu, F. K. (2023).\n\nRevolutionizing education with AI: Exploring the transformative potential of ChatGPT.\n\nContemporary Educational Technology, 15(3), ep429.\n\n- AI regulation (2023).\n\nLegal and Ethical Aspects of ChatGPT: EU Parliament’s Amendment, French Experts’ Opinion on Ethical Issues and Other Useful Resources.\n\nRetrieved from \n- Al Braiki, B., Harous, S., Zaki, N., & Alnajjar, F. (2020).\n\nArtificial intelligence in education and assessment methods.\n\nBulleting of Electrical Engineering and Informatics, 9(5), 1998–2007.\n\n- Aoun, J. E. (2017).\n\nRobot-proof: Higher education in the age of artificial intelligence.\n\nThe MIT Press.\n\n- Atlas, S. (2023).\n\nChatGPT for higher education and professional development: A guide to conversational AI.\n\n- Bholat, D., & Susskind, D. (2021).\n\nThe assessment: Artificial intelligence and financial services.\n\nOxford Review of Economic Policy, 37(3), 417–434.\n\n- Buckley, R. P., Zetzsche, D. A., Arner, D. W., & Tang, B. W. (2021).\n\nRegulating artificial intelligence in finance: Putting the human in the loop.\n\nThe Sydney Law Review, 43(1), 43–81.\n\n- Cassidy, C. (2023).\n\nUniversities to return to pen and paper exams after students caught using AI to write essays.\n\nThe Guardian.\n\n- Cavendish, C. (2023).\n\nChatGPT will force school exams out of the dark ages.\n\nFinancial Times.\n\n- Chan, C.K.Y.\n\n(2023).\n\nIs AI changing the rules of academic misconduct?\n\nAn in-depth look at students’ perceptions of ’AI-giarism’.\n\n- Chan, C. K. Y., & Hu, W. (2023).\n\nStudents’ voices on generative AI: Perceptions, benefits, and challenges in higher education.\n\n- Chan, CKY & Chen, S. (2023).\n\nStudent Partnership in Assessment in Higher Education: A Systematic Review.\n\n- Chan, C. K. Y., & Lee, K. K. W. (2023).\n\nThe AI generation gap: Are Gen Z students more interested in adopting generative AI such as ChatGPT in teaching and learning than their Gen X and Millennial Generation teachers?\n\narxiv:2305.02878\n- Chan, C. K. Y., & Tsi, L. H. Y.\n\n(2023).\n\nThe AI Revolution in Education: Will AI Replace or Assist Teachers in Higher Education?\n\n[Preprint].\n\narxiv:2305.01185\n- Chan, C. K. Y., & Zhou, W. (2023).\n\nDeconstructing Student Perceptions of Generative AI (GenAI) through an Expectancy Value Theory (EVT)-based Instrument [Preprint].\n\narxiv:2305.01186\n- Chatterjee, S. (2020).\n\nAI strategy of India: Policy framework, adoption challenges and actions for government.\n\nTransforming Government, 14(5), 757–775.\n\n- Civil, B.\n\n(2023, March 16).\n\nChatGPT can hinder students’ critical thinking skills: Artificial intelligence is changing how students learn to write.\n\nThe Queen’s Journal.\n\n- Cotton, D. R. E., Cotton, P. A., & Shipway, J. R. (2023).\n\nChatting and cheating: Ensuring academic integrity in the era of ChatGPT.\n\nInnovations in Education and Teaching International.\n\n- Dexe, J., & Franke, U.\n\n(2020).\n\nNordic lights?\n\nNational AI policies for doing well by doing good.\n\nJournal of Cyber Policy, 5(3), 332–349.\n\n- Eggmann, F., Weiger, R., Zitzmann, N. U., & Blatz, M. B.\n\n(2023).\n\nImplications of large language models such as ChatGPT for dental medicine.\n\nJournal of Esthetic and Restorative Dentistry.\n\nContinued references list with above-mentioned sources.",
    "# Russell Group principles on the use of generative AI tools in education\n\nOur universities are committed to the ethical and responsible use of generative AI and to preparing our staff and students to be leaders in an increasingly AI-enabled world.\n\nThe rise of generative artificial intelligence (AI) has the potential for a profound impact on the ways in which we teach, learn, assess, and access education.\n\nOur universities wish to ensure that generative AI tools can be used for the benefit of students and staff – enhancing teaching practices and student learning experiences, ensuring students develop skills for the future within an ethical framework, and enabling educators to benefit from efficiencies to develop innovative methods of teaching.\n\nValuable work undertaken by organisations such as the Quality Assurance Agency for Higher Education (QAA) and Jisc has helped develop the sector’s understanding of the opportunities and considerations of generative AI, and the Department for Education (DfE) has set out its position on the use of generative AI in the pre-university education sector.\n\nRussell Group universities have contributed sector-wide insight and have been proactively working with experts to revise and develop policies that provide guidance to students and staff.\n\nCollaboration, coordination, and consistency on this issue across the education and professional sectors – including professional bodies, schools, FE colleges and employers – will be crucial.\n\nIn recognition of this, Russell Group universities have collectively developed the following principles that will guide the approach to generative AI tools across our universities and, we hope, beyond:",
    "## Universities will support students and staff to become AI-literate.\n\nStaff should be equipped to support students to use generative AI tools effectively and appropriately in their learning experience.\n\nUniversities will adapt teaching and assessment to incorporate the ethical use of generative AI and support equal access.\n\nUniversities will ensure academic rigour and integrity is upheld.\n\nUniversities will work collaboratively to share best practice as the technology and its application in education evolves.\n\nGenerative AI tools are capable of processing vast amounts of information to generate responses but they have significant limitations.\n\nIt is important that all students and staff understand the opportunities, limitations and ethical issues associated with the use of these tools and can apply what they have learned as the capabilities of generative AI develop.\n\nThese include:\n\n- Privacy and data considerations: whether a generative AI tool is designed to learn directly from its users’ inputs or not, there are risks to privacy and intellectual property associated with the information that students and staff may enter.\n\n- Potential for bias: generative AI tools produce answers based on information generated by humans which may contain societal biases and stereotypes which, in-turn, may be replicated in the generative AI tool’s response.\n\nNote: Links to resources: 1 [Jisc, 2 QAA, 3 DfE.]\n\n- Inaccuracy and misinterpretation of information: data and information contained within generative AI tools is garnered from a wide range of sources, including those that are poorly referenced or incorrect.\n\nSimilarly, unclear commands or information may be misinterpreted by generative AI tools and produce incorrect, irrelevant or out-of-date information.\n\nThis means that accountability for the accuracy of information generated by these tools when transferred to another context lies with the user.\n\n- Ethics codes: users of generative AI tools should be aware that while ethics codes exist, they may not be embedded within all generative AI tools and that their incorporation, or otherwise, may not be something that users can easily verify.\n\n- Plagiarism: generative AI tools re-present information developed by others and so there is the risk of plagiarised content and/or copyright infringement being submitted by a user as their own, and artwork used by image generators may have been included without the creator’s consent or licence.\n\n- Exploitation: the process by which generative AI tools are built can present ethical issues.\n\nFor example, some developers have outsourced data labelling to low-wage workers in poor conditions.\n\nOur universities will provide guidance and training to help students and staff understand how generative AI tools work, where they can add value and personalise learning, as well as their limitations.\n\nBy increasing AI-literacy, our universities will equip students with the skills needed to use these tools appropriately throughout their studies and future careers, and ensure staff have the necessary skills and knowledge to deploy these tools to support student learning and adapt teaching pedagogies.",
    "## Staff should be equipped to support students to use generative AI tools effectively and appropriately in their learning experience.\n\nOur universities will develop resources and training opportunities, so that staff are able to provide students with clear guidance on how to use generative AI to support their learning, assignments, and research.\n\nThe appropriate uses of generative AI tools are likely to differ between academic disciplines and will be informed by policies and guidance from subject associations, therefore universities will encourage academic departments to apply institution-wide policies within their own context.\n\nUniversities will also be encouraged to consider how these tools might be applied appropriately for different student groups or those with specific learning needs.\n\nEngagement and dialogue between academic staff and students will be important to establish a shared understanding of the appropriate use of generative AI tools.\n\nEnsuring this dialogue is regular and ongoing will be vital given the pace at which generative AI is evolving.",
    "## Universities will adapt teaching and assessment to incorporate the ethical use of generative AI and support equal access.\n\nUniversities continually update and enhance their pedagogies and assessment methods in response to drivers including new research, technological developments and workforce needs – adapting to the use of generative AI technology is no different.\n\nIncorporating the use of generative AI tools into teaching methods and assessments has the potential to enhance the student learning experience, improve critical reasoning skills and prepare students for the real-world applications of the generative AI technologies they will encounter beyond university.\n\nAppropriate adaptations to teaching and assessment methods will vary by university and discipline, and protecting this autonomy is vital.\n\nAll staff who support student learning should be empowered to design teaching sessions, materials and assessments that incorporate the creative use of generative AI tools where appropriate.\n\nProfessional bodies will also have an important role in supporting universities to adapt their practices, particularly in relation to accreditation.\n\nAs the technologies develop and new generative tools become available, elements of generative AI used within universities may reside behind paywalls or be restricted to paying subscribers.\n\nUniversities will need to consider how best to respond to a potential proliferation of such subscription tools and attempt to ensure fairness of access so that students and staff can access the generative AI tools and computing resources they need in support of their teaching and learning practices.",
    "## Universities will ensure academic rigour and integrity is upheld.\n\nAll 24 Russell Group universities have reviewed their academic conduct policies and guidance to reflect the emergence of generative AI.\n\nThese policies make it clear to students and staff where the use generative AI is inappropriate, and are intended to support them in making informed decisions and to empower them to use these tools appropriately and acknowledge their use where necessary.\n\nSuch clear and transparent policies are critical to maintaining consistent and high standards of learning, teaching and assessment across Russell Group universities.\n\nEnsuring academic integrity and the ethical use of generative AI can also be achieved by cultivating an environment where students can ask questions about specific cases of their use and discuss the associated challenges openly and without fear of penalisation.",
    "## Universities will work collaboratively to share best practice as the technology and its application in education evolves.\n\nNavigating this ever-changing landscape will require collaboration between universities, students, schools, FE colleges, employers, sector and professional bodies, with the ongoing review and evaluation of policies, principles and their practical implementation.\n\nOur universities will regularly evaluate policies and guidance for staff and students relating to generative AI tools and their impact on teaching, learning, and assessment practices.\n\nThis will include monitoring the effectiveness, fairness, and ethical implications of the integration of generative AI tools into academic life, and adapting policies and procedures to ensure they remain valid as generative AI technologies evolve.\n\nFostering relationships between higher education institutions, schools, employers, professional bodies who accredit degrees, AI experts, leading academics and researchers, as well as ensuring an inter-disciplinary approach to addressing emerging challenges and promoting the ethical use of generative AI, will be crucial.\n\nRussell Group universities recognise the challenges that lie ahead and will continue to value the input of others, along with contributing expertise to the national and international discussions around generative AI and its applications within teaching, learning, assessment and support.",
    "## Introduction\n\nAs you ponder the possibilities of generative AI in your specific teaching context, both for good and ill, there are many matters to reflect on and address, including the following: course policies regarding the use of generative AI; the legal and ethical appropriateness of using generative AI for teaching and learning; the accuracy of the content and views expressed by generative AI; and the pedagogy of using generative AI.\n\nBelow you will find seven important issues or considerations for each of us to keep in mind in our teaching as we respond to generative AI:\n- Course policies - student use of AI\n- Plagiarism detection\n- FERPA, privacy, and choice\n- Accuracy and biases of output\n- Reinvigoration of your teaching and students’ learning\n- Cultivation and growth of 21st-century skills\n- Effective prompting through prompt engineering",
    "### Summary\n\nMany have said artificial intelligence will disrupt many aspects of society.\n\nWith generative AI, these disruptions may extend to our ideas and assumptions on academic integrity Whatever expectations or policies regarding the use of generative AI a faculty member ultimately adopts and uses in teaching a course, students should come to receive and understand them clearly and early in the course, through explicit syllabus statements and discussions in class.\n\nInstructors may like to consider inviting students to contribute to the formulation of these expectations and policies.",
    "### Academic Integrity and Policies\n\nOne of the most dominant concerns regarding generative AI is its effects on academic integrity.\n\nWith tools like ChatGPT able to compose meaningful and clear work that students can submit as their own, many may wonder and worry about whether generative AI may account for much of the content appearing in submissions.\n\nA recent survey indicates that most institutions and departments have not yet developed guidelines and policies regarding when and how students can use generative AI, if at all (Surovell, 2023).\n\nGiven the widespread lack of institutional guidance, faculty members have faced the task of articulating and describing their own policies.\n\nAs per the Standard of Conduct, the University of Missouri System has decided to leave to each individual faculty member or their departments the decision of whether students can use generative AI in their coursework.\n\nThe default policy is that if an instructor does not explicitly allow the use of generative AI, it is forbidden.\n\nWith this policy, the University of Missouri System has decided that each individual instructor is best equipped to judge and decide what role generative AI should play in students’ learning, if any.\n\nConsider your own teaching context: What policies involving generative AI would ensure students achieve the set learning outcomes and objectives and build 21st-century skills, such as creativity and communication (Vivekanandan, 2019), while still being realistic and offering opportunities for students to learn new technologies?\n\nWhatever policy is adopted, a faculty member may like to at least integrate the following two processes into the instruction.\n\nThese processes will give students explicit guidelines regarding the use of generative AI in their studies:\n\n- **Syllabus statements**: The syllabus for a class should include a section discussing generative AI and indicate any standards or guidelines for both appropriate and inappropriate use thereof.\n\nThis section should clearly define the border between the two to ensure students understand what the instructor expects.\n\nIt is essential to identify and describe the contexts (when, where, how, why, and for what) students can use generative AI in the course.\n\n- **A discussion early in the course**: In addition to developing a syllabus statement, instructors should engage students in a conversation about generative AI.\n\nRather than lecturing them, invite them to share how they could most effectively use generative AI while maintaining responsibility for and actively participating in their learning.\n\nSomething that you may like to do is use the discussion with students to generate the syllabus statement.\n\nIn other words, you could collaborate with students in each course you teach to develop the expectations and policies surrounding the use of generative AI.\n\nInvolving students in the process of determining these policies (and perhaps others in the course) “fosters student ownership in the classroom and responsibility among the students for their behaviors” (Nollmeyer, 2018).",
    "### Sample Syllabus Statements\n\nAs you consider your own policy to include in your syllabus, please rest assured: Various syllabus statements about generative AI have come to exist and become publicly available across higher education (Texas A&M University Center for Teaching Excellence).\n\nThese syllabus statements, which individual faculty members have developed in the absence of institutional mandates, range from the absolute restriction of generative AI to the enthusiastic embrace of it.",
    "#### No Use Permitted Whatsoever\n\n“Intellectual honesty is vital to an academic community and for my fair evaluation of your work.\n\nAll work submitted in this course must be your own, completed in accordance with the University’s academic regulations.\n\nYou may not engage in unauthorized collaboration or make use of ChatGPT or other AI composition software” (Princeton University).\n\n“Since writing, analytical, and critical thinking skills are part of the learning outcomes of this course, all writing assignments should be prepared by the student.\n\nDeveloping strong competencies in this area will prepare you for a competitive workplace.\n\nTherefore, AI-generated submissions are not permitted and will be treated as plagiarism” (The University of Iowa).",
    "#### Use Permitted Under Certain Circumstances and in Certain Contexts\n\n“We recognize that there are a variety of AI programs available to assist writers.\n\nAI programs are not a replacement for human creativity, originality, and critical thinking.\n\nWriting is a craft that you must develop over time to develop your own individual voice as a writer.\n\nHowever, within limited circumstances, and with proper attribution, AI programs may be used as a tool” (Bryant University).\n\n“There are situations and contexts within this course where you will be asked to use AI tools to explore how they can be used.\n\nOutside of those circumstances, you are discouraged from using AI tools to generate content (text, video, audio, images) that will end up in any student work (assignments, activities, responses, etc) that is part of your evaluation in this course.\n\nAny student work submitted using AI tools should clearly indicate what work is the student’s work and what part is generated by the AI.\n\nIn such cases, no more than 25% of the student work should be generated by AI.\n\nIf any part of this is confusing or uncertain, please reach out to me for a conversation before submitting your work” (University of Colorado).",
    "#### Broader Use Accepted Within Certain Guidelines\n\n“AI is allowed with attribution: Use of AI tools, including ChatGPT, is permitted in this course for students who wish to use them.\n\nTo adhere to our scholarly values, students must cite any AI-generated material that informed their work (this includes in-text citations and/or use of quotations, and in your reference list).\n\nUsing an AI tool to generate content without proper attribution qualifies as academic dishonesty” (University of Massachusetts Amherst).\n\nUse of AI tools, including ChatGPT, is permitted in this course for students who wish to use them.\n\nTo be consistent with our scholarly values, students must cite any AI-generated material that informed their work and use quotation marks or other appropriate indicators of quoted material when appropriate.\n\nStudents should indicate how AI tools informed their process and the final product, including how you validated any AI-generated citations, which may be invented by the AI.\n\nAssignment guidelines will provide additional guidance as to how these tools might be part of your process for each assessment this semester and how to provide transparency about their use in your work” (The University of Iowa)\n\nYou will find further sample syllabus statements from Temple University and the University of Tennessee, Knoxville.\n\nFor more information about how students could cite generative AI tools, please review this page from the Modern Language Association and this page from the American Psychological Association.",
    "### Summary\n\nOne of the more common responses to generative AI so far has been to use detection software.\n\nHowever, such tools present issues and challenges.\n\nWe may need to reconceptualize and transform our ideas about plagiarism and academic honesty in response to generative AI.\n\nMissouri Online offers a sprint, a short asynchronous professional development opportunity for faculty, on academic integrity that considers the causes of cheating and how to prevent it, but here we will primarily consider AI-plagiarism detection software and what it means for learning and teaching.",
    "### AI Detection Software\n\nRather than adopting and implementing clear guidelines for the use of generative AI, faculty members may seek a policy of restricting and forbidding its use.\n\nIn addition, they may resort to AI detection software (Caren, 2023).\n\nWhile these tools may offer a seemingly convenient solution to the question of academic integrity in this new age, they also present issues:\nSuch solutions may identify false positives, wrongly imperiling some students’ academic studies (Fowler, 2023).\n\nThe formulaic writing style of non-native writers as they develop CALP, cognitive academic language proficiency, may resemble the writing style of generative AI, leading to further false positives (Liang et al., 2023).\n\n“New AI language models are more powerful and better at generating even more fluent language, which quickly makes our existing detection tool kit outdated” (Heikkilä, 2023).\n\nYou may learn more about issues with AI detectors here.\n\nGiven all this, “there is little actual scientific evidence to show that AI-generated text can be effectively detected” (Eaton, 2023).\n\nWith the advent of generative AI and the problems with detecting it, we must perhaps evolve and change our ideas of what constitutes academic integrity and plagiarism.\n\nFor a long time, academic integrity involved writing original content and citing any other sources used.\n\nThe expectation with academic writing has been to respect and acknowledge the work of others as distinct while also presenting one’s own insights and conclusions.\n\nIn other words, academic integrity has generally involved treating ideas with an individualistic attitude: They are the property of a single individual or group of individuals.",
    "### Reconsidering Plagiarism and Academic Honesty\n\nThis definition of academic integrity becomes more problematic once one encounters the chimera that is a student’s work produced with assistance and contributions from generative AI.\n\nWith the power of generative AI, Eaton (2023) describes the possibility of academia needing to transition into a postplagiarism world.\n\nBelow you will find an infographic she produced to describe the six characteristics of a postplagiarism world.\n\nPlease view the above image in higher resolution here.\n\nAs provocative as the idea of a postplagiarism world may seem, it remains clear that we must adjust our approach to and provide more clarification about academic integrity.\n\nStudents are likely more confused than we are about these expectations, in which the slightest and most innocuous use of generative AI (to revise and edit a draft written entirely by the student) could constitute academic misconduct.\n\nThe conversation needs to change.",
    "### Supporting Students in This Brave New World\n\n“We argue that it is time to shift this narrative in favor of one highlighting a distributed accountability when it comes to academic misconduct—that is, leaders, administrators, educators, and students are to share the responsibility.\n\nEducational bodies and institutions should therefore allocate adequate resources to support staff and students deal effectively with Generative AI-related challenges and optimize opportunities presented by its tools” (Lim et al., 2023, p. 9).\n\nRegardless of one’s own response to Eaton’s six tenets (2023) and the concept of “distributed accountability” proposed by Lim et al.\n\n(2023), it is clear we must all anticipate and respond to the use of generative AI by our students.\n\nWhether we approve or disapprove of any use of generative AI and whether we plan to use detection software or not, we must communicate to our students our expectations and policies.",
    "#### Scenario\n\nStudents have submitted their final research essays for the semester in a course on the economics of globalization.\n\nThis essay, in which they must present and analyze a positive or negative economic effect of globalization, is one that many students decide to ask ChatGPT to outline for them, Grammarly to proofread for them, and Quillbot to refine for them.\n\nHowever, students still have conducted all the research and thoroughly engaged with the content.\n\nOnce these students submit their work, you receive an alert that their work has been identified as possibly being generated by AI.\n\nHow do you respond to this information?\n\nIf you learned of the process they used, would you consider their work plagiarized or not, and why would you take such a position?\n\nRed light (unacceptable): It is clear students have offloaded much of the “thinking” for this assignment to generative AI.\n\nTherefore, the work they have submitted does not meet your standards and expectations for their learning.\n\nYellow light (uncertain/ambiguous): While you do not mind the use of generative AI for stages later in the writing process (revision and editing, for example), you believe students should still take ownership of and be responsible for higher-order considerations in their work (content and ideas, for example).\n\nTherefore, you may like to discuss with them appropriate use cases and inappropriate use cases with generative AI.\n\nGreen light (acceptable): Students are still “learning” in your eyes: They are engaging with the content and receiving guidance and feedback from a partner in the writing process.\n\nThough generative AI may play a role in the workflow of students, it is clear they are still achieving the learning outcomes.",
    "### Summary\n\nThough such new technologies as ChatGPT and Claude could play a role in the workflow of both students and the instructor, it is important to keep the law in mind.\n\nOne specific matter is ensuring that the work of students is not shared with third parties without their consent.\n\nAs per the Family Educational Rights and Privacy Act, parties without a legitimate academic interest should not receive access to students’ academic records without approval.\n\nTherefore, instructors should not:\nRequire students to use third-party generative AI tools.\n\nUpload or copy and paste the work of students into generative AI tools without consent from each individual student.\n\nIf you give students the option to use generative AI as part of their learning, please also design your instruction such that students who decline to use such services can enjoy and benefit from a comparable learning experience.",
    "### Background and Context\n\nUnder FERPA, or the Family Educational Rights and Privacy Act, educational institutions receiving federal funding must ensure the privacy of students’ academic records.\n\nSuch institutions of learning “must have written permission from the…eligible student in order to release any information from a student's education record” (U.S. Department of Education, n.d.).\n\nIt is possible that requiring students to use ChatGPT may share academic records and other PII, or personal identifying information, with OpenAI and its third-party vendors.\n\nThe same may also hold true for other generative AI applications.\n\nUnless the user has disabled chat history with the service (OpenAI, 2023), any information shared with ChatGPT will contribute to its evolving LLM (large language model), the corpus of text that it uses to generate responses to human prompts.\n\n“What we do know is that any information shared with ChatGPT or other OpenAI programs in an educational setting can then be shared by that program elsewhere” (Turner, 2023).",
    "### Principles to Keep in Mind\n\nIn light of the legal requirements of FERPA and the uncertainty surrounding precisely what happens with the data shared with OpenAI or other companies, there are two fundamental principles to keep in mind:\n- Faculty should refrain from mandating that students use generative AI for an activity or assignment.\n\nWhile you can certainly allow and invite students to use generative AI in your directions, you should not require students to use these services, nor should you penalize them for declining to do so.\n\nIn addition, you should clearly communicate to and discuss with students the inherent privacy risks associated with using these tools for learning purposes.\n\n- Faculty should avoid using any content from, or discussing any specific concerns about or communications with, students with generative AI without the students’ explicit consent.\n\nIt is also important for faculty to remain mindful of and abide by FERPA while using generative AI for teaching purposes.\n\nAfter all, research has found educators use ChatGPT more than students do (The Walton Family Foundation, 2023).\n\nFaculty may turn to generative AI for a variety of purposes: to give feedback on the results section of a Master’s thesis, to compose an email response to an undergraduate student asking for an extension on assignment due to a family emergency, and more.\n\nIn these situations, by copying and pasting students’ work or communication with you into a separate generative AI tool, you are violating FERPA: You are disclosing students’ academic records and/or PII to a third party to which the student did not consent.\n\nIt is essential that instructors refrain from importing students’ work or communication into generative AI, from using third-party generative AI tools to analyze and give feedback on students’ work, and from sharing academic records and/or PII about students to expedite their workflow.",
    "### Ensuring Equity and Student Choice\n\nFor students who decline to use generative AI for one reason or another, instructors should ensure equity and universal design in the learning experience.\n\nIn other words, if students using tools like ChatGPT or Google Bard would receive a considerable academic advantage from using generative AI, it is essential you craft and integrate avenues for other students to enjoy similar opportunities.\n\nFor example, you could teach an art history course and direct students to use DALL-E to produce artificially generated artwork and then share them in a discussion forum to review and discuss the characteristics and hallmarks of various movements, such as Impressionism or Cubism.\n\nIn this event, you must offer students a similarly effective avenue for reviewing the schools virtually, such as by downloading and sharing images from elsewhere.\n\nIf you are to ask students to use generative AI as part of their learning, please also consider the pricing associated with some of the services.\n\nFor example, only the paid version of ChatGPT uses the most recent model of GPT, GPT-4, while the free version uses an older version.\n\nThough the paid version may offer access to a more robust and powerful LLM, it costs twenty dollars a month, something that can come to constitute a considerable expense for students.\n\nGiven rising concerns about the affordability of a college education, you are advised to refrain from mandating the use of paid tools.\n\nPlease also note that faculty are advised and encouraged to only ask students to use the technology tools vetted and approved by the University of Missouri System for learning and teaching purposes.\n\nShould you mandate that students make use of another application or service that has not received approval, you accept the risks and potential liability in the event of a data breach or similar event.\n\nWith Simplified Tuition, you may not be able to require students to purchase unauthorized software.",
    "#### Scenario\n\nYou teach an introductory computer science course and have directed students to ask ChatGPT to generate the HTML code needed to create, maintain, and update a professional portfolio, which will serve as the final assignment.\n\nStudents will then analyze, critique, and update the code produced before building their own HTML portfolio.\n\nA student privately messages you her concerns about using this service: Uncertain what ChatGPT does with the data she shares, she worries about the risks.\n\nHow do you respond to these concerns she has raised?\n\nShould you inform her she is exaggerating the privacy and safety risks of using ChatGPT and expect her to proceed to use the service?\n\nShould you acknowledge her reservations and offer an alternative activity?\n\nWhat should you do?",
    "### Summary\n\nThough many of us may treat and view generative AI as an “expert” due to the datasets on which it has been trained, it is not: The internet, which often serves as the source of much if not all of a tool’s LLM, is not intrinsically an expert on any topic, as we all know.\n\nGenerative AI can only generate content based on patterns it has detected, and it cannot appraise or determine the accuracy or social impact of what it creates.\n\nOnly humans are capable of fully assessing the quality of material and of empathizing with others.\n\nOnly we can understand.\n\nTherefore, in integrating and using generative AI as part of students’ learning and workflow, you should take the time to encourage and scaffold for students the critical engagement and use of these tools.\n\nThere are two fundamental and necessary components of such critical engagement and use:\n- AI literacy skills, which includes prompt engineering\n- Social awareness and engagement (to identify biases and prejudices toward various groups)\n\nThe latter concern does not receive much attention in discussions about teaching students to use generative AI.\n\nHowever, developing this awareness will enable students to not only more effectively and comprehensively assess and judge the output of generative AI but will also foster and invite the development of empathy, something currently in decline (American Psychological Association, 2019) but increasingly needed in our complex and dynamic society.",
    "### Accuracy of Output\n\nGenerative AI can create text based on what it has learned through its large language model.\n\nHowever, as the case study above demonstrates, it cannot assess the accuracy, validity, and truthfulness of the content it produces.\n\nAs ChatGPT itself observed in discussing what generative AI cannot do, it cannot:\n- Guarantee complete accuracy and reliability in generated content.\n\n- Distinguish between real and generated content in all cases.\n\nGenerative AI merely uses its LLM to produce text relevant to the prompt from users.\n\nOften that output uses correct and true information borrowed from the web.\n\nSometimes, though, that output contains “hallucinations.” The Kennesaw State University School of Data Science and Analytics (2023) states, “For the most part, when people talk about an AI hallucination, they mean that a generative AI process has responded to their prompt with what appears to be real, valid content, but which is not.” Hiller (2023) found that in response to a prompt about teaching with technology in a science class, ChatGPT produced six citations, five of which were fake.\n\nThis pattern of numerous false citations likely persists in the work generative AI may produce for students.\n\nThough it is possible that generative AI will improve and gain effectiveness against producing hallucinations, for the time being, faculty should teach students AI literacy skills.\n\nKey among these is the ability to analyze and critique the output from generative AI for quality, accuracy, and correctness.\n\nIf students are working on a research project, such as the legal brief assignment described in the scenario below, it is important to ask and remind students to corroborate and check any citations or studies presented by generative AI.\n\nSome argue that because of the possibility of hallucinations, students should not exclusively rely on generative AI for sources and citations for an assignment (Wellborn, 2023).\n\nInstead, students should review and check any citations or summaries created by generative AI for accuracy.\n\nThough generative AI may provide a basis for the discussion at hand, students should corroborate and expand on it.\n\nThe technology is rapidly progressing in this area (Tay, 2023).",
    "### Biases of Output\n\nGenerative AI simply analyzes a prompt and then “parrots” an answer back from its LLM (large language model).\n\n“The model (for ChatGPT) was trained using text databases from the internet.\n\nThis included a whopping 570GB of data obtained from books, webtexts, Wikipedia, articles and other pieces of writing on the internet.\n\nTo be even more exact, 300 billion words were fed into the system” (Hughes, 2023).\n\nIf some biased or prejudiced sources are present among the training data and contribute to the construction of the LLM, then those biases or prejudices may appear once again in the output from generative AI.\n\nIt is also important to note that not including substantial content from or with certain perspectives or demographic groups in the training data may also produce biased content.\n\nSuch content “can manifest in a myriad of ways, ranging from gender bias, racial and ethnic bias, socioeconomic bias, cultural bias, content bias, and ideological bias in terms of political, philosophical, and religious perspectives” (Trivedi, 2023, p. 23).\n\nUltimately, generative AI may reinforce and perpetuate social marginalization.\n\nThe research so far has established two relevant and urgent concerns:\n\n- **Marginalization of conservative perspectives in the viewpoints generated by LLMs**: “Models trained on the internet alone tend to be biased toward less educated, lower income, or conservative points of view.\n\nNewer models, on the other hand, further refined through curated human feedback tend to be biased toward more liberal, higher educated, and higher income audiences” (Myers, 2023).\n\nFor example, the latest GPT model (GPT-4) relies on and uses RLHF (Reinforcement Learning from Human Feedback) (Malhotra, 2023).\n\nBecause of this, conservatives have voiced and raised concerns about the potential bias displayed in the output of generative AI.\n\nChatGPT has been found to display a “pro-environmental, left-libertarian ideology” (Hartmann et al., 2023).\n\nBy asking questions related to the political compass test, Rutinowski et al.\n\n(2023) reached a similar conclusion: “ChatGPT seems to hold a bias towards progressive views” (p. 1).\n\nA study by Santurkar et al.\n\n(2023) also determined that the perspectives expressed by LLMs sharply diverged from those held by various demographic groups in the United States.\n\n- **Dated and dangerous stereotypes based on gender and race, especially in images generated by AI**: These concerns about bias also extend to artificially generated images.\n\nA Bloomberg analysis (Nicoletti & Bass, 2023) found that Stable Diffusion, an image generator driven by artificial intelligence, placed men with lighter skin tones in higher-paying jobs and women and individuals with darker skin tones in lower-paying or domestic jobs.\n\nSocial stratification in these images was found to be higher than exists in the data from the Bureau\n\nof Labor Statistics.\n\nIn addition, “more than 80% of the images generated for the keyword ‘inmate’ were of people with darker skin, even though people of color make up less than half of the US prison population, according to the Federal Bureau of Prisons” (Nicoletti & Bass, 2023).\n\nThe research has established that generative AI may create content with bias across multiple modalities.\n\nTherefore, faculty should draw students’ attention to these issues and invite students to appraise and review generative AI’s output for biased viewpoints or inaccurate and harmful stereotypes.\n\nStudents should approach the content they receive from generative AI with a critical and socially engaged eye.\n\nWith conservatives often expressing concerns about their views being marginalized on college campuses and with students becoming more diverse and non-traditional as society changes, it is important for us to treat them all with respect and to acknowledge when generative AI may fail them.",
    "#### Scenario\n\nYou teach an online course on constitutional law for the University of Missouri School of Law.\n\nYou have asked students to select a clause within a constitutional amendment of their choice, generate a hypothetical legal case, and then compose a corresponding legal brief describing the relevant precedents associated with the clause and/or amendment and established by the United States Supreme Court.\n\nThis legal brief must cite specific decisions, discuss how the Supreme Court’s understanding of the clause and the amendment may have evolved over time, and develop an argument about how this understanding applies to and informs the case at hand.\n\nYou receive a legal brief from a student about the Equal Protection Clause of the Fourteenth Amendment, discussing how its role in society has expanded from its passage to the present day.\n\nHowever, as you review the brief, you notice something: In addition to including cases like Brown v. Board of Education, Obergefell v. Hodges, and others, the legal brief contains citations for cases you do not recognize at all.\n\nYou begin doing research and realize these cases do not exist.\n\nThey never happened.\n\nConcerned, you confront the student.\n\nOnce you do so, the student admits that she relied on ChatGPT to write a good portion of the legal brief and did not bother to check if all the included cases were actually real.\n\nYou decide that while the student will lose points on the “relevant case law” section of the rubric, this could serve as a learning experience for her and others.\n\nWhat would you like to tell her and her fellow students?\n\n*Note: Actual events inspired this scenario.\n\n*",
    "### Summary\n\nGenerative AI will and should challenge all of us to change how we teach and how students learn.\n\nWith new technologies, the opportunity emerges for us to innovate and experiment, creating fresh and engaging learning experiences for students.\n\nIf properly designed, structured, and implemented, the use of generative AI by students could potentially contribute to and enrich their learning.\n\nAs we consider the future of teaching and learning with generative AI, please keep in mind the following:\n- Our role in the classroom, physical or virtual: Heeding the trite, either/or dichotomy of the “guide on the side” versus the “sage on the stage,” we should practice humility, take a both/and approach, and remain open-minded about the role we may come to play in students’ learning experience in this new world filled with AI and its contributions to education.\n\n- The importance of focusing on the journey: Using a process-centered approach to teaching and learning will guarantee that we design potent learning experiences.\n\nBy concentrating more on the journey, we can offer more structured, meaningful, and active activities and assignments for students that foster metacognition and reflection, encourage students to take ownership of their learning, allow for more formative assessments, and ensure students do not come to depend on generative AI exclusively.",
    "### Divergent Metaphors of Teaching and Learning\n\nIt is among the most pervasive cliches in teaching: We should aspire to serve as a “guide on the side” rather than the “sage on the stage.” With the advent of the internet, students could obtain information and knowledge with ease and convenience, and so teachers faced the challenge of moving from delivering knowledge to creating and sustaining learning experiences.\n\nNow that generative AI has emerged, we must further embrace that role as a guide on the side.\n\nAfter all, generative AI can:\n- Step beyond search engines, which merely present available content and information of relevance to a topic, to synthesize and produce content and information.\n\n- Offer feedback and clarification on content and information.\n\nNotwithstanding concerns about accuracy, if correctly guided and prompted, generative AI could effectively teach a student about the causes of the decline of the Roman Empire or produce an essay about the differences between dark matter and dark energy.\n\nIt seems that with delivering information, evaluating work, and creating content, generative AI could potentially replace the work of the teacher and student.\n\nFor this reason, much of the conversation about generative AI approaches learning as a sort of factory in which the teacher offers information and instructions in a top-down manner and students must work in isolation to manufacture a product.\n\nHowever, with the potential of generative AI, we perhaps can no longer view ourselves as experts depositing knowledge in the minds of students, demonstrating and adhering to what Paulo Freire (1993) calls a “banking model of education.” Instead of conforming to such a model, in which we serve as the “sage on the stage,” we should view ourselves as conductors, “guides on the side,” facilitating a symphony of learning, in which students use instruments that include but are not limited to generative AI to create something beautiful and memorable.",
    "### Why Educators Still Matter\n\nThough theoretically valid, concerns about generative AI supplanting the work of teaching and learning ultimately approach students with a limited conceptualization of what takes place in the physical or virtual classroom.\n\nGenerative AI cannot perform certain tasks and cannot accomplish certain hallmarks of good teaching.\n\nPlease review what generative AI cannot do and identify where educators could still play a role in the learning experience of students.\n\nUnlike (and according to) generative AI, only educators can offer students the following:\n- Intuition and empathy: Human teachers can establish emotional connections with their students, creating a supportive and empathetic learning environment.\n\nThey have the capacity to sense and respond to the emotional and social dynamics within a classroom.\n\nThey can gauge the mood of the students, detect signs of distress, and provide appropriate support.\n\nThis intuition and empathy are essential for fostering a positive and inclusive learning environment.\n\n- Creativity and improvisation: Teachers often need to think on their feet, adjusting their lesson plans or explanations in real-time based on students' reactions and questions.\n\nThey can employ creative teaching techniques, analogies, and examples to enhance understanding and engagement, tailoring their approach to the specific needs of the students.\n\n- Motivation and inspiration: Human teachers can inspire students and instill a passion for learning.\n\nThrough their own enthusiasm, storytelling, and real-life experiences, they can ignite curiosity and encourage students to explore subjects beyond the curriculum.\n\n- Mentorship and guidance: Human teachers often serve as mentors and role models for students, offering guidance not only in academics but also in personal growth, career choices, and character development.\n\nThey can provide valuable advice, share wisdom, and nurture the holistic development of their students.\n\n- Social and interpersonal skills: Teaching involves social interactions, and human teachers possess the ability to navigate and facilitate these interactions effectively.\n\nThey can foster collaboration, communication, and teamwork among students, promoting social skills and emotional intelligence.\n\n- Ethical decision-making: Teaching involves making ethical decisions in various situations, such as handling sensitive topics, addressing student behavior, and respecting diverse perspectives.\n\nHuman teachers can exercise judgment, empathy, and moral reasoning to navigate these complexities.\n\nIn other words, they can engage in critical thinking and reflection (reflection will also play a critical role in how students will learn with generative AI).",
    "### Process-Centered Teaching\n\nEducators can demonstrate why they matter still in the classroom by taking a process-centered approach, in which we demonstrate an investment in the learning journeys of students.\n\nOne of the most common concerns with generative AI’s impact on education is that students will rely on it to produce much if not all of the product they submit.\n\nIf a student is asked to write a research essay or some code to execute a task, he or she may turn to ChatGPT or Bing for some of the content, only to then expand on and change some of it.\n\nWith generative AI playing a significant role in the ultimate submission, educators naturally have concerns about the extent to which students have engaged with the assignment and concepts at hand and achieved the learning outcomes set.\n\nNotice: We are focusing on the ultimate work the student turns in for a grade and nothing that has taken place beforehand.\n\nThe conversation about generative AI often focuses on the destination.\n\nEducators express concern that by using generative AI, students will “teleport” to the destination without doing the hard work of actually journeying cognitively toward it.\n\nHowever, underlying and unacknowledged in these worries is that educators may often focus exclusively on the destination in their pedagogy and not guide and facilitate students’ journeys in reaching it.\n\nIt is a universal cliche: Life is about the journey and not the destination.\n\nThe same applies to learning and the work we would like to see from students.\n\nGenerative AI may challenge us to focus more on the journey of students.\n\nIn other words, we may adopt a process-centered approach rather than a product-centered one in our teaching.\n\nWhat are the differences between product-centered teaching and process-centered teaching?\n\nPlease review the table below, adapted from White (1991), that discusses how these approaches diverge from one another in their pedagogy, their epistemology, their curricula, and their power dynamics.",
    "#### Differences between Product-Centered and Process-Centered Teaching\n\nYou may notice that adopting a process-centered approach also calls for you to reconceptualize and reinvigorate our role and approach as an educator.\n\nHowever, you may wonder: How exactly may product-centered and process-centered teaching affect how students use generative AI?\n\n- Product-centered teaching and generative AI’s ensuing role in learning: It is important to note here that for many major assignments, the instructor may provide an introduction to it for the class, answer some questions through a Q & A, and then leave students to work on and complete it in isolation until the due date.\n\nFocusing primarily on the product that students will submit, this approach neglects to provide students the scaffolding and structure to move forward and thrive with the project.\n\nThis absence of structured guidance to facilitate movement forward leaves students on their own and the door open to the potentially excessive use of generative AI, impacting students’ learning.\n\nAfter all, generative AI can create the entire product in question with some guided prompting.\n\n- Process-centered teaching and generative AI’s ensuing role in learning: With a more process-centered approach, in which the teacher accompanies, guides, and equips students in their learning, students will engage in more deliberation and contemplation about their work.\n\nIn addition, students will experience and enjoy numerous activities and interactions that facilitate their progression through their learning and foster metacognition.\n\nAfter all, “process is much harder to fake” (Dietz & Keys, 2023).\n\nThough it is likely (and acceptable) that students will turn to generative AI throughout the process, students will not rely on but rather collaborate with tools like ChatGPT and Duet AI.",
    "### Instructional Strategies for Process-Centered Teaching\n\nSome strategies to cultivate a process-centered approach that would accommodate and acknowledge the role generative AI could play, include:\n- Reflections and discussions on one’s personal experience with and attitudes toward a topic throughout (before, while, and after working on a project)\n- Gathering and synthesis of research on a topic to identify themes and insights that generative AI may lack (annotated bibliography, etc.)\n\n- A tool like PowerNotes could support and facilitate this process well.\n\nPlease note that PowerNotes is currently only available for the University of Missouri flagship campus in Columbia.\n\n- One-on-one conferences with students to discuss a project\n- Critiques of the output from generative AI for various stages in the process (brainstorming, outlining/planning, drafting, revising, and editing)\n- Expression across multiple modalities (outlining or planning through creating a slideshow or infographic, doing peer review through podcasts, etc.)\n\n- Other activities (KWLs, etc.)\n\nand classroom assessment techniques (CATs), or formative check-ins (Iowa State Center for Excellence in Learning and Teaching, n.d.)\n- KWLs: Students identify what they know and what they want to know at the beginning of a unit or project.\n\nAt the end of the unit or project, students then articulate and describe what they have learned.",
    "**The Problem**\n\nThroughout your career so far, you may have used a top-down approach to teaching environmental biology: deliver lectures and then assess how well students have learned the content through exams.\n\nThis approach concentrates on the product (the achievement of the learning outcomes) and not on the process (the students’ engagement with and exploration of contemporary issues at the intersections of biology and sustainability).\n\nYou have relied on a familiar and comfortable routine for your curriculum for years now: deliver content, whether by traditional lecture for face-to-face sections and through video for asynchronous online sections, and ask students to reflect on and discuss the concepts at hand, either through think-pair-shares or discussion forums.\n\nYou then ask students to complete open-book assessments, such as quizzes or tests, to demonstrate their learning.\n\nIt has come to your attention that a significant portion of your students are now using ChatGPT for assistance during these open-book exams, even though you have told them to rely on the digital textbook and authoritative websites.\n\nYou believed offering open-book assessments would allow students to focus on higher-order thinking about matters involving biodiversity and sustainability, but it now seems your students may no longer bother to engage with the concepts as deeply or critically as they previously had.\n\nInstead, students are now merely relying on ChatGPT for information about the sixth mass extinction and the Anthropocene, information they then build on or modify to answer the essay questions.\n\nYou begin to wonder: Should you change how you teach, and, if so, how?",
    "**The Solution**\n\nYou decide to reduce your reliance on traditional, unidirectional teaching practices, such as the lecture, partially.\n\nWith the cornucopia of information available on the internet and through generative AI, students can locate and find information and ideas about the sixth mass extinction and other topics.\n\nApproaching your work as a guide on the side rather than the sage on the stage, you create structured opportunities for students to use the internet and generative AI to learn more about, create content regarding, and reflect on these matters in small groups.\n\nStudents can then share their findings, insights, and learning with the remainder of the class through presentations and other strategies.\n\nAltogether, the change you have implemented is embracing inquiry-based learning: You have changed the class such that it now frequently challenges students to learn about a specific topic or issue, either individually or collaboratively, and then share their findings with the class.\n\nStudents are invited and encouraged not only to use insights from generative AI but also to corroborate and expand on them through finding and integrating relevant academic research.",
    "### Summary\n\nAs we design learning experiences that allow and accommodate the use of generative AI, we must remember to reinforce and build students’ skills in communication and collaboration.\n\nWe must also encourage and invite creativity and innovation through offering more choices, more modalities, and more open-ended assignments and activities.\n\nOne last important consideration in this regard is that we should strive to summon students to higher levels of Bloom’s Taxonomy.\n\nWith a well-designed curriculum, generative AI could support students’ in learning to communicate more effectively, in stretching their imaginations and creative muscles, and in facilitating higher-order thinking.",
    "### Defining, Teaching, and Assessing 21st Century Skills\n\n“To participate effectively in the increasingly complex societies and globalized economy that characterize today’s world, students need to think critically, communicate effectively, collaborate with diverse peers, solve complex problems, adopt a global mindset, and engage with information and communications technologies, to name but just a few requirements” (Vivekanandan, 2019).\n\nWhile we may not explicitly account for these 21st-century skills in developing our learning outcomes and curriculum, we should consider how our teaching will cultivate and contribute to the growth of them.\n\nHowever, the question may naturally and inevitably arise: How can we ensure students grow in these skills and do not offload the work of communication, creation, and thinking critically to tools like ChatGPT, Claude, or Duet AI in full?\n\nAuthentic or alternative assessments and activities can play a substantial role in ensuring students still practice and build these 21st-century skills while potentially using generative AI.\n\nMany different authentic or alternative assessments and activities exist and may help with meeting the challenges and answering the pedagogical questions posed by generative AI.\n\nIn addition, there are some authentic or alternative assessments and activities that can deliberately and conscientiously call for the use of AI.\n\nBelow you will find some suggestions for authentic or alternative assignments and activities that both include the use of generative AI and ensure students remain active in their learning and practice some key 21st-century skills, including communication/collaboration, creativity/innovation, and critical thinking.\n\nAfter we consider these suggestions, we will relate them to a scenario of teaching an environmental biology course.",
    "### Communication and Collaboration\n\nHere are some examples of assignments and activities, suggested by ChatGPT.\n\nTeachers can use these strategies and others to foster and invite communication and collaboration among students, who may leverage generative AI as part of their learning and workflow.\n\n- **Collaborative writing**: Assign students a writing task where they work together to create a piece of written content related to their discipline.\n\nThey can use generative AI to generate ideas, prompts, or even assist in the writing process, taking turns contributing and editing the content.\n\n- **Presentation and debate**: Assign students a topic or issue within their discipline, and have them prepare and deliver presentations to the class.\n\nAfter each presentation, encourage a debate where students can ask questions, challenge viewpoints, and engage in constructive discussions using generative AI-generated content as reference material.",
    "### Creativity and Innovation\n\nHere are some examples of assignments and activities, suggested by ChatGPT.\n\nTeachers can use these strategies and others to cultivate and encourage creativity and innovation among students, who may leverage generative AI as part of their learning and workflow.\n\n- **Content creation using multimedia**: Instead of traditional essays or presentations, encourage students to produce content with multimedia created with generative AI tools.\n\nThey can incorporate AI-generated text and visuals, interactive elements, and multimedia components to communicate their ideas creatively, capturing the attention and imagination of the audience.\n\nYou will find a discussion of Microsoft Designer, one tool that would support this, here (Dvorak, 2023).\n\n- **Data visualization with AI**: Task students with creating dynamic and visually appealing data visualizations using generative AI.\n\nThey can use AI algorithms to analyze large datasets from their discipline and generate visual representations that communicate complex information in innovative ways.\n\nStudents can explore different visualization techniques, colors, and interactive features to make their data visualizations engaging and creative.",
    "### Critical Thinking\n\nHere are some examples of assignments and activities, suggested by ChatGPT.\n\nTeachers can use these strategies and others to call for and bring about critical thinking and problem-solving among students, who may leverage generative AI as part of their learning and workflow.\n\n- **Argumentation and debate**: Assign controversial topics or issues for students to research and develop arguments.\n\nEncourage them to critically analyze the strengths and weaknesses of different perspectives, fact-checking, comparing, evaluating, synthesizing, and using AI-generated information from various tools to support their claims while also considering its potential limitations.\n\n- **Reflection and self-assessment**: Incorporate regular reflection activities where students can assess their own critical thinking skills, including how effectively they use AI tools.\n\nEncourage them to identify areas for improvement and develop strategies for enhancing their critical thinking abilities.",
    "#### Scenario\n\nIn the environmental biology course you teach, you have decided to accept and embrace the use of generative AI by students as part of their learning.\n\nHowever, how can you ensure students still practice and build 21st-century skills while making use of tools like ChatGPT and Google Bard?\n\nBelow you will find some examples of possible ideas and strategies.",
    "### Communication and Collaboration\n\nYou have introduced the HIPPO acronym that identifies the causes of ongoing defaunation: habitat loss, invasive species, pollution, population growth, and overharvesting.\n\nStudents then join teams, in which they must work together to learn more about their assigned cause from the HIPPO acronym.\n\nAs students engage with one another to learn more, they not only rely on ChatGPT for common perspectives (and potential misconceptions) but also locate and use peer-reviewed research and other authoritative sources for more accurate and reliable insights and information.\n\nAfter gathering further information, students collaborate on a shared presentation on the graphic design app Canva.\n\nThey are working together to develop the materials they will share with the class through a Panopto video recording of the aforementioned slideshow.\n\nThrough this activity, students learn to collaborate effectively with others and to use written, oral, and visual communication strategically to convey information and ideas.",
    "### Creativity and Innovation\n\nYou have decided to transform your curriculum and design a multigenre research project (Langstraat, n.d.) that will play a key role in students’ learning this semester.\n\nStudents will select an ongoing issue in environmental biology, such as defaunation.\n\nThen they will do research on that topic, gathering and analyzing sources, and then use themes or ideas from their sources to create a multigenre research project.\n\nThis multigenre research project will not explicate but rather render the topic at hand (Romano, 1995): Students will not explain it but rather bring it to life through various artifacts, such as first-person diary entries from or podcast interviews with affected species, infographics featuring statistics, or PSA videos to hypothetically share on social media.\n\nWhile composing these artifacts, students will use ideas and possibilities produced by generative AI tools but will have to apply and compose them across various genres.\n\nBeing asked to compose these various artifacts, even with input from generative AI, will challenge students to use their imagination.\n\nWhile practicing communication in these various genres, students will also become familiar with the discourse and best practices for each modality.",
    "### Critical Thinking\n\nTo conclude the unit on the sixth mass extinction in your environmental biology course, you decide to eliminate the test and replace it with an alternative assessment: a reflective portfolio in which students must keep video diaries for a week and then reflect on how their actions each day may contribute to or prevent ongoing defaunation.\n\nStudents cannot rely on ChatGPT or other tools for this as much as they would have for a generic essay about the topic, and the assignment forces them to grapple with and analyze how current human systems as a whole cause defaunation.\n\nStudents will engage with higher levels of Bloom’s Taxonomy (Armstrong, 2010): applying what they have learned about HIPPO, analyzing and evaluating the environmental impact of their own actions, and synthesizing these observations to identify how they as individuals and human systems altogether could change to better support ecosystems.",
    "### Summary\n\nAs students use generative AI for the purposes discussed above, we must provide explicit instruction and structured guidance in crafting and submitting effective prompts.\n\nCalled “prompt engineering,” this process will ensure that when generative AI may play a role in students’ learning, they can use it to achieve desired results.",
    "### Four Elements of an Effective Prompt\n\nAn effectively written prompt should contain the following four elements, though they may do so to varying degrees:\n- **Instruction**: the task assigned to the generative AI tool.\n\nSome examples include:\n    - Chained prompting: posing follow-up questions or prompts\n    - Self-critiques of output: asking generative AI to review or critique its own previous output\n- **Context**: additional information about the “situation” surrounding the output\n    - Identifying the rhetorical situation: explicitly articulating the purpose, audience, and genre of the output\n    - Being specific and detailed with prompt\n- **Input data**: further information in the input to generative AI\n- **Output indicator**: specification of the output desired output\n    - Indication of preferred writing style (Flesh-Kincaid reading levels, amount of technicality and jargon, etc.)\n\nand length (one paragraph, a five-paragraph essay, etc.)\n\nof output\n    - Request for certain presentation format (through tables, lists, outlines, etc.)\n\n- Use of a prompt previously created by generative AI (created in response to your asking it for a prompt you will later use)",
    "### Some Tips for Writing Prompts\n\nWhat are some simple and effective strategies for creating an effective prompt?\n\nHere are some tips that you may like to share with students so that they can use them in their workflow with generative AI.\n\n- Start simple.\n\n- Break down complex tasks.\n\n- Use clear instructions.\n\n- Separate elements.\n\n- Specify details of output, such as length.\n\n- Be precise.\n\n- Focus on what to do.",
    "#### Scenario\n\nYou have asked students to use Claude, ChatGPT, or any program of their choice to compare and contrast various coding languages in your computer science course.\n\nHowever, as you circulate around the room, you find many students consider the responses vague and perhaps even meaningless: The output lacks concrete details, according to many.\n\nYou decide to bring the class back together to discuss the importance and value of chained prompting, in which students take an iterative approach to achieve the output they desire.\n\nIn addition, you explain the importance of offering specific contexts as part of the input to generative AI, so that there is a more meaningful situation for Claude or ChatGPT to consider and discuss.",
    "## Conclusion\n\nAs generative AI comes to play more of a role in the world, it is natural for us to worry and wonder about how it will affect the learning of students.\n\nThere is understandable concern, anxiety, and frustration on the part of faculty.\n\nHowever, generative AI could amount to less of a threat and more of an opportunity with thoughtful and effective pedagogy.\n\nThis document is not an absolute and comprehensive guide to navigating the use of generative AI by students in your teaching.\n\nInstead, it is only a starting point for you to begin to contemplate how to adjust your policies and pedagogy.\n\nAs you begin to consider how to best integrate generative AI in your teaching, please do not hesitate to discuss and brainstorm your ideas and concerns with Missouri Online.\n\nYou are welcome to reach out to an instructional designer at .",
    "## References\n\n- American Psychological Association [TheAPAVideo].\n\n(2019, December 4).\n\nThe decline of empathy and the rise of narcissism with Sara Konrath, PhD [Video].\n\nYouTube.\n\n- Armstrong, P. (2010).\n\nBloom’s Taxonomy.\n\nVanderbilt University Center for Teaching.\n\n- Caren, C. (2023, April 4).\n\nThe launch of Turnitin’s AI writing detector and the road ahead.\n\nThe Turnitin Blog.\n\nad-ahead\n- Dietz, L., & Keys, J.\n\n(2023).\n\nExploring the pedagogical possibilities of generative AI.\n\nNorthwestern University Searle Center for Advancing Learning and Teaching.\n\nve-ai.pdf\n- Dvorak, B.\n\n(2023, May 12).\n\nDesign like pro with Microsoft Design.\n\nEdTech - Fresno Unified School District.\n\n- Eaton, S.E.\n\n(2023, February 25).\n\n6 tenets of postplagiarism: Writing in the age of artificial intelligence.\n\nLearning, Teaching, and Leadership.\n\nn-the-age-of-artificial-intelligence/\n- Eaton, S.E.\n\n(2023).\n\n6 tenets of postplagiarism: Writing in the age of artificial intelligence [Infographic].\n\nLearning, Teaching, and Leadership.\n\n- Eaton, S.E.\n\n(2023, May 6).\n\nThe use of AI-detection tools in the assessment of student work.\n\nLearning, Teaching, and Leadership.\n\nassessment-of-student-work/\n- Finley, A.\n\n(2021).\n\nHow college contributes to workforce success: Employer views on what matters most.\n\nAmerican Association of Colleges and Universities.\n\n- Fowler, G.A.\n\n(2023, April 3).\n\nWe tested a new ChatGPT-detector for teachers.\n\nIt flagged an innocent student.\n\nThe Washington Post.\n\nn-turnitin/\n- Freire, P. (2018).\n\nPedagogy of the oppressed.\n\nContinuum Books.\n\n- Hartmann, J., Schwenzow, J.\n\n& Witte, M. (2023).\n\nThe political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation.\n\narXiv:2301.01768 [cs/CL].\n\n- Heikkilä, M. (2023, February 7).\n\nWhy detecting AI-generated text is so difficult (and what to do about it).\n\nMIT Technology Review.\n\n-text-is-so-difficult-and-what-to-do-about-it/\n- Hiller, M. (2023, February 20).\n\nWhy does ChatGPT generate fake references?\n\nMacquarie University TECHE Blog.\n\n- Hughes, A.\n\n(2023, June 30).\n\nChatGPT: Everything you need to know about OPENAI’s GPT-4 tool.\n\nBBC Science Focus.\n\n- Iowa State University Center for Excellence in Learning and Teaching.\n\n(n.d.).\n\nClassroom and learning assessment techniques.\n\nRetrieved June 23, 2023, from  ssessment-techniques-quick-strategies-to-check-student-learning-in-class/\n- Kennesaw State University School of Data Science and Analytics.\n\nGenerative AI: It’s all a hallucination!\n\nSchool of Data Science and Analytics Blog.\n\n- Langstraat, L. (n.d.).\n\nMultigenre projects table of contents.\n\nThe Writing Gallery from Writing@CSU.\n\nRetrieved June 28, 2023, from \n- Liang, W., Yuksekgonul, M., Mao, Y., Wu, E., & Zou, J.\n\n(2023).\n\nGPT detectors are biased against non-native English writers.\n\narXiv:2304.02819v2 [cs/CL].\n\n- Lim, W.M., Gunasekara, A., Pallant, J.L., Pallant, J.I., & Pechenkina, E. Generative AI and the future of education: Ragnarok or reformation?\n\nA paradoxical perspective from management educators.\n\nThe International Journal of Management Education, 21(2), 1-13.\n\n- Malhotra, T. (2023, March 21).\n\nExploring the differences between ChatGPT/GPT-4 and traditional language models: The impact of reinforcement learning from human feedback (RLHF).\n\nMarktechpost.\n\npt-gpt-4-and-traditional-language-models-the-impact-of-reinforcement-learning-fr om-human-feedback-rlhf/\n- McAdoo, T. (2023, April 7).\n\nHow to cite ChatGPT.\n\nAPA Style Blog.\n\n- Modern Language Association.\n\n(2023, March 17).\n\nHow do I cite generative AI in MLA style?\n\nMLA Style Center.\n\n- Myers, A.\n\n(2023, May 22).\n\nAssessing political bias in language models.\n\nStanford HAI (Human-Centered Artificial Intelligence) News.\n\n- Nicoletti, L. & Bass, D. (2023).\n\nHumans are biased.\n\nGenerative AI is even worse.\n\nBloomberg.\n\n- Nollmeyer, G. (2018, November 27).\n\nMaking rules alongside students.\n\nClassroom Management Toolbox.\n\n- OpenAI.\n\n(2023, April 25).\n\nNew ways to manage your data in ChatGPT.\n\nOpenAI Blog.\n\n- Ott, H. (n.d.).\n\nBloom’s Taxonomy revised.\n\nAcademic assessment.\n\nned.pdf\n- Romano, T. (1995).\n\nWriting with passion: Life stories, multiple genres.\n\nHeinemann.\n\n- Rutinowski, J., Franke, S., Endendyk, J., Dormuth, I., & Pauly, M. (2023).\n\nThe self-perception and political biases of ChatGPT.\n\narXiv:2304.07333 [cs/CY].\n\n- Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., & Hashimoto, T. (2023).\n\nWhose opinions do language models reflect?\n\narXiv:2303.17548 [cs/CL].\n\n- Schafran, A.\n\n(2023).\n\nThe impact of artificial intelligence on design practice [Honors thesis, California State University, Long Beach].\n\nThe California State University ScholarWorks.\n\n- Southern Illinois University Edwardsville.\n\n(n.d.).\n\nPersonal narrative and digital storytelling.\n\nRetrieved June 28, 2023, from\n rative-Digital-Storytelling-Assignment-1.pdf\n- Surovell, E. (2023, March 16).\n\nFaculty members still aren’t sure what to make of ChatGPT.",
    "Retrieved June 28, 2023, from\n rative-Digital-Storytelling-Assignment-1.pdf\n- Surovell, E. (2023, March 16).\n\nFaculty members still aren’t sure what to make of ChatGPT.\n\nThe Chronicle of Higher Education.\n\n-chatgpt?cid=gen_sign_in\n- Tay, A.\n\n(2023, April 3).\n\nThe new Bing chat and Elicit.org - the power of search engines with large language models (LLM) like GPT.\n\nSingapore Management University Libraries Topics & Insights.\n\nengines-large-language-models-llm-gpt\n- Texas A&M University Center for Teaching Excellence.\n\n(n.d.) Generative AI syllabus statement considerations.\n\nRetrieved June 23, 2023, from  enAI-SyllabusStatementConsiderations.pdf\n- The University of Missouri.\n\n(n.d.).\n\nSign up for PowerNotes.\n\nMizzouOne.\n\nRetrieved July 7, 2023, from \n- The Walton Family Foundation.\n\n(2023, March 1).\n\nChatGPT used by teachers more than students, new survey from Walton Family Foundation finds [Press release].\n\nnts-new-survey-from-walton-family-foundation-finds\n- Trivedi, A.\n\n(2023).\n\nExploring gender bias in large language models.\n\nIn Policy Lab on AI and Bias (Ed.\n\n), A Promethean moment: Towards an understanding of generative AI and its implications on bias (pp.\n\n19-31).\n\nUniversity of Pennsylvania Carey Law School.\n\n- Turner, A.\n\n(2023, April 25).\n\nChatGPT’s impact on education and student data privacy.\n\nJD Supra.\n\n- U.S. Department of Education.\n\n(n.d.) Family Educational Rights and Privacy Act (FERPA).\n\n- Vivekanandan, R. (2019, February 14).\n\nIntegrating 21st-century skills into education systems: From rhetoric to reality.\n\nEducation Plus Development Blog from the Brookings Institution.\n\n-21st-century-skills-into-education-systems-from-rhetoric-to-reality/\n- Weiser, B.\n\n(2023, May 27).\n\nHere’s what happens when your lawyer uses ChatGPT.\n\nThe New York Times.\n\n- Wellborn, A.\n\n(2023, March 9).\n\nChatGPT and fake citations.\n\nDuke University Libraries News.\n\n- White, R. (1991).\n\nThe ELT curriculum: design, innovation, and management.\n\nBasil Blackwell.\n\n(855) 675-0755  \n  \nteaching.missouri.edu  \n130 Heinkel Building, Columbia, Missouri 65211",
    "# Generative AI: A New Era of Enlightenment \n\nBuilding a Responsive Strategy at Elon University  \nDate: August 10, 2023\n\nThe strategic plan for 2030 Boldly Elon sets the direction to build upon decades of innovation and national leadership in experiential and innovative teaching pedagogy, setting the standard for engaged learning and mentoring.\n\nMaintaining such a strategic positioning requires that Elon University continue to be responsive to its ever-changing dynamic environment, which now includes rapidly evolving and potentially disruptive generative Artificial Intelligence (AI) technologies.\n\nThe introduction of generative AI in Fall 2022 has left higher education institutions and companies grappling with its impact on society and daily operations.\n\nGiven the speed of generative AI's deployment thus far, the need to advance digital transformation, reskill talent, and prepare future generations for the new world of AI is greater than ever.\n\nA recent McKinsey report predicts that because of generative AI half of all work will be automated between 2030 and 2060 (McKinsey, 2023).\n\nWith its ability to process and generate human language, generative AI can assist humans in a variety of tasks including responding to complex questions, serving as a personal tutor, generating programming code, writing a resume, and translating languages.\n\nGenerative AI has even passed standardized tests like the Bar, GRE, and SAT and has outperformed medical students on complex clinical exams (Hadhazy, 2023).\n\nHigher education institutions have rushed to respond to generative AI—such as ChatGPT, LLaMA, GitHub Copilot, and Bard—with institutional use policies that, in some cases, included a complete ban of generative AI over fears of student plagiarism.\n\nWe believe that the question is not whether to use generative AI in higher education, but how to do so appropriately and ethically.\n\nThe goal of this whitepaper is to offer strategic insights to help us examine how our campus could engage ethically with AI advancements and prepare for the ways it may impact education.\n\nThis whitepaper offers a broad snapshot of how Elon might responsibly and strategically plan for engaging AI on our campus.\n\nAs AI technologies are emerging and changing quickly, this whitepaper does not aim to answer all the questions that are appropriate and important to ask, because such answers are uncertain and should involve discussions with the wider university community before they are answered.\n\nThis whitepaper is a conversation starter, offered as a way to engage our community in a dialogue that will continue throughout the next academic year.\n\nThe whitepaper is organized into the following sections: \n\nSection 2 proposes draft language for \"Elon's Generative AI Statement,\" building upon Elon's \"AI Awareness & Readiness Survey\" and the analysis of peer- and aspirant-institutions' AI statements  \nSection 3 highlights some benefits of AI and its potential for engaged learning  \nSection 4 outlines some risks and challenges of AI that are especially important for educational institutions to consider mitigating  \nSection 5 offers interim considerations for generative AI in instructional settings for our community to consider, to ensure informed and ethical engagement with AI advancements  \n\nIn line with the approaches of the other institutions, including Elon's peer- and aspirant-institutions, and considering Elon's Mission Statement and unique culture, the following draft of \"Elon's Generative AI Statement\" is offered for discussion:  \nAs a pioneer of experiential learning and a national leader in engaged learning for student and alumni success, Elon University prepares ethical, resilient, and agile graduates who are active and responsible citizens that shape the future.\n\nIn line with our mission, we recognize the importance of equipping our students with the necessary skills to embrace technology for enhanced learning and engagement in their personal, professional, and civic lives.\n\nGenerative AI technologies hold immense potential for accelerating the co-creation of knowledge, enhancing effectiveness and efficiency.\n\nHowever, they can also introduce novel challenges to student engagement; pedagogy design and assessments; diversity, equity, and inclusion concerns; ethical considerations; and honor code expectations.\n\nAt Elon, the decision to incorporate these technologies into teaching and learning is at the discretion of our faculty.\n\nWhile some faculty members may choose to utilize generative AI in their instruction in select instances, others may opt to refrain from its use.\n\nIt is the responsibility of faculty to make their policies clear and explicit and to discuss them openly with students.\n\nIt is the responsibility of students to adhere to the guidelines established by their faculty.\n\nEmploying the output of these technologies in violation of the policy set by the faculty will be considered an Elon honor code violation.",
    "# Elon University AI Principles:\n\nThe use of AI at Elon should begin with the primacy of human health, well-being, dignity, safety, privacy, and security.\n\nThe use of AI is appropriate and effective in some applications and inappropriate and ineffective in others.\n\nFaculty should decide whether, when, and how AI should be used in courses and clearly communicate those boundaries to students.\n\nStaff should incorporate the use of AI in their work with full awareness and support of their supervisors.\n\nIn all situations intellectual honesty and transparency about the use of AI is paramount.\n\nWe see AI as a tool to enrich and enhance teaching, learning, creativity, and human development.\n\nWe believe the role of AI should be to augment, not fully replace the vital human relationships between teachers and learners, or within groups of peer learners.\n\nWe see digital and AI literacy as an essential part of an Elon education.\n\nSince AI is a multidisciplinary field, students should learn about AI’s intersections with philosophy and ethics, social sciences, health sciences, business, communications, government and legal studies, creative arts, and many other fields.\n\nWe should strive to help learners gain critical thinking and analysis skills appropriate for functioning in an AI-assisted world, including concepts such as social responsibility and citizenship, information literacy, how AI might reinforce human bias and discrimination, its implications for personal privacy, intellectual property, and the ways it can abet deception through fraud and fakery.\n\nEveryone in our community should have the opportunity to access and engage with AI systems, including physical access as well as the skills and capability to use these systems efficiently and equitably.\n\nAI systems used at Elon should never compromise the privacy of students’ personal information.\n\nWhile AI systems may be utilized, faculty and staff should maintain a primary role in the evaluation of students’ learning progress, behaviors, and outcomes.\n\nAI systems used at Elon should be transparent and neutral – they should disclose the positionality of their data and models and should not manipulate learning processes in unethical, deceptive, or subliminal ways.\n\nThe use of AI in research must be done within the bounds of rigorous ethical standards.\n\nResearchers should take all necessary steps to understand the likely benefits of AI-enabled research, the limits that should be imposed on its application, and the risks (known and unknown) and potential negative consequences that might emerge from these technologies.\n\nGenerative AI has the potential to transform, and perhaps even revolutionize, education.\n\nWhile much of this is still speculative—as AI tools continue to proliferate and educators learn to harness their power— there are several potential educational benefits of generative AI that are repeatedly mentioned in the literature.\n\nThe broad argument is that generative AI will free instructors from mundane and repetitive tasks, allowing them to spend more time creating immersive and engaging learning experiences geared toward the individualized needs of students.\n\nAs educators gain AI literacy, it could lead to even more profound shifts in how they create content, teach, and engage students.\n\nThe following section summarizes some of the primary ways technology leaders predict generative AI will change education.",
    "## Educational Enhancement\n\nGenerative AI can enhance education, making it more engaging, accessible, and tailored to individual student needs.\n\nGenerative AI, for instance, can put classroom skills and knowledge into practice through the generation of interactive learning simulations (Lebo & Brown, 2023).\n\nFor example, medical students might “interact” with a generative AI trained to mimic the symptoms of a patient.\n\nSuch simulations can enhance the realism of learning objectives, providing opportunities for faculty to tailor scenarios to class content and student learning needs.\n\nGenerative AI can also serve as a personalized tutor.\n\nFor instance, Khan Academy, a non-profit learning platform for K-12 students, has partnered with OpenAI to create an AI guide that serves as a tutor and student coach in math, computer science, and writing topics.\n\nWith the proper IT infrastructure, faculty could use generative AI to tailor content and feedback to students based on their needs and learning styles (Seo et al., 2021).\n\nThese AI tools can also \"engage\" with learners, for example, by helping students practice language skills by interacting with an AI through customized conversation-like responses (Kharbach, 2023) or by making revision suggestions to submitted text, according to the users' prompts.\n\nThese personalized, interactive uses of generative AI can enhance student engagement and improve learning outcomes (Adiguzel et al., 2023).",
    "## Instructor Empowerment and Skill Development\n\nInstructors could use generative AI tools to create a range of curricular materials, like syllabi, assignments, test and quiz questions, case studies, self-paced practice problems, and study guides, as well as administrative and professional documents like memos, reports, proposals, self-assessments, and visual presentations (Pavlik, 2023; Gupta & Bhaskar, 2020).\n\nFurthermore, educators can assist students with learning needs by utilizing AI tools with text-to-speech or speech-to-text functions (Dillard, 2022; Kasneci et al., 2023; Tilili et al., 2023).\n\nBeyond creating materials, AI technologies may foster improved teaching skills and competency by providing inspiration, promoting self-reflection, and granting deeper insights into students' learning processes (Adiguzel et al., 2023).\n\nFor instance, AI-generated metrics can provide instructors with detailed analytics on each student's performance, such as the type of questions they ask or the amount of progress they have made in a course.\n\nWith such metrics, faculty can refine assignments, better target areas of difficulty for students, and create tailored learning paths.",
    "## Enhanced Academic Research\n\nAI technologies (such as scite.ai, elicit.org, consensus.app, and semanticscholar.org) are valuable tools for researchers (Mariani, 2021).\n\nAI applications can help streamline and automate the process of collecting research, writing literature reviews, and enabling knowledge discovery (Mariani, 2021).\n\nUpon analyzing a given text, generative AI has the ability to generate informative research questions.\n\nAI-powered tools can also synthesize and organize large amounts of research data, concisely summarizing research findings (Heid, 2023).\n\nWith AI-supported research, faculty can generate more detailed results and efficiently manage data.\n\nAI-enhanced research methods can also be used to identify patterns and trends that would be difficult to identify manually, allowing faculty to explore research areas they might not have considered.",
    "## Increased Efficiency\n\nGenerative AI technologies (such as Microsoft Co-Pilot) can automate various routine tasks, allowing staff and faculty to focus their time on other important activities.\n\nFor example, AI tools can automate the gathering of data, the development of reports, and the scheduling of meetings and events, all of which can help instructors and staff save time.",
    "## Content Creation\n\nThe ability of generative AIs to create content has the potential to alter education in at least two ways.\n\nFirst, as mentioned above, instructors can use these tools to quickly and efficiently create a range of curricular and professional materials (Pavlik, 2023; Gupta & Bhaskar, 2020).\n\nSecond, students can use these tools to respond to writing assignments and prompts during different parts of the writing process.\n\nWhile it is essential for students to understand the limitations and biases of content created by generative AIs, the value of effective prompt engineering, and the importance of critiquing and revising AI outputs, these AI tools have the potential to enhance student writing as engaged learning (Hesse, D. et al., 2023; Byrd, A. et al., 2023; Selber, 2023).\n\nIt is important to be aware of and mitigate the potential risks and ethical concerns associated with the use of generative AI in educational environments.\n\nThese concerns have led 1,000 technology leaders to call for a six-month halt in \"giant\" AI experiments (Metz and Schmidt, 2023).\n\nIn addition, there is concern that AI developers are creating applications that are difficult to understand, predict, and reliably control.\n\nThese concerns include, but are not limited to, the following:",
    "### Transparency\n\nA lack of transparency into the source data and how the responses are generated could make these generative AI models difficult to explain and understand.\n\nFor example, a generative AI solution that summarizes the literature about engaged learning may not be able to reference the right articles that led it to its conclusions about the data.\n\nThat's why being able to verify the information produced by generative AI is an essential skillset to develop and build.\n\nHow can we help our students critically assess AI output quality?",
    "### Overreliance\n\nOverreliance on AI systems can lead to several negative consequences, such as users uncritically accepting incorrect AI outputs; loss of critical thinking skills; inhibiting users from developing original ideas; misunderstanding concepts; inability to apply knowledge; detachment from the inquiry process; limited exposure to diverse perspectives; and users losing confidence that they can learn on their own without the assistance of AI (OpenAI, 2023; Passi & Vorvoreanu, 2022).\n\nHow can we train our community to use strategies for mitigating user overreliance on AI and help students develop frameworks for interacting successfully with AI systems?",
    "### Academic Integrity\n\nIt is reasonable to assume that students have been utilizing generative AI technologies to complete their class assignments.\n\nHowever, given the speed with which AI emerged during this last academic year, it is also reasonable to assume that most faculty members understandably did not incorporate an \"Acceptable AI Use” policy into their syllabi.\n\nThis creates an academic integrity gap that could be addressed by clear expectations around generative AI use in academic settings.\n\nHow can we prepare our community to address the academic integrity gap?",
    "### Misinformation Concerns\n\nThe rise of deep fake images and videos and manipulation of information pose a major near-term threat that calls for AI regulations from the US government and close attention from educational institutions.\n\nStudies have already demonstrated the ability of machines to produce images and sounds nearly identical to their real-life counterparts (Metz and Blumenthal, 2019).\n\nSuch advancements could potentially hasten the propagation of misleading or false information.\n\nHow can we help our community to mitigate the risks of deep fake materials?",
    "### Output Quality\n\nWhile the answers generated by AI may seem logical and accurate, there can be instances where the information produced is incorrect, a phenomenon often referred to as a \"hallucination.\"\n\nIt is therefore crucial for students to cross-check AI-generated information with reliable external sources.\n\nMoreover, content produced by generative AI can be harmful and biased, as the data training the model represents data sources that include bias.\n\nAdditionally, these tools have been primarily trained on Western perspectives and may not present a globally inclusive view.\n\nHow can we help our community develop a clear understanding of the limitations of generative AI?",
    "### Equity and Access\n\nSome generative AI tools are freely available, but more advanced models like GPT-4 are subscription-based.\n\nEnsuring that all students, faculty, and staff have equal access to tools like ChatGPT and the training to use them effectively is a critical step in bridging the digital divide on campus.\n\nWhat infrastructure do we need to build to ensure equitable access to these tools?",
    "### Job Loss\n\nThe impacts of generative AI on the job market, both in the short- and long-term, remain uncertain.\n\nWhile some studies show significant productivity gains (McKinsey, 2023), others indicate potential disruptions in specific labor sectors, which may impact students, staff, and faculty.\n\nHow can we help our community use AI to strengthen their professional lives while mitigating any potential downsides?\n\nThe following interim consideration recommendations have been assembled based on published research, discussions in public forums, and feedback received from faculty and staff across the university units.",
    "#### Understand the Opportunities and Limitations of AI\n\n- Develop AI Literacy\n- Attend campus workshops and review campus online resources to become familiar and stay current with the use of generative AI and Large Language Models, their benefits and limitations, and their potential for facilitating engaged learning.\n\n- Experiment with different AIs in your everyday work, as you prepare for class and conduct research, to become familiar with the range of AIs available and their possibilities for academic work.\n\n- Participate in discussions with colleagues within and among disciplines on the appropriate use of AI.\n\n- Review the AI statements issued by Elon and by your professional organizations and publications, as the use of AI technologies in teaching and research is often discipline-specific.\n\n- Be open to the creative use of generative AI tools to enhance students' engaged learning experiences and help them achieve your course learning outcomes.\n\nFor example, students might use generative AI tools as a \"discussion partner,\" as an online content tutor, as a research assistant, or as an aid during specific parts of the writing process (e.g., to brainstorm topics or questions; to generate an initial outline as a starting point; to request specific kinds of feedback, etc.).\n\n- Consider where in your curriculum generative AI will be discussed and used, and consider scaffolding across classes, just like you would scaffold content and learning outcomes.\n\n- Determine how you would measure successful use of the generative AI to support your courses and your students.\n\n- Develop an understanding of how AI technologies may be used, or are already being used, in the workplace in relation to the field in which you teach.\n\n- Be aware that AI-detection tools, much like existing plagiarism detection tools, have limitations and can set up an adversarial rather than a trust and empowerment relationship between students and faculty.\n\n- Engage in discussions with your colleagues to determine resources needed to support your use of AI for your majors and schools.",
    "#### Understand the Risks Associated with AI and its Ethical Use\n\n- Consider what constitutes ethical use of AI in your discipline, in your research, and in your teaching.\n\n- If students may use AI tools in your courses, clearly communicate what constitutes acceptable use and how a student should cite or disclose the use of generative AI relative to their academic work.\n\n- Be aware of the risk of unauthorized and unverified AI tools for sensitive information as they may present data privacy concerns.\n\nCritical information protected under the Family Educational Rights and Privacy Act (FERPA) could potentially be leaked onto the internet, leading to security risks and legal implications.\n\n- Be aware of the potential for fabricated \"hallucinations\" (i.e., inconsistent and nonsensical generated content) and inaccurate content produced by generative AI.\n\nConsider when and how content from generative AI should be verified.",
    "#### Develop and Communicate Generative AI Course Policies\n\n- Consider how the development of AI technology might require you to revise and redesign your course assignments, quizzes, and tests to avoid unethical or dishonest use of generative AI.\n\nFor example, widely used class assignments such as take-home essays and writing assignments that are not scaffolded might be weakened by the advance of generative AI tools as they can generate convincing content.\n\n- Be transparent with students about course and assignment expectations regarding the use of generative AI technologies.\n\n- Add a clear \"Acceptable AI Use\" statement to your syllabi.\n\nYou might also choose to add more specific \"Acceptable AI Use\" statements to individual assignments.\n\nThe decision to use AI in the classroom and under what circumstances is up to individual faculty members.\n\nPlease see Appendix C for examples.\n\n- Explain to your students the benefits and limitations of generative AI technologies relative to your class and clarify the values and learning goals that led you to adopt your \"Acceptable AI Use\" policy.\n\n- Clearly articulate course expectations and repercussions for violating the \"Acceptable AI Use\" policy in alignment with other university policies.\n\n- When students are permitted to use AI for class assignments:\n  - Practice using AI as a class, and together critique their benefits and limitations.\n\n- Be clear about the role of AI in your class, highlighting its role as supplementing the classroom experience rather than replacing it.\n\n- Ask students to write a brief reflection or summary about which AI they used, critique the output quality, and explain how they revised the output (the Human-AI loop).\n\n- Ask students to turn in transcripts produced during AI use.\n\n- If you are asking your students to use paid AI technology, add the resource into your syllabus, so students know the cost of the tool.\n\n- Remind the students of the values in Elon's Honor Code and Code of Conduct: Honesty, Integrity, Responsibility, and Respect.\n\nStudents are expected to exemplify our values through their academic work, including their use of generative AI.\n\n- Highlight the importance of the Human-AI loop: AIs may assist humans in creating content, but AI outputs should not be used without human intervention and revision.\n\n(Please see Humans in the Loop: The Design of Interactive AI Systems -stanford.edu)\n  - Be honest with students about your experience and comfort level with AI technologies, which are emerging and changing frequently.\n\nDisclose when you use AI technologies to design class and learning assignments to your students.",
    "#### Promote Equitable and Inclusive Use of AI\n\n- Consider how the integration of generative AI technologies into the classroom helps or hinders students' success depending on their unique needs and experiences.\n\n- Consider how AI tools such as text-to-speech, speech-to-text, or generative picture technology can enhance your class accessibility.\n\n- Ensure all students have access to technology and instructions when adopting generative AI tools in your classroom.\n\nPay attention to the cost of subscription AIs and consider using free alternatives if the cost is prohibitive.",
    "#### Understand the Opportunities and Limitations of AI\n\n- Review the AI statements from professional organizations relevant to your areas of expertise that guide your work.\n\n- Develop AI Literacy: participate in workshops to become familiar and stay current with the use of generative AI, its benefits and limitations as well as data privacy, bias, and security concerns.\n\n- Experiment with different AIs to learn how they might augment your work efficiency and effectiveness so you can allocate more time to higher-impact tasks.\n\n- Explore using AI tools in your professional work (outlining drafts, summarizing main points, etc.).\n\n- Discuss the use of AI in your work tasks with your supervisor to have a candid conversation and seek any input and guidance they may have.\n\n- Determine how you would measure successful use of the generative AI to support your work.",
    "#### Consider How it Could be Applied in Your Unit\n\n- When using AI for work tasks such as creating new content for marketing, forecasting numbers for finance, or coding for software development:\n  - Critique the output.\n\nAI outputs are not meant to be copied and used without human intervention or revision (the Human-AI loop).\n\n- Note which AI you used, what worked well, and any limitations you noticed, so you can keep track of the ones you would like to use in the future and to share best practices and new ideas with colleagues.\n\n- Experiment using AIs with staff (or students) you supervise and discuss their benefits and downsides.\n\n- Select a pilot to try the use of AI safely in a business operation.\n\nHaving the structures and support through change management practices will be critical as we transition to a new way of working with emerging technology.\n\n- Consider an open discussion with IT about implementing an Azure platform with OpenAI access to explore, configure and use the technology appropriately with proprietary and sensitive data.",
    "#### Understand Risks Associated with AI and its Ethical Use\n\n- Consider what constitutes ethical use of AI in your discipline, in your field of work, and in your teaching.\n\n- Establish guardrails to help your team experiment safely and responsibly.\n\nWhat data is allowed or not allowed to share?\n\nWhen is it necessary and/or recommended to fact-check content produced by generative AI tools?\n\nWhat are the mechanisms for fact-checking a suggested output?\n\n- Be aware of the risk of unauthorized and unverified AI tools for sensitive information as they may present data privacy concerns.\n\nCritical information protected under the Family Educational Rights and Privacy Act (FERPA) could potentially be leaked onto the internet, leading to security issues and legal actions.\n\n- Be aware of the potential for fabricated \"hallucinations\" and inaccurate content produced by generative AI.\n\nConsider when and how content from generative AI should be verified.",
    "### Student Considerations\n\n- Students are responsible for knowing their instructors’ AI policies.\n\nIf it is not communicated clearly to students in course or assignment statements or by the instructor, they should ask their instructor if it is acceptable to use AI and under what conditions.\n\nAs an emerging technology that may be implemented differently in different disciplines and courses, it is important for students to understand that course expectations toward AI use may vary across classes, disciplines, and schools.\n\n- Students should have access to ongoing AI training across all their years at Elon.\n\n- Build an understanding of when it's appropriate to use generative AI to support their work.\n\n- Understand the limitations and benefits of using generative AI across different disciplines and courses.\n\n- Students are responsible for understanding and abiding by Elon's Honor Code and Code of Conduct: Honesty, Integrity, Responsibility, and Respect.\n\nStudents are expected to exemplify Elon's values through their academic work, including their use of generative AI.",
    "### Building an Infrastructure for Faculty and Staff Professional Development Considerations\n\nIn order to define AI responsible use policies and principles at Elon, above and beyond the teaching context, we recommend reviewing resources like the European Union's Ethics Guidelines of Trustworthy AI and the Organization for Economic Cooperation and Development (OECD) AI principles.\n\nThe following materials were organized using a people, processes, technology, and workshop/training framework:",
    "#### People\n\n- Identify AI champions in departments or schools, to establish open communication between faculty/staff and professional development leaders and to help respond promptly to faculty/staff needs, questions, and requests and to relate these concerns to the coordinating infrastructure.\n\nOffer champions compensation for their time and effort, as they will be expected to be leaders in their fields and for their colleges.\n\n- Offer incentives to faculty with appropriate expertise to work alongside IT professionals to build AI infrastructure.\n\n- Train and recruit IT professionals to develop and run an AI infrastructure leveraging existing IT infrastructure.",
    "#### Processes\n\n- Consider connections between Elon's Data Competency QEP (Quality Enhancement Plan) and Data Nexus.\n\nFor example, how and to what extent do data competent students use AI?\n\n- Organize events and create opportunities throughout the academic year to have conversations about AI adoption that include students, faculty, and staff.\n\n- Develop an infrastructure for coordinating/organizing professional development opportunities across campus and commit sufficient budget resources for this coordination, workshops, speakers, grants, technology requirements, etc.\n\nPossibilities might include an \"AI Readiness Committee;\" relying upon existing programs to manage coordination; defining a more centralized structure such as an \"Institute or Center for Ethical AI Use;\" a working group; or a combination of these options.\n\n- Ensure that Elon stays informed about emerging AI ethical concerns, government and industry AI discussions, and policies and adjust policies accordingly.\n\n- Establish a cross-functional responsible AI adoption committee and develop processes to monitor and audit AI systems deployment in business operations on-campus.",
    "#### Technology\n\n- Create online resources for the provost's website (\"Generative AI Statement,\" Recommendations); CWE: Center for Writing Excellence website (AI writing, research, and writing pedagogy resources); CATL: Center for the Advancement of Teaching and Learning website (AI general pedagogy resources); Library (AI research resources); as well as other units with interests and expertise to share.\n\nThese programs could link to each other's websites and aim for sharing a limited number of useful, focused resources.\n\n- Develop an \"AI Playground\" so faculty, staff, and students can experiment with different, emerging, and free as well subscription AIs.\n\nOffering the Elon community these opportunities is an important way to mitigate the potential problem of unequal access and encourage faculty and staff to integrate AIs into their work and teaching, when appropriate.",
    "#### Workshops and Training\n\n- Draw upon strengths of Spring 2023 precedent, with multiple programs on campus collaborating to offer general and school/discipline-specific AI workshops and grants (CWE, CATL, TLT (Teaching and Learning Technologies), Librarians, First-Year Writing, faculty/staff with expertise and interest).\n\n- Design inclusive and collaborative professional development opportunities that recognize faculty and staff's diverse AI experiences and draws upon their expertise and interest.\n\n- Design professional development so that it meets participants where they are in the learning process, responds to their needs and questions, and provides many \"touch points\" or opportunities over time, and in different formats (for example, workshops, reading groups, online resources, individual consultations, and grant opportunities).\n\n- Design training for students based on their discipline.",
    "## References\n\nAdiguzel, T., Kaya, M. H., Cansu, F. K. (2023).\n\nRevolutionizing Education with Ai: Exploring the Transformative Potential of Chatgpt.\n\nContemporary Educational Technology, 3(15), ep429.\n\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P et al.\n\n(2023).\n\nSparks of artificial general intelligence: Early experiments with gpt-4.\n\narXiv preprint arXiv:2303.12712.\n\nByrd, A., Flores, L., Green, D., Hassel, H., et al.\n\n(2023).\n\nMLA-CCCC Joint Task Force on Writing and AI Working Paper: Overview of the Issues, Statement of Principles, and Recommendations.\n\nDepartment of Education.\n\nOffice of EducaAugust 8thechnology.\n\nArtificial Intelligence.\n\n(Accessed August 8, 2023).\n\nGupta, K. P. and Bhaskar, P. (2020).\n\nInhibiting and Motivating Factors Influencing Teachers' Adoption of Ai-based Teaching and Learning Solutions: Prioritization Using Analytic Hierarchy Process.\n\nJournal of Information Technology Education: Research, (19), 693-723.\n\nHadhazy, A.\n\n(2023).\n\nChatGPT Out-scores Medical Students on Complex Clinical Care Exam Questions.\n\nHAI Stanford University.\n\nAvailable at:  15thscores-medical-students-complex-clinical-care-exam-questions (Accessed August 8, 2023).\n\nHeidt, A.\n\n2023.\n\nArtificial-intelligence search engines wrangle academic literature.\n\nNature, 620(7973): 456–457.\n\nHesse, D., Rademaekers, J., Blakeslee, A., Britt-Smith, L., Moroski-Rigney, K., Craig, S., and Rosinski, P. (January 2023).\n\nAWAC: Association for Writing Across the Curriculum, Statement on Artificial Intelligence Writing Tools in Writing Across the Curriculum Settings.\n\nJohri, A., Katz, A. S., Qadir, J., Hingle, A.\n\n(2023).\n\nGenerative Artificial Intelligence and Engineering Education.\n\nJournal of Engineering Education, 3(112), 572-577.\n\nKang, C. and Metz, C. (2023).\n\nFTC Opens Investigation into ChatGPT Maker Over Technology's Potential Harms.\n\nThe New York Times.\n\n17th&region=BELOW_MAIN_CONTENT&block=storyline_flex_guide_recirc (Accessed August 8, 2023).\n\nKharbach, M. (2023).\n\nAI in Education – Benefits and Limitations.\n\nEducators Technology.\n\n8thy.com/2023/02/ai-in-education-benefits-and-limitations.html (Accessed August 8, 2023).\n\nLebo, C., & Brown, N. (2022).\n\nIntegrating Artificial Intelligence (AI) Simulations into Undergraduate Nursing Education: An Evolving AI Patient.\n\nNursing Education Perspectives, 10-1097.\n\nMariani, M. M., Perez‐Vega, R., Wirtz, J.\n\n(2021).\n\nAi In Marketing, Consumer Research and Psychology: A Systematic Literature Review and Research Agenda.\n\nPsychology &Amp; Marketing, 4(39), 755-776.\n\nMcKinsey (2023).\n\nThe economic potential of generative AI: The next productivity frontier.\n\nMcKinsey Digital.\n\n17thic-potential-of-generative-ai-the-next-productivity-frontier (Accessed August 8, 2023).\n\nMetz, C and Blumenthal, S. (2019).\n\nHow AI Could Be Weaponized to Spread Disinformation.\n\nThe New York Times.\n\n17thechnology/ai-text-disinformation.html?searchResultPosition=5 (Accessed August 8, 2023).\n\nMetz, C. and Schmidt, G. (2023).\n\nElon Musk and Others March 29th Pause on AI, Citing' Profound Risks to Society'.\n\nThe New York Times, March 29.\n\n10th023/03/29/technology/ai-artificial-intelligence-musk-risks.html (Accessed August 8, 2023).\n\nMilmo, D. (2023).\n\nSarah Silverman Sues OpenAI and Meta claiming AI training infringed copyright.\n\nThe Guardian.\n\n17thl/10/sarah-silverman-sues-openai-meta-copyright-infringement (Accessed August 8, 2023).\n\nOpenAI (2023).\n\nGPT -4 TecJuly 17thReport.\n\nAvailable at  (Accessed August 8, 2023).\n\nPassi, S. and Vorvoreanu, M. Overreliance on AI: Literature review.\n\nAETHER: Ai Ethics and Effects in Engineering and Research.\n\n10th2022/06/Aether-Overreliance-on-AI-Review-Final-6.21.22.pdf (Accessed August 8, 2023).\n\nPavlik, J. V. (2023).\n\nCollaborating With Chatgpt: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education.\n\nJournalism &Amp; Mass Communication Educator, 1(78), 84-93.\n\nSelber, S. (2023).\n\nPWR (Professional Writing and Rhetoric) Approach to Artificial Intelligence.\n\nPenn State: Program in Writing and Rhetoric.\n\n(Accessed August 8, 2023).\n\nSeo, K., Tang, J., Roll, I., Fels, S., Yoon, D. (2021).\n\nThe Impact of Artificial Intelligence on Learner– instructor Interaction in Online Learning.\n\nInternational Journal of Educational Technology in Higher Education, 1(18).\n\nWang, Y.\n\n(2023).\n\nExploration On the Operation Status and Optimization Strategy of Networked Teaching of Physical Education Curriculum Based on Ai Algorithm.\n\nInternational Journal of Information Technologies and Systems Approach, 3(16), 1-15.",
    "## Introduction\n\nThis policy establishes the framework for using generative Artificial Intelligence (hereinafter referred to as AI) in teaching, learning, and evaluation at ISM University of Management and Economics, UAB (hereinafter referred to as ISM University).\n\nIt is intended to help navigate this advanced landscape, acknowledging the increasing significance of generative AI in shaping the diverse aspects of our lives, especially in the field of education.\n\nISM University is committed to fostering competency in using this groundbreaking technology among our students and faculty to achieve academic excellence.\n\nISM University aims to empower students and faculty with the necessary skills to thrive in an ever-evolving technological landscape.\n\nTo achieve this goal, ISM University places a strong emphasis on building generative AI literacy within ISM University’s community.\n\nMastery of these AI skills will prove vital in navigating the increasingly technologically integrated society of the future.\n\nThis policy applies to all faculty members, students, and administrative staff involved in teaching and learning processes at ISM University.\n\nIt will be reviewed annually or as necessitated by changes in technology or regulation.\n\nThe ISM University’s Digital Committee is responsible for ensuring its relevancy and effectiveness.",
    "## Definition of Generative AI\n\nGenerative AI is a subset of artificial intelligence that utilizes machine learning techniques to generate information.\n\nGenerative AI uses advanced algorithms and models to generate new data similar to the input data it was trained on.\n\nThis unique ability allows it to create content that did not previously exist or transform existing data into a new format.\n\nFor instance, generative AI can be used to create realistic images, synthesize voices, generate text, compose music and conduct other tasks.\n\nAI tools can offer substantial benefits to academia, helping students and teachers alike in research, writing, exploration of complex topics and other activities.\n\nBy leveraging these capabilities, ISM University aims to enhance the teaching and learning experience, thus pushing the boundaries of conventional education.",
    "### Generative AI in Teaching\n\nFaculty members can choose to use AI tools in their teaching, as long as these tools help meet the goals and requirements of the course and are in line with ISM University’s Code of Ethics.\n\nFaculty members are encouraged to try out AI tools to make classes more engaging and help students learn better.\n\nThis can include using AI tools for things like research, improving content made by AI, competing on AI challenges, explaining how to think through problems, and making all-around assignments.\n\nFaculty members can also use AI tools to help them prepare for teaching and to give students feedback.\n\nFaculty members should clearly explain students how they should be using AI in the class.\n\nThey should talk about this in class and put it in the course syllabus.\n\nFaculty members need to know what are the benefits and drawbacks of the application of AI tools.\n\nThis includes being aware of any biases in the AI, and any ethical or data privacy issues.\n\nFaculty members should know the basics of AI working principles.\n\nThis will help them use AI properly and ethically in their teaching.\n\nFaulty members are encouraged to have regular training on how to use AI tools.\n\nThis training should cover not just the technical information but also ethical and legal knowledge of the technology.",
    "### Authorship and Accountability for AI-Assisted Works\n\nGenerative AI tools should not be acknowledged as authors because they cannot ensure the work’s accuracy, originality, and integrity - all of which are prerequisites for authorship.\n\nAs a result, students are responsible for all materials created with the assistance of AI technologies.\n\nStudents are encouraged to revise the output, as AI can produce potentially inaccurate, incomplete, or biased content.\n\nStudents must ensure that their paper is free from plagiarism, including in the text and images generated by AI.\n\nIt is a student’s obligation to provide attribution to all quoted content as per Formatting and Citing Requirements for Student Academic Papers: APA Style Summary (7th edition), updated on 1 September, 2023.\n\nIn cases where assignments contain factual inaccuracies or incorrect citations, the students will be held accountable, even if these errors originate from properly cited AI tools.\n\nSuch circumstances will result in the assignment receiving a lower grade and/or other penalties as per ISM University’s Code of Ethics.\n\nUnauthorized use of AI tools, where prohibited, will be treated as a violation of academic integrity, resulting in penalties as outlined by ISM University’s Code of Ethics.",
    "### Referencing AI-Generated Content\n\nIt is imperative that students provide an appropriate citation for the AI tool used, which involves:\n\n- Acknowledging the AI tool in references list and in-text citation.\n\n- Specifying the purpose of AI tools usage.\n\n- Providing a brief process description.\n\nThis applies even when the AI-generated content has been edited by the student or student-generated content was edited by AI tools.",
    "#### Describing the Purpose and Process of using Generative AI in Academic Papers\n\nAn explanation of how the AI tool was employed in the research should be provided in the Methods section or a comparable part of the academic paper.\n\nIn specific cases, a Professor can require to justify human input of AI generated content.\n\nFor literature reviews, essays, or response and reaction papers, the introduction can serve as a suitable location to describe the usage of the tool.\n\nThe text should include: the prompt used and the corresponding AI-generated text as a response.\n\nExample:\n\nIn response to the query \"Can music enhance cognitive performance?\n\n\", ChatGPT produced the following statement: \"Research suggests that listening to music can have a positive impact on cognitive performance, such as improving focus and memory retention\" (OpenAI, 2023).\n\nReference\n\nOpenAI.\n\n(2023).\n\nChatGPT (Mar 14 version) [Large language model].\n\nWhen dealing with extensive outputs from generative AI, like ChatGPT, it is recommended to incorporate the full text into an appendix of the paper or within online supplementary materials.\n\nThis approach gives readers access to the precise response generated.\n\nDue to the characteristic of ChatGPT which generates unique responses for every interaction, even with the same prompt, it becomes essential to document the exact produced text.\n\nIf the decision is made to add appendices or online supplementary resources, it is important to ensure that each one is referenced at least once within the core text of the APA style paper.\n\nExample:\n\nIn response to a secondary prompt such as \"Can you clarify the concept?\n\n\", the text produced by ChatGPT explained that \"networks of neurons work in tandem to facilitate various mental operations\" and \"the dedicated roles of specific networks can adapt based on experience and environmental contexts\" (OpenAI, 2023; refer to Appendix B for the entire conversation).\n\nReference\n\nOpenAI.\n\n(2023).\n\nChatGPT (Version dated Mar 14) [Large language model].",
    "## Closing Remarks\n\nISM University is committed to promoting responsible and ethical use of generative AI in teaching, learning, and student evaluations.\n\nThese guidelines are to be used as a framework for the successful integration of AI into our academic setting.\n\nISM University recognizes that advancements in technology or changes in overarching policies may necessitate updates and revisions to this policy.",
    "## References\n\nCambridge University Press.\n\n(2023).\n\nAuthorship and Contributorship.\n\nHartie School.\n\n(2023).\n\nArtificial Intelligence Tools at the Hartie School Teaching Guidelines for Faculty and Students.\n\nMonash University.\n\n(2023).\n\nPolicy and practice guidance around acceptable and responsible use of AI technologies.\n\nStanford University.\n\n(2023).\n\nGenerative AI Policy Guidance.\n\nThe University of Edinburgh.\n\n(2023).\n\nGuidance for students on the use of Generative AI (such as ChatGPT).\n\nUniversity of Puget Sound.\n\n(n.d.).\n\nAI-based writing tools: guidance and policy.\n\nUniversity of Toronto.\n\n(2023).\n\nChatGPT and Generative AI in the Classroom.\n\nOpenAI.\n\n(2023).\n\nChatGPT (Mar 14 version) [Large language model].",
    "### Guidelines for Using ChatGPT and other Generative AI tools at Harvard\n\nDear Members of the Harvard Community,\n\nWe write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI’s ChatGPT and Google Bard.\n\nThe University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\n\nGenerative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.",
    "### Initial guidelines for use of generative AI tools:\n\n- **Protect confidential data:** You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University’s Information Security Policy.\n\nInformation shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.\n\n- **You are responsible for any content that you produce or publish that includes AI-generated material:** AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called “hallucinations”), or may contain copyrighted material.\n\nReview your AI-generated content before publication.\n\n- **Adhere to current policies on academic integrity:** Review your School’s student and faculty handbooks and policies.\n\nWe expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools.\n\nIn the meantime, faculty should be clear with students they’re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work.\n\nStudents are also encouraged to ask their instructors for clarification about these policies as needed.\n\n- **Be alert for AI-enabled phishing:** Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale.\n\nContinue to follow security best practices and report suspicious messages to .\n\n- **Connect with HUIT before procuring generative AI tools:** The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds.\n\nIf you have procured or are considering procuring generative AI tools or have questions, contact HUIT at .\n\n- **Vendor generative AI tools must be assessed for risk by Harvard’s Information Security and Data Privacy office prior to use.\n\n**\n\nIt is important to note that these guidelines are not new University policy; rather, they leverage existing University policies.\n\nYou can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.\n\nSincerely,\n\nAlan M. Garber  \nProvost\n\nMeredith Weenick  \nExecutive Vice President\n\nKlara Jelinkova  \nVice President and University Chief Information Officer",
    "## About the AI Asia Pacific Institute\n\nThe AI Asia Pacific Institute focuses on addressing the social, legal, and ethical risks associated with artificial intelligence in order to unlock its potential for creating a sustainable world.\n\n---\n\nCopyright © 2023 AI Asia Pacific Institute.\n\nAll rights reserved.\n\nNo part of this publication may be reproduced or transmitted in any form or by any means, including photocopying and recording, or by any information storage and retrieval system.",
    "### Important disclaimer\n\nThe reader is advised and needs to be aware that the information contained in this publication may be incomplete or unable to be used in any specific situation.\n\nNo reliance or actions must therefore be made on that information without seeking prior expert professional, scientific and technical advice.\n\nTo the extent permitted by law, the AI Asia Pacific Institute (including its members and advisors) excludes all liability to any person for any consequences, including but not limited to all losses, damages, costs, expenses and any other compensation, arising directly or indirectly from using this publication (in part or in whole) and any information or material contained in it.\n\n---\n\nAfter engaging in a series of discussions with experts in the field of artificial intelligence (‘AI’), the AI Asia Pacific Institute previously disseminated a briefing paper that delves into the risks and prospects associated with ChatGPT.\n\nBuilding upon the foundation laid by the briefing paper, this Policy Brief explores nascent methodologies applicable to the regulation of generative AI systems.\n\nGenerative AI systems are models that generate new output—whether text, audio, or visual material—based on data they have been trained on.\n\nChatGPT is one example of a generative AI system.\n\nOther examples include Alibaba’s ‘Tongyi Qianwen’, Google’s ‘Bard’, and Microsoft’s ‘Vall-E’.",
    "### China\n\nChina is a central figure in the development of AI, and is expected to play a leading role in shaping the contours of the ‘Fourth Industrial Revolution’.\n\nChina has adopted a top-down approach to AI regulation, characterized by strong government intervention and control.\n\nThe Chinese government has issued comprehensive national strategies and plans for AI development, outlining specific goals, targets, and policy frameworks.\n\nIn contrast, Western countries generally follow a more decentralised approach featuring a combination of government regulations, industry self-regulation, and collaborative initiatives.\n\nOn a brief comparative analysis, the regulatory developments emerging from China place a greater emphasis on social responsibility and group and community relations, with relatively less focus on individualistic rights.\n\nThe regulatory framework includes the ‘Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)’ (‘Draft Measures for Generative AI Services’); Provisions on the Administration of Deep Synthesis of Internet Information Services (entered into force on 10 January 2023); and the Personal Information Protection Law (entered into force on 1 November 2021).\n\nAn official translation of the Draft Measures for Generative AI Services is not available at the time of writing.\n\nTherefore, this Policy Brief relies on Stanford University’s April 2023 translation.\n\nCrucially, a holistic analysis of China’s high-level regulatory developments requires policymakers to assess the structural, cultural, and political context that shapes its approach to and development of AI.\n\nAs is the case with all jurisdictions, a nuanced understanding of the domestic and international interests is paramount for policymakers.",
    "### The European Union\n\nThe EU’s regulatory framework includes the General Data Protection Regulation (‘GDPR’) and the proposed ‘AI Act’ (‘Draft EU AI Act’).\n\nUnder the 16 May 2023 compromise amendments to the Draft EU AI Act, generative AI systems will be considered ‘foundation models’, as they are ‘designed to optimize for generality and versatility of output’ and ‘trained on a broad range of data sources and large amounts of data’ (para 60(e)).\n\nThe explanatory paragraph (60g) states that this is intended to subject generative AI systems to specific requirements that distinguish such systems from ‘high-risk’ AI systems.\n\nAccordingly, article 28b requires providers of foundation models to ensure their systems comply with EU regulations prior to becoming available on the market by documenting risk assessment and mitigation; incorporating appropriate data sources and examining for biases.\n\nThe Draft EU AI Act has also received criticism from the AI ecosystem on the argument that it imposes excessive regulatory burdens and stifles innovation, contributing to Europe’s further disadvantage in AI.\n\nThe Draft AI Act is still undergoing review and revision, and it is expected that the final version will address some of the concerns raised during the consultation process.",
    "### The United States\n\nThe US does not have comprehensive federal regulations specifically focused on AI.\n\nHowever, various initiatives and efforts are underway to address AI-related concerns.\n\nThis mainly consists of Agency Guidance: federal agencies such as the Federal Trade Commission (‘FTC’), the National Institute of Standards and Technology (‘NIST’), and the Food and Drug Administration (‘FDA’) have all issued guidance and recommendations related to AI.\n\nFor instance, the FTC provides guidelines on consumer protection and privacy concerns associated with AI technologies, while NIST offers technical standards and best practices to promote trustworthy AI development and deployment and has proposed the AI Risk Management Framework, which is intended to be voluntary.\n\nSupplementing the Agency Guidance is the Blueprint for an AI Bill of Rights (2022) (the ‘Blueprint’).\n\nThe Blueprint—which is not binding—is intended to support the development of policies and practices to protect the civil rights and promote democratic values in the building, deployment, and governance of automated systems.\n\nUS regulation is lagging relative to China and the EU, with no actual or proposed binding federal legislation to date.\n\nThere have been ongoing discussions and legislative proposals in the U.S. Congress regarding AI regulation.\n\nThese efforts seek to establish federal laws that address a wide range of AI-related issues, including privacy, bias, transparency, and accountability.\n\nSeveral bills have been introduced, but as of now, no comprehensive federal AI legislation has been enacted.\n\nWhile this absence of federal legislation may be thought to encourage innovation, an alternative possibility is that clarifying the regulatory framework—or lack thereof—can increase investment in AI due to greater legal certainty.",
    "## Regulating Risks Posed by Generative AI Systems\n\nOf paramount importance is the proper categorisation of the risks posed by generative AI systems, distinguishing between the short-term and long-term risks.\n\nThis Policy Brief focuses on the short-term risks outlined in the Briefing Paper, which are (i) impersonation and disinformation; (ii) privacy and security; (iii) bias and discrimination; and (iv) intellectual property infringement.\n\nThe regulatory strategies used, or proposed, to mitigate these risks are discussed in turn.",
    "### Impersonation and Disinformation\n\nGenerative AI systems have the capacity to mimic individuals, resulting in the proliferation of advanced disinformation and fraudulent activities.\n\nThis section considers two epistemic threats.\n\nThe first threat concerns disinformation created by malicious agents, like phishing messages.\n\nThe second threat concerns factual failures and reasoning errors generated by AI systems, referred to as ‘hallucinations’.\n\nThese threats can be thought of as human-made disinformation and AI-generated misinformation, respectively.\n\nIn regard to human-made disinformation, regulations can be imposed on both the providers and users of generative AI systems.\n\nThe EU’s 2022 Strengthened Code of Practice on Disinformation provides a set of guidelines for signatories to better self-regulate.\n\nCommitment 15 provides that signatories can mitigate human-made disinformation by warning users of the AI systems and proactively detecting such content.\n\nThis standard mirrors article 52(1) of the Draft EU AI Act, which provides that, unless it is ‘obvious from the circumstances and the context of use’, providers must inform natural persons they are interacting with their AI systems.\n\nTo prevent the distribution of manipulative content, article 52(3) provides that users of AI systems who create deepfakes must disclose that they have been artificially generated.\n\nA report by the European Parliamentary Research Service notes that this labelling obligation ‘could be a first step towards mitigating potential negative impacts’, but is insufficient to address other issues.\n\nThe Draft EU AI Act neither contains guidelines for disclosure nor includes sanctions for non-compliance (article 71).\n\nFurther, it is unclear how actors sharing deepfakes anonymously would be held accountable.\n\nChina’s ‘Provisions on the Administration of Deep Synthesis of Internet-based Information Services’, which came into force on 10 January 2023, are instructive here.\n\nThese provisions prohibit persons from using generative AI systems to produce or disseminate false information (article 6); require the authentication of persons using deepfake technologies (article 9); and establish mechanisms for refuting rumours created by such systems (article 11).\n\nPolicymakers should consider whether different sociopolitical conditions would affect the feasibility of these regulations within their own countries.\n\nBecause Chinese internet users have a greater ‘digital fingerprint’, as they are often required to link their online accounts to their government-ID-linked phone numbers, malicious agents may struggle to conceal their identities online.\n\nThere may be competing policy objectives, as suggested by measure 15.2 of the 2022 Strengthened Code of Practice on Disinformation, which notes that policies to detect and sanction the impermissible use of deepfake technologies must be trustworthy and ‘respect the rights of end-users’.\n\nWith respect to AI-generated misinformation, regulators ought to maintain a balanced approach.\n\nIn a blog post concerning GPT-4, OpenAI acknowledged that ‘new risk surfaces’ will emerge as models become more powerful and their fields of content expand.\n\nIt is conceivable that the complete eradication of hallucinations may be an unattainable objective.\n\nGreater attention is required in the EU to address this issue.\n\nAs has been observed elsewhere, the original proposal of the Draft EU AI Act did not foresee the proliferation of generative AI systems.\n\nThe GDPR addresses this gap to a limited extent.\n\nIn circumstances where the hallucination provides false information about a particular individual, a claim may arise under article 5 of the GDPR, which provides that personal data must be accurate, and reasonable steps must be taken to rectify or erase inaccurate data.\n\nIn addition to the aforementioned scenario, the absence of provisions governing hallucinations presents a potential peril whereby individuals may excessively depend on systems without engaging in discerning assessment of their outputs.\n\nIt must be noted that the providers of AI systems are not entirely blameworthy for hallucinations in all circumstances.\n\nUsers may unintentionally pose a question in a manner that causes the system to ‘untether’ from factual training data.\n\nFor example, when asked to provide ‘at least five examples, together with quotes from relevant newspaper articles’ of sexual harassment by American law professors, ChatGPT falsely listed the (real) George Washington University Law School professor, Jonathan Turley.\n\nThe response (falsely) claimed that Turley was a member of Georgetown University Law Center, citing a (non-existent) Washington Post article that supposedly reported the harassment occurred during a class trip to Alaska (which Turley clarified had never happened before).",
    "As indicated within the recommendations section, regulatory measures aimed at mitigating the incidence and associated hazards of hallucinations may necessitate comprehensive coverage of both the development and utilization of generative AI systems.",
    "### Privacy and Security\n\nModels trained on personal data can generate highly realistic and identifiable information, creating risks for privacy and security.\n\nThis concern transcends the realm of individual privacy and security, encompassing broader considerations.\n\nIn April 2023, Samsung banned their employees from using ChatGPT following concerns that internal sensitive code that had been uploaded could be provided to other users.\n\nThe GDPR is more relevant to privacy and security issues than the Draft EU AI Act.\n\nThe GDPR requires consent from individuals before collecting personal data (article 7); provides individuals with the rights to access their personal data (article 15) and delete it (article 17); and contains measures to protect personal data from unauthorised access, use, or disclosure (article 21).\n\nSince March 2023, OpenAI and the ‘Garante’—Italy’s data protection authority—have contested the lawfulness of ChatGPT’s data collection processes.\n\nThis reveals the regulatory challenges that generative AI systems pose to the GDPR.\n\nSpecifically, the Garante called for a range of measures concerning data processing, which sought to resolve four identified breaches of the GDPR.\n\nFirstly, OpenAI did not initially prevent minors from accessing ChatGPT.\n\nSecondly, ChatGPT can generate inaccurate information about people (ie hallucinate).\n\nWhether the failure to explain how personal data is processed, and failure to explain responses that utilise personal data are, breaches of the GDPR is a contested issue.\n\nThe ‘right to explanation’ is not mentioned in the GDPR’s articles, and the relevant Recital (Recital 71) has no legal force.\n\nNevertheless, article 5 of the GDPR concerns erasure or rectification of inaccurate personal data.\n\nThirdly, users were not provided with an explanation as to how their data was being collected.\n\nArticle 17 of the GDPR provides for the ‘right to be forgotten’ such that an individual can request to have their data removed from the model.\n\nIt is worth noting that an absolute ‘right to be forgotten’ may be unattainable.\n\nThere are concerns that personal data becomes embedded in generative AI systems, making it ‘nearly impossible to remove all traces of an individual’s personal information’.\n\nLastly, the large amounts of personal information being collected to train future iterations of ChatGPT could not be justified under any of the six bases in article 6(1) of the GDPR.\n\nThe US Blueprint lists ‘data privacy’ as a guiding principle.\n\nThe Blueprint calls for privacy protection by default, with an ongoing review for privacy risks; minimising data collection by confining it to situations where it is ‘strictly necessary to [achieve] the [system provider’s] identified goals’; and ensuring best practices are followed to prevent data leaks beyond the consented use case.\n\nFurther, the Blueprint outlines rights for the peoples whose data is being collected, including the rights to access that data; know who has access to that data; correct the data where necessary; and request the deletion of their data.\n\nCertain domains (eg health, employment, education) are identified as deserving of enhanced data protection.\n\nThis includes an ethical review of sensitive data that may limit opportunities or access to services; auditing data quality to ensure it is not inaccurate; limiting the extent to which sensitive data can be shared, sole or made public; and additional reporting requirements where necessary.\n\nChina’s Draft Measures for Generative AI Services contains similar measures.\n\nThe consent of data subjects is required for personal data used in the pre-training and optimisation of generative AI systems (article 7(3)).\n\nFurther, providers must ‘ensure the data’s veracity, accuracy, objectivity, and diversity’ (article 7(4)).\n\nOnce this data is collected, providers have an obligation to protect it and treat it appropriately.\n\nPursuant to the lex generalis article 11, providers must not ‘illegally preserve input information from which it is possible to deduce the identity of users, … conduct profiling on the basis of information input by users and their usage details, and … provide information input by users to others’.\n\nLast, the ‘right to be forgotten’ is recognised in article 13, which provides that providers must ‘promptly handle individual requests concerning revision, deletion, or masking of their personal information’.",
    "### Bias and Discrimination\n\nGenerative AI systems can reflect biases present in their training data, entrenching discriminatory narratives.\n\nOne of the highlighted issues presented by experts is the importance of improving the transparency of AI systems.\n\nHowever, gaining complete insight into how generative AI systems are trained may be an unattainable objective.\n\nThe combination of complex training algorithms, proprietary considerations, large-scale data requirements, iterative processes, and the evolving nature of research and development pose challenges that can limit transparency and make it difficult to fully understand the intricacies of the training process.\n\nThe European Commission has recently proposed auditing requirements for very large online platforms (‘VLOPs’) and search engines (‘VLOSEs’), which would be inserted into the Digital Services Act.\n\nThe current proposal would impose an obligation on VLOPs and VLOSEs to provide vetted researchers with privileged access to data; subject themselves to annual independent audits; and publish reports on content moderation, risk assessments and risk mitigation.\n\nAt the time of writing, ChatGPT and other generative AI systems are not included in the list of 17 VLOPs or 2 VLOSEs.\n\nHowever, paragraph 29 of the proposal indicates that these systems may be subject to requirements if they are used by the listed VLOPs and VLOSEs.\n\nIn addition, the newly proposed article 28b of the Draft EU AI Act provides that foundation models—which include generative AI systems like ChatGPT—must contain ‘only datasets that are subject to appropriate data governance measures’.\n\nProviders of such models must therefore take appropriate measures to examine the suitability of the data sources and mitigate possible biases.\n\nIn China, generative AI systems must ‘respect social virtue and good public custom’.\n\nThus, article 4(2) of the Draft Measures for Generative AI Services provides that measures must be taken to prevent discrimination ‘on the basis of race, ethnicity, religious belief, nationality, region, sex, age, or profession’.\n\nThis must occur throughout the lifespan of the AI system, including during the stages of algorithm design, selecting training data, model generation and optimisation, and service provision.\n\nLastly, the US Blueprint lists ‘algorithmic discrimination protections’ as a guiding principle.\n\nThe Blueprint calls for a ‘proactive assessment of equity’ during a system’s design phase; ‘representative and robust data’ that mitigates potential biases; proactive testing to guard against discrimination via proxy data; and disparity assessment and mitigation.",
    "### Intellectual Property Infringement\n\nFinally, generative AI systems raise questions about intellectual property rights and the implications of generating copyrighted works.\n\nProviders have already faced liability risks, with class action suits having been brought against companies like Github for their code-generating AI and Midjourney for their art-generating tool.\n\nSimilar regulations have been proposed across China and the US.\n\nIn China, article 4(3) of the Draft Measures for Generative AI Services provides that generative AI systems must ‘respect intellectual property rights and commercial ethics’.\n\nArticle 7(2) accordingly prohibits the use of data that infringes intellectual property rights from being used as pre-training or optimisation material for generative AI systems.\n\nIn the US, the proposed AI Risk Management Framework states that training data subject to copyright should align with relevant intellectual property right laws.\n\nMeanwhile, article 28b(4)(c) of the Draft EU AI Act imposes an obligation on providers of generative AI systems to publicly disclose the training data used that is protected under copyright law.\n\nThis transparency provision was favoured over a blanket prohibition on the use of copyrighted material as training data.\n\nUltimately, this is a balancing act between protecting individuals’ (intellectual property) rights on one hand and fostering innovation on the other.\n\nThe application of copyright law to machines—as opposed to the human creative process—raises a fundamental question moving forward: should copyright law protect an artist’s creative expression or their style more broadly?\n\nThis discourse has been further convoluted by the recent position adopted in Japan, wherein copyrighted materials incorporated within AI datasets are exempted from the purview of copyright law, except in cases where such utilisation would unreasonably jeopardize the rights of the copyright owner.",
    "## Conclusion and Recommendations\n\nThe regulatory framework surrounding AI is characterised by rapid evolution.\n\nAs of the present writing, the United States Government is actively soliciting public input for forthcoming AI regulations, while divergent opinions persist among experts within the AI community regarding the potential suspension of training activities for systems surpassing the capabilities of GPT-4.\n\nThis section presents a conclusive overview of the proposed methodologies aimed at regulating the aforementioned risks associated with AI.\n\nOf paramount importance is the proper categorization of these risks, distinguishing between the short-term and long-term risks posed by generative AI systems.\n\nAs mentioned already, this Policy Brief places high emphasis on the short-term risks of the technology.",
    "#### Impersonation and Disinformation\n\n- For users, inform them that they are interacting with generative AI systems.\n\n- For users, inform them of the AI system’s limitations, which improves their ability to critically evaluate its output.\n\n- For users, establish platforms where they can refute the validity of harmful generated content.\n\n- For users, require identity verification before enabling access to generative AI systems.\n\n- For providers, require detection procedures to identify malicious generated content.\n\n- For providers, require content produced by generative AI systems to be labelled as generated.\n\n- For providers, require ‘stress testing’ of models to test the vulnerability to hallucinations.",
    "#### Privacy and Security\n\n- For users, enable age-gating to protect the personal data of minors.\n\n- For users, provide notice and require their consent to collect and use data for training purposes.\n\n- For providers, minimise the amount of personal data collected for training purposes.\n\n- For providers, allow users to opt-out of having their personal data used for training purposes.\n\n- For providers, establish mechanisms that allow users to delete their personal data from existing datasets.",
    "#### Bias and Discrimination\n\n- For providers, establish bias mitigation measures throughout the lifespan of their systems, including the following stages: algorithm design, selecting training data, model generation and optimisation, and service provision.\n\n- For providers, exercise enforcement on the disclosure of data training models.\n\n- For providers, require regular independent audits and reporting on risk assessment and mitigation measures.",
    "### Long-Term Risks\n\nThe longer-term risk that generative AI systems may pose is in accelerating society’s arrival at Artificial General Intelligence (‘AGI’).\n\nAGI refers to the ability of an AI system to perform a variety of tasks in different contexts and environments.\n\nThe concept of AGI exists in contrast to our current, ‘narrow’ AI systems, which perform tasks in specified contexts.\n\nWithin the AI community, there exists a division of perspectives regarding the fundamental inquiries surrounding the feasibility of AGI and, if indeed attainable, the prospective timeframe for its realization.\n\nAssuming that AGI proves viable, apprehensions arise regarding its potential to profoundly and adversely impact humanity, potentially engendering existential perils.\n\nThese concerns demand equitable recognition and warrant further scrutiny.\n\nIn this vein, there are recommendations that policymakers increase investment in ‘safety and alignment research’ so that our understanding of AI’s long-term risks keep pace with technological developments.\n\nNonetheless, it is of utmost significance to uphold the differentiation between short-term and long-term risks, thereby averting potential diversions from the pivotal role that regulation may play in offering remedies to the aforementioned risks.",
    "## Endnotes\n\n1.\n\nOne prominent example has been the ‘hundreds’ of fake profiles of Ukrainian President, Volodymyr Zelenskyy, that attempt to trick individuals into sending them money: Peter Suciu, ‘There Are Now Hundreds Of Volodymyr Zelenskyy Impersonators On Social Media’, Forbes (online, 16 March 2022) <\n2.\n\nThis term has not been universally approved of, largely on the basis that it frames the problem in a manner that appears to anthropomorphize AI systems rather than depict the issue as one of ‘untethered’ text generation: Ben Zimmer, ‘“Hallucination”: When Chatbots (and People) See What Isn’t There’, Wall Street Journal (online, 20 April 2023) <\n3.\n\nDeepfakes are AI-generated audio or visual content.\n\nThey need not be manipulative, and have recognisably beneficial purposes in areas like education and entertainment.\n\nHowever, deepfake technology can be used maliciously.\n\nA 2019 report by Deeptrace Labs found that the vast majority of deepfake videos (96%) were non-consensual pornography: see Deeptrace Labs, The State of Deepfakes (Report, 2019) 1, 6.\n\n4.\n\nAgain, there is no official English translation of these regulations, so the Stanford University translation is relied on: Rogier Creemers and Graham Webster, ‘Translation: Internet Information Service Deep Synthesis Management Provisions (Draft for Comment) – Jan. 2022’, DigiChina Stanford (online, 4 February 2022) <\n5.\n\nSee, eg, Toby Walsh, ‘The Singularity May Never Be Near’ (2017) 38(3) AI Magazine 58.\n\nCf Sébastien Bubeck et al, ‘Sparks of Artificial General Intelligence: Early experiments with GPT-4’ [2023].",
    "## Quick Links\n\n- Instructor resources for Artificial Intelligence & ChatGPT tools\n- Tips for students\n\nArtificial intelligence is everywhere and with it comes new opportunities and challenges.\n\nAs these new platforms evolve so must the way instructors, TAs and students use them.\n\nGiven the rapid pace of change and growth in artificial intelligence - this issue will be ongoing for the foreseeable future.\n\nAt present, it is important for instructors to be explicit about whether artificial intelligence or tools like ChatGPT are allowed to be used to complete assignments, tests or exams, and if so, the extent to which it is allowed, and if it should be cited and how to cite it.\n\nA student who does not comply with the instructors rules about the use of such tools will be subject to Policy 71 and an investigation into academic misconduct.\n\nA memo from the Associate Vice-President, Academic about Chat GPT (and other AI tools) can be found here.\n\nFAQ for Instructors and TAs can be found here.\n\nWiley has also put out a publication on AI in Higher Education: Current Uses and Future Applications which can be found here.\n\nThinking about Generative Artificial Intelligence in the Classroom (Video): An overview for University of Waterloo instructors on considerations for generative artificial intelligence in teaching and learning.\n\nIncludes recommendations instructors may wish to implement in their teaching practice.",
    "## Syllabus AI Policies\n\nPOLICY IDEAS FOR THE USE OF AI IN HIGHER EDUCATION\n\nArtificial Intelligence (AI) is one of the key transformative technologies of the coming century.\n\nIts impact on research institutions and universities is already significant today, and is likely to continue growing intensively over the coming years.\n\nFor students and scholars, AI can be a powerful tool that enhances their learning experience or research endeavors – but, if used improperly, it can also degrade the quality of a student’s education and undermine the rigor and impact of scholarly research.\n\nMuch depends on the specific pedagogical or intellectual context within which AI tools are being used, and on the ways in which those tools either support or weaken the underlying goals of a given academic endeavor.\n\nSince these contexts vary widely, it is the policy of Vanderbilt University to leave it up to individual faculty to decide whether and how AI tools are used in their courses or research work.\n\nBelow we include the most recent Guidance from Faculty Affairs, which stresses the importance of each faculty member deciding whether or how to incorporate generative AI into course expectations.\n\nIn an effort to assist faculty in making these their own policies, the AIGC has catalogued example policies regarding the academic uses of AI as falling along a broad spectrum that ranges from permissive to restrictive.\n\nHere we present examples of such policies that have been adopted by Vanderbilt faculty hailing from a variety of disciplines.\n\nWe have divided these examples into four basic gradations: permissive; slightly restrictive; moderately restrictive; and completely restricted.\n\nIf you would like to submit your own policy to be included, please click here\n\n(",
    "### POLICY STATEMENT:\n\nArtificial Intelligence is a valuable, dynamic, and vital tool.\n\nAs with all other tools, you must abide by all applicable laws and policies when you use AI, and you should strive to use AI ethically.\n\nYou should follow guidelines (including prohibitions) provided by your dean, instructor, supervisor, or other individual(s) overseeing your work.\n\nYou should disclose the use of AI in an appropriate way.\n\nYou should exercise sound judgment in your use of AI.\n\nWhen AI is used, it should be used in ways that are consistent with university policies on confidentiality and privacy.\n\nThis applies to all aspects of research, service work, and creative expression.\n\nYou are the author of content that you produce with AI and responsible for its accuracy, impact, and compliance with relevant laws and policies.",
    "### PRINCIPLES/GOALS:\n\n- Faculty should decide whether and how generative AI is used in courses.\n\n- Faculty should clearly communicate expectations to students.\n\n- Faculty should clearly communicate what constitutes academic dishonesty.\n\n- Students are responsible for understanding the rules of engagement for using AI in each of their courses and seeking out information if they do not understand or are unsure how to comply.\n\n- Faculty and students are responsible for using AI appropriately and ethically.\n\n- Faculty and students should disclose the use of AI in their work if such disclosure is expected.\n\n- Faculty and students are the authors of content generated by AI and are responsible for that content as they are with content that they author.\n\n- AI should be used in ways that respect confidentiality and privacy.\n\n- AI should be used legally, ethically, and reasonably.",
    "### Scott Crossley, Peabody\n\nGenerative AI.\n\nYou can use generative AI models (ChatGPT, GPT, DALL-E, Stable Diffusion, Midjourney, GitHub Copilot, and anything after) as needed, for any purpose, at no penalty as long as you recognize the model’s contribution.\n\nFailure to acknowledge the use of AI models will be penalized as plagiarism.\n\nHowever, please note that AI models tend to include incorrect information, fake citations, and inaccurate outputs.\n\nYou are responsible for any inaccurate, biased, offensive, or otherwise unethical content submitted regardless of whether it originally comes from you or an AI model.",
    "### David Schlundt, Psychology\n\nThe worlds of science and academics are rapidly changing with the introduction of complex-language AI software.\n\nThis software, like ChatGPT, can be very useful when researching topics.\n\nYou are encouraged to explore AI tools to find and organize knowledge.\n\nI strongly recommend that you avoid pasting AI output into your papers.\n\nYou will have to back up what you write with citations to studies, literature reviews, and other scholarly works.\n\nHelping you become a better writer is one of the most important course goals.\n\nYou may use AI but write the papers using your own words and only use citations that you have accessed and reviewed for accuracy.\n\nAll papers will be reviewed for plagiarism using Turnitin software.\n\nThe software now includes the ability to detect text that was generated using AI.\n\nPlagiarism is a violation of the Honor code.\n\nCurrently, I will treat copying text from AI as a reason for deducting points to further encourage you to write in your own words.",
    "### Michael Bess, History\n\nFor the purposes of this course, the use of AI tools such as GPT4, Bing, Claude, or Bard falls under two categories:\n- text-generation tool (prohibited);\n- research, brainstorming, and editorial aid (permitted).\n\nTurning to AI text-generation tools can all too easily degrade the quality of a student’s learning experience in thinking through the material for themselves and creating their own work.\n\nOn the other hand, certain targeted usages of AI tools can powerfully augment the learning experience for students, when conducted in a way that enhances their critical thinking, creativity, and research capabilities.\n\nPlease read the detailed description of this policy (and the rationale for it) below.\n\nFailure to adhere to this policy, or to acknowledge your use of such a tool, constitutes a violation of Vanderbilt’s honor code.\n\nFeel free to consult me if you have any questions about this.\n\nClick here for a full description and rationale for Bess's policy\t\n\nUnattributed (adapted from Monica Linden, Brown University).\n\nThe use of AI has exploded in just this last year.\n\nAlthough many of the assignments in this class will not lend themselves to use AI (I think) because a lot of it is personal reflection, it’s possible that it could be used in some instances.\n\nSo for our purposes, I ask that you report your use, using the following guidelines.\n\nThere will be no deduction in your grade for the reported use of AI as this is an experimental time.\n\nAt the bottom of your assignments please write the statement that most closely describes your use.\n\nIf you have used it extensively, I will call you in for a conversation on a case-by-case basis.\n\n- I did not use Generative AI for this assignment.\n\n- Generative AI helped with grammar and readability, but the original writing was my own.\n\n- Generative AI helped with idea generation, but I wrote the assignment myself.\n\n- A small amount of my work (just a couple of sentences) was written by Generative AI.\n\n- Generative AI significantly wrote my work, but I edited it.\n\nAn example of unacceptable use of AI would be to have it write a song on a particular topic without making significant changes.\n\nIf you do use it to give you some ideas, you will need to submit a screenshot of the prompt and the song the AI wrote along with your assignment.\n\nIf you do use AI in your work, you must properly cite it.\n\nLook at these guidelines for how to cite properly in APA style: \n\n(\n\nI reserve the right to modify the guidelines as needed.",
    "### Ole Molvig, History/CSET\n\nIn this course, students are allowed limited and intentional use of AI text-generation tools such as ChatGPT, Bing, or Bard in crafting essays or other assignments for this course.\n\n(The only exception is for specific assignments in which we explicitly instruct students to use such a tool.)\n\nCare should be taken in the use of AI for education.\n\nTurning to AI text-generation tools can all too easily degrade the quality of your own learning experience in thinking through the material for yourself and writing your own essay.\n\nTherefore it is not allowed to simply use AI text-generation tools such as ChatGPT, Bing, or Bard merely to replace your own effort.\n\n(In other words, you may not simply enter the essay prompt, hit generate, and turn the work in as your own.)\n\nAI tools may be employed in the brainstorming, research, and editing of class assignments, but the final, submitted essay must be primarily written by the student author.\n\nText predominantly generated by an AI must be cited as such, as you would a human author.\n\nNote, rarely will an AI expert be a preferred citation to a human expert.\n\nIn the use an AI text-generation tool, you must include a citation of this tool in your references, indicating in detail the ways in which you used this tool in crafting your assignment.\n\nFailure to acknowledge your use of such a tool constitutes a violation of Vanderbilt’s honor code.\n\nFor a full description of this policy (and the rationale for it), please see the statement in the class pack; do feel free to reach out to us if you have any questions about this policy.",
    "### Richard Alan Peters II, Electrical and Computer Engineering\n\nUse of generative AI.\n\nYou are free to use generative AI algorithms such as Chat-GPT in your work.\n\nHowever, You must:\n- Cite any text that the AI generated (even if you edited it) with a bibliography entry that includes the name and version of the AI model that you used, the date and time it was used, and includes the exact query or prompt that you used to get the results.\n\n- Cite, as described in rule 1, any code that you had it generate for you.\n\nI recommend that you not ask it to write code for you.\n\nDoing so will probably be more work than simply writing it yourself.\n\nBecause:\n  - You must thoroughly test the code to prove that it works.\n\n- You must explain what you did to verify that it works.\n\n- To demonstrate that you understand it, you must comment every single logical object be it a data structure or line or short block of code that it generates.\n\nI.e.\n\nexactly what that bit of code does and how it does it.\n\n- The code must follow the other rules.\n\nFor example, the assignment may have stated restrictions on methods, procedures, external libraries, or programs.\n\n- It must generate the results that are asked for in the assignment instructions.\n\nI hope that by following these rules you will learn how to use generative AI as an assistant to increase your productivity in writing and coding.\n\nIf you fail to follow these rules, that will be an honor code violation and you will be referred to the Honor Council.",
    "## STRONGLY LIMITED POLICIES\n\nUnattributed\n\nYou may use AI programs e.g.\n\nChatGPT to help generate ideas and brainstorm.\n\nHowever, you should know that the material generated by these programs may be inaccurate, incomplete, or otherwise problematic.\n\nYou may not submit any work generated by an AI program as your own.\n\nIf you include material generated by an AI program, it should be cited like any other reference material (with due consideration for the quality of the reference, which may be poor).\n\nAny plagiarism or other form of cheating will be dealt with severely under relevant Vanderbilt policies.\n\nOf course, there are many cases where I may not be able to tell if you cheated or not.\n\nNote however that, for any reason, including doubts about whether the paper was written with external help, any student can be required to come in for an oral examination on their paper.\n\nIn such a case, a grade for the oral examination will replace that for the paper.",
    "### Anna Castillo, Spanish\n\nPolicy regarding ChatGPT and other AI tools: The objective of this course is to help you to develop your own voice as a film critic while cultivating your intellectual creativity.\n\nThe use of generative AI to substantially complete your homework goes against this objective and the honor code.\n\nHaving said this, learning to use artificial intelligence (AI) language models is an emerging skill, and I can give you advice about using it in a responsible and effective manner.\n\nIf you have any doubts, ask me.\n\nIt is important not to outsource your learning in this course.\n\n(translated from Spanish)",
    "### Ray Friedman, Owen\n\nYou are writing about your own personal experiences, which are not in the AI databases.\n\nTherefore, chat bots cannot answer the assignments.\n\nChatbots can sometimes help with writing.\n\nIf you would like to use AI to help your writing, please submit a) your own version (an outline, at least) and b) the AI-enhanced final version.\n\nI will assess your final version only but want to also have a clear sense of your initial input.\n\n(Careful!\n\nAI can also sometimes produce vapid buzz-word-filled sentences that make no sense.\n\nIt is up to you to decide what words best represent your ideas.)",
    "## RESTRICTED POLICIES\n\nUnattributed\n\nDue to the centrality of the craft of writing in this course, students may use NO AI text-generation tools (such as ChatGPT, Bing, or Bard, or similar) in any way in crafting their essays or other assignments for this course.\n\nTurning to AI text-generation tools can all too easily degrade the quality of the student’s learning experience in thinking through the material for themselves and creating their own essay.\n\nThe use of such a tool constitutes a violation of Vanderbilt’s honor code.\n\nFeel free to reach out to me if you have any questions about this policy.\n\n---\n\n© ( 2024 Vanderbilt University · All rights reserved.\n\nSite Development: Digital Strategies (Division of Communications) (\n\nVanderbilt University is committed to the principle of equal opportunity.\n\nVanderbilt University does not discriminate against individuals on the basis of their race, sex, sexual orientation, gender identity, religion, color, national or ethnic origin, age, disability, military service, or genetic information in its administration of educational policies, programs, or activities; admissions policies; scholarship and loan programs; athletic or other University-administered programs; or employment.\n\nAccessibility information.\n\nVanderbilt®, Vanderbilt University® and the Vanderbilt logos are trademarks of The Vanderbilt University.",
    "# AI Course Policies and Assignment Guidelines\n\nBefore you write a policy about AI, you need to understand how available AI tools might be used by your students.\n\nSpend some time engaging with AI to see how it can be used productively, or not, with the assignments you intend to give your students.\n\nYou may find this “play” will lead to assignment revision, and it will certainly help you develop a useful approach to guiding students on appropriate and inappropriate uses of AI.\n\nVery likely, you will find that your guidance and policies will need to be catered to different types of assignments.\n\nAs of 05/15/2023, the University’s Academic Dishonesty policy has changed slightly to include a clause on work completed by entities that are not human: “Academic dishonesty is any attempt by a student to submit 1) work completed by another person or entity without proper citation or 2) to give improper aid to another student in the completion of an assignment, such as plagiarism.” This change helps establish, at institutional level, that submitting AI-generated content in place of one’s own work constitutes plagiarism.\n\nAt the individual course level, you may consider adding a clarifying statement to your syllabus, such as:\n\n“Use of Artificial Intelligence (AI) to produce or help content without proper attribution or authorization, when an assignment does not explicitly call or allow for it, is plagiarism.”\n\nA few other AI-related things to consider adding to the syllabus:\n\n- Information about generative AI’s tendency to hallucinate + clear ground rules about students’ accountability for verifying any AI outputs they consult or reference;\n- A notice about using AI ethically and safely.\n\n(ChatGPT acknowledges that they may share account holders’ personal information with third parties, including vendors and service providers — see their Privacy Policy.\n\nTeach your students to never share personal and sensitive information with generative AI chatbots.)\n\n- A description of under what circumstances students will be permitted or encouraged to use generative AI in your course.\n\n- Information about how students should cite or credit AI.\n\nAll courses will very likely not have the same policy.\n\nThat’s part of what we need to teach students — that faculty are designing courses and assignments with different purposes and directions, and thus careful attention to directions around the appropriate use of AI needs to become part of students’ regular practice.\n\nIf you are looking for ideas for syllabus language, Montclair faculty statements are being collected, and Lance Eaton of College Unbound has organized this diverse collection of statements from faculty active in AI discussions.\n\nLast Modified: Monday, August 21, 2023 8:26 am\n\nVS\n\n1 Normal Ave. Montclair, NJ 07043\n\n973-655-4000\n\nCampus Map",
    "# K-12 Generative Artificial Intelligence (Gen AI) Readiness Checklist Questionnaire Introduction\n\nThe Council of the Great City Schools (CGCS) and CoSN (Consortium for School Networking) worked in partnership with Amazon Web Services (AWS) to develop a K-12 Generative AI (Gen AI) Readiness Checklist Questionnaire.\n\nThis questionnaire is designed to guide K-12 school districts in understanding key factors to consider before implementing Gen AI technologies.\n\nWhile not an exhaustive readiness assessment, the checklist serves as a preliminary tool for district leaders—including superintendents, district leaders and technology experts—to evaluate the safety, privacy, security, and ethical implications of using Gen AI.\n\nThe goal is to help districts prepare adequately, ensuring data privacy and security, and avoiding bias or algorithmic discrimination, while gaining a foundational understanding of the related tactical considerations.\n\nThe K-12 Gen AI Readiness Checklist Questionnaire is the first tool that CGCS, CoSN, and AWS are planning to introduce to school districts to help them prepare their organizations for adoption of this transformative technology.\n\nOur next phase will build upon the foundational considerations outlined in this checklist.\n\nWe plan to introduce a more comprehensive tool that will allow districts to self-evaluate their readiness for implementing Gen AI technologies.\n\nThis rubric will help districts identify their maturity level in terms of Gen AI readiness and pinpoint specific areas that require further attention to ensure safe and secure adoption of this transformative technology.\n\nWe hope that school districts will find the K-12 Generative AI (Gen AI) Readiness Checklist Questionnaire both useful and informative.\n\nThis tool is intended to serve as a starting point for intelligently and thoughtfully implementing AI technologies that align with instructional and operational objectives.\n\nWe encourage district leaders to adapt this resource to their unique needs.\n\nOnce the Creative Commons License is finalized, districts will have the flexibility to modify and share the tool, further contributing to its value as a community resource.",
    "# Table of Contents\n\nPage 3: Executive Leadership Readiness (24 Questions)  \nPage 6: Operational Readiness (9 Questions)  \nPage 7: Data Readiness (25 Questions)  \nPage 10: Technical Readiness (16 Questions)  \nPage 12: Security Readiness (7 Questions)  \nPage 13: Legal/Risk Management (12 Questions)  \nPage 15: Appendix 1 – Glossary  \nPage 18: Appendix 2 – Policies  \nPage 20: Our Contributors",
    "# Creative Commons License\n\nWe would like to extend our thanks to everyone who contributed to the development of this Readiness Checklist, including:\n\n- **Willie Burroughs**: Director of Management Services, Council of the Great City Schools  \n- **Tom Ryan**: Co-Founder, K12 STAG; former Chief Information Officer of Albuquerque & Santa Fe Public Schools; representative of CGCS  \n- **Shahryar Khazei**: President, Macaw Consulting; former Chief Information Officer of Los Angeles Unified School District; representative of CGCS; CoSN Board Member  \n- **Tami Lundberg**: Chief Technology Officer, Fresno Unified School District  \n- **Don Wolff**: Chief Technology Officer, Portland Public Schools  \n- **Eva Mendoza**: Chief Information Technology Officer, San Antonio Independent School District; CoSN Board Member  \n- **Richard Charles**: Chief Information Officer, Denver Public Schools  \n\n**CoSN**  \n- **Keith Krueger**: CEO  \n- **Michael Nehmer**: Communications Specialist  \n- **Jennifer Prescott**: Director, Marketing & Communications  \n\n**AWS**  \n- **Annmarie Lehner**: K-12 Business Development & Strategy Leader, Amazon Web Services (AWS)  \n- **Derek Root**: K-12 Business Development Manager, AWS  \n- **Praveen Katta**: K-12 Business Development Manager, AWS  \n- **Kevin McCandless**: K-12 Solutions Architect, AWS  \n- **Mary Strain**: AL/ML Education Lead, AWS  \n- **Vadim Omeltchenko**: Sr.\n\nSolutions Architect Specialist AI/ML, AWS  \n- **Monica Carranza**: Digital Innovation Leader, AWS",
    "# Preparing for Generative AI in the 2024 Election: Recommendations and Best Practices Based on Academic Research\n\nThis white paper is the result of a partnership between the University of Chicago Harris School of Public Policy and the Stanford Graduate School of Business intended to generate non-partisan, expert recommendations on various critical AI governance issues.\n\nIn late August 2023, the two institutions hosted a convening of experts from academia, technology, industry, and civil society who explored the challenges of generative artificial intelligence for the 2024 U.S. Election and opportunities for establishing best practices and governance guidelines.",
    "### Executive Summary\n\nThe rapid development of generative AI technology is transforming the political landscape, presenting both challenges and opportunities for the 2024 US election.\n\nThis document provides a research-based overview of the potential impact of generative AI and offers best practices to safeguard the electoral process.\n\nThe purpose of our review is not to recommend many specific legal or policy actions, but rather to promote clear understanding among voters, journalists, civil society, tech leaders, and other stakeholders about both the risk and promise of AI for electoral democracy in the hope of fostering a more productive public discussion of these issues.",
    "#### Manipulation and Microtargeting:\n\nThere is concern that AI-powered micro-targeting or emotionally manipulative chatbots will persuade voters to behave contrary to their interest or polarize the electorate.\n\nHowever, social scientific evidence suggests mass persuasion or manipulation of voters is unlikely and that the greater concern is the perception of this manipulation, rather than an actual direct effect on electoral outcomes.",
    "#### Information Centralization:\n\nTo the extent that generative AI tools concentrate in a small number of tech firms, there will be concerns about the power they wield over political information.\n\nThis will create thorny issues of content moderation, bias, and distrust.\n\n* University of Chicago  \n† Stanford University  \n‡ Columbia University",
    "#### For tech companies:\n\n- Continue building on recent efforts to watermark and label AI-generated content while clearly communicating that these efforts are not a panacea.\n\n- Plan table-top exercises to anticipate best responses to late-breaking “October surprises” that rely on AI-generated content.\n\n- Study how labeling AI-generated material affects users’ understanding of the information environment, building on recent academic research in this area.\n\n- Ensure that AI chatbots point users toward authoritative information from official state sources regarding other narrowly defined issues of clear fact, such as how and where to vote.\n\n- Monitor uses of chatbots for microtargeting and misleading content intended to deter participation—but primary focus should remain on deepfakes and the information environment.",
    "#### For journalists:\n\n- Disincentivize misinformation and manipulation by avoiding covering stories whose only case for newsworthiness is the use of AI-generated content.\n\n- Aid with AI-literacy through non-sensationalist coverage of AI’s role in elections.\n\n- Partner with civil society to evaluate the provenance of widely distributed political content, and participate in tabletop exercises to consider how to cover and rapidly evaluate October surprises.",
    "#### For the future:\n\n- Both government and big tech companies should limit centralization of power over content generation and moderation in a small number of platforms.\n\n- Encourage a diversity of generative AI tools and involve users and external stakeholders in setting content guardrails and governance structures.\n\n- The political risk associated with power centralization should be among the considerations when designing AI safety regulations that might increase market concentration, such as licensing requirements.",
    "## Introduction\n\nThe rapid development of generative AI technology over the past year has created new challenges as well as some new opportunities for democracy.\n\nThis document provides a research-based overview of the potential impact of generative AI and offers best practices to safeguard the electoral process.\n\nThe purpose of our review is not to recommend many specific legal or policy actions, but rather to promote clear understanding among voters, journalists, civil society, tech leaders, and other stakeholders about both the risk and promise of AI for electoral democracy in the hope of fostering a more productive public discussion of these issues.\n\nDemocracy relies on electoral accountability.\n\nVoters are asked to elect candidates they support and to fire politicians they think are not doing a good job.\n\nDemocracy therefore requires a healthy information environment in which voters can monitor what politicians are doing, learn what candidates promise to do if elected, and assess what policies might be needed in response to societal challenges.\n\nBy facilitating the automated creation of highly detailed text, image, and videos that are difficult to distinguish from human-generated content, generative AI could negatively impact the information environment.\n\nIt could be used to create false or misleading information, feed a general sense of radical skepticism or nihilism about the reliability of information, and even allow political actors to dismiss real, damaging information as AI-generated fake news.\n\nBy assisting in the creation of micro-targeted political content (for example, for use in online political ads), generative AI could polarize voters or contribute to a sense of a fractured polity in which people live in different informational environments from one another or are manipulated into voting for the wrong candidate or into not voting at all.\n\nGenerative AI also powers chatbots capable of engaging in human-like conversation.\n\nStartups, campaigns, and other actors are likely to use these chatbots to engage in political conversations.\n\nThis may not be inherently problematic—indeed, one could imagine increased engagement leading to better-informed voters.\n\nBut some may object to the inauthentic nature of these conversations, the potential for emotional manipulation, or the intentional or unintentional spread of false or misleading information.\n\nMoreover, extremist groups might use these chatbots in an effort to recruit people to their causes.\n\nFinally, and more generally, the growth of generative AI could lead to further centralization of the online information ecosystem, with a small number of large tech companies deciding which ideas and values can be expressed and which are out of bounds.\n\nThis could threaten free expression and further erode Americans’ trust in the political environment.\n\nAs an extraordinarily promising new technology, though, generative AI also holds promise for democratic governance.\n\nResearchers are already studying ways to create chatbots that can discuss politics with people.\n\nBecause generative AI is good at synthesizing large amounts of information, and if conversing through chatbots proves to be a more natural and engaging way for people to consume political information, then training chatbots to understand political issues and the positions of parties and candidates may be an effective way to help Americans become better informed about politics.\n\nThese tools could also help under-resourced campaigns to communicate more effectively with voters, and could help voters to communicate with their representatives more easily.\n\nWhile it’s essential that we safeguard the 2024 election and the democratic process more generally in the face of disruptive new technologies, we should also seek to maximize these opportunities for generative AI to improve our democracy.",
    "## Voters’ Information Environment\n\nGenerative AI has the capacity to alter the voters’ information environment in at least two ways.\n\nFirst, generative AI allows for the creation of highly convincing deepfakes—images and videos that are difficult for non-specialists to distinguish from genuine content.\n\nSecond, AI chatbots are a new, direct source of information for certain voters, especially younger ones.",
    "### Deepfakes\n\nUnscrupulous actors are likely to disseminate deepfakes, perhaps experimentally tested and tuned for maximal impact, unconstrained by norms of truth telling or public accountability.\n\nEarly instances of these concerns have already arisen in real world elections.\n\nDeepfakes played a role in the recent Turkish elections.\n\nAt a large political rally, President Recep Tayyip Erdoğan showed a fake video linking his chief opponent, Kemal Kılıçdaroğlu, to the leader of the PKK, a Kurdish group classified by the State Department’s as a foreign terrorist organization.\n\nSeparately, an online Kılıçdaroğlua supporter used AI to generate a video that appeared to show the candidate delivering a campaign speech in perfect English.\n\nAnd here in America, the Republican National Committee released a campaign video that used AI-generated images of President Biden and Vice President Harris to show a dystopian future following a Biden-Harris victory in 2024.\n\nThis video was not deceptive—it seems unlikely any viewers thought it showed current reality—but it did use AI to try to manipulate voters’ perceptions of political opponents and to garner press attention and notoriety.\n\nThere are several reasons to be concerned about the use of deepfakes in campaigns.\n\nFirst, of course, voters may well be deceived by such content.\n\nIt is particularly concerning that AI-manufactured content could be released very close to election day in order to generate fake scandals within a time frame that makes fact checking difficult.\n\nThese “October surprises” may be especially difficult to respond to if they are generated or shared by major political candidates.\n\nTech companies, civil society, and journalists may want to consider plans to rapidly analyze these October surprise deepfakes as they arise, focusing only on the most salient ones that gain the most reach online.\n\nSecond, wide-spread circulation of manufactured content may undermine voters’ trust in the broader information environment.\n\nIf voters come to believe that they cannot trust any digital evidence, it becomes difficult to seriously evaluate those who seek to represent them.\n\nThird, politicians may use this undermining of the credibility of the information environment to dismiss genuine information.\n\nLate in the Turkish election, a tape came to light showing compromising images of a candidate, Muharrem İnce.\n\nWhile İnce eventually withdrew, he also claimed the video was a deepfake.\n\nIf voters genuinely can’t tell the difference between what is fake and what is real, it is not hard to imagine that such denials will become a commonplace.\n\nMuch of the public discussion around how to address such concerns is focused on establishing content provenance.\n\nThe most prominent approach involves watermarking and labeling, something that major AI companies including Meta, Google, and OpenAI have publicly pledged to implement.\n\nTikTok has announced a similar approach.\n\nThe idea is that the large AI companies should build-in code that “watermarks” content–making content produced or modified by generative AI detectable–and that media and social media outlets should then label such content as “AI generated”.\n\nOther approaches involve building technologies that make it possible for creators to cryptographically sign content in ways that are not preserved if the content is altered without permission, thereby allowing users to attribute responsibility for content to a particular creator.\n\nThe goal of all of these steps is to allow voters to make better inferences about the reliability of information.\n\nWhile watermarking, labeling, and signing are certainly worthwhile, they are not a silver bullet due to several practical constraints.\n\nOnly a small percentage of online content can be expected to be cryptographically signed.\n\nAnd, while AI companies are exploring a variety of watermarking technologies, all current approaches can be worked around by sophisticated actors.\n\nFor generative text models, shorter texts are harder to detect.\n\nEven advanced cryptographic techniques for watermarking require “a few hundred” tokens for reasonable detection, which is well over the length of a typical Tweet.\n\nMoreover, because of the many approaches being pursued, the industry has not agreed on a set of standards; this lack of harmonization makes labeling more difficult for media and social media companies.\n\nFinally, labeling content as AI generated requires making judgment about substantial gray areas.\n\nIn addition to being used to create deceptive or false content, generative AI tools are used for a host of innocuous tasks, such as red-eye reduction, editing for fluency, and the like.\n\nWatermarking and labeling any content that has been touched by an AI risks being so over-inclusive that labels become meaningless.\n\nBut the alternative requires laborious and contentious judgment calls.\n\nMoreover, the effects of labeling on human behavior and perception are not yet well understood.",
    "But the alternative requires laborious and contentious judgment calls.\n\nMoreover, the effects of labeling on human behavior and perception are not yet well understood.\n\nIt is possible that users will respond to seeing labels alerting them that content is “AI generated” by assuming it is untrue.\n\nSuch a response would become problematic if there is useful, informative AI-generated content that gets systematically dismissed, or if users fallaciously conclude that any content not AI-generated must therefore be true.\n\nRecent research offers helpful evidence on what labels might be most effective, and further research by academics and platforms could help to refine best practices for labeling.\n\nWithout effective methods to establish the veracity of digital content, an alternative path forward is improved verification for content providers.\n\nEven if we can’t know whether digital content has been altered, account verification and verifiable content signing can increase confidence that the content that is released by an entity is truly coming from them, not an imposter.\n\nIf content is determined to be misleading or false, being able to confidently trace the content to its origin is a useful step towards creating accountability.\n\nRecent events have shown us the impacts of misleading account verification.\n\nIn the midst of recent changes to its verification program, X (formerly Twitter) users were able to create accounts that appeared to be verified and official but were, in fact, imposters.\n\nOne user created a fake but official-looking Eli Lilly account that tweeted “We are excited to announce insulin is now free.” In the wake of this tweet, Eli Lilly’s stock fell over 4%.",
    "### Voter Information\n\nAnother concern in the information environment is that voters might seek technical facts about elections—for instance, voting dates, polling locations, registration procedures, voting eligibility—from AI chatbots.\n\nRecent polling suggests that seeking such factual information is the election-related activity voters are most likely to use generative AI for.\n\nBut research by the bipartisan policy center finds that chatbots are neither a reliable nor an appropriate tool for this use case.\n\nIn recent years, internet search has been the most common source of such information for voters.\n\nTo the extent that AI chatbots are viewed as substitutes for search, this could be a substantial concern.\n\nChatbots have not, at this point, displaced search as a source of information.\n\nOnly about one quarter of adult Americans had accessed ChatGPT by July of this year.\n\nHowever, that number is substantially higher among those of young voting age—over 40% of Americans age 18-29 have done so.\n\nThe two most common uses were for entertainment and to access new information.\n\nThus, while it is unlikely that chatbots will be a source of widespread misinformation on technical election information, it is important that the creators of such chatbots take steps to reduce the risk.\n\nThe most straightforward approach is to train generative AIs to inform users that the chatbot is not a reliable source of technical election information and to direct users seeking such information to authoritative sources, such as the websites of state or local election administrators.\n\nSeveral of the large AI companies are already pursuing such an approach.",
    "## Recommendations and Best Practices\n\n- Campaigns and political parties should publicly pledge not to use deceptive AI generated content in campaign materials.\n\n- Tech companies should continue to build on their recent efforts to collaborate around watermarking and labeling AI-generated content, and support further research on the effects of labeling this content on users’ overall understanding of the information environment.\n\n- Tech companies should ensure that chatbots inform users that they are not a reliable source of technical election information and point users toward authoritative sources, as they have done in past election cycles.\n\n- Tech companies, journalists, and civil society should carry out tabletop exercises exploring how to respond to the release of late-breaking deepfake content, especially under scenarios in which the content is generated or promoted by a major political candidate.\n\n- State election boards should emphasize that existing voter intimidation and deception laws apply to AI generated content an outside group or campaign may use; the fact that the content was generated by AI is not a defense for voter intimidation or deception.\n\n- Journalists should disincentivize misinformation and manipulation by avoiding covering stories whose only case for newsworthiness is the use of AI-generated content.\n\n- Journalists and Civil Society should collaborate to provide trustworthy information on content authenticity.",
    "## Microtargeting and Manipulation\n\nIn an article published in The Conversation in June, political scientist Archon Fung and law professor Lawrence Lessig describe a dystopian scenario in which political campaigns use generative AI to develop highly persuasive messages tailored to individual voters.\n\nAI tools use reinforcement learning to get better and better at changing minds, far surpassing the capability of human campaign strategists.\n\nFong and Lessig warn that well-resourced candidates with access to such technology could win elections despite holding unpopular policy views, undermining electoral accountability.\n\nAlthough the specific AI-fueled context in this scenario is new, the general fear of candidates using persuasive technology to convince voters to vote against their preferences is not.\n\nSimilar arguments were made about targeted digital advertising when it became widespread in campaigns in the late 2000s through mid 2010s, and about broadcast advertising on television and radio when those technologies were young.\n\nPolitical scientists have evaluated the plausibility of similar claims to Fung and Lessig’s in the context of existing persuasive technologies – direct mail, television, and non-AI-generated digital ads – and have generally found persuasive effects close to zero.\n\nWhile there is some evidence that television advertising can have small but nonzero effects on vote shares in general elections, this effect is believed to operate mainly through changing the composition of the electorate (by motivating partisans to turn out or not), not by persuading voters to change their minds on policy issues.\n\nTwo further checks on the potential power of any communication technology, including AI, come from the competitive nature of elections– which implies that any persuasive arguments developed by one campaign are likely to be countered by its opponent–and the fact that any messaging voters receive from campaigns is swamped in volume by information from the mass media, social media, and friends and family.\n\nTo the extent that generative AI-produced ads are different from human-produced ads, the difference lies in the potential for mass customization on a scale not feasible with human-produced content.\n\nExisting technologies do allow for some customization, so this difference is one of degree rather than of fundamental character.\n\nEvidence about candidates’ use of existing targeting and message-tailoring technologies is, therefore, informative about how candidates would be likely to use generative AI.\n\nInterestingly, candidates do not appear to take much advantage of the large difference in targeting precision between broadcast TV ads and digital ads on social media platforms like Facebook; candidates run very similar campaign messages in both settings and do not present significantly different versions of their policy positions to different groups of platform users.\n\nIt does not appear that candidates perceive large gains from targeting, perhaps because of the potential for news outlets or their opponents to expose and broadcast any narrowly-targeted messages outside of their intended audiences.\n\nWe are, accordingly, skeptical of the potential for generative AI to undermine political accountability by duping voters into supporting candidates whose policy agendas they oppose.\n\nWhile it is possible that AI-enabled campaign advertising is substantially more effective than other existing campaign advertising technology, our accumulated experience and knowledge of previous waves of technological innovation in campaigns suggests that this is unlikely.\n\nNonetheless, there are some potential consequences about which some concern is warranted.\n\nFirst, one reason that campaigns find it difficult to persuade voters is that their opponents and the news media provide a counterweight to the information and arguments they offer.\n\nWere either the competitiveness of elections or the quality of political information provided by news media to decline, the scope for campaign persuasion by any means, and in particular through AI-generated targeted messages, would increase.\n\nSecond, generative AI is costly to use.\n\nIf it came to be perceived by candidates as a necessary tool to mount a serious campaign, it could raise the financial cost required to enter a race.\n\nThe arms-race effect of such competitive pressure could impact the pool of candidates willing to run, even in the absence of any net effect on voter behavior.\n\nThere is now good evidence from multiple settings that increases in campaign spending levels tend to shrink the candidate pool, advantage wealthier candidates, and increase incumbency advantages.\n\nThird, AI-enabled campaigning might be perceived by voters as unfair or illegitimate, particularly if its adoption is not symmetric across the parties.\n\nNews media might contribute to such perceptions by uncritically reporting claims about AI campaigns’ effectiveness from self-interested technology vendors.",
    "News media might contribute to such perceptions by uncritically reporting claims about AI campaigns’ effectiveness from self-interested technology vendors.\n\nLosing candidates could use such claims, were they widely believed, to cast doubt on the legitimacy of their opponent’s victory.",
    "## Recommendations and Best Practices\n\n- Journalists should inform the public that the likelihood of generative AI ads or chatbot conversations massively affecting electoral outcomes is low.\n\n- Journalists, civil society, and voters should exercise extreme skepticism when startups or other companies claim to be able to use new technology to persuade large numbers of voters to switch their votes or to not turn out.\n\n- Tech companies should continue to monitor concerning uses of chatbots for microtargeting, persuasion, and particularly misleading content intended to deter participation.\n\nWhere appropriate, they should consider forbidding such behavior in their terms of services as some already do.\n\nHowever, in the short amount of time before the 2024 election, issues with deepfakes and the information environment are likely to be more pressing and should be the primary focus.\n\n- Campaigns and political parties should avoid allocating undue resources to microtargeting and chatbots, which may foster mistrust even if they are not effective at political persuasion.",
    "## Opportunities\n\nThough some of the most salient examples of generative AI being used in politics involve nefarious applications such as deepfakes and inauthentic news stories, these technologies also have the potential to be harnessed for civic engagement and education.\n\nPositive use cases include summarizing policies and salient political events for lay audiences, developing more accessible voting advice applications, and facilitating communication between citizens and lawmakers.\n\nBeyond the potential to empower voters, generative AI could also enable under-resourced campaigns to improve their outreach and communication strategies.\n\nDistilling policies and presenting them in an accessible format is a capability of large language models (LLMs) that could enhance voter learning.\n\nFor example, one can ask Anthropic’s Claude, an LLM with a 100,000 token context window, to generate summaries of House and Senate resolutions.\n\nDoing so for H.R.\n\n5745, a 57-page bill introduced in the 118th Congress aimed at regulating digital assets, produces a summary correctly highlighting its aim to “create new regulations for cryptocurrencies, stable coins, and other digital money.” If one continues the chat by claiming to be a “cryptocurrency expert,” Claude can return details about new reporting requirements introduced in the bill.\n\nThe ability for these tools to tailor information based on user knowledge provides an opportunity for citizens to engage with the legislative process in a more transparent and efficient way.\n\nLLMs could also enhance voter learning in electoral settings.\n\nVoting advice applications (VAAs) have become ubiquitous in the US and Europe.\n\nVAAs solicit voters’ stances across several issues and match voters to politically congruent candidates.\n\nThough previous research has found positive effects on voter turnout and vote choice, VAAs assume that voters possess stable issue preferences, and might appeal to those who are already politically engaged.\n\nIntegrating generative AI into the development of VAAs could allow lower propensity voters to pose simple questions about political parties (e.g., “where does the Republican Party stand on abortion?”) and receive information about party positioning within seconds.\n\nInstead of responding to an extensive list of policy items, as is the case with existing VAAs, AI-guided conversations could help make interacting with VAAs less intimidating and more accessible to a broader spectrum of voters.\n\nIn implementing tools such as VAAs or even summarizing policies, a natural question is whether LLMs will be “biased” toward certain viewpoints.\n\nMethods like retrieval-augmented generation (RAG) can be used to mitigate these concerns by grounding the generation of content in a variety of sources.",
    "# Article\n\nWhen a user interacts with an LLM, RAG ensures that relevant sources are fetched from a preloaded database, inserted into the prompt, and used to populate the answer.\n\nIt is possible to load an external database with sources such as party platforms and instruct the LLM to only summarize the information contained in those sources.\n\nThis produces outputs that are generally free of editorializing and accurately represent the source material.\n\nOutside of electoral settings, extensions of RAG could be used to help voters learn about topics discussed in city council meetings, scour publicly available but underutilized civic data, and understand local policies.\n\nUsing RAG is akin to using a search engine that retrieves relevant information and summarizes it in an accessible fashion.\n\nThis makes it an ideal method for simplifying vast amounts of data, which can be useful for voters making choices in political contexts where knowledge is limited due to a lack of media coverage.\n\nFor example, one could use RAG to summarize recent approvals of housing developments or a list of policies considered by a city council in the previous week.\n\nBeyond political learning, generative AI could also be used to facilitate communication between citizens and elected officials.\n\nWriting to lawmakers can be an intimidating process for many citizens, as they might know how to articulate their concerns or they may be unsure about the relevant norms for such communication.\n\nLLMs can improve this process by enabling citizens to craft more persuasive messages.\n\nThey can also provide real-time assistance with respect to grammar, style, and tone, allowing citizens to communicate more effectively.\n\nIn the future, these tools may also be used by legislators to proactively identify common concerns among citizens, further improving responsiveness.\n\nGenerative AI could also help level the playing field with respect to political campaigns.\n\nUnder-resourced campaigns may face challenges in creating content that appeals to voters.\n\nGenerative image and text tools can enable these campaigns to draft more compelling speeches, press releases, social media posts, and other materials.\n\nThese methods can also be used to create tailored materials for different audiences.\n\nTo the extent that Generative AI allows financially constrained campaigns to maintain a veneer of professionalism, it could reduce imbalances between lesser-known candidates and more established politicians.\n\nIn sum, although negative use cases involving generative AI have received significant media attention, there are positive use cases that could enhance civic engagement and education.\n\nFrom powering political information-retrieving chat bots to improving citizen-to-legislator communication, the summarization and style transfer capabilities of LLMs could be used to improve democratic processes.\n\nEncouraging the public to use these tools to productively cut through the morass of everyday politics can serve as a powerful counterweight to misinformation, whether human or AI-generated.",
    "# Recommendations and Best Practices\n\nEncourage campaigns, journalistic outlets, and technology companies to create pathways that help voters to use Generative AI to summarize complex policies and political platforms.\n\nPolitical parties, technology companies, and civic organizations should help under-resourced campaigns learn to use AI tools to generate higher quality and more informative content.\n\nLawmakers and other actors should create pathways for citizens to use AI tools to communicate directly with their offices.",
    "# Information Centralization\n\nThere are good reasons to believe that the AI market is likely to end up concentrated in the hands of a small number of dominant firms who build and deploy foundation models—the large, flexible models that can be used for a wide variety of applications.\n\nTo the extent that generative AI tools end up clustered in a small number of large tech platforms, it may further exacerbate concerns that tech companies have too much power over the information environment.\n\nAlready, OpenAI, Microsoft, Meta, and other tech companies offering AI-related services have faced concerns over what content their chatbots and other generative AI tools will and won’t countenance, with some critics worrying that the tools will say offensive things, while others worry that the generative AIs exhibit a left-wing political bias.\n\nMore generally, there are hard questions about (a) what content is allowed to enter a mode’s training algorithms, (b) what prompts users are permitted to ask and which are rejected for being offensive, dangerous, or otherwise off limits, and (c) what output the tools will and won’t provide.\n\nAs generative AI use expands, the power these companies have over these rules could become problematic in much the same way that the power of social media companies over online speech has over the past 10 years.\n\nThere are two main risks.\n\nFirst, companies could err, and create content guardrails that are unfair, biased, or confusing in ways that perturb the information environment.\n\nSecond, people might further lose trust in the information environment, concluding that big tech companies who don’t share their values are unduly influencing what ideas are generated or gain widespread distribution.\n\nIn the US, these concerns could come from both sides of the ideological spectrum.\n\nJust as we’ve seen with regards to social media, the right will worry that generative AI tools are biased against conservative ideas and values, while the left will worry that these tools are biased towards conservative ideas and values.\n\nThese concerns rest on the widespread use of generative AI and so may not come to fruition before the 2024 election.\n\nIndeed, currently, polling shows that there is bipartisan support among Americans for a variety of interventions by government and industry to reduce the risk that generative AI poses to elections.\n\nHowever, it is entirely possible that public attitudes will polarize once Americans experience their first important “AI election” in 2024.\n\nAs such, these concerns bear careful watching, and companies and society would be wise to think now about how to resist the centralization of information provision before the problem becomes more developed.\n\nThere are at least two potential ways to mitigate this issue.\n\nFirst, society could take steps to ensure that there are many providers of generative AI tooling, so that no one company has undue power over content generation guardrails.\n\nThis could include exploring policies to prevent market concentration in the generative AI industry, and perhaps is a reason to be cautious about endorsing policies that envision restricting the set of companies licensed to provide generative AI tools (though these policies may have other important benefits).\n\nSecond, generative AI platforms could avoid taking on this power over content moderation by giving decision-making power concerning guardrails over to users, industry-level self-regulatory bodies, civil-society based regulatory bodies, or other external stakeholders.\n\nAlready a number of companies have announced efforts along these lines.\n\nBuilding on their experiment with the Oversight Board to make binding content decisions for the company, Meta has recently announced a “community forum” concerning generative AI guardrails, in which users are randomly recruited and paid to learn about, debate, and make recommendations concerning AI policy issues.\n\nOpenAI has announced funding for a similar exploration.\n\nMeanwhile, Anthropic has implemented an effort to develop a “constitution” through consultation with the community.\n\nWhile these efforts are interesting, they are still nascent.\n\nThe recruited users can only consider a small number of issues and, at most, make recommendations rather than binding decisions.\n\nFor these self-governance efforts to reassure skeptical Republicans and Democrats that tech companies are not shaping the expression of values in society, users would have to be given substantially greater power over more issues, to make binding decisions on them, and to communicate these decisions to the public so that the public could understand that tech companies have truly given over this power.\n\nExpanding the power of self-governance institutions like these is difficult, though, due to the expense and the practical challenges associated with recruiting and paying users to deliberate and decide.",
    "# Conclusion\n\nWe have reviewed a number of essential ways that generative AI will affect the 2024 election, focusing both on the ways that it threatens to disrupt the information environment and affect the outcome of the election and voters’ faith in the process itself, as well as the ways it can be used to inform voters and improve democratic functioning.\n\nIf there is a single theme to our review, it is that voters, journalists, and everyone who cares about elections should regard claims about new technology with great skepticism.\n\nIn some instances, the threat to the election is caused by generative AI itself—such as late-breaking “October surprises” driven by highly persuasive fake information.\n\nPeople should be skeptical of these surprises given these threats, and they should find ways to seek out information verification from sources they trust.\n\nIn other instances, the threat is not from the technology itself but from over-credulousness about its power.\n\nWhen startups claim to be able to use generative AI to change the minds of large numbers of voters, you should approach the claim with a great deal of skepticism.\n\nFinally, you should also be skeptical about the promise of quick fixes to new technological problems.\n\nWhile the idea of “watermarking” generative AI content is a valuable one, it is clear it will not be a panacea for the 2024 election.\n\nWatermarking is simultaneously too easy to evade, and also aimed at only one part of the problem: nothing about watermarking will tell you whether or not you should believe the claims and information in a piece of content.\n\nThere is ample misleading content that is not AI-generated, and there will be plenty of perfectly accurate AI-generated content.\n\nUltimately there will be no substitute for your skepticism, common sense, and trusted sources.",
    "# Author Bios and Disclosures\n\nEthan Bueno de Mesquita is the interim dean and Sydney Stein Professor in the Harris School of Public Policy at the University of Chicago.\n\nBueno de Mesquita discloses that he receives consulting income from Meta Platforms, Inc for work related to governance issues.\n\nBrandice Canes-Wrone is Professor of Political Science and Maurice R. Greenberg Senior Fellow of the Hoover Institution, Stanford University.\n\nAndrew B.\n\nHall is the Davies Family Professor of Political Economy in the Graduate School of Business at Stanford University, and a Professor of Political Science.\n\nHe is a Senior Fellow at the Stanford Institute for Economic Policy Research and a Senior Fellow (courtesy) at the Hoover Institution.\n\nHall discloses that he receives consulting income from Meta Platforms, Inc for work related to Augmented Reality strategy, and from Andreessen-Horowitz for work related to web3 governance.\n\nKristian Lum is Research Associate Professor in the Data Science Institute at the University of Chicago.\n\nShe was previously a Senior Staff Machine Learning Researcher on the ML Ethics, Transparency and Accountability Team at Twitter.\n\nGregory J. Martin is Associate Professor of Political Economy in the Graduate School of Business at Stanford University.\n\nYamil Ricardo Velez is an Assistant Professor of Political Science at Columbia University.",
    "## Overview\n\nSince 2020, there has been an exponential increase in the investment and development of generative artificial intelligence (AI) services.\n\nGenerative AI is a type of artificial intelligence system that generates new text, images, or other media in response to prompts.\n\nNotable generative AI systems include ChatGPT, Microsoft Copilot, and Google Bard.",
    "## Precautions\n\nGenerative AI has potential applications across a wide range of industries, including art, writing, and software development.\n\nHowever, there are also concerns about the potential misuse of these tools and any data shared with the services.\n\nWhen you provide information to these tools, such as queries, student essays, grant proposals, source code, or datasets, it is the same as posting the information on a public website.\n\nIndiana University encourages its affiliates to experiment with using these generative AI services, as long as no institutional data is submitted to them without approval.\n\nAt IU, Microsoft Copilot ( (formerly Bing Chat Enterprise) is available for use by IU faculty and staff, and it is the recommended way to use generative AI within the IU environment.\n\nAs part of the university's enterprise agreement with Microsoft, Copilot is approved to interact with data classified up to and including University-Internal data ( To use Copilot, you must be logged in with your Microsoft 365 at IU ( account (your @iu.edu email address and your IU passphrase (acpu)).\n\nFor information about browser and app compatibility, see About Bing Chat Enterprise/Copilot at IU (bing).\n\nTo date, no other generative AI tools have been approved for data beyond Public classification, and these have not been through the Software and Services Selection Process (SSSP).\n\nPrior to the sharing of any institutional data, these services will need to go through review to ensure the necessary contracts and safeguards are in place to protect the data submitted and to ensure the algorithms in use are ethical, transparent, and beneficial to the IU community.",
    "### Types of institutional data that should not be submitted to public versions of generative AI tools, even when anonymized, include:\n\n- Data classified as University-Internal or higher (for examples, visit the Data Classification Matrix \n\n- Any data that may be considered student, faculty, or staff intellectual property, unless the individual submitting that intellectual property created it",
    "### Specific examples that are not appropriate for the public versions of generative AI tools include:\n\n- Sharing names and information about a real student, employee, research participant, or patient\n- Asking an AI service to summarize and grade a student paper or assignment\n- Sharing employee-related data such as performance or benefit information for communication drafting or analysis\n- Asking an AI service to generate code for IU systems protecting institutional data or sharing IU source code for editing\n- Sharing grant proposals still under review",
    "## Acceptable uses\n\nWith these precautions in mind, there are numerous ways to use generative AI tools without submitting university data or intellectual property.\n\nUsing general queries to generate content to pull information from the AI resources is a good way to engage with the products.\n\nStudents should use generative AI in ways that align with university academic integrity policies and communicate with their instructors before using generative AI in their coursework.\n\nSchools and departments may elect to further restrict generative AI.\n\nFrom a data management perspective, examples of acceptable uses of generative AI include:\n\n- **Syllabus and lesson planning**: Instructors can use generative AI to help outline course syllabi and lesson plans, getting suggestions for learning objectives, teaching strategies, and assessment methods.\n\nCourse materials that the instructor has authored (such as course notes) may be submitted by the instructor.\n\n- **Correspondence when no student or employee information is provided**: Students, faculty, or staff may use fake information (such as an invented name for the recipient of an email message) to generate drafts of correspondence using AI tools, as long as they are using general queries and do not include institutional data.\n\n- **Professional development and training presentations**: Faculty and staff can use AI to draft materials for potential professional development opportunities, including workshops, conferences, and online courses related to their field.\n\n- **Event planning**: AI can assist in drafting event plans, including suggesting themes, activities, timelines, and checklists.\n\n- **Reviewing publicly accessible content**: AI can help you draft a review, analyze publicly accessible content (for example, proposals, papers and articles) to aid in drafting summaries, or pull together ideas.\n\nEven if you use generative AI tools for activities that do not share personal or institutional data, you should still check the tool's output for accuracy.\n\nSince these tools have been known to produce inaccurate content (sometimes called \"hallucinations\"), verify any factual information generated by an AI tool, and make sure to reference the tool as you would any content source.",
    "## Learn more and get help\n\nIf you have any privacy-related concerns about generative AI tools, or questions about the type of data that can be shared with it, email .\n\nFor more information, see:\n\n- AI at IU (\n- Precautions about using ChatGPT at IU (bigz)\n- About Bing Chat Enterprise/Copilot at IU (bing)\n- About AI detection tools (bimt)\n\nThis is document biit in the Knowledge Base.\n\nLast modified on 2023-12-14 14:16:11.\n\nConnect with UITS\n\n- Have a question?\n\nAskIU (\n- Chat with a consultant (\n- Email the UITS Support Center (\n- Research technology support (\n\nFind\n\n- Staff directory (\n- Careers (\n- Facilities (\n\nServices\n\n- One.IU (\n- Canvas (\n- Zoom (\n\nResources\n\n- Knowledge Base (\n- Web Content Management System (\n- Training (\n\nEmail\n\n- About email at IU (\n- Exchange (\n\nAccessibility ( Privacy (/d/priv) Copyright ( © 2024 The Trustees of Indiana University (",
    "## Purpose of these Guidelines\n\nThe following guidance aims to help instructors thinking about the impact of generative artificial intelligence (AI) tools on teaching and learning.\n\nSpecifically these guidelines suggest instructors:\ninclude a syllabus statement regarding use of AI tools;\ncreate transparent and productive learning environments by explicitly discussing appropriate, creative, and/or ethical AI use within a course, discipline, and/or profession; and\nprevent situations in which a student unintentionally engages in academic dishonesty while using AI tools.\n\nThe University of Arizona Catalog includes a list of required syllabus policies, including The Code of Academic Integrity.\n\nThe phrase “graded work/exercises must be the product of independent effort unless otherwise instructed” can be interpreted differently.\n\nTherefore, instructors should also include a statement specifically addressing course policy on the use of generative AI and discuss this policy with students.\n\nIf applicable the syllabus should include information about the use of AI detection applications, because these involve sharing student-created work.\n\nExample syllabus statements are provided below.\n\nStudents will potentially face different policies in different courses/assignments in regard to the use of generative AI.\n\nAlso, there are many different ways students might interact with generative AI in their learning processes.\n\nClarification about expectations is necessary.\n\nThis is similar to the potentially different meanings of the phrase “open book exam.” Does “open book exam” include using the textbook and notes?\n\nRecorded lectures?\n\nThe internet?\n\nTalking with classmates, friends, and family?\n\nIt is extremely important to be direct and specific about course policies concerning AI.\n\nStarting with the syllabus, engage with students as partners in learning about types of AI as related to the course, discipline, and profession; and discussing issues of academic integrity.",
    "### Learning Goals for Students\n\nGenerative AI tools can augment learning and may be a technology that students will use in the future, but their use may not be relevant or appropriate in all situations.\n\nEvaluate how using AI or learning about AI fits with a course’s goals and objectives.\n\nConsider whether or not the use of generative AI:\n- aligns with the learning outcomes and objectives of the course;\n- provides multiple means of engagement, representation, action or expression of learning;\n- ameliorates differences in students’ language or preparatory experiences;\n- facilitates process, practice, or develops durable critical thinking skills;\n- prepares students for future employment and equips them with digital and information literacies; and/or\n- makes the learning experience more interactive, personalized, or stimulating.",
    "### Academic Integrity\n\nThe University of Arizona Code of Academic Integrity prohibits “all forms of academic dishonesty, including...cheating, fabrication, facilitating academic dishonesty, and plagiarism.” Definitions of “cheating” vary.\n\nTherefore, students need guidelines and explanations from you about what counts as “cheating” or “plagiarism” within the context of a specific course or assignment.\n\nStudents may not be aware that AI policies can and will vary between courses, sections, instructors, and departments, so take time to support them in understanding and abiding by different policies.\n\nWhen introducing assignments, clearly articulate the expectations and/or boundaries of when, where, and how generative AI might be used—and, if used, how it will be documented.\n\n(See guidance on citation/acknowledgment in section III below.)\n\nIn considering academic integrity in your course, also think about ways to foster honest behavior and if you can prevent or detect cheating.\n\nNote that AI-detecting tools are not reliably accurate.\n\nThe University Center for Assessment, Teaching, and Technology can help you to review and re-think course assessment plans and assignments, and strategize how to promote and protect academic integrity.",
    "### Accessibility, Equity, Risks, and Ethics\n\nSpecific students are likely to be more familiar with generative AI tools.\n\nAccording to a May 2023 Pew Research Study, Americans with higher household incomes and formal education were more likely to know about ChatGPT, and White adults who have heard of ChatGPT are consistently less likely than their Asian, Hispanic or Black counterparts to have used the chatbot for fun, work or education.\n\nSome people see this technology as a way to mend inequities.\n\nAt the moment many generative AI tools are free, but more powerful versions of these tools are starting to charge a subscription fee.\n\nThere are risks and practical precautions to keep in mind and discuss with your students: The companies that own these tools collect information, so you and your students should not enter confidential information.\n\nReview the privacy policies of any tool you use or suggest as class material.\n\nLarge language models are trained on information from the internet, so their output contains misconceptions, biases, violence, racism, sexism, etc.\n\nthat exist in that data.\n\nThe use of intellectual property by large language models is debated; lawsuits about infringement on intellectual property rights are pending.\n\nGenerative AI makes up (hallucinates) information that is non-existent but sounds plausible.\n\nGenerative AI can be steered by the bias of human-written prompts, too.\n\nGenerative AI tools are not search engines; the output needs to be checked.\n\nNot all generative AI tools meet the requirements for accessibility as defined by the Americans with Disabilities Act.\n\nAI companies have been criticized for unfair labor practices.",
    "### Instructor Workload\n\nWhatever your policy decision is, workload may increase.\n\nThis includes, but is not limited to, creating a thoughtful syllabus policy; adjusting or adapting instruction, coursework, and assessments, and addressing students’ questions about allowable AI use.\n\nIt may be more time-consuming to disallow the use of generative AI in courses, especially large sections.\n\nAI-detection tools are not reliable and the necessary evaluation by a human reader takes time.\n\nSo, too, do discussions with students and documentation of violations.\n\nConsult with the University Center for Assessment, Teaching, and Technology if you would like to talk about your particular courses or learn more about generative AI, teaching, and learning.",
    "## Example Syllabus Statements\n\nWhether or not instructors permit or discourage the use of generative AI technologies in courses, all courses should include at least a syllabus statement, ideally with additional discussion, that explains the reasons for the course policy regarding generative AI use.\n\nYour reasons might include achievement of the learning objectives, developing professional skills and literacies, and/or academic integrity.\n\nPlease use these sample syllabus statements as a starting place for developing your own.",
    "### Generative AI use is NOT allowed for any purpose\n\nIf your intent is to disallow all use of generative AI, please think about:\n- How will you explain the benefits of this approach for students’ learning, personal, and professional development?\n\n- How might you adjust assignments or assessment methods to discourage use of an AI tool?\n\n- How will you detect and handle violations of this course policy?",
    "### Generative AI use is allowed for certain purposes/assignments, but not for others\n\nIf your intent is to allow any or limited use of generative AI in your course, please give thought to:\n- When, where, and how you communicate expectations: Make sure that students have information about what is/is not appropriate, how to cite/acknowledge use, and opportunity to ask questions on particular assignments.\n\n- Equity and inclusion: How will you ensure that all students have access to tools and support for learning about steering generative AI and critically analyzing output?",
    "## III.\n\nHow to Cite/Acknowledge use of AI\n\nCitation and acknowledgment practices for generative AI are evolving along with the tools.\n\nRemind your students that they are responsible for the quality of the work that they submit.\n\nIndicate in the syllabus and/or assignment instructions how to cite AI-generated content and acknowledge use of AI tools.",
    "### Acknowledging Assisted Processes\n\nStudents might use AI tools to assist them during the learning and composing processes.\n\nAI tools can help summarize and synthesize texts.\n\nAI tools can help develop focused research topics and resource search terms, produce outlines, and generate paragraphs of text or composite images.\n\nIf students use AI tools as part of their process during the learning activities and assessment projects, be sure to tell them where and how they can document their use of AI tools.\n\nIf it is a more traditional research project, it might include a methods section.\n\nOr, you might assign a process reflection memo that students submit as a parallel part of their project.",
    "### Citing Generated Content\n\nRemind students that when any AI-generated content is paraphrased, quoted, or incorporated into work, it should be cited; this includes AI-generated visuals or other media.\n\nIf the AI tool makes its interactions shareable, tell students to include a link or URL to the original prompt.\n\nThe MLA Style Center: How do I cite generative or APA Style: How to cite ChatGPT include other examples.\n\nIt may take time for more specific citation guidelines to settle; therefore, you may find yourself developing guidelines specific to your course or assignments.",
    "### Checking Suggested References\n\nDepending on the prompt, generative AI tools may create biased, illogical, or false information and non-existent sources.\n\nRemind students they should thoroughly check or investigate any content generated by AI large-language models and that they are ultimately responsible for the quality and content of all submitted work.",
    "## IV.\n\nStatement of Use of AI Content Detection\n\nAI content detectors are available to assist in scrutinizing student work.\n\nBe aware that AI detectors are not fully reliable.\n\nThe evaluation of an AI detection tool alone is not sufficient evidence to conclude that a student has violated any rules.\n\nA human instructor must evaluate the circumstances and converse with a student in order to determine whether or not misconduct has occurred as well as formulate an appropriate course of action.\n\nIf you choose to use AI detection, include a statement in plain language that describes what tool(s) will be used.\n\nBe transparent with students in writing before any assessment or grading begins.\n\nIf possible, include a link to the privacy policy of the tool(s).\n\nTurnItIn’s detection tool is integrated into the Brightspace D2L learning management system at UArizona although it is currently (8/7/2023) inactivated.\n\nFor updates on TurnItIn, please contact the University Center for Assessment, Teaching, and Technology.\n\nIf you use other detection tools, link to their data privacy policy when available.\n\nThe statement above aligns with the University’s value of respect for data privacy and standard 6.4 of the Quality Matters Rubric for Higher Education Online/Hybrid courses which specifies that \"The course provides learners with information on protecting their data and privacy.\"\n\nIn addition to a written statement, discuss academic integrity early and in-depth with students.\n\nThis is an opportunity to learn what students think about cheating, as well as to clarify instructions and rules about the use of generative AI.\n\nOther ways instructors can cultivate honesty include: spreading learners’ workload evenly throughout the course; requiring a description of process; reflecting on the pros/cons of AI; writing during class time; checking in with students throughout the term about what’s going well and what isn’t.\n\nContact the Dean of Students with questions about Code of Academic Integrity procedures.\n\nv 2.1.1\n\nThis document was authored by members of the Syllabus Guidance Team of the University of Arizona AI Access & Integrity working group, August 2023.\n\nWe are grateful for feedback that contributed to improving the guidance.\n\nAuthors: Isabella Carrillo, Shelly Rodrigo, Scott Eisenberg, Caitlin Hills, Alison Jameson, Kristin Chorba, Moe Momayez, Jess Zeitler, Gretchen Gibbs",
    "## Executive Summary\n\nPurpose and Audience: This resource aims to provide educators at the University of Delaware (UD) with practical and ethical guidance on integrating AI into teaching and learning.\n\nTargeting educators across all levels and academic unit leaders, it is designed to initiate, not conclude, discussions about AI in education.\n\nContext and Scope: With the advent of generative AI, new ethical, practical, and pedagogic challenges have emerged, necessitating the guidance and considerations included herein.\n\nThis resource deliberately excludes AI applications unrelated to education, such as self-driving cars, and does not reference specific AI tools to maintain relevance in a rapidly evolving field.\n\nReasons for Integrating AI in Education:\n\n- Improvement in Education: AI can enhance educational priorities by providing immediate feedback, fostering creativity, and enabling critical thinking.\n\n- Prevalence in Society: The growing ubiquity of AI tools in society makes it essential for educators to prepare learners for their ethical and effective use.\n\n- Awareness of Risks: Educators should ensure learners are aware of the risks associated with AI to guide its positive development.\n\nKey Considerations for AI Integration:\n\n- Human-Centered Approach: AI should amplify human abilities without replacing the human elements in education.\n\nEducators should remain central to and responsible for teaching and decision-making.\n\n- Transparency: Clear expectations regarding AI use should be set, with educators modeling ethical AI usage and appropriate disclosure, especially regarding data privacy.\n\n- Ethical Considerations: Educators should ensure that AI tools are accessible, are used in ways that protect learners’ personally identifiable information, and that an examination of possible bias is considered and unpacked in the classroom.\n\nEducators should also develop and maintain their own knowledge of these tools.\n\n- Educational Effectiveness: AI usage should align with best instructional practices and theories of learning.\n\n---\n\n1 Executive summary produced with the assistance of ChatGPT 4.0.\n\nAll output was evaluated and then revised to ensure accuracy of content and preserve the authors’ intended meaning of key concepts and terms.",
    "### Purpose\n\nThis resource provides educators with guidance regarding ways to utilize AI within teaching and learning in ways that are effective, ethical, and equitable and that will empower educators and learners–both traditional and non-traditional learners–alike.\n\nThese are considerations, not requirements.\n\nEducators do not have to teach with or about AI, but there’s good reasons to do so, and should someone wish to, here are some things to consider.\n\nThis document is not and cannot be exhaustive.\n\nIt concerns a very complex set of ideas with details that not only evolve over time, sometimes very quickly, but also have different meanings in different contexts.\n\nThis university-level document should begin discussions and decisions made by individual educators and groups of educators (e.g., departments, schools, programs), not end them.",
    "### Audience\n\nThis resource and the considerations offered here are primarily intended for UD educators responsible for teaching courses at any level, associates through doctoral.\n\nA secondary audience includes leaders of academic units (e.g., deans, department chairs, school directors) who may wish to utilize or adapt these considerations when creating unit- or discipline-specific AI-in-education policy documents for their units.",
    "### Context: Generative AI as a Catalyst for Educational Transformation\n\nThe Center for Integrative Research in Computing and Learning Sciences defines AI as:\n\n“A branch of computer science.\n\nAI systems use hardware, algorithms, and data to create ‘intelligence’ to do things like make decisions, discover patterns, and perform some sort of action.\n\nAI is a general term and there are more specific terms used in the field of AI.\n\nAI systems can be built in different ways, two of the primary ways are: (1) through the use of rules provided by a human (rule-based systems); or (2) with machine learning algorithms.\n\nMany newer AI systems use machine learning…[employing] an algorithm [that] will identify rules and patterns in the data without a human specifying those rules and patterns.\n\nThese algorithms build a model for decision making as they go through data.\n\n(You will sometimes hear the term machine learning model.)\n\nBecause they discover their own rules in the data they are given, ML systems can perpetuate biases.\n\nAlgorithms used in machine learning require massive amounts of data to be trained to make decisions.”\nWhile AI has been a focus of scholarship and development since at least the 1950s, recent advances in generative AI have catalyzed great interest, concern, and activity, particularly in regards to education.\n\nGenerative AI refers to “AI techniques that learn a representation of artifacts from data and use it to generate unique content (including images, video, music, speech, and text) that preserves a likeness to the original data.” These Considerations are offered in response to the unique ethical, practical, and educational challenges arising from generative AI.",
    "### Scope\n\nGiven that AI is a broad topic, this resource is necessarily limited in scope, specifically to that of the use of AI and automation within teaching and learning.\n\nFor example, while AI has been leveraged in the application of self-driving cars, that application of AI is outside the scope of this resource.\n\nAlso, the field of AI - including generative AI - is constantly evolving and doing so rapidly.\n\nConsiderations are thus offered without reference to specific AI applications (e.g., ChatGPT, DallE), retaining the focus on those considerations that would plausibly generalize to whichever tool or application may be used.\n\nFinally, as the field of AI advances, new considerations will emerge.\n\nThus, this resource is offered as a starting point, meant to begin and not end conversations around this important topic.",
    "### AI Can Improve Education\n\nAI can help educators achieve educational priorities in ways that can easily be used with many learners and can be widely accessible.\n\nFor instance, AI has the potential to provide opportunities for immediate and substantive feedback, appraising growth and progress, and exercising creativity and critical thinking.",
    "### AI is Increasingly Prevalent\n\nThese tools are becoming widely available and used throughout modern society.\n\nEducators have an obligation to provide opportunities for learners to learn about AI to ensure learners are prepared to use AI and automation systems effectively and ethically within employment and society more generally as engaged and informed citizens.\n\nMany learners are already using AI tools, and for educators to consider AI in their own classrooms allows them to better set expectations for how learners can and should use AI in their courses.",
    "### AI Raises a Number of Concerns and Risks\n\nAlthough AI is increasingly being used throughout society, there are risks associated with AI.\n\nEducators have a responsibility of ensuring that learners are cognizant of those risks to ensure that learners use AI safely, ethically, and effectively.\n\nIn this way, educators may help shape the future of AI in a positive direction.",
    "## Considerations for Integrating AI Within Teaching and Learning\n\nUniversity of Delaware educators contemplating the use of artificial intelligence (AI) tools and models should weigh and prioritize the following considerations.\n\nThese considerations are offered with the goal of ensuring that using AI preserves – even strengthens – but never diminishes the value and values of a UD education.",
    "#### Human-centered\n\nAI within teaching and learning should be “human centered.” To be human-centered means that the goal of implementing AI should be to “increase human self-efficacy, creativity, responsibility, and social connections” and to “amplify human abilities, empowering people in remarkable ways while ensuring human control.” Accordingly, the human-centered application of AI within teaching and learning should facilitate educators to focus more on what they are uniquely capable of and facilitate learners to engage in higher levels of thinking.\n\nImportantly, the application of AI should not reduce the energy directed at teaching and learning, rather it should create a productive division of labor, shifting the focus towards new, more innovative, and higher levels of critical engagement.\n\nEducators should be “in the loop,” remaining responsible for teaching and educational decision making.\n\nWhile AI can streamline tasks like feedback, assessment creation, grading, and material development, saving time and effort, it's crucial that educators actively supervise and confirm the validity of AI outputs.\n\nUltimately, the responsibility for instruction rests solely with the educator; the AI cannot take responsibility.",
    "#### Transparent\n\nEducators should make their expectations regarding learners' use/non-use of AI explicit.\n\nEducators should be explicit about what uses and tools are and are not appropriate.\n\nMoreover, and perhaps more importantly, educators should not just set those policies but also to teach learners, or direct learners towards resources, the knowledge and skills they need to successfully comply with those policies.\n\nIf learners are allowed to use AI tools, educators should ensure that there are clear expectations about how to cite their use, and whether or not learners should submit supplemental artifacts with their work (e.g., the prompt[s] they used to develop and refine their work, revisions made to the original output).\n\nEducators should help learners understand that expectations likely will not generalize from one instructor to the next.\n\nSpecial attention should be paid in courses that have multiple sections taught by different instructors to establish some consistency.\n\nEducators are cautioned against using AI detectors as their reliability has shown to be weak, often generating an unacceptable rate of false positives (i.e., human-written text flagged incorrectly as AI generated).\n\nEducators should make their own use of AI tools transparent.\n\nEducators should model transparent and ethical behaviors and practices when using AI tools in educational contexts.\n\nFor example, AI tools used to create or modify educational materials – syllabi, slides, etc.\n\n– should be appropriately cited or acknowledged.\n\nEducators who are using AI tools that access student data or provide input into decisions about learners (e.g., recommendations about grades) should also be appropriately and ethically transparent about their use of those tools.",
    "#### Ethical\n\nEducators should consider issues of access, equity, bias, and fairness when using AI within teaching and learning.\n\nEducators should ensure that AI tools and models are accessible to learners in every appropriate interpretation of \"accessible\" e.g., financial, technical, cultural.\n\nThis includes appropriate expectations regarding learners' AI literacy and how that is built and supported.\n\nEducators are encouraged to evaluate the outputs of the AI system given the potential of AI to amplify existing socio-cultural biases.\n\nAn examination of potential bias should be considered and unpacked in the classroom.\n\nEducators should solicit feedback from learners regarding their perceptions of those outputs, be they scores, feedback, text, images etc., to ascertain whether some groups of learners are systematically experiencing different levels of usability and usefulness with the AI tool.\n\nAI use should be aligned with policies and practices protecting learners’ data and privacy.\n\nConsistent with existing laws and policies governing data privacy such as FERPA, educators should not submit or encourage learners to submit personally identifiable information or other confidential data to AI systems that do not provide sufficient protections and appropriate assurances of privacy.\n\nLaws concerning generative AI, intellectual property and copyright, especially around fair use, are unsettled at this time.\n\nEducators should understand how data used in AI tools are stored, used, and protected.\n\nFor example, educators should not upload copyrighted materials (including most materials accessed through the library).\n\nThe greater the consequences, the more educator expertise, knowledge, and involvement is necessary.\n\nEducators should be appropriately knowledgeable about AI tools they use and the tools they require or encourage students to use.\n\nWithout appropriate knowledge, it will be difficult – if not impossible – for educators to remain “in the loop” and provide oversight on the effective uses of AI in their instruction.\n\nThe level of knowledge should be balanced against the intended use and impact of the tool with more intensive and consequential uses requiring more detailed knowledge on the part of the educator.\n\nThe greater the consequence attached to the outcome/decision, the more important it is to retain human control/autonomy.\n\nFor example, providing learners with feedback has relatively low stakes.\n\nLearners may elect to fully adopt, partially adopt, or ignore feedback given to them.\n\nIn contrast, grading – especially summative grading where learners are not given opportunities to revise or resubmit – has higher consequences.\n\nAI and automation might be used in both cases, but the latter case would require much greater educator oversight.",
    "#### Educationally Effective\n\nUse of AI within teaching and learning should be aligned with best practices in instruction and theories of how we learn.\n\nAI most productively supports teaching and learning when integrated with modern learning principles: AI tools and models should be employed in alignment with our best understanding of effective teaching and learning, including but not limited to an emphasis on practice and feedback, organization of knowledge, incorporation of prior knowledge and culture, metacognition, and the development of self-regulation and self-direction.\n\nAI doesn’t change everything.\n\nWhen using AI in the classroom, the basics should still apply: “What do I want learners to be able to do as a result of this activity?” Effective use of AI requires some explicit instruction in the use of AI.\n\nEducators are encouraged to instruct learners in productive ways of using AI tools, developing learners’ knowledge, skills, critical thinking, and ethical awareness of AI to develop their AI literacy.",
    "# Guidelines for Use of Generative AI Tools\n\nWe are providing initial guidelines on the use and procurement of generative artificial intelligence (AI) tools—such as OpenAI’s ChatGPT and Google Bard—that can generate content in response to prompts.\n\nWe support responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.\n\nGenerative AI is a rapidly evolving technology, and we will watch developments and incorporate feedback from the community to adjust our approach as needed.",
    "### Protect Confidential Data\n\nYou should not enter data at Levels 2-3 ( (Pretty much anything that should not be on a public webpage) into unapproved generative AI tools.\n\nThe approval procedure is described here ( Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties.",
    "### You Are Responsible for Any Content That You Produce or Publish That Includes AI-Generated Material\n\nAI-generated content can be false, misleading, or worse—AI sometimes makes up events and facts.\n\nIn addition, AI can infringe Intellectual Property rights, and your use of the results can be considered infringement—subjecting you to lawsuits and damages.\n\nOn the other side of the Intellectual Property coin, AI results are not well-protected by Intellectual Property laws.\n\nAll of these risks require you to read and understand the terms that come along with the AI you are using.",
    "### There is Almost Never “No Contract.”\n\nThere may be no charge, but Intellectual Property law requires an End User License Agreement, and most producers will have Terms of Use.\n\nThese are usually very favorable to producer and very unfavorable to you and Tufts.\n\nThat said, somebody needs to read them, comprehend the risk, and make a responsible decision to accept it.\n\nIf you are using this AI for a Tufts matter, you should be reaching out to TTS for a technology review (see No.\n\n5 below).",
    "### Adhere to Current Policies on Academic Integrity\n\nReview your School’s student and faculty handbooks and policies.\n\nFor example, misuse of ChatGPT already violates our Policies Regarding Student Behavior.\n\n( We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools.\n\nIn the meantime, faculty should be clear with students they’re teaching and advising about their policies on permitted uses and required disclosures, if any, of generative AI in classes and on academic work.\n\nStudents are also encouraged to ask their instructors for clarification about these policies as needed.",
    "### Connect with TTS Before Procuring Generative AI Tools\n\nThe University is working to ensure that tools procured on behalf of Tufts have the appropriate privacy and security protections and provide the best use of Tufts funds.\n\nIf you have procured or are considering procuring generative AI tools or have questions, please contact ( TTS using these instructions.\n\nVendor generative AI tools must be approved by TTS as described here (\n\nIt is important to note that these guidelines are not new University policy; rather, they leverage existing University policies.\n\n( Careers (/careers) Policies and Guidelines (/about/policies-and-guidelines) Remote Assistance (",
    "# Generative AI Resources for Instructors\n\nThe introduction of ChatGPT has sparked discussions in higher education about its use and the challenges and opportunities it presents for instructors in the classroom.\n\nAs with any technology used in instruction, following best practices is key to success.\n\nAs always, focusing on learning objectives and being transparent with students will continue to be key to quality teaching and learning.\n\nWith generative AI, it is also paramount that instructors know what AI is and how it can be ethically used in preparation and instruction.\n\nTo access a free version of ChatGPT 3.5, visit OpenAI and set up your account.",
    "## Overview of Generative Artificial Intelligence (AI)\n\nThe following videos were created by Mississippi State's Assistant Teaching Professor & Associate Director of Data Science Dr. Jonathan Barlow.\n\n- Generative AI Part 1: Background, Limitations, Access\n- Generative AI Part 2: How to Prompt ChatGPT\n- Generative AI Part 3: Plugins for ChatGPT\n- Generative AI Part 4: What's Next and Other Cool Tools",
    "## How Can Instructors Use AI?\n\nInstructional use of Generative AI in higher education courses will continue to evolve.\n\nHere are just a few common uses for AI in the classroom:\n\n- Generate examples, demonstrations, alternate test questions, & discussion prompts\n- Review written objectives, directions, & prompts\n- Recommend additional resources\n- Review structure of written materials",
    "## Getting Started: AI for Instruction\n\nFor further ideas on possibilities for AI use in preparation and instruction, as well as ways to address the topic with students and in your syllabi, visit Brandeis University's CTL resources here.\n\nThe articles listed below provide an overview of early generative AI use in higher education.\n\n- Johnson, SM, et.al.\n\n(2023).\n\nTeaching in the Age of AI.\n\nVanderbilt University Center for Teaching.\n\n- McMurtiie, B. and Beckie Supiano.\n\n(June 13, 2023).\n\nCaught Off Guard by AI.\n\nChronicle of Higher Education.\n\n- McCormack, M. (April 2023).\n\nAdopting and Adapting to Generative AI in Higher Ed Tech.\n\nEducause.\n\n- Mittelstadt, M. (2023).\n\nChatGPT Guidance for Instructors.\n\nUniversity of Georgia Center for Teaching and Learning.",
    "## Syllabus Statement Examples & Considerations\n\nAs Generative AI continues to evolve, it is clear that \"proof\" of plagiarism is becoming more problematic.\n\nThe Turnitin tool we have used before could directly compare student writing to websites, articles, and other writing; whereas, AI writing detection is less definitive.\n\nWe in the Center for Teaching and Learning would reiterate a few of the points from Turnitin.com AI Writing Detection's own site: \"Remember there is no 'right' or 'target' score with the AI writing indicator\"; \"Always dig deeper and analyze in conjunction with your institution's policies on the use of AI writing tools, in coordination with the assignment's rubric\"; and \"Engage with your student if you have doubts about the authenticity of the submission...use this [AI Writing Detection results] to have an honest, open dialogue with students regarding their work.\"\n\nUltimately, your openness about your AI policies with students throughout the semester will be the best indicator as to how ethically your students use the tools in your class.\n\nThe Student Honor Code Office suggests placement of one of the three syllabus statement options below and notice of the use of turnitin.com AI Writing Detection on course syllabi.",
    "**AI Policy: Permitted in This Course with Attribution**\n\nIn \\[course name\\], students are encouraged to use Generative AI Tools such as ChatGPT to support their work.\n\nStudents must give credit and cite any AI-generated material according to rules of documentation including in-text citations, quotations, and references.\n\nStudents must also include the following statement in assignments to indicate use of a Generative AI Tool: \"The author acknowledges the use of \\[Generative AI Tool Name\\] in the preparation of this assignment for \\[brainstorming, grammatical correction, citation, etc\\].\"",
    "**AI Policy: Permitted When Assigned with Attribution**\n\nIn \\[course name\\], students are permitted to use Generative AI tools such as ChatGPT for specific assignments, as designated by the instructor.\n\nStudents must give credit and cite any AI-generated material according to \\[APA, MLA, etc.\\] rules, including in-text citations, quotations, and references.\n\nSubmitted work may be filtered through turnitin.com AI Writing Detection.\n\nThis review may initiate further discussion about the authenticity of the submission which could result in a more formal review through the Honor Code Council.\n\nStudents must also include the following statement in assignments to indicate use of a Generative AI Tool: \"The author acknowledges the use of \\[Generative AI Tool Name\\] in the preparation of this assignment for \\[brainstorming, grammatical correction, citation, etc\\].\"",
    "**AI Policy: AI Use Discouraged in This Course**\n\nIn \\[course name\\], it is expected that all submitted work is produced by students themselves.\n\nUse of a Generative AI tool to complete an assignment constitutes academic dishonesty and will be reported as an Honor Code Violation.\n\nSubmitted work may be filtered through turnitin.com AI Writing Detection.\n\nThis review may initiate further discussion about the authenticity of the submission which could result in a more formal review through the Honor Code Council.",
    "## Preamble\n\nThis policy derives from a case study conducted on Tuesday January 24, 2023, by the 47 juniors and seniors in CDS DS380, “Data, Ethics, and Society.” The focus of the case study was the use of OpenAI’s ChatGPT in university education.\n\nThe case study involved analyzing ideas such as skill building and cheating, as well as the interests of stakeholders.\n\nThe challenge posed within the case study was to devise a policy for use in their own class that takes full account of the aims of university education and the perspectives of relevant stakeholders, and can be shared for consideration in other classes.\n\nThe case study was revisited each class session thereafter until a consensus was achieved.\n\nThe result was the Generative AI Assistance (GAIA) policy.\n\nThe fundamental concerns of class members included wanting to learn how to use AI text generators skillfully while enhancing rather than damaging their own developing abilities as writers and thinkers, to honor stakeholder interests, and to ensure that grading is fair between people who do not use AI text generators and people who do use them, in a variety of different ways.\n\nThis introduces new demands on those charged with grading student work.\n\nThus, the GAIA policy stresses transparency, fairness, and obligations for both students and teachers.",
    "## The Generative AI Assistance (GAIA) Policy\n\nWe welcome AI language-generation tools (collectively, large language models, or LLMs) into the learning process, in a way that preserves fairness, optimizes student skill building, and honors relevant stakeholder perspectives.\n\nThese include our own perspectives as students eager to learn and build careers, and the perspectives of our families who send us to university, our professors who are charged with teaching us vital skills, our university that has a responsibility to attest to our competency with diplomas, our future employers who invest in us because of our abilities and character, and our colleagues who lack privileged access to valuable resources.\n\nTo that end, the GAIA policy adopts a few commonsense limitations on an otherwise embracing approach to LLMs.\n\nAs students, we should give credit to LLMs whenever they are used, even if only to generate ideas rather than usable text.\n\nWhen writing papers or take-home assignments using LLMs, we should include in an appendix our entire exchange(s) with the LLM, highlighting the most relevant sections.\n\nWe should write a narrative at the beginning of the appendix explaining precisely how we used the LLM (to generate ideas, turns of phrase, elements of text, long stretches of text, lines of argument, pieces of evidence, etc.).\n\nThis narrative should also explain why we used the LLM (to save time, to surmount writer’s block, to stimulate our thinking, to handle mounting stress, to clarify prose, to experiment for fun, etc.).\n\nWe should not use LLMs to help with in-class examinations, tests, or assignments, unless they are explicitly organized around an LLM (e.g.\n\nan in-class assignment might challenge students to elicit from a LLM the most profound statement on the future of AI).\n\nIf we choose to use an LLM, we should use it wisely and intelligently, aiming to deepen our grasp of the subject matter and to support our learning.\n\nOur professor should seek to understand how LLMs work, including their strengths and weaknesses, as well as tools for detecting LLM-generated output, in order both to optimize their value for our learning, and to incorporate that awareness into grading procedures.\n\nOur professor should:\n- Treat work submitted by students who declare no use of LLMs as the baseline for grading.\n\n- Use a lower baseline for students who declare use of LLMs, depending on how extensive the usage, while rewarding creativity, critical nuance, and the correction of factual inaccuracies or superficial interpretations in response to suggestions made by the LLM.\n\n- Employ AI-text-generated-detection tools to evaluate the degree to which machine-generated text is likely to be present in student work.\n\n- Impose a significant penalty for low-energy or unreflective reuse of wording generated by LLMs, to the point of assigning zero points for merely reproducing LLM output.\n\nFurthermore:\n- We acknowledge that some may prefer stronger restrictions on the use of LLMs, and therefore that the GAIA policy may make them feel uncomfortable.\n\n- We acknowledge that some aspects of this policy are non-enforceable and depend on goodwill, a sense of fairness, and honorable character.\n\n- We acknowledge that this policy will need to be revisited when subscription versions of LLMs become available, which may be affordable for some students but not for others.\n\n- We acknowledge that this policy may need to be revised in light of other policies that would supersede it or due to novel technological developments.",
    "# Sample Syllabus Statements for the Use of AI Tools in Your Course\n\nThe following guidance is provided to assist you in developing coherent policies on the use of generative AI tools in your course.\n\nPlease adjust the guidance to fit your particular context.\n\nRemember also to note in specific assignment descriptions where AI use is allowed or disallowed.",
    "## Acceptable and Unacceptable Use of AI\n\n[This syllabus statement is useful when you are allowing the use of AI tools for certain purposes, but not for others.\n\nAdjust this statement to reflect your particular parameters of acceptable use.\n\nThe following is an example.]\n\nThe use of generative AI tools (e.g.\n\nChatGPT, Dall-e, etc.)\n\nis permitted in this course for the following activities:\n- Brainstorming and refining your ideas;\n- Fine tuning your research questions;\n- Finding information on your topic;\n- Drafting an outline to organize your thoughts; and\n- Checking grammar and style.\n\nThe use of generative AI tools is not permitted in this course for the following activities:\n- Impersonating you in classroom contexts, such as by using the tool to compose discussion board prompts assigned to you or content that you put into a Zoom chat.\n\n- Completing group work that your group has assigned to you, unless it is mutually agreed upon that you may utilize the tool.\n\n- Writing a draft of a writing assignment.\n\n- Writing entire sentences, paragraphs or papers to complete class assignments.\n\nYou are responsible for the information you submit based on an AI query (for instance, that it does not violate intellectual property laws, or contain misinformation or unethical content).\n\nYour use of AI tools must be properly documented and cited in order to stay within university policies on academic honesty.\n\nFor example, [Insert citation style for your discipline.\n\nSee these resources for APA guidance, and for other citation formats.].\n\nAny assignment that is found to have used generative AI tools in unauthorized ways [insert the penalty here*].\n\nWhen in doubt about permitted usage, please ask for clarification.",
    "## Use Encouraged and Permitted\n\n[This syllabus statement is useful when you are allowing, and perhaps encouraging, broad use of generative AI tools.]\n\nYou are welcome/expected to use generative AI tools (e.g.\n\nChatGPT, Dall-e, etc.)\n\nin this class as doing so aligns with the course learning goal [insert the course learning goal use of AI aligns with].\n\nYou are responsible for the information you submit based on an AI query (for instance, that it does not violate intellectual property laws, or contain misinformation or unethical content).\n\nYour use of AI tools must be properly documented and cited in order to stay within university policies on academic honesty.\n\nFor example, [Insert citation style for your discipline.\n\nSee these resources for APA guidance, and for other citation formats.]",
    "## Use Prohibited\n\n[This syllabus statement is useful when you are forbidding all use of generative AI tools for any purpose in your class.]\n\nThe use of generative AI tools (such as ChatGPT, DALL-E, etc.)\n\nare not permitted in this class; therefore, any use of AI tools for work in this class may be considered a violation of Temple University’s Academic Honesty policy and Student Conduct Code, since the work is not your own.\n\nThe use of unauthorized AI tools will result in [insert the penalty here*].\n\n* We highly recommend that you have conversations in your department about the appropriate penalties for unauthorized use of an AI.\n\nIt is important to think about the appropriate level of penalty for first-time offenders and those who repeatedly violate your policies on the use of AI.",
    "## Artificial Intelligence and Academic Integrity\n\nThere has been a significant increase in the popularity and availability of a variety of generative artificial intelligence (AI) tools, including ChatGPT, Sudowrite, and others.\n\nThese tools will help shape the future of work, research, and technology — but when used in the wrong way, they can stand in conflict with academic integrity at Ohio State.\n\nAll students have important obligations under the Code of Student Conduct ( to complete all academic and scholarly activities with fairness and honesty.\n\nOur professional students also have the responsibility to uphold the professional and ethical standards found in their respective academic honor codes.\n\nSpecifically, students are not to use “unauthorized assistance in the laboratory, on field work, in scholarship or on a course assignment” unless such assistance has been authorized specifically by the course instructor.\n\nIn addition, students are not to submit their work without acknowledging any word-for-word use and/or paraphrasing” of writing, ideas or other work that is not your own.\n\nThese requirements apply to all students — undergraduate, graduate, and professional.\n\nTo maintain a culture of integrity and respect, these generative AI tools should not be used in the completion of course assignments unless an instructor for a given course specifically authorizes their use.\n\nSome instructors may approve of using generative AI tools in the academic setting for specific goals.\n\nHowever, these tools should be used only with the explicit and clear permission of each individual instructor, and then only in the ways allowed by the instructor.",
    "## Introduction\n\nSeattle University asserts that academic honesty and integrity are important values in the educational process.\n\nAcademic dishonesty in any form is a serious offense against the academic community.\n\nActs of academic dishonesty or fraud will be addressed according to the Academic Integrity Policy.\n\nStandards, procedures, definitions of terms, and timelines are set forth below.\n\nShould the student have concerns about specifics of the procedure in a given case, s/he can bring these concerns to the attention of the dean’s designee.\n\nShould the need arise, the timeline can be extended or procedure adjusted with the approval of the dean or dean’s designee.\n\nGrade, academic probation and other penalties imposed under the Academic Integrity Policy must be appealed under this policy and may not be appealed through the Academic Grading Grievance Policy or Academic Probation Policy.",
    "## Standards of Conduct\n\nStudent conduct that is academically dishonest and evidences lack of academic integrity or trustworthiness is prohibited.\n\nA non-exhaustive list of prohibited conduct includes:\n\n- **Plagiarism** - The use of the work or intellectual property of other persons or the outputs of Generative Artificial Intelligence (AI) programs (e.g., ChatGPT, DALL-E, Github Copilot) presented as one’s own work without appropriate citation or acknowledgment.\n\nWhile different academic disciplines have different modes for attributing credit, all recognize and value the contributions of individuals to the general corpus of knowledge and expertise.\n\nStudents are responsible for educating themselves as to the proper mode of attributing credit in any course or field.\n\nA student does not need to have intended to plagiarize; the unacknowledged use of another’s work is sufficient.\n\nExamples of plagiarism include, but are not limited to, copying, paraphrasing, summarizing, or borrowing ideas, phrases, sentences, paragraphs, code, images, or an entire paper from another person’s work or AI program’s output without proper citation and/or acknowledgment.\n\n- **Cheating on Exams and Other Assignments** - Acting dishonestly or deceptively in connection with an assignment, examination or other activity related to a course.\n\nExamples of cheating include: \n  - Submitting work as one’s own that has been substantially written by someone else\n  - Copying another person’s work during an examination\n  - Allowing another person to copy one’s work\n  - Using, receiving, or transmitting unauthorized materials or other information during an examination\n  - Obtaining test materials before they are administered\n  - Sharing test materials with someone else before they are administered\n  - Having someone take an exam in one’s place\n  - Taking an exam for someone else\n  - Using AI output for any form of academic work including, but not limited to, writing, computer programming, or mathematics assignments if prohibited by the faculty member.\n\n- **Special Notes on Plagiarism and Cheating**:\n  - Faculty members may have specific policies regarding the use of AI programs, including prohibiting their use, and such policies should be communicated to students.\n\n- The use of any electronic device during an examination is expressly prohibited unless explicitly authorized by the instructor of the course.\n\nAll electronic devices, including cell phones, must be turned off and kept in a secure location out of view.\n\nStudents with disabilities who require electronic devices as an approved accommodation during exams are required to coordinate the use of electronics during exams with their course instructor and Disabilities Services.\n\nFailure to follow these guidelines may result in the filing of an academic integrity violation.\n\n- It is the responsibility of the student to consult with the faculty member concerning what constitutes permissible collaboration.\n\n- **Academic Fraud** - Misrepresenting one’s academic work.\n\nExamples of fraud include:\n  - **Submitting False Data** - Any attempt to fabricate, alter, or contrive data in such a way as to be deliberately misleading.\n\n- **Falsifying Academic Documentation** - Any attempt to forge or alter academic documentation such as admission applications, transcripts, letters of recommendation, certificates of enrollment or good standing, and registration forms, for oneself or others.\n\n- **Falsifying Academic Credentials or Standing** - Any attempt to claim an academic credential or standing that has not been earned.\n\n- **Submitting Work for Multiple Purposes** - Students are not permitted to submit their own work, in identical or similar form, for multiple purposes without the prior and explicit approval of all faculty members to whom the work will be submitted.\n\nThis policy applies to work first provided in connection with classes at Seattle University, other institutions attended by the student or work completed on behalf of other organizations.",
    "## Process for Reporting an Academic Integrity Violation\n\nFaculty, who have reason to believe that a student has committed an act of academic dishonesty, will, within five business days of discovering evidence of academic dishonesty, inform the student of the allegation and evidence in person, by phone, or by email.\n\nThe faculty will at the same time inform the department chair or program director.\n\nThe faculty member will initiate a meeting with the student and may include a third party, typically the department chair or program director or designee to present the evidence.\n\nThis meeting will take place as soon as possible following student notification of the allegation (see Appendix for definition of terms including “notification,” and timelines).\n\nIf the instructor is the department chair or program director, the dean or designee will refer the case to the chair or program director of a closely related department.\n\nThe purpose of the meeting is to provide an opportunity for the faculty member to present and discuss the evidence with the student and the circumstances that led to the faculty member’s concerns or allegation of an academic integrity violation.\n\nThe meeting also provides an opportunity for the student to ask questions to clarify the nature of the violation and the penalties that may be imposed.\n\nIf the student is unable or unwilling to confer, the faculty member may, after making reasonable attempts to confer, proceed with the process of reporting the violation and determining penalties.\n\nIf the faculty member then determines that the student committed an act of academic dishonesty, s/he will determine the appropriate penalties and will fill out the Academic Integrity Violation Report form.\n\nThis form, along with all supporting material, will be forwarded to the dean's designee of the college or school in which the course is offered and to the chair or program director of the department in which the course is offered within ten business days of the student-faculty meeting.\n\nA copy of the Academic Integrity Violation Report form and all supporting material are given to the student in person or via email at the same time it is delivered to the dean’s designee and department chair or program director.\n\nA copy of the form will be sent via certified mail by the dean’s designee to the student’s address of record.\n\nThe dean's designee will review the report and the student's record and may implement further penalties, as may be appropriate depending on the seriousness of the violation.\n\nThe dean's designee will forward copies of the report and the supporting materials to the Office of the Provost.\n\nThe Office of the Provost will keep the report on file and will maintain a file of cases of violations of the Academic Integrity Policy.\n\nIn cases of multiple violations, the Office of the Provost will notify the dean's designee in the school in which the student is a major and other academic officials if appropriate.",
    "## Penalties\n\nA student found to have violated the Academic Integrity Policy shall be subject to penalties imposed by the faculty member reporting the violation and any additional penalties that may be imposed by the dean or the provost.\n\nNo penalty may be imposed for violation of the Academic Integrity Policy without submission of the Academic Integrity Violation Report form.\n\nPossible penalties assigned by the faculty member and the dean include:\n\n- **No Credit** - The faculty member may give the student no credit (“zero”) for the assignment, examination, or other activity with respect to which the academic dishonesty has occurred, and may count the no-credit (“zero”) as an “F,” or allow the student to fully or partially make up such assignment, examination or activity.\n\nThe faculty member may also lower the student’s course grade or fail the student for the course.\n\n- **Reprimand** - A letter of reprimand may be issued for minor violations against the Academic Integrity Policy.\n\nThe letter of reprimand is issued by the dean or other appropriate academic official and will be placed in the student's university academic file.\n\nInformation about the letter is not shared without the student's consent except as permitted by the Family Educational Rights and Privacy Act (FERPA).\n\n- **Probation** - At the dean’s discretion the student may be placed on academic probation, beginning with the quarter following the violation.\n\nThe dean will specify the length of the probation.\n\n- **Suspension** - At the dean’s discretion, for serious or multiple violations, the student may be suspended from the school or college for up to two quarters.\n\nThe dean will specify the length of the suspension at the time of suspension.\n\nThis sanction will be permanently noted on the student's transcript with a notation of “Suspended: Acad Integrity”.\n\n- **Expulsion from University** - The dean may recommend to the provost that the student be expelled from the university.\n\nSee Section VI for further explanation.\n\n- **Denial of Recommendation** - For programs involving professional certification, the university may be asked to provide students with recommendations for the professional certificate.\n\nIn deciding whether to provide such a recommendation, the university considers personal characteristics, conduct, and potential to serve effectively and ethically in the profession for which the student is seeking training or certification.\n\nA finding that a student has violated the Academic Integrity Policy may affect the ability or willingness of the program to provide such a recommendation.\n\nAppeals of decisions not to recommend a student for a professional certificate because of a finding of academic dishonesty may only be reviewed under the Seattle University Professional Conduct: Appeal Procedures.\n\nAppeals of decisions not to recommend a student for a professional certificate because of a finding of academic dishonesty will not be reviewed under the appeal processes outlined below (Section V).",
    "## Appeal\n\nA student found to have violated the Academic Integrity Policy may appeal findings and/or penalties imposed by the faculty member, dean, or dean’s designee.\n\nAll proceedings associated with the appeal process are confidential and taping is not permitted.\n\n- Appeals of decisions by faculty and/or resultant penalties should be sent to the dean of the school in which the course is offered.\n\n- Appeals of penalties imposed by a dean or dean’s designee should be sent to the provost.\n\n- Decisions by the provost and provost’s designee, including expulsion from the university, may not be appealed.",
    "### Appeal of Decisions by Faculty\n\nStudent appeals of findings or penalties imposed by the faculty member will be heard by a school-based review panel.\n\n- **For appeals of findings**: It is the responsibility of the student to demonstrate, based on clear, cogent, and convincing evidence, that the faculty member was in error in determining that the student was in violation of the Academic Integrity Policy.\n\n- **For appeals of penalties**: It is the responsibility of the student to demonstrate, based on clear, cogent, and convincing evidence, that the faculty member acted in an arbitrary and capricious manner in determining the penalty(s) for a violation of the Academic Integrity Policy.",
    "### Appeal of Decisions by the Dean\n\nStudent appeals of findings and/or penalties imposed by the dean or dean’s designee will be heard by a university review panel.\n\nIt is the responsibility of the student to demonstrate that the dean or dean’s designee acted in an arbitrary and capricious manner.",
    "### Content of Appeal\n\nThe appeal must be in writing and must contain the date, the student’s name, the name of the course, the faculty member’s name, and student’s signature.\n\nIn addition, the student must describe in detail whether the violation or the penalty (or both) is being appealed, the evidence that supports the appeal, and the remedy that the student proposes.",
    "### Resolving the Appeal\n\n**Step 1: Notifying the faculty member that an appeal has been filed**  \nAfter receiving the appeal, the dean’s designee (in all cases here and below on the appeal process, provost’s designee performs a similar function for university panels) will send the faculty member a copy of the appeal.",
    "**Step 2: Composition of Appeal Review Panel**\n\n- **Review of faculty decisions by School based panel**  \n  School-based review panels will be composed of three full-time faculty members appointed on an annual basis by the dean.\n\nMembers of school-based review panels should not serve on hearings if they were involved in determining the initial finding or penalty.\n\nIf a member of a school review panel sanctions a student for an act of academic dishonesty, and that sanction is appealed, the faculty member will be replaced for that hearing.\n\n- **Review of dean’s decision by University based panel**  \n  The university review panel will be composed of three full-time faculty members appointed on an annual basis by the provost.\n\nThese will not include members of the school-based panels.\n\nAlso, members of the university review panel should not serve on hearings if they were involved in determining the initial finding or penalty.\n\nIf a member of the university review panel reports an act of academic dishonesty, and the penalties imposed by the dean’s designee are appealed to the university review panel, the provost or provost’s designee will replace that faculty member for that hearing.\n\n(The process for appeals of a penalty imposed by a dean follows the same process as the appeal of a decision or penalty imposed by a faculty member as noted below.)\n\n**Step 3: Challenges for Bias**  \nAfter selecting the review panel members who will hear the appeal, the dean’s designee will send both the faculty member and the student a list of the proposed panel members by email.\n\nThe faculty member and the student each may challenge members of the appeal panel for bias within two business days.\n\nSuch challenges must be in writing and must identify the reasons for the challenge.\n\nThe dean’s designee will rule on the challenges, and, if appropriate, appoint new members to the panel.\n\n**Step 4: Scheduling the Hearing**  \nThe dean’s designee will schedule the appeal review hearing to be held within ten business days of receiving the appeal.\n\nAs a general rule, the hearing should be scheduled so that both the student and the faculty member have at least five business days to prepare for the hearing.\n\nThe dean’s designee does, however, have the right to extend these time limits to accommodate the schedules of the student, the faculty member, and the panel members who will hear the appeal.\n\nThe dean’s designee oversees the distribution of the written documentation to the panel in sufficient time to insure a careful advance review of the materials.\n\nNeither the student nor the faculty member may bring legal representation to the hearing.\n\nHowever, both the student and faculty member may bring to the hearing a support person of their choice from within the university community.\n\nIf the student and/or faculty person elects to invite a support person, that information should be provided to the dean's designee at least two business days in advance of the hearing.\n\n**Step 5: Conducting the Hearing**  \nThe dean’s designee will preside over the hearing.\n\nAs a general rule, the hearing will be conducted as follows:\n\nThe dean’s designee will open the hearing by introducing the student, the faculty member, and the panel members who will review the appeal.\n\nSupport persons, should they be present, will also be introduced, but are not permitted to speak, question, or otherwise participate in the hearing process.\n\nThe dean’s designee will ask the student to present their reasons for overturning the decision of the faculty member.\n\nThe student has the burden of providing by clear, cogent, and convincing evidence that the faculty member failed to exercise reasonable professional judgment in determining that the student was in violation of the Academic Integrity Policy and/or acted in an arbitrary and capricious manner in assigning penalties in conjunction with the violation.\n\nThe dean’s designee will ask the faculty member to respond to the student’s evidence and to explain the evidence and the process leading to the decision.\n\nThe dean’s designee will provide the student with an opportunity to respond to the faculty member’s statements and evidence.\n\nThe dean’s designee will give the panel members hearing the appeal an opportunity to ask both the student and faculty member questions.\n\nThe dean’s designee will end the hearing by asking if the student and faculty member have any final statements and by telling the student and faculty member that the panel members, but not the dean’s designee, will meet in a closed session to decide the merits of the appeal.\n\nThe student and faculty member are each allowed, but are not required to make a final statement summarizing their respective positions.\n\nThey and the dean’s designee will then leave the hearing.\n\nThe appeal panel will then meet (or set a meeting time) to conduct their deliberations.",
    "They and the dean’s designee will then leave the hearing.\n\nThe appeal panel will then meet (or set a meeting time) to conduct their deliberations.\n\n**Step 6: Deciding the Appeal**  \nTo change the finding of or the penalty for an academic integrity violation, a simple majority of the panel members must agree to the change.\n\nIf a majority of the panel members does not agree to the change, no change will be made.\n\n**Step 7: Preparing the Written Opinion**  \nWithin five business days of the conclusion of the hearing process, the review panel will issue a written report of their decision addressed to the dean’s designee and deliver it to the dean’s designee.\n\nThe dean’s designee will send by email copies of the report to the student and the faculty member.\n\nThe dean’s designee will also send by certified mail a copy of the report to the student.\n\nUnless step 8 below applies, the panel’s decision is final and cannot be appealed.",
    "**Step 8: Limited Appeal**\n\n- **Findings by a school based review panel**  \n  If the student or faculty member believes that the appeal process failed to follow the appropriate procedures and that the procedural violation materially affected the appeal decision, they may file a limited written appeal with the dean of the school that offered the course within 5 business days of notification of the outcome of the appeal.\n\nIn this limited appeal, the student or faculty member should set out the procedure that they believe was not followed, any information they have to support the claim and a statement explaining how the alleged procedural error affected the appeal decision.\n\nThe dean will review the written material submitted, conduct any investigation they believe is necessary, and make a decision within five business days as to whether material procedural error occurred.\n\nThis decision is final and not subject to further appeal.\n\nIf the dean finds that a material procedural error occurred, then the matter shall be remanded to the appeal panel to remedy the material procedural error and reexamine its original decision and make such changes as are deemed appropriate.\n\nThe decision of the appeal panel on remand is final and not subject to further appeal.\n\n- **Findings by a university based review panel**  \n  If the student or dean believes that the appeal process failed to follow the appropriate procedures and that the procedural violation materially affected the appeal decision, they may file a limited written appeal with the provost/provost’s designee within five business days of notification of the outcome of the appeal.\n\nIn this limited appeal, the student or dean should set out the procedure that they believe was not followed, any information they have to support the claim and a statement explaining how that alleged procedural error affected the appeal decision.\n\nThe provost/provost’s designee will review the written material submitted, conduct any investigation they believe is necessary, and make a decision within five business days as to whether material procedural error occurred.\n\nThis decision is final and not subject to further appeal.\n\nIf the provost/designee finds that a material procedural error occurred, then the matter shall be remanded to the appeal panel to remedy the material procedural error and reexamine its original decision and make such changes as are deemed appropriate.\n\nThe decision of the appeal panel on remand is final and not subject to further appeal.",
    "## Expulsion\n\nA dean may recommend to the provost or provost’s designee that a student be expelled from the university for multiple or serious violation(s) of the Academic Integrity Policy.\n\nThe recommendation for expulsion will be considered only after the student has exhausted any appeal(s), should the student choose to file (an) appeal(s).\n\nIn the event of a recommendation of expulsion, the provost will convene a panel.\n\nThe panel will be chaired by the provost’s designee with two deans from schools or colleges other than that in which the student is enrolled or in which the violation occurred.\n\nThe provost’s designee shall notify the student via email and certified mail to the student’s address of record and convene the panel within five business days of receipt of the dean’s recommendation for expulsion (Attachment B).\n\nThe panel will review the documentation and notify the provost of their decision within ten business days of the student’s notification.\n\nThe provost or his/her designee will inform the student of the panel’s decision via email and certified mail within 2 business days of the decision.\n\nThe student may remain in classes while the panel is reviewing the documentation (academic violation reports, appeals).\n\nFollowing notification of expulsion, the student will be dropped from all classes for which he/she is registered at the time of expulsion.\n\nThe decision of the panel is final.\n\nAppeals of decisions to expel a student will not be reviewed under the appeal processes outlined above.\n\nThis sanction will be permanently noted on the student's transcript with the notation of “Expelled: Acad Integrity”.\n\nWhere the circumstances warrant, the president, provost or provost’s designee may also revoke degrees or academic credit already earned.",
    "### Definitions of Terms in this policy:\n\n- **Dean’s Designee** - an individual appointed by the dean, often an associate dean or assistant dean.\n\n- **Provost’s Designee** - an individual appointed by the provost, often an associate provost.\n\n- **Notice** - Notice will be deemed as given four business days after certified mail is sent.\n\n- **Business day** - weekdays, Monday through Friday, when the university is open for business and not including university holidays.\n\n- **Expulsion** - immediate dismissal from the university and no right to return.\n\nIf presently enrolled, student will be dropped from all classes.",
    "### Attachment B\n\nTimeline for Deadlines and Activities Associated with Penalties Imposed or Recommended by the Dean or Dean’s Designee under the Academic Integrity Policy\n\n*An interim update to the Academic Integrity Policy was approved by the Provost on March 24, 2023 to respond to concerns related to the use of Generative Artificial Intelligence (AI) programs (e.g., ChatGPT, DALL-E, Github Copilot).\n\n**Former Policy Titled: Academic Honesty Policy 2004-01  \n\nOriginally Issued: 9/19/2011  \nUpdated: 06/15/12  \n11/15/13",
    "## ChatGPT in Teaching and Learning\n\nThere are many artificial intelligence (AI) tools that can generate “human-like” responses to a wide range of questions and statements.\n\nAmong the most popular generative AI (or GenAI) tools is ChatGPT ( a text-based tool that can produce essays, reports, lesson plans, and more.\n\nBoston College students will likely use content from these tools in a variety of ways, including as substitutes for their own thinking and writing.\n\nLike other technologies that have created new opportunities for academic dishonesty (e.g.\n\nWikipedia, calculators, etc.\n\n), ChatGPT invites instructional responses that promote academic integrity and authentic student learning without sacrificing trust in instructor-student or student-student relationships.\n\nChatGPT’s parent company, OpenAI, has provided some responses to the most commonly asked questions ( about the tool on its website.",
    "## Limitations of ChatGPT\n\nWhile ChatGPT can produce text that can pass for human-created work, it does have many significant limitations depending on the version being used.\n\nAs of this writing, OpenAI provides a brief comparison ( of the distinctions between versions 3.5 (free) and 4.0 (paid).\n\nFor those using the free version, key limitations include:\n\n- ChatGPT does not use any information after January 2022.\n\nCurrent events and trends are not a part of its database.\n\n- ChatGPT does not access the internet.\n\nIt relies completely on the information already present in its database.\n\n- ChatGPT is not “unbiased.” It is limited by the culturally specific and language-specific ( information that was used to train it.\n\n- ChatGPT will often make up facts, produce misleading information, and include content unrelated to a user’s question.\n\nThis is because the technology is constantly changing and adjusting to how users interact with it.\n\nThe technology has no native way to differentiate fact from fiction.",
    "## Faculty Concerns\n\nFaculty have raised several questions and concerns since ChatGPT was released in November 2022:\n\n- How will we know if students are submitting original work?\n\n- Should we ban the use of ChatGPT and other AI tools?\n\n- If we assume students will use ChatGPT, how can the content it produced be cited?\n\n- Are there ways that ChatGPT can be used to help students learn?\n\n- Can we use ChatGPT for our own course design?",
    "## Tools for Detecting AI\n\nSeveral tools do exist that claim to detect AI-generated content.\n\nFor-profit services like Originality.AI ( claim to have 94% accuracy in identifying text produced by ChatGPT.\n\nAmong the free tools are:\n\n- openai-detector (\n- Gltr.io (\n- GPTZero (\n- CrossPlag (\n\nWhile these tools may be helpful in some cases, there are also significant downsides to using these tools:\n\n- Research shows that detectors are unreliable ( and biased against several populations (\n- It is unsustainable for faculty to run every one of their students’ submissions through these services.\n\nIt is not only an enormous time-consuming process, it is also likely that the services will be inaccurate.\n\n- Using these tools creates an adversarial relationship with students ( wherein they are taught that being caught cheating is worse than choosing to use the service in the first place.\n\n- As of January 2023, it is unclear whether or not there are FERPA violations in submitting student work through these plagiarism detectors.\n\nBelow are alternative instructional responses to ChatGPT, including short- and long-term interventions.",
    "## Instructional Responses to ChatGPT\n\nInstructional responses to the use of ChatGPT and similar tools vary depending on how much time and energy faculty have to make course-level or assignment-level changes.\n\nThe short-term solutions provided below are meant to serve as an immediate response that should be revisited when time permits.\n\nThe long-term approaches, on the other hand, are meant to prompt students to think more critically about the technology and their own intellectual formation.\n\nWhile the advent of ChatGPT poses new questions to address, strategies that limit academic dishonesty ( in general remain as relevant and effective as they have been before AI.",
    "#### Update Your Syllabus\n\nThe following sample statements should be taken as starting points to craft your own policy.\n\nAs of January 23, 2023, the Provost’s Office at BC has not issued a policy regarding the use of AI in coursework.\n\nWhen adding an AI-specific policy to a syllabus, consider how to personalize the policy ( depending on the norms of each department and course, along with BC’s institutional policies and protocols (\n\nIt is important to note that while you may suggest to students that using GenAI to complete assignments will be considered plagiarism, it is very difficult to prove given that GenAI will produce unique responses to every prompt.",
    "**Syllabus Statement 1 (Discourage Use of AI)**\n\nArtificial Intelligence (AI) Tool Usage: AI tools can generate text, images, and other media very quickly.\n\nSince a central goal of this course is to help you become independent and critical thinkers, you are discouraged from using AI tools to create text, video, audio, or images that end up in your work (assignments, activities, responses, etc).\n\nIf any part of this is confusing or uncertain, please reach out to me for a conversation before submitting your work.\n\n*Note: This statement assumes the syllabus has an academic integrity policy and/or statement about how plagiarized work will be treated.\n\n*",
    "**Syllabus Statement 2 (Treat AI-generated text as a source)**\n\nArtificial Intelligence (AI) Tool Usage: AI tools can generate text, images, and other media very quickly.\n\nSince a central goal of this course is to help you become independent and critical thinkers, you are discouraged from using AI tools to create text, video, audio, or images that end up in your work (assignments, activities, responses, etc).\n\nIf any AI-generated content is used for your assignments, you must clearly indicate what work is yours and what part is generated by the AI.\n\nIn such cases, no more than 10% of the student work should be generated by AI.\n\nAny AI-generated work not cited and/or used for more than 10% of your assignment will receive .\n\nIf any part of this is confusing or uncertain, please reach out to me for a conversation before submitting your work.\n\n*Note: This statement assumes that students are told which citation styles to use for secondary sources.\n\nThe instructor would indicate the penalty for not following the policy.\n\n*\n\nAs of January 15, 2024, a regularly-updated list of existing policies ( for the use of AI tools at various institutions is available online.\n\nBC faculty choosing to adapt these policies for their own syllabi may need to edit them in light of BC institutional policies and protocols ( as well as departmental and course context.",
    "#### Discuss ChatGPT With Students\n\nAllocate some time in class to discuss ChatGPT.\n\nThe conversation can be a way to discuss ethics in education, your expectations, as well as student perspectives.\n\nDiscussion prompts and questions can include:\n\n- What are some reasons students would want to use ChatGPT?\n\n- In what ways does ChatGPT impact the course goals?\n\nIn what ways can AI prevent or enable students in meeting these goals?\n\n- What are the differences between plagiarism, remixing, influencing, and originality?\n\n- How does the use of Wikipedia compare to the use of ChatGPT?\n\n- What is Academic Integrity and why is it so valued at Boston College?\n\nCTE’s resource on how to make Academic Integrity transparent ( can be a useful guide for these conversations.\n\nAdditionally, the BC Library has a helpful guide for orienting students to Generative AI (",
    "#### Collect a Writing Sample\n\nToward the beginning of the semester, ask students to write in class a short version of what you might ask them to write later in the semester.\n\nThis “diagnostic” can be a source of comparison if a student later submits work that seems significantly different.",
    "### Course Design: Long-Term Approaches\n\nCultivating a learner-centered course climate is fundamental to designing a course that de-incentivizes the use of tools like ChatGPT to cheat (see the CTE’s resource on Underlying Reasons for Academic Dishonesty ( In general, the more that assessments can make thinking visible, the less likely a tool like ChatGPT will be able to be used to replace a student’s own problem-solving techniques.",
    "#### Break Up Major Assignments\n\nDivide major assignments into smaller graded components that build on each other.\n\nThis “scaffolding” approach requires students to incorporate feedback on earlier assignments to improve their later assignments.\n\nThe CTE’s guidance on assignment design ( explains how making such changes can positively harness student motivation and deter academic dishonesty.",
    "#### Distribute the Grading\n\nWhen students are asked to produce one or two exemplar assignments for a semester, they are often pressured to focus on the product rather than the process of learning.\n\nBy creating more lower-stakes graded assignments, instructors encourage students to demonstrate learning as a continuous activity.\n\nFaculty can see the progress of their students and easily discover how thinking on a subject has evolved.",
    "#### Assess Student Workload\n\nAs indicated in the resource, Underlying Reasons for Academic Dishonesty ( one common reason students might use a tool like ChatGPT to cheat is because the overall workload expected of them seems excessive.\n\nWhat is considered excessive, of course, is relative.\n\nOne good way to measure the workload is by assuming that a student will take at least three times longer to accomplish a task than it would for a faculty member.\n\nRice University has also produced a workload estimator ( that can be used to measure the time it might take a student to complete certain kinds of tasks.",
    "### Assignment Design: Short-Term Interventions\n\nTry running your assignment prompts through ChatGPT itself.\n\nIf you are finding the responses to be on par for what you would expect, make small changes to make it more challenging for the tool to be used inappropriately, and plan for longer-term adjustments to how the technologies should be engaged, if it all.",
    "#### Add Reflection to Assignment\n\nAsk students to provide a reflective essay in addition to the paper or exam itself.\n\nThe objective would be to allow students to show their thinking and provide explanations for why they made the content and stylistic choices they did.\n\nSometimes called “exam wrappers ( these reflections personalize each submission and give students a chance to explain in detail how their thinking led to the product.",
    "#### Teach with ChatGPT\n\nAsk students to analyze the output of the AI for a question that could easily be asked as part of an assignment.\n\nIn the analysis, see if students can differentiate the output with something that a human would produce.\n\nFor example:\n\n- Does the output have a “style” of writing that makes it distinct?\n\n- Does the AI rely on clichés or casual speech in a manner that is inappropriate for the topic?\n\n- How would a student rewrite the AI output to be more accurate or more distinct?\n\nA thoughtful framework on ways to teach students to write with AI ( from Glenn Kleiman can be referenced for further reading.\n\nIf choosing to teach with ChatGPT, privacy concerns should be discussed openly.\n\nSee Georgetown’s resource on the topic under the section, Privacy and Data Collection ( Review ChatGPT’s privacy policy ( before asking students to sign up for the service.\n\nStudents should be made aware that signing up authorizes ChatGPT to share “Personal Information” with third parties without notice and that they must provide their cell phone number to the service.\n\nIn addition, ChatGPT’s parent company OpenAI ( discloses that:\n\n- It can access any information fed into or created by its technology\n- It uses log-in data, tracking, and other analytics \n- The technology does not respond to “Do Not Track”",
    "#### Plan for Social Annotation of Text\n\nTools like Perusall ( can be enormously useful in not only ensuring students read a given text, but also in prompting critical engagement with the readings.\n\nIf interested in using Perusall, you can review CTE’s resources on Persuall ( and set up a consultation with us if needed by emailing  (mailto:).",
    "#### Create Assignments that Require Multiple Modes\n\nSince ChatGPT is text-based, it can only render output that uses traditional sentence structure and syntax.\n\nIn place of written essays, faculty can ask students to create multimodal submissions: podcasts, posters, mind maps with annotations, short videos, etc.\n\nA very helpful — if overwhelming — resource for alternative assessments ( can be a useful guide when rethinking assignment types.\n\nThe “Guide to Alternative Assessments 2.0” ( PDF is particularly salient when thinking of multimodality.",
    "## Future Impact of AI on Teaching and Learning\n\nThe strategies suggested above respond to what we know about AI tools today; however, it’s clear that such technologies will improve and so our strategies will have to improve with them.\n\nIn the long run, instructional responses that engage the technology and its limits — rather than seek to simply ban them — promise to be more effective ways to meet learning goals across disciplines.\n\nSuch strategies may also help faculty find new ways to respond to other persistent challenges in higher education, such as:\n\n- How can AI tools be used ethically and strategically for our curriculum?\n\n- How can they be used to teach students about information literacy, data privacy, and intellectual property?\n\n- Can AI be used to promote more equitable learning experiences for students, especially those who have faced structural barriers to resources?\n\nSee Equity and Academic Integrity ( for a reference on how accusations of cheating are disproportionately aimed at underrepresented learners.\n\n- Should we find more ways to assess the process of learning and not just the product?\n\nThe CTE will continue to update this resource with suggestions, strategies, and perspectives that can inform faculty decisions on these questions.",
    "# 2023 EDUCAUSE Horizon Action Plan: Generative AI\n\nJenay Robert and Nicole Muscanell\n\n2023 EDUCAUSE Horizon Action Plan: Generative AI\n\nJenay Robert and Nicole Muscanell, 2023 Horizon Action Plan: Generative AI (Boulder, CO: EDUCAUSE, 2023)\n\n© 2023 EDUCAUSE\n\nThis report is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License.\n\nEDUCAUSE is a higher education technology association and the largest community of IT leaders and professionals committed to advancing higher education.\n\nTechnology, IT roles and responsibilities, and higher education are dynamically changing.\n\nFormed in 1998, EDUCAUSE supports those who lead, manage, and use information technology to anticipate and adapt to these changes, advancing strategic IT decision-making at every level within higher education.\n\nEDUCAUSE is a global nonprofit organization whose members include U.S. and international higher education institutions, corporations, not-for-profit organizations, and K–12 institutions.\n\nWith a community of more than 100,000 individuals at member organizations located around the world, EDUCAUSE encourages diversity in perspective, opinion, and representation.\n\nFor more information, please visit educause.edu.",
    "## INTRODUCTION\n\nIn 2023, generative AI emerged as the most rapidly adopted technology in history.\n\nAll members of the higher education community, from students to administrators, are trying to determine what impact generative AI tools can, will, and should have on life, learning, and work.\n\nTo make matters more complex, there is no consensus about how or even whether generative AI should play a role in the future of higher education.\n\nSome faculty, staff, and administrators insist that generative AI should be banned from educational contexts, concerned that such tools undermine their foundational goal to teach students to be independent and critical thinkers.\n\nOther leaders embrace these tools, positing that generative AI will be the most disruptive and transformational technology our community has ever seen.\n\nIndeed, most of the higher education community falls somewhere in the middle, embracing the potential of generative AI with excitement while practicing pragmatic caution and scrutiny.\n\nAmid all of this uncertainty, one thing is clear: If higher education leaders want to influence the future of generative AI at their institutions, there is no time to waste.\n\nTo this end, a panel of teaching and learning professionals joined EDUCAUSE strategic foresight experts to envision an ideal future of generative AI in higher education in 10 years.\n\nUsing a 10-year horizon allows us to consider significant advancements in society, technology, politics, the economy, and the environment, without looking so far into the future that any projections become unrealistic.\n\nBuilding on the trends, technologies, and practices described in the 2023 EDUCAUSE Horizon Report: Teaching and Learning Edition, the panel crafted its vision of the future along with practical actions that individuals, units and departments, and groups of collaborators can take to make this future a reality.",
    "## GOALS FOR OUR FUTURE STATE\n\nAsked to describe the state of generative AI that they would like to see in higher education 10 years from now, panelists collaboratively constructed the future described below.\n\nGenerative AI developers and researchers have found ways to ensure that processes and outputs are equal, representative, and unbiased.\n\nDevelopers prioritize ethical considerations such as algorithmic transparency, data privacy and security, accessibility, equity, and inclusion.\n\nEnd users have full control over how institutions and companies use their data and intellectual property.\n\nAI training for students and professionals is built on a foundation of ethical practices.\n\nGenerative AI tools are designed with safeguards to ensure they operate within the confines of developers’ goals.\n\nGenerative AI developers and end users build specific, reference-based datasets for various higher education contexts.\n\nThought leaders in higher education provide insights about data quality.\n\nGenerative AI supports access to and the accessibility of educational technology.\n\nStructural and arguably arbitrary roadblocks to student success are diminished or eliminated.\n\nFor example, generative AI gets rid of the writing gateway for knowledge demonstration.\n\nGenerative AI facilitates multilingual education by automatically translating spoken and written communication into any language in real time, allowing educators to teach students all over the world and enriching cross-cultural experiences for students.\n\nThe development of digital tools is democratized by the aid of generative AI tools.\n\nEnd users can create their own digital applications, and technology is not controlled by industry.\n\nGenerative AI powers interactive and immersive learning experiences such as simulation and virtual reality environments, making complex concepts more engaging and tangible for students.\n\nFaculty, staff, and students are able to think critically about generative AI tools and outputs.\n\nThey are able to discern appropriate uses for generative AI tools and evaluate generative AI products.\n\nSearch tools powered by generative AI provide users with credible sources of information, scaffolding information-gathering and fact-checking.\n\nFaculty have the necessary professional development and ongoing support to implement generative AI tools in teaching and learning.\n\nEducators teach students how to use generative AI tools ethically, responsibly, and effectively.\n\nGenerative AI is used to enhance human relationships and complete tasks humans don’t want to do.\n\nHuman time is freed to work on and solve big issues related to health, economics, and other areas.\n\nIn higher education, more time is available to focus on meaningful interactions between faculty, staff, and students.\n\nGenerative AI tools enable new connections between learners who share interests, and education environments are more focused on supporting these connections than on knowledge exchange because traditional pedagogical processes are more efficient and effective.\n\nGenerative AI tools report learning analytics findings in real time.\n\nFaculty and staff leverage this information to make data-informed decisions, implement targeted interventions, and continuously improve teaching methods.\n\nLearning analytics tools also provide students and faculty with detailed, targeted, and constructive feedback on assignments, projects, and assessments.\n\nDigital assistants provide students with personalized, lifelong guides for learning.\n\nPowered by generative AI, assistants are built with personas similar to a coach or friend.\n\nInstitutions use these assistants to provide wraparound (i.e., educational and psychosocial) support for students.\n\nAssistants integrate insights from earlier educational experiences and ongoing individualized learning needs to address students’ unique preferences, strengths, and challenges.\n\nHigher education experiences are a seamless continuation from secondary education, with a focus on continuous upskilling and metacognition.\n\nAfter postsecondary graduation, students are still connected to lifelong learning pathways for gaining additional certifications and skills.\n\nAssistants also help students develop critical thinking, innovation, and creativity by presenting them with unique challenges such as creative prompts, writing exercises, and problem-solving scenarios.",
    "### Individuals\n\n- Cultivate a culture of experimentation.\n\nEncourage faculty, students, and staff to experiment with AI by providing dedicated time, incentives, and access to tools and discovery opportunities.\n\nExperimentation will foster user agency, innovation, and creativity.\n\nIt will also help individuals embrace new technologies and will lessen the digital divide.\n\n- Establish a community of practice.\n\nCreate a safe and controlled environment, such as an AI sandbox, where faculty, staff, and students can collaborate and experiment with AI tools and applications without risk to production systems.\n\n- Establish stakeholder inclusion as a best practice.\n\nEncourage cross-group collaboration, including students and external stakeholders (e.g., K–12, industry).\n\nCreate peer learning communities where stakeholders can share experiences, challenges, and best practices, and encourage collaboration and interdisciplinary projects that explore innovative applications of AI in various fields.\n\n- Provide opportunities to self-educate.\n\nOffer easy-to-access resources such as online courses and tutorials, webinars and workshops, and educational materials such as handbooks, guides, and case studies that introduce faculty, staff, and students to AI concepts, applications, and best practices.\n\n- Invest in and support ongoing AI literacy initiatives.\n\nDevelop comprehensive, institution-wide training programs and resources that individuals with varying levels of technical expertise can understand and use, and create a repository of regularly updated educational resources related to AI integration in higher education.\n\n- Ensure that faculty members have access to AI tools.\n\nProvide faculty with the tools and platforms they need to experiment and incorporate AI into their teaching and research activities.\n\nProvide them with an adequate support system that includes AI experts who can provide guidance and assistance to faculty members as they integrate AI into their teaching and research.\n\n- Increase stakeholder commitment to ethics.\n\nHelp stakeholders commit to being ethical users of AI by providing comprehensive and up-to-date training and resources on cybersecurity, privacy, and effective versus ineffective uses of AI, along with the risks and benefits of use.\n\n- Establish AI facilitator roles on campuses.\n\nFacilitators can help faculty and staff navigate their identity as AI users and educators.\n\nImplement professional development opportunities and resources such as trainings, workshops, coaching and mentoring programs, peer learning communities, and educational materials to help faculty and staff become facilitators.\n\nEstablish recognition and reward systems that recognize those who excel in their supportive facilitator roles.\n\n- Be an engaged leader.\n\nSupport your colleagues on the generative AI journey by engaging regularly in professional development and training opportunities, which will help you make well-informed and bold decisions about new institutional policies.\n\nProvide dedicated time to communicate with units and departments about important issues surrounding AI usage at your institution.",
    "### Units and Departments\n\n- Reimagine curriculum and assessments.\n\nHelp faculty incorporate AI into courses in ways that create more personalized and authentic learning experiences while also incentivizing learning, creativity, and innovation over grades.\n\nEncourage units and departments to use a collaborative and evidence-based approach, and provide them with time and budget for redesign, along with an updated curriculum review process.\n\n- Train the next generations of AI users.\n\nEncourage units and departments to continually think about how AI will evolve over time and across disciplines so that they can anticipate, identify, and acknowledge new skills and knowledge that students need now and in the future.\n\n- Employ or train AI integration specialists.\n\nThese specialists can assess communication and collaboration needs within units and departments and identify opportunities for AI-powered solutions.\n\n- Invest in state-of-the-art centralized AI infrastructure.\n\nThe infrastructure should include high-performance computing resources, cloud services, and AI development tools to support research and implementation efforts.\n\nProvide shared access to AI tools, data storage, and technical support for all units and departments.\n\n- Provide dedicated IT support.\n\nEnsure that each unit and department has IT support to address technical issues related to AI integration and collaboration.\n\nEmploy an institution-wide technical support team to assist departments in the implementation, integration, and troubleshooting of AI solutions.\n\n- Develop departmental protocols and guidelines.\n\nEstablish guidelines that address security, privacy, and data management and ensure that sensitive information shared through AI-powered tools is secure and complies with data protection regulations.",
    "### Multi-Unit Collaboration\n\n- Establish interdisciplinary AI centers.\n\nCreate centers where individuals from different units and departments can collaborate on projects and explore innovative applications of AI in various academic fields and solve complex problems.\n\n- Establish an AI implementation task force.\n\nInclude representatives from different units and departments to oversee resource allocation, assess unit and departmental needs, and identify potential AI use cases.\n\n- Form an AI policy committee.\n\nThe committee should comprise representatives from multiple administrative units and departments to develop and oversee shared AI policies and governance.\n\nThe committee should review and develop policies and shared practices in the areas of fair use, assessment and academic integrity, institutional review board processes, and the scholarship of teaching and learning.\n\n- Appoint an AI Ethics and Compliance Officer.\n\nThe officer should be responsible for risk management and ensuring that AI practices across units and departments align with ethical guidelines, data privacy regulations, and institutional policies.\n\nSeek legal expertise to navigate complex legal frameworks and ensure that institution-wide and unit/department AI policies adhere to relevant laws and regulations.\n\n- Establish dedicated communication channels.\n\nCreate channels to inform all units and departments about shared AI policies and governance, in addition to AI literacy and integration trainings and opportunities.\n\nEncourage units and departments to use these channels to collaborate and share best practices.\n\n- Recruit a dedicated interdisciplinary team of AI experts.\n\nAllocate resources to recruit a team of interdisciplinary AI experts with diverse skill sets, including data scientists, AI researchers, software engineers, and domain specialists.",
    "### Multi-Institution Collaboration\n\n- Establish formal partnerships with other institutions.\n\nForm a commission or consortium consisting of members from multiple institutions and disciplines to promote sustained collaboration and joint AI initiatives.\n\nThe commission or consortium should focus on governance and policy across institutions, with specific attention to accreditation issues and student outcomes.\n\n- Establish cross-institutional AI research centers.\n\nCreate collaborative spaces for researchers to work together on cutting-edge AI projects.\n\nEstablish resource-sharing agreements that enable institutions to pool their expertise, data, and AI tools for collective use and benefit.\n\n- Establish multi-institutional funding models.\n\nCreate funding opportunities to support collaborative AI research and projects that involve multiple institutions, encouraging knowledge exchange and cross-institutional cooperation.\n\n- Appoint a dedicated outreach team.\n\nThe team should comprise representatives from each collaborating institution.\n\nThe outreach team should be responsible for overseeing, coordinating, and promoting AI collaboration efforts, ensuring effective communication and the alignment of goals.\n\n- Support knowledge exchange between institutions.\n\nOrganize virtual or in-person knowledge exchange forums, webinars, workshops, and conferences that bring together faculty, researchers, and administrators from different institutions to discuss their AI initiatives, successes, and challenges and to explore potential collaboration opportunities.\n\n- Encourage multi-institutional publications.\n\nSupport and incentivize multi-institutional research publications, white papers, and reports that share insights and findings from collaborative AI research and projects.\n\n- Establish a dedicated multi-institutional collaborative platform.\n\nProvide access to collaborative tools and support, allowing institutions to easily share AI-related research, best practices, and resources and to foster effective cross-institution outreach.\n\n- Partner with K–12 stakeholders and institutions.\n\nWork with K–12 stakeholders to develop a more holistic understanding of students’ early experiences with AI, in addition to their future academic and workforce needs, which can help institutions redesign and/or develop curricular programs to address these needs.",
    "### Activity: Understand Your Institution’s Needs\n\nThe best plan of action always starts with understanding the starting line.\n\nUse this activity to explore the current needs and interests of stakeholders at your institution.\n\n- Make a list of key stakeholders at your institution who will be interested in shaping the future of generative AI.\n\nConsider a wide range of perspectives from a variety of operational units: academic units, information technology, information security, privacy, student services, legal, etc.\n\n- Select a sample of individuals from your list and talk to them about the present and future states of generative AI at your institution.\n\nSome guiding questions are: What is our current state of generative AI?\n\nHow do we currently support or suppress generative AI?\n\nWhat are our biggest challenges related to generative AI?\n\nWhat are our greatest assets related to generative AI?\n\n- Take notes on key takeaways from your conversations.\n\nInclude references for resources and documents such as websites and institutional policies.\n\n- Reflect on your findings and consider how they align with the findings in this report.\n\nIs your institutional vision aligned with panelists’ ideas?\n\nAre any individuals at your institution ready to take some of the actions described in this report?\n\n- Make a plan for next steps.\n\nUsually, this starts with identifying key members of your professional network who can partner with you.\n\nConsider stakeholders who are ready to hit the ground running, colleagues who already have considerable influence and can break down barriers, and colleagues or units that might be resistant to change and need help seeing your vision.",
    "### Activity: Build an Action Roadmap\n\nNow that you have a clear picture of where you are, where you want to go, and who is going to help, you’re ready to develop an Action Roadmap (adapted from the Institute for the Future).\n\nThis activity is best accomplished collaboratively, so consider working with some of the individuals you identified in the activity “Understand Your Institution’s Needs.” Start with the right side of this tool, describing the goals and elements of your preferred future, using the future described in this report as inspiration.\n\nThen, review the findings you generated with “Understand Your Institution’s Needs” and describe the short-, mid-, and long-term actions that will carry you from today’s reality to the future you want to see.\n\n- Describe your preferred future\n\n© 2023 Institute for the Future.\n\nAll rights reserved.\n\nSR-2300B | CC BY-NC-ND 4.0",
    "## METHODOLOGY\n\nThis 2023 Horizon Action Plan: Generative AI is grounded in the perspectives and knowledge of an expert panel of practitioners and thought leaders who represent the higher education teaching and learning community.\n\nThe members of this group were sought out for their unique viewpoints as well as their contributions and leadership within their domain.\n\nDependent as the Horizon Report efforts are on the voices of its panel, every effort was made to ensure those voices were diverse and that each could uniquely enrich the group’s work.\n\nFor this action plan, we adopted and adapted different components of the Institute for the Future (IFTF) foresight methodology.\n\nFirst, we asked panelists to review the trends, technologies and practices, and scenarios from the 2023 EDUCAUSE Horizon Report: Teaching and Learning Edition and to describe their own vision of the preferred future of generative AI.\n\nSecond, panelists were directed to list the threats and opportunities that might imperil or bring momentum to their preferred future and to brainstorm possible actions in response to those threats and opportunities.\n\nThe data produced as a result of these efforts have been used to create the action plan featured in this report.\n\nEDUCAUSE staff provided group facilitation and technical support but minimal influence on the content of the panel’s inputs and discussions.\n\nThis was done to reduce the potential introduction of bias into the results and to allow for this organized group of experts themselves to discuss and converge on a set of actions for the future based on their own expertise and knowledge.\n\nThe panel discussions were held remotely on July 20, 2023, by Zoom.",
    "## EXPERT PANEL ROSTER\n\nWe would like to acknowledge and express our deepest gratitude to the panel of experts listed below who were responsible for generating all of the big ideas summarized throughout this resource.\n\nTheir brilliant thinking and rich discussions were the foundation of this work, and this resource simply would not exist had it not been for their dedication to this project and their passion for serving higher education.\n\n- James Hutson, Lead XR Disruptor/Department Head, Lindenwood University\n- Jamie Nelson, Associate Director, Educational Innovation, University of Illinois Urbana-Champaign\n- Avi Hyman, Director, Academic, Research & Collaborative (ARC) Technologies, University of Toronto\n- Wendy Lampner, Director of Online, Continuing, and Professional Education, University of Akron\n- Christa Morrison, Business Systems Analyst, McMaster University\n- Janet Frizzarin, Program Manager, Senior Specialist Digital Learning, Lodz University of Technology\n- Mark Daley, Professor and Chief Digital Information Officer, Western University\n- Szymon Machajewski, Associate Director, Learning Technologies and Innovation, University of Illinois Chicago\n- Kathe Pelletier, Director, Teaching and Learning Program, EDUCAUSE\n- Jenay Robert, Senior Researcher, EDUCAUSE\n- Nicole Muscanell, Researcher, EDUCAUSE\n- Nichole Arbino, Communities Program Manager, EDUCAUSE\n- Mark McCormack, Senior Director of Research and Insights, EDUCAUSE\n- Jamie Reeves, Director of Community, Product, and Portfolio Management, EDUCAUSE\n- Belle McDonald, Portfolio Manager, EDUCAUSE",
    "### Background\n\nThe emergence and proliferation of generative Artificial Intelligence (AI) systems, features, and add-ons provide great opportunities but also raise concerns over security and the unintentional sharing of confidential data.\n\nProviding any data to generative AI tools or services as part of a query is equivalent to posting the data on a public-facing website; generative AI systems “learn” by collecting and storing user-provided data.\n\nThis data may then be used as output provided to others.",
    "### Guidance\n\nAs an instructor, you can use AI platforms to assist with some aspects of teaching and course design that do not involve sensitive data, including:\n\n- Using ChatGPT to assist in the creation of course materials, assignments, rubrics, etc., while paying particular attention to truthfulness, accuracy, and relevance.\n\n- Supporting students through the use of ChatGPT and other AI platforms in educationally appropriate ways.",
    "#### Do NOT\n\n- Submit HIPAA-protected data or research data to any AI system, including patient names and identifiers, health records, or information related to payment for healthcare.\n\n- Submit confidential data, sensitive data, or personally identifiable information relating to research projects.\n\n- Use Zoom bots (e.g., Otter.ai) to record, transcribe, or summarize Zoom meetings.\n\nYour Zoom meeting may inadvertently introduce sensitive or confidential information, and connection to non-University-supported systems may constitute public disclosure.",
    "#### Exceptions\n\nThe University of Miami will review generative AI systems to see if they can be approved and licensed for use with sensitive data.\n\nThose that are approved will be added to it.miami.edu/ai-tools ( Please read the information about each application carefully to understand its purpose, uses, limitations, and restrictions.",
    "## Sample Syllabus Language Suggestions and Samples\n\nThe widespread availability of generative AI tools requires our attention and careful consideration as faculty.\n\nWe encourage faculty to reach out to colleagues to consider what approach you and your field should take.\n\nAdditionally, we encourage all faculty to communicate expectations and approaches to students.\n\nTo that end, we should have language in our syllabi so that students know what is allowed and what is not in each class.\n\nThe text below, based on similar documents produced by UNC and other universities, is designed as a template for possible inclusion in class syllabi for this Fall semester.\n\nIt should be modified as appropriate for individual faculty, style of pedagogy, and subject field.\n\nOur Faculty Senate’s Academic Standards Committee will take up this topic during the Fall semester, with the goal of proposing possible modifications to the Academic Integrity Policy.",
    "### Suggested text for inclusion in syllabi:\n\nChatGPT and other Generative Artificial Intelligence (AI) software may be useful tools for enhancing learning, productivity, and creativity.\n\nFor instance, they can assist with brainstorming, finding information, and creating materials, such as text, images, and other media.\n\nHowever, these tools must be used appropriately and ethically, and you must understand their limitations.\n\nIn particular, it is important to realize that all AI software has the following limitations:\n\n- How output is arrived at is not clear as the internal processes used to produce a particular output within the generative AI cannot be determined.\n\n- AI output is typically based on data harvested from unknown online sources.\n\nAs such, it may reflect biases that should be acknowledged.\n\nAI output may also be inaccurate or entirely fabricated, even if it appears reliable or factual.\n\n- AI evokes a range of intellectual property concerns; sourcing and ownership of information is often unclear and is currently the subject of ongoing litigation.\n\nIf you use AI tools in any part of your work, you are responsible for the final product of that work, both academically and in the workforce.",
    "## Principles\n\nAI should help you think, not think for you.\n\nAI tools may be used to help generate ideas, frame problems, and perform research.\n\nIt can be a starting point for your thought process, analysis, and discovery.\n\nDo not use them to do your work for you, e.g., do not enter an assignment question into ChatGPT and copy and paste the response as your answer.\n\nThe use of AI must be open and documented.\n\nThe use of any AI in the creation of your work must be declared in your submission and explained.\n\nYour faculty can provide guidance as to the format and contents of the disclosure.\n\nThe undeclared use of any AI (including text, images, program code, musical notation, etc.)\n\nin any work may be considered as plagiarism.\n\nEngage with AI Responsibly and Ethically.\n\nEngage with AI technologies responsibly, critically evaluating AI-generated outputs and considering potential biases, limitations, and ethical implications in your analysis and discussions.\n\nEnsure that the data used for AI applications are obtained and shared responsibly.\n\nNever pass off as your own work generated by AI.\n\nYou are 100% responsible for your final product.\n\nYou are the user; if the AI tool makes a mistake, and you use it, then it’s your mistake.\n\nIf you don’t know whether a statement about any item in the output is true, then it is your responsibility to research it.\n\nIf you cannot verify it as factual, you should delete it.\n\nYou hold full responsibility for AI-generated content.\n\nIdeas must be attributed, and sources must be verified.\n\nThese principles are in effect unless the instructor gives you specific guidelines for an assignment or exam.\n\nIt is your responsibility to ensure you are following the correct guidelines.\n\nNot following them will result in a breach of the Academic Integrity Policy.\n\nData that are confidential or personal should not be entered into generative AI tools.\n\nPutting confidential or personal data into these tools exposes you and others to the loss of important information.\n\nTherefore, do not do so.\n\nSee point 3 above.\n\nThe rules and practices on the use of AI may vary from class to class, discipline to discipline.\n\nDo not assume that what is acceptable in a Computer Science class will be acceptable in a Philosophy class.\n\nIt is the student’s responsibility to stay informed as to the instructor’s expectations.\n\nWhen in doubt, ask.",
    "### AI Text Detectors\n\nThe use of systems that claim to detect AI-generated text (e.g., GPTZero, Copyleaks) is not recommended.\n\nSubmitting students’ coursework to these systems may constitute a FERPA violation since students’ work is considered an educational record.\n\nFurthermore, AI-generated text cannot be reliably detected ( (Sankar Sadasivan et al., 2023), and some evidence indicates that text-detecting tools may demonstrate bias against texts written by non-native English speakers ( (Liang, et al., 2023).\n\nLiang, W., Yuksekgonul, M., Mao, Y., Wu, E., & Zou, J.\n\n(2023).\n\nGPT detectors are biased against non-native English writers.\n\nPatterns (4)7.\n\nSankar Sadasivan, V., Kumar, A., Balasubramanian, S., Wang, W., & Feizi, S. (2023).\n\nCan AI-Generated Text be Reliably Detected?.\n\narXiv e-prints, arXiv-2303.",
    "### Students With Disabilities\n\nUsing AI platforms as part of teaching and learning may have differential impacts on students with disabilities.\n\nSome strategies may help them in the learning process, while other applications of AI might hinder them.\n\nFor more information, refer to:\n\n- How ChatGPT Could Help or Hurt Students With Disabilities (\n  \n- 3 Ways AI Can Help Students with Disabilities (",
    "### What is ChatGPT?\n\nChatGPT is an artificial intelligence (AI) tool that uses natural language processing techniques to respond to user-generated prompts.\n\nWhile other chatbots are typically limited to pre-programmed responses, ChatGPT can produce original responses in real time.\n\nChatGPT can generate everything from essays and emails to poems and lesson plans.\n\nIt can revise text with improved grammar and spelling or modulate the style or tone of a provided set of text, as requested by the user.\n\nChatGPT is just one of many generative AI tools.\n\nChatGPT may occasionally incorporate fabricated or incorrect information in its responses.\n\nBecause of the way it operates, it is difficult to trace the source and provenance of information provided by ChatGPT (except when it is connected to the internet, e.g., Bing-integrated GPT).\n\nIn addition, because ChatGPT was trained on a corpus of text that was originally written by humans, ChatGPT’s responses may reflect the biases present in the text it was trained on.",
    "### Tips for Instructors\n\nTalk to students about your expectations for academic honesty.\n\nMany students who engage in academic dishonesty do so impulsively or without significant reflection on the choice.\n\nAnticipate this behavior by proactively engaging students in an open discussion about academic honesty in your course, including your expectations regarding ChatGPT.\n\nRemind students that they are required to follow UGA’s Academic Honesty Policy and talk to your students about how that policy applies to your course.\n\nFor example, is ChatGPT off-limits, sometimes OK to use, or always encouraged?\n\nSee below for sample syllabus language related to ChatGPT, or explore the growing repository of classroom policies related to generative AI available here.\n\nUse of artificial intelligence or word mixing software to write your paper or disguise plagiarized work is considered unauthorized assistance in this course.\n\nSuspected unauthorized assistance, or other violations of UGA’s “A Culture of Honesty,” will be reported to the Office of Academic Honesty.\n\nOr:\n\nWe encourage you to use AI tools to explore the field and help you study.\n\nHowever, you must take full responsibility for any AI-generated materials you incorporate in your course products.\n\nIf you include AI-generated materials in your course products, it should be cited like any other reference material.\n\nAll information incorporated into course products must be verified, ideas must still be attributed, and facts must be true.\n\nDo not rely solely on AI detector tools (see the “Additional Resources” section at the end of this document for examples) to identify whether an assignment was created using generative AI.\n\nThese tools can return false positives and fail to detect some AI-generated material.\n\nIf you suspect a piece of work was completed using generative AI, carefully analyze the work.\n\nIf you believe a violation of your course policies may have occurred, instructors should report the incident to the Office of Academic Honesty.\n\nA facilitated discussion will be scheduled for you to talk with the student about their process for completing the assignment and whether a violation occurred.\n\nConsider the limitations of ChatGPT when designing course assignments.\n\nAs of August 1, 2023, ChatGPT (v3.5):\n- Cannot access information that is not already included in its corpus.\n\nThis includes text or internet resources generated after 2021, notes or other specifics from your course.\n\nHowever, GPT4 and other GPT integrated products do have access to more updated information.\n\n- Cannot dissect or produce non-text-based responses, although other AI-generators can produce images and art.\n\n- May fabricate references (including contrived DOIs) when prompted to produce citations.\n\nInstructors might deliberately ask students to engage with ChatGPT, or other Generative AI tools, as an educational endeavor.\n\nFor example, students might analyze how ChatGPT generates text for different audiences, attempt to find the original sources of text used to generate a particular ChatGPT response, compare and contrast ChatGPT output in response to various prompts, or improve upon a given ChatGPT prompt.\n\nIf you decide to incorporate use of generative AI tools in your course, inform students how/whether you would like them to document or cite their use of generative AI.\n\nDiscuss with students the importance of transparency and trustworthiness to the process of generating knowledge, the pursuit of a degree, and to the ideals of higher education.\n\nIf you ask your students to use ChatGPT, review the privacy policy and provide an opt out.\n\nChatGPT is currently released as a “free research preview,” meaning that current use of the tool is helping to improve it for future use.\n\nOpenAI (the company that designed ChatGPT) collects data from its users which may also be shared with third party entities.\n\nIf you plan to use ChatGPT in an educational setting, it is recommended that you review the privacy policy and terms of use with your students, and provide them with the option to opt out (or alternate means to engage with ChatGPT output) if they do not feel comfortable having their data collected and shared as stated in these policies.",
    "### Additional Resources\n\n- The Homework Apocalypse (Substack post from Ethan Mollick, 7/1/2023)\n- Using AI to Implement Effective Teaching Strategies in Classrooms: Five Strategies, including Prompts (DOI: 10.2139/aarn.4391243, 3/17/2023)\n- Teaching: What you Need to Know about ChatGPT (Chronicle of Higher Education newsletter, 3/16/2023)\n- Teaching Writing in an Age of AI (Teaching in Higher Ed podcast with guest John Warner, 1/19/2023)\n- ChatGPT is not all you need.\n\nA State-of-the-Art Review of large Generative AI models (DOI:10.48550/arXiv.2301.04655, 1/11/2023)\n- The Learning with AI Initiative (University of Maine, ongoing)\n- Growing collection of ideas for ChatGPT prompts (Github, ongoing)\n- ChatGPT: Understanding the New Landscape and Short-term Solutions (C. Alby, ongoing)\n- Chat GPT detectors: GPTZero, OpenAI Detector, AI Writing Check, GLTR, Turnitin’s AI writing detection.\n\n“ChatGPT Guidance for Instructors” by Megan L. Mittelstadt (2023) is licensed under a CC BY NC 4.0 International License, meaning that you can freely use, remix, and share it as long as you give attribution and do not use it for commercial purposes.\n\nPortions of this work itself are derived from the CC BY NC work “ChatGPT and Education” by Torrey Trust, PhD.\n\nPhillip Griffeth and Courtney Cullen, UGA Office of Academic Honesty, served as consultants in the creation of this guide.",
    "## Possible AI Syllabus Statements\n\nWe encourage all instructors to include a policy in their course syllabus regarding the use (and misuse) of generative AI (e.g., chatGPT) in their course.\n\nWhether students in your course are forbidden from using chatGPT or expected to explore its limits, a policy helps ensure that your expectations for appropriate interaction with generative AI tools are clear to students.\n\nOnce you decide on a policy, make sure you articulate it clearly for your students, so that they know what is expected of them.\n\nBelow are examples of sample language you may adopt for your own policy.\n\nFeel free to modify it or create your own to suit the needs of your course:",
    "#### A maximally restrictive draft policy (from Harvard University)\n\nWe expect that all work students submit for this course will be their own.\n\nIn instances when collaborative work is assigned, we expect for the assignment to list all team members who participated.\n\nWe specifically forbid the use of ChatGPT or any other generative artificial intelligence (AI) tools at all stages of the work process, including preliminary ones.\n\nViolations of this policy will be considered academic misconduct.\n\nWe draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.\n\n(Source)",
    "#### \"Use Prohibited\" Example Statement (from the University of Delaware)\n\nStudents are not allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course.\n\nEach student is expected to complete each assignment without substantive assistance from others, including automated tools.\n\n(Kevin R. Guidry)  \n(Source)",
    "#### Example Statement from Creative Writing (from Salem State University)\n\nSince writing, analytical, and critical thinking skills are part of the learning outcomes of this course, all writing assignments should be prepared by the student.\n\nDeveloping strong competencies in this area will prepare you for a competitive workplace.\n\nTherefore, AI-generated submissions are not permitted and will be treated as plagiarism.\n\nEthics still matter and plagiarism is a serious Salem State University offense.\n\n(Lis Horowitz)  \n(Source)",
    "#### A fully-encouraging draft policy (from Harvard University)\n\nThis course encourages students to explore the use of generative artificial intelligence (GAI) tools such as ChatGPT for all assignments and assessments.\n\nAny such use must be appropriately acknowledged and cited.\n\nIt is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility.\n\nViolations of this policy will be considered academic misconduct.\n\nWe draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.\n\n(Source)",
    "#### \"Use is freely permitted with no acknowledgement\" Example Statement (from the University of Delaware)\n\nStudents are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course; no special documentation or citation is required.\n\n(Kevin R. Guidry)  \n(Source)",
    "#### Example Statement from EDUC 6191: Core Methods in Educational Data Mining (from the University of Pennsylvania)\n\nWithin this class, you are welcome to use foundation models (ChatGPT, GPT, DALL-E, Stable Diffusion, Midjourney, GitHub Copilot, and anything after) in a totally unrestricted fashion, for any purpose, at no penalty.\n\nHowever, you should note that all large language models still have a tendency to make up incorrect facts and fake citations, code generation models have a tendency to produce inaccurate outputs, and image generation models can occasionally come up with highly offensive products.\n\nYou will be responsible for any inaccurate, biased, offensive, or otherwise unethical content you submit regardless of whether it originally comes from you or a foundation model.\n\nIf you use a foundation model, its contribution must be acknowledged; you will be penalized for using a foundation model without acknowledgement.\n\nHaving said all these disclaimers, the use of foundation models is encouraged, as it may make it possible for you to submit assignments with higher quality, in less time.\n\nThe university's policy on plagiarism still applies to any uncited or improperly cited use of work by other human beings, or submission of work by other human beings as your own.\n\n(Ryan S. Baker)  \n(Source)",
    "#### Example Statement from Advanced Quantitative Analyses (from Clemson University)\n\nArtificial Intelligence Policy: Are all of our classes now AI classes?\n\nI expect you to use AI (e.g., ChatGPT, Dall-e-2) in this class.\n\nIn fact, some assignments will require it.\n\nLearning to use AI is an emerging skill, and I will provide basic tutorials about how to leverage it for our work.\n\nHowever, be aware of the limits of these software systems.\n\nAI is vulnerable to discrimination because it can inadvertently (or intentionally) perpetuate existing biases present in the data it is trained on.\n\nFor example, if an AI system is trained on data that contains a bias against a certain group of people, the system may make decisions that are unfair or discriminatory towards that group.\n\nThere are several reasons why AI systems can perpetuate discrimination:  \n\n- Bias in the training data: If the training data contains biases, the AI system may learn and replicate those biases in its decision-making.\n\n- Lack of diversity in the training data: If the training data does not include a diverse range of examples, the AI system may not perform well on diverse inputs, which may lead to discrimination.\n\n- Lack of transparency: Some AI systems can be difficult to understand and interpret, making it challenging to detect and correct for biases.\n\n- Lack of accountability: Without proper oversight and accountability, it can be difficult to identify and address discrimination in AI systems.\n\nIt is important to keep in mind that these biases can be unconscious, unintended and hard to detect, but they can have serious consequences if they are not addressed.\n\nAI can be a valuable tool for augmenting human decision-making and critical thinking, but it is not a replacement.\n\nAI is a tool, just like a pencil or a computer.\n\nHowever, unlike most tools you need to acknowledge using it.\n\nPay close attention to whatever information you use in your own work that is produced from AI, and explain how/what you used at the end of assignments.\n\nMy recommendation is to screen shot and save everything (i.e., what prompts you used, what answers were produced, where, why, and how).\n\nThis is new territory, but basic attribution rules still apply.\n\nCite everything, otherwise you are likely violating academic integrity policies.\n\nIf you provide minimum effort prompts, you will get low quality results.\n\nYou will need to refine your prompts to get better outcomes.\n\nThis will take time and practice.\n\nDon't trust anything the systems says.\n\nAssume it is wrong, unless you already know the answer and can verify with trusted sources.\n\nIt works best for topics you deeply understand.\n\nUse your best judgement to determine if/where/when to use these tools.\n\nThey don't always make products easier and/or better.\n\nLarge language models and chatbots are \"look back\" machines.\n\nThey don't advance knowledge (yet).\n\nChatGPT-3 uses data from 2021 and earlier (a lot has changed since 2021).\n\nNote...some of this was written with Ai; OpenAI.\n\n(2021).\n\nGPT-3 API.\n\nRetrieved from   \n(Ryan Gagnon)  \n(Source)",
    "#### Example Statement from Specialization for Insects (from the Wharton School University of Pennsylvania)\n\nI expect you to use AI (ChatGPT and image generation tools, at a minimum), in this class.\n\nIn fact, some assignments will require it.\n\nLearning to use AI is an emerging skill, and I provide tutorials in Canvas about how to use them.\n\nI am happy to meet and help with these tools during office hours or after class.\n\nBe aware of the limits of ChatGPT:  \nIf you provide minimum effort prompts, you will get low quality results.\n\nYou will need to refine your prompts in order to get good outcomes.\n\nThis will take work.\n\nDon’t trust anything it says.\n\nIf it gives you a number or fact, assume it is wrong unless you either know the answer or can check in with another source.\n\nYou will be responsible for any errors or omissions provided by the tool.\n\nIt works best for topics you understand.\n\nAI is a tool, but one that you need to acknowledge using.\n\nPlease include a paragraph at the end of any assignment that uses AI explaining what you used the AI for and what prompts you used to get the results.\n\nFailure to do so is in violation of the academic honesty policies.\n\nBe thoughtful about when this tool is useful.\n\nDon’t use it if it isn’t appropriate for the case or circumstance.\n\n(Ethan Mollick)  \n(Source)",
    "#### Mixed draft policy (from Harvard University)\n\nCertain assignments in this course will permit or even encourage the use of generative artificial intelligence (GAI) tools such as ChatGPT.\n\nThe default is that such use is disallowed unless otherwise stated.\n\nAny such use must be appropriately acknowledged and cited.\n\nIt is each student’s responsibility to assess the validity and applicability of any GAI output that is submitted; you bear the final responsibility.\n\nViolations of this policy will be considered academic misconduct.\n\nWe draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.\n\n(Source)",
    "#### Example Statement on the Use of AI-Assisted Programming Tools (from Georgetown University)\n\nLarge language models, such as ChatGPT (chat.openai.com) are rapidly changing the tools available to people writing code.\n\nGiven their use out in the world, the view we will take in this class is that it does not make sense to ban the use of such tools in our problem sets or projects.\n\nFor now, here is my guidance on how these can and should be used in our class: First and foremost, note that output from ChatGPT can often be confidently wrong!\n\nRun your code and check any output to make sure that this actually works.\n\nSuch AI assistants will give you a good first guess, but these are really empowering for users who invest in being able to tell when the output is correct or not.\n\nIf you use ChatGPT or similar resources, credit it at the top of your problem set as you would a programming partner.\n\nWhere you use direct language or code from ChatGPT, please cite this as you would information taken from other sources more generally.\n\n(Andrew Zeitlin, McCourt)  \n(Source)",
    "#### Example Statement on the Use of ChatGPT (from Georgetown University)\n\nPart of treating others with respect is giving appropriate credit for ideas and scholarly works (including code).\n\nIf you consult with other students on an assignment, report this in the work that you turn in.\n\nIf in your code you use a library or implementation from another source, indicate that as well (minimally by including a URL in a comment).\n\nDo not generate new content with prompt-based AI tools like ChatGPT or CodePilot without permission from instructors unless specifically allowed by the assignment.\n\n(Using, for example, Grammarly as a language aid is OK.) Instructors reserve the right to request an oral explanation of answers.\n\n(Nathan Schneider, Computer Science)  \n(Source)",
    "#### Example Statement (from the University of Pennsylvania)\n\nYou may use AI programs e.g.\n\nChatGPT to help generate ideas and brainstorm.\n\nHowever, you should note that the material generated by these programs may be inaccurate, incomplete, or otherwise problematic.\n\nBeware that use may also stifle your own independent thinking and creativity.\n\nYou may not submit any work generated by an AI program as your own.\n\nIf you include material generated by an AI program, it should be cited like any other reference material (with due consideration for the quality of the reference, which may be poor).\n\nAny plagiarism or other form of cheating will be dealt with severely under relevant Penn policies.\n\n(Holly Fernandez Lynch)  \n(Source)",
    "#### Example Statement from CORE-2096: Digital Literacies and Intercultural Learning (from the American University in Cairo)\n\nTransparency: When/if you use Artificial Intelligence (AI) platforms in your assignments, please write a note to clarify where in your process you used AI and which platform(s) you used.\n\nWe will discuss this more throughout the semester in class, and you are encouraged to reflect on this in your writing as well.\n\nPlease note that what the AI writing tools generate is often inaccurate and you may have to exert effort to create something meaningful out of them.\n\nI also hope that when the assignment is about reflecting on your own opinion or experience, you will do so.\n\n(Maha Bali; @Bali_Maha)  \n(Source)",
    "#### \"Use only with prior permission\" Example Statement (from the University of Delaware)\n\nStudents are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course if instructor permission is obtained in advance.\n\nUnless given permission to use those tools, each student is expected to complete each assignment without substantive assistance from others, including automated tools.\n\n(Kevin R. Guidry)  \n(Source)",
    "#### \"Use only with acknowledgement\" Example Statement (from the University of Delaware)\n\nStudents are allowed to use advanced automated tools (artificial intelligence or machine learning tools such as ChatGPT or Dall-E 2) on assignments in this course if that use is properly documented and credited.\n\nFor example, text generated using ChatGPT-3 should include a citation such as: “Chat-GPT-3.\n\n(YYYY, Month DD of query).\n\n“Text of your query.” Generated using OpenAI.\n\nMaterial generated using other tools should follow a similar citation convention.\n\n(Kevin R. Guidry)  \n(Source)",
    "#### Example Statement from Introduction to Critical Theory (from George Washington University)\n\nPolicy on the use of generative artificial intelligence tools:\n\nUsing an AI-content generator such as ChatGPT to complete assignment without proper attribution violates academic integrity.\n\nBy submitting assignments in this class, you pledge to affirm that they are your own work and you attribute use of any tools and sources.\n\nLearning to use AI responsibly and ethically is an important skill in today’s society.\n\nBe aware of the limits of conversational, generative AI tools such as ChatGPT.\n\nQuality of your prompts: The quality of its output directly correlates to the quality of your input.\n\nMaster “prompt engineering” by refining your prompts in order to get good outcomes.\n\nFact-check all of the AI outputs.\n\nAssume it is wrong unless you cross-check the claims with reliable sources.\n\nThe currently AI models will confidently reassert factual errors.\n\nYou will be responsible for any errors or omissions.\n\nFull disclosure: Like any other tool, the use of AI should be acknowledged.\n\nAt the end of your assignment, write a short paragraph to explain which AI tool and how you used it, if applicable.\n\nInclude the prompts you used to get the results.\n\nFailure to do so is in violation of academic integrity policies.\n\nIf you merely use the instructional AI embedded within Packback, no disclosure is needed.\n\nThat is a pre-authorized tool.\n\nHere are approved uses of AI in this course.\n\nYou can take advantage of a generative AI to:  \n- Fine tune your research questions by using this tool  Enter a draft research question.\n\nThe tool can help you find related, open-ended questions.\n\n- Brainstorm and fine tune your ideas; use AI to draft an outline to clarify your thoughts.\n\n- Check grammar, rigor, and style; help you find an expression.\n\n(Alexa Alice Joubin)  \n(Source)",
    "#### Example Statement from HI 371, Baseball as American History (from Bentley University)\n\nA Few Words about Generative AI (e.g.\n\nChatGPT)  \nWriting is integral to thinking.\n\nIt is also hard.\n\nNatural language processing (NLP) applications like ChatGPT or Sudowrite are useful tools for helping us improve our writing and stimulate our thinking.\n\nHowever, they should never serve as a substitute for either.\n\nAnd, in this course, they cannot.\n\nThink of the help you get from NLP apps as a much less sophisticated version of the assistance you can receive (for free!)\n\nfrom a Bentley Writing Center tutor.\n\nThat person might legitimately ask you a question to jump-start your imagination, steer you away from the passive voice, or identify a poorly organized paragraph, but should never do the writing for you.\n\nA major difference here, of course, is that an NLP app is not a person.\n\nIt’s a machine which is adept at recognizing patterns and reflecting those patterns back at us.\n\nIt cannot think for itself.\n\nAnd it cannot think for you.\n\nWith that analogy in mind, you will need to adhere to the following guidelines in our class.\n\nAppropriate use of AI when writing essays or discussion board entries:  \n- You are free to use spell check, grammar check, and synonym identification tools (e.g., Grammarly, and MS Word).\n\n- You are free to use app recommendations when it comes to rephrasing sentences or reorganizing paragraphs you have drafted yourself.\n\n- You are free to use app recommendations when it comes to tweaking outlines you have drafted yourself.\n\nInappropriate use of AI when writing essays or discussion board entries:  \n- You may not use entire sentences or paragraphs suggested by an app without providing quotation marks and a citation, just as you would to any other source.\n\nCitations should take this form: OpenAI, chatGPT.\n\nResponse to prompt: “Explain what is meant by the term ‘Triple Bottom Line’” (February 15, 2023,   \n- You may not have an app write a draft (either rough or final) of an assignment for you.\n\nEvidence of inappropriate AI use will be grounds for submission of an Academic Integrity report.\n\nSanctions will range from a zero for the assignment to an F for the course.\n\nI’m assuming we won’t have a problem in this regard but want to make sure that the expectations are clear so that we can spend the semester learning things together—and not worrying about the origins of your work.\n\nBe aware that other classes may have different policies and that some may forbid AI use altogether.\n\n(Chris Beneke)  \n(Source)",
    "#### Example Statement from CS6750: Human-Computer Interaction; CS7637: Knowledge-Based AI (from the Georgia Institute of Technology)\n\nWe treat AI-based assistance, such as ChatGPT and Github Copilot, the same way we treat collaboration with other people: you are welcome to talk about your ideas and work with other people, both inside and outside the class, as well as with AI-based assistants.\n\nHowever, all work you submit must be your own.\n\nYou should never include in your assignment anything that was not written directly by you without proper citation (including quotation marks and in-line citation for direct quotes).\n\nIncluding anything you did not write in your assignment without proper citation will be treated as an academic misconduct case.\n\nIf you are unsure where the line is between collaborating with AI and copying from AI, we recommend the following heuristics:  \n\n- Never hit “Copy” within your conversation with an AI assistant.\n\nYou can copy your own work into your conversation, but do not copy anything from the conversation back into your assignment.\n\nInstead, use your interaction with the AI assistant as a learning experience, then let your assignment reflect your improved understanding.\n\n- Do not have your assignment and the AI agent itself open on your device at the same time.\n\nSimilar to above, use your conversation with the AI as a learning experience, then close the interaction down, open your assignment, and let your assignment reflect your revised knowledge.\n\nThis heuristic includes avoiding using AI assistants that are directly integrated into your composition environment: just as you should not let a classmate write content or code directly into your submission, so also you should avoid using tools that directly add content to your submission.\n\nDeviating from these heuristics does not automatically qualify as academic misconduct; however, following these heuristics essentially guarantees your collaboration will not cross the line into misconduct.\n\n(David Joyner)  \n(Source)",
    "#### Example Statement 1\n\nIt is important to remember that chatGPT and other AI tools are not a replacement for your own critical thinking and original ideas.\n\nThe ultimate goal of this course and any tool used to submit work is to enhance your own learning and understanding, not to undermine it.\n\nIf you have questions about what is permitted, please reach out to me.",
    "#### Example Statement 2\n\nAs a college student, it is your responsibility to maintain the highest standards of academic integrity.\n\nRepresenting work generated by artificial intelligence as one's own work is considered to be academically dishonest.\n\nThis includes (a) ensuring that all work submitted for grades is your own original work, and (b) properly citing any sources that you use.",
    "#### Example Statement 4\n\nIf you consult with other students on an assignment, report this in the work that you turn in.\n\nIf in your code you use a library or implementation from another source, indicate that as well (minimally by including a URL in a comment).\n\nDo not generate new content with prompt-based AI tools like ChatGPT or CodePilot without permission from instructors unless specifically allowed by the assignment.\n\n(Using, for example, Grammarly as a language aid is OK.) Instructors reserve the right to request an oral explanation of answers.",
    "## Abstract\n\nNew AI tools have the potential to change the way workers perform and learn, but little is known about their impacts on the job.\n\nIn this paper, we study the staggered introduction of a generative AI-based conversational assistant using data from 5,179 customer support agents.\n\nAccess to the tool increases productivity, as measured by issues resolved per hour, by 14% on average, including a 35% improvement for novice and low-skilled workers but with minimal impact on experienced and highly skilled workers.\n\nWe provide suggestive evidence that the AI model disseminates the best practices of more able workers and helps newer workers move down the experience curve.\n\nIn addition, we find that AI assistance improves customer sentiment, increases employee retention, and may lead to worker learning.\n\nOur results suggest that access to generative AI can increase productivity, with large heterogeneity in effects across workers.\n\nJEL Classifications: D80, J24, M15, M51, O33  \nKeywords: Generative AI, Large Language Models, Technology Adoption, Worker Productivity, Worker Learning, Experience of Work, Organizational Design.\n\nCorrespondence to , , and .\n\nWe are grateful to Daron Acemoglu, David Autor, Amittai Axelrod, Eleanor Dillon, Zayd Enam, Luis Garicano, Alex Frankel, Sam Manning, Sendhil Mullainathan, Emma Pierson, Scott Stern, Ashesh Rambachan, John Van Reenen, Raffaella Sadun, Kathryn Shaw, Christopher Stanton, Sebastian Thrun, and various seminar participants for helpful comments and suggestions.\n\nWe thank Max Feng for providing excellent research assistance and the Stanford Digital Economy Lab for funding.\n\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of Stanford University, MIT, or the NBER.\n\nThe emergence of generative artificial intelligence (AI) has attracted significant attention, but few studies have examined its economic impact.\n\nWhile various generative AI tools have performed well in laboratory settings, excitement about their potential has been tempered by concerns that these tools may be less effective in real-world settings, where they may encounter unfamiliar problems, face organizational resistance, or provide misleading information in a consequential environment (Peng et al., 2023a; Roose, 2023).\n\nIn this paper, we study the adoption of a generative AI tool that provides conversational guidance for customer support agents.\n\nThis is, to our knowledge, the first study of the impact of generative AI when deployed at scale in the workplace.\n\nWe find that access to AI assistance increases the productivity of agents by 14%, as measured by the number of customer issues they are able to resolve per hour.\n\nIn contrast to studies of prior waves of computerization, we find that these gains accrue disproportionately to less-experienced and lower-skill workers.\n\nWe argue that this occurs because generative AI systems work by capturing and disseminating the patterns of behavior that characterize the most productive agents, including knowledge that has eluded automation from earlier waves of computerization.\n\nComputers and software have transformed the economy with their ability to perform certain tasks with far more precision, speed, and consistency than humans.\n\nTo be effective, these systems typically require explicit and detailed instructions for how to transform inputs into outputs: when engineers write code to perform a task, they are codifying that task.\n\nYet because many workplace activities—such as writing emails, analyzing data, or creating presentations—rely on tacit knowledge, they have so far defied automation (Polanyi, 1966; Autor, 2014).\n\nMachine learning (ML) algorithms work differently from traditional computer programs: instead of requiring explicit instructions to function, these systems infer instructions from examples.\n\nGiven a training set of images, for instance, ML systems can learn to recognize specific individuals even though one cannot fully explain what physical features characterize a given person’s identity.\n\nThis ability highlights a key, distinguishing aspect of ML systems: they can learn to perform tasks even when no instructions exist—including tasks requiring tacit knowledge that could previously only be gained through lived experience (Polanyi, 1966; Autor, 2014; Brynjolfsson and Mitchell, 2017).\n\nIn addition, ML systems are often trained on data from human workers, who naturally differ in their abilities.\n\nBy seeing many examples of tasks—making sales pitches, driving a truck, or diagnosing a patient, to name a few—performed well and poorly, these models can implicitly learn what specific behaviors and characteristics set high-performing workers apart from their less effective counterparts.\n\nThat is, not only are generative AI models capable of performing complex tasks, they might also be capable of capturing the skills that distinguish top workers.",
    "That is, not only are generative AI models capable of performing complex tasks, they might also be capable of capturing the skills that distinguish top workers.\n\nThe use of ML tools may therefore expose lower-skill workers to new skills and lead to differential changes in productivity.\n\nWe study the impact of generative AI on productivity and worker experience in the customer service sector, an industry with one of the highest rates of AI adoption (Chui et al., 2021).\n\nWe examine the staggered deployment of a chat assistant using data from 5,000 agents working for a Fortune 500 software firm that provides business process software.\n\nThe tool we study is built on a recent version of the Generative Pre-trained Transformer (GPT) family of large language models developed by OpenAI (OpenAI, 2023).\n\nIt monitors customer chats and provides agents with real-time suggestions for how to respond.\n\nIt is designed to augment agents, who remain responsible for the conversation and are free to ignore its suggestions.\n\nWe have three sets of findings.\n\nFirst, AI assistance increases worker productivity, resulting in a 14% increase in the number of chats that an agent successfully resolves per hour.\n\nThis increase reflects shifts in three components of productivity: a decline in the time it takes an agent to handle an individual chat, an increase in the number of chats that an agent handles per hour (agents may handle multiple chats at once), and a small increase in the share of chats that are successfully resolved.\n\nThe productivity impacts of AI assistance are highly uneven.\n\nWe find that less-skilled and less-experienced workers improve significantly across all productivity measures we consider, including a 35% increase in the number of issues they are able to resolve per hour.\n\nAccess to the AI tool helps newer agents move more quickly down the experience curve: treated agents with two months of tenure perform just as well as untreated agents with more than six months of tenure.\n\nIn contrast, we find minimal impacts on the productivity of more-experienced or more-skilled workers.\n\nIndeed, we find evidence that AI assistance may decrease the quality of conversations by the most skilled agents.\n\nThese results contrast, in spirit, with studies that find evidence of skill-biased technical change for earlier waves.\n\nOur second set of results investigates the mechanism underlying our main findings.\n\nWe show that AI recommendations appear useful to workers: agents who follow recommendations more closely see larger gains in productivity, and adherence rates increase over time for all workers, particularly those who were initially more skeptical.\n\nWe also find that engagement with AI recommendations can generate durable learning.\n\nUsing data on software outages—periods in which the AI software fails to provide any suggestions—we show that workers see productivity gains relative to their pre-AI baseline even when recommendations are unavailable.\n\nThese outage-period gains are more pronounced for workers who had more prior exposure to AI assistance or who had followed AI recommendations more closely.\n\nFinally, we analyze the text of agents’ chats and provide suggestive evidence that access to AI drives convergence in communication patterns: low-skill agents begin communicating more like high-skill agents.\n\nOur third set of results focuses on agents’ experience of work.\n\nWork in contact centers is often difficult.\n\nAgents are regularly exposed to hostile treatment from upset (and anonymous) customers, and because much work is outsourced, many agents work overnight shifts in order to service US business hours.\n\nAI assistance may help agents communicate more effectively but could also increase the likelihood that agents are perceived as mechanical or inauthentic.\n\nWe show that access to AI assistance markedly improves how customers treat agents, as measured by the sentiment of their chat messages.\n\nWe also find that customers are less likely to question the competence of agents by requesting to speak to a supervisor.\n\nThese changes come alongside a substantial decrease in worker attrition, which is driven by the retention of newer workers.\n\nOur overall findings show that access to generative AI can increase the productivity and retention of individual workers.\n\nWe emphasize, however, that our paper is not designed to shed light on the aggregate employment or wage effects of generative AI tools.\n\nFirms may respond to increasing productivity among novice workers by hiring more of them, de-skilling positions, or seeking to develop more powerful AI systems that can replace lower-skill workers entirely.\n\nUnfortunately, our data do not allow us to observe changes in wages, overall labor demand, or the skill composition of workers hired for the job.\n\nOur results also highlight the longer-term incentive challenges that AI systems bring.",
    "Our results also highlight the longer-term incentive challenges that AI systems bring.\n\nTop workers are generally not paid for their contributions to the training data that AI systems use to capture and disseminate their skills.\n\nYet, without these contributions, AI systems may be less effective in learning to resolve new problems.\n\nOur work therefore raises questions about how workers should be compensated for the data they provide to AI systems.\n\nOur paper is related to a large literature on the impact of technological adoption on worker productivity and the organization of work (e.g., Rosen, 1981; Autor et al., 1998; Athey and Stern, 2002; Bresnahan et al., 2002; Bartel et al., 2007; Acemoglu et al., 2007; Hoffman et al., 2017; Bloom et al., 2014; Michaels et al., 2014; Garicano and Rossi-Hansberg, 2015; Acemoglu and Restrepo, 2020; Felten et al., 2023).\n\nMany of these studies, particularly those focused on information technologies, find evidence that IT complements higher-skill workers (Akerman et al., 2015; Taniguchi and Yamada, 2022).\n\nBartel et al.\n\n(2007) show that firms that adopt IT tend to use more skilled labor and increase skill requirements for their workers.\n\nAcemoglu and Restrepo (2020) study the diffusion of robots and find that the negative employment effects of robots are most pronounced for workers in blue-collar occupations and those with less than a college education.\n\nThere have been substantially fewer studies involving AI-based technologies, generative or not.\n\nAcemoglu et al.\n\n(2022); Zolas et al.\n\n(2020); Calvino and Fontanelli (2023) examine economy-wide data from the US and OECD and show that the adoption of AI tools is concentrated among large, young firms with relatively high productivity.\n\nSo far, evidence on the productivity impacts of these technologies is mixed: for example, Acemoglu et al.\n\n(2022) finds no detectable relationship between investments in AI-specific tools, while Babina et al.\n\n(2022) finds evidence of a positive relationship between firms’ AI investments and their subsequent growth and valuations.\n\nThese studies all caution that the productivity effects of AI technologies may be challenging to identify at the macro-level because AI-adopting firms differ substantially from non-adopters.\n\nIn this paper, we provide micro-level evidence on the adoption of a generative AI tool across thousands of workers working at a given firm and its subcontractors.\n\nOur work is more closely related to several other studies examining the impacts of generative AI in lab-like settings.\n\nPeng et al.\n\n(2023b) recruit software engineers for a specific coding task (writing an HTTP server in JavaScript) and show that those given access to GitHub Copilot complete this task twice as quickly.\n\nSimilarly, Noy and Zhang (2023) conduct an online experiment showing that subjects given access to ChatGPT complete professional writing tasks more quickly.\n\nChoi and Schwarcz (2023) give law students access to AI assistance on a law school exam.\n\nConsistent with our findings, Noy and Zhang (2023), Choi and Schwarcz (2023) and Peng et al.\n\n(2023a) find that ChatGPT compresses the productivity distribution, with lower-skill workers benefiting the most.\n\nOur paper, however, is the first to examine longer-term effects in a real-world workplace where we can also track patterns of learning, customer-side effects, and changes in the experience of work.",
    "## Generative AI and Large Language Models\n\nIn recent years, the rapid pace of AI development and public release tools such as ChatGPT, GitHub Copilot, and DALL-E have attracted widespread attention, optimism, and alarm (The White House, 2022).\n\nThese technologies are all examples of “generative AI,” a class of machine learning technologies that can generate new content—such as text, images, music, or video—by analyzing patterns in existing data.\n\nIn this section, we provide background on generative AI as a technology and discuss its potential economic implications.",
    "### Technical Primer\n\nThis paper focuses on an important class of generative AI, large language models (LLMs).\n\nLLMs are neural network models designed to process sequential data (Bubeck et al., 2023).\n\nAn LLM is trained by learning to predict the next word in a sequence, given what has come before, using a large corpus of text (such as Wikipedia, digitized books, or portions of the Internet).\n\nThis knowledge of the statistical co-occurrence of words allows it to generate new text that is grammatically correct and semantically meaningful.\n\nThough \"large language model\" implies human language, the same techniques can be used to produce other forms of sequential data (\"text\") such as protein sequences, audio, computer code, or chess moves (Eloundou et al., 2023).\n\nRecent progress in generative AI has been driven by four factors: computing scale, earlier innovations in model architecture, the ability to \"pre-train\" using large amounts of unlabeled data and refinements in training techniques.\n\nFirst, the quality of LLMs is strongly dependent on scale: the amount of computing power used for training, the number of model parameters, and dataset size (Kaplan et al., 2020).\n\nFirms are increasingly devoting more resources to increasing this scale.\n\nThe GPT-3 model included 175 billion parameters, was trained on 300 billion tokens, and generated approximately $5 million dollars in computing costs alone; the GPT-4 model, meanwhile, is estimated to include 1.8 trillion parameters, trained on 13 trillion tokens, at a rumored computing-only cost of $65 million (Li, 2020; Brown et al., 2020; Patel and Wong, 2023).\n\nIn terms of model architecture, modern LLMs use two earlier key innovations: positional encoding and self-attention.\n\nPositional encodings keep track of the order in which a word occurs in a given input.\n\nMeanwhile, self-attention assigns importance weights to each word in the context of the entire input text.\n\nTogether, this approach enables models to capture long-range semantic relationships within an input text, even when that text is broken up into smaller segments and processed in parallel (Vaswani et al., 2017; Bahdanau et al., 2015).\n\nNext, LLMs can be pre-trained on large amounts of unlabeled data from sources such as Reddit or Wikipedia.\n\nBecause unlabeled data is much more prevalent than labeled data, LLMs can learn about natural language on a much larger training corpus (Brown et al., 2020).\n\nBy seeing, for instance, that the word \"yellow\" is more likely to be observed with \"banana\" or \"sun\" or \"rubber duckie,\" the model can learn about semantic and grammatical relationships even without explicit guidance (Radford and Narasimhan, 2018).\n\nThe resulting model can be used in multiple applications because its training is not specific to a particular set of tasks.\n\nFinally, general-purpose LLMs can be further \"fine-tuned\" to generate output that matches the priorities of any specific setting (Ouyang et al., 2022; Liu et al., 2023).\n\nFor example, a model trained to generate social media content would benefit from receiving labeled data that contain not just the content of a post or tweet, but also information on the amount of user engagement it received.\n\nSimilarly, an LLM may generate several potential responses to a given query, but some of them may be factually incorrect or contain toxic language.\n\nTo discipline this model, human evaluators can rank these outputs to train a reward function that prioritizes desirable responses.\n\nThese types of refinements can significantly improve model quality by making a general-purpose model better suited to its specific application (Ouyang et al., 2022).\n\nTogether, these innovations have generated meaningful improvements in model performance.\n\nThe Generative Pre-trained Transformer (GPT) family of models, in particular, has attracted considerable media attention for their rapidly expanding capabilities.",
    "### The Economic Impacts of Generative AI\n\nComputers have historically excelled at executing pre-programmed instructions, making them particularly effective at tasks that can be reduced to explicit rules (Autor, 2014).\n\nConsequently, computerization has disproportionately reduced demand for workers performing “routine” tasks such as data entry, bookkeeping, and assembly line work, reducing wages in these jobs (Acemoglu and Autor, 2011).\n\nAt the same time, computerization has also increased the demand for workers who possess complementary skills such as programming, data analysis, and research.\n\nTogether, these changes have contributed to increasing wage inequality in the United States and have been linked to a variety of organizational changes (Katz and Murphy, 1992; Autor et al., 2003; Michaels et al., 2014; Bresnahan et al., 2002; Baker and Hubbard, 2003; OECD, 2023).\n\nIn contrast, generative AI tools do not require explicit instructions to perform tasks.\n\nIf asked to write an email denying an employee a raise, generative AI tools will likely respond with a professional and conciliatory note.\n\nThis occurs because the model will have seen many examples of workplace communication in which requests are declined in this manner.\n\nImportantly, the model produces such an output even though no programmer has explicitly specified what tone would be appropriate for what context, nor even defined what a tone like \"professional\" or \"conciliatory\" means.\n\nIndeed, the ability to behave \"appropriately\" is one that cannot be fully articulated even by those who possess it.\n\nRather, people learn to do so from experience and apply unconscious rules in the process.\n\nThis type of \"tacit knowledge\" underlies most tasks humans perform, both in and out of the workplace (Polanyi, 1966; Autor, 2014).\n\nThe fact that generative AI models display such skills suggests that they can acquire tacit knowledge that is embedded in the training examples they encounter.\n\nThis ability expands the types of tasks that computers may be capable of performing to include non-routine tasks that rely on judgment and experience.\n\nFor example, Github Copilot, an AI tool that generates code suggestions for programmers, has achieved impressive performance on technical coding questions and, if asked, can provide natural language explanations of how the code it produces works (Nguyen and Nadi, 2022; Zhao, 2023).\n\nMeanwhile, “AI-assistant” services such as Claude can be used to produce convincing business case analyses, including reading and interpreting financial statements and offering strategic assessments.\n\nBecause many of these tasks—coding, financial analysis, etc.—are currently performed by workers who have either been insulated or benefited from prior waves of technology adoption, the expansion of generative AI has the potential to shift the relationship between technology, labor productivity, and inequality (The White House, 2022).\n\nGenerative AI tools can not only expand the types of tasks that machines can perform, they may also reveal valuable information about how the most productive human workers differ from others.\n\nThis is because the ML models underlying generative AI systems are commonly trained on data generated by human workers and, consequently, encounter many examples of people performing tasks both well and poorly.\n\nIn learning to predict good outcomes on such data, ML models may implicitly identify characteristics or patterns of behavior that distinguish high and low performers, including subtleties rooted in tacit knowledge.\n\nGenerative AI systems then take this knowledge and use it to produce new behaviors that embody what top performers might do.\n\nThis ability could be used in different ways: firms may choose to replace lower-skill workers with AI-based tools, such tools could be used to demonstrate best practices to help lower-skill workers improve or help less experienced workers get up to speed more quickly.\n\nIn either case, generative AI tools may have differential impacts by worker ability, even amongst workers performing the same tasks.\n\nDespite their potential, generative AI tools face significant challenges in real-world applications.\n\nAt a technical level, popular LLM-based tools, such as ChatGPT, have been shown to produce false or misleading information in unpredictable ways, generating concern about their ability to be reliable in high-stakes situations.\n\nSecond, while LLM models often perform well on specific tasks in the lab (OpenAI, 2023; Peng et al., 2023b; Noy and Zhang, 2023), the types of problems that workers encounter in real-world settings are likely to be broader and less predictable.\n\nThis raises concerns both about whether AI tools will be able to provide accurate assistance in every circumstance and—perhaps more importantly—about whether workers will be able to distinguish cases where AI tools are effective from those where they are not.",
    "Finally, the efficacy of new technologies is likely to depend on how they interact with existing workplace structures.\n\nPromising technologies may have more limited effects in practice due to the need for complementary organizational investments, skill development, or business process redesign.\n\nBecause generative AI technologies are only beginning to be used in the workplace, little is currently known about their impacts.",
    "### Customer Support and Generative AI\n\nWe study the impact of generative AI in the customer service industry, an area with one of the highest surveyed rates of AI adoption.\n\nCustomer support interactions are important for maintaining a company’s reputation and building strong customer relationships, yet, as in many industries, there is substantial variation in worker productivity (Berg et al., 2018; Syverson, 2011).\n\nNewer workers are also often less productive and require significant training.\n\nAt the same time, turnover is high: industry estimates suggest that 60% of agents in contact centers leave each year, costing firms $10,000 to $20,000 dollars per agent (Buesing et al., 2020; Gretz and Jacobson, 2018).\n\nTo address these workforce challenges, the average supervisor spends at least 20 hours per week coaching agents with lower performance (Berg et al., 2018).\n\nFaced with variable productivity, high turnover, and high training costs, firms are increasingly turning to AI tools (Chui et al., 2021).\n\nAt a technical level, customer support is well-suited for current generative AI tools.\n\nFrom an AI’s perspective, customer-agent conversations can be thought of as a series of pattern-matching problems in which one is looking for an optimal sequence of actions.\n\nWhen confronted with an issue such as “I can’t login,” an AI/agent must identify which types of underlying problems are most likely to lead a customer to be unable to log in and think about which solutions typically resolve these problems (“Can you check that caps lock is not on?”).\n\nAt the same time, they must be attuned to a customer’s emotional response, making sure to use language that increases the likelihood that a customer will respond positively (“that wasn’t stupid of you at all!\n\nI always forget to check that too!”).\n\nBecause customer service conversations are widely recorded and digitized, pre-trained LLMs can be fine-tuned for customer service using many examples of both successfully and unsuccessfully resolved conversations.\n\nCustomer service is also a setting where there is high variability in the abilities of individual agents.\n\nFor example, top-performing agents are often more effective at diagnosing the underlying technical issue given a customer’s problem description.",
    "## Data Firm and AI Tool Deployment\n\nWe studied a company providing AI-based customer service support software, focusing on its deployment at a Fortune 500 enterprise software company specializing in business process software for SMBs in the U.S.\n\nAgents work from the Philippines and the U.S., dealing with technical support questions from small business owners.\n\nSessions average at 40 minutes, requiring detailed product knowledge, problem-solving, and customer interaction skills.\n\nProductivity is measured by average handle time, resolution rate, and net promoter score.\n\nManagers provide agent feedback through weekly one-on-one sessions.",
    "### AI System Design\n\nThe AI system combines a recent GPT version with ML algorithms fine-tuned for customer service.\n\nIt is trained on customer-agent conversations labeled with various outcomes and characteristics.\n\nThe system prioritizes responses that express empathy and provide technical documentation while limiting unprofessional language.\n\nIt provides real-time suggestions to agents based on conversation history and is designed to augment, not replace, human agents.",
    "### Summary Statistics\n\nOur study included data from 3 million chats by 5,179 agents, with specific detail on sample characteristics.\n\nMost agents are located outside the U.S., mainly in the Philippines.\n\nWe analyze resolutions per hour (RPH), average handle time (AHT), chats per hour (CPH), resolution rate (RR), and net promoter score (NPS) at the agent-month level.",
    "# Analysis on AI Adherence Over Time\n\nWe note, however, that this relationship could also be driven by other factors: selection (agents who choose to adhere are more productive for other reasons); or selection on gains (agents who follow recommendations are those with the greatest returns).\n\nTo further explore this, we consider worker's revealed preference: do they continue to follow AI recommendations over time?\n\nIf our results were driven purely by selection, we would expect workers with low adherence to continue having low adherence, since it was optimal for them to do so.\n\nFigure 10 plots the evolution of AI adherence over time, for various categories of agents.\n\nPanel A begins by considering agents who differ in their initial AI compliance, which we categorize based on terciles of AI adherence in the first month of model deployment.\n\nHere, we see that compliance either stays stable or grows over time.\n\nThe most initially compliant agents continue to comply at the same rates (just above 50%).\n\nLess initially compliant agents increase their compliance over time: those in the bottom tercile initially follow recommendations less than 20% of the time but, by month five, their compliance rates have increased by over 50%, to just over half of the time.\n\nNext, Panel B divides workers up by tenure at the time of AI deployment.\n\nMore senior workers are initially less likely to follow AI recommendations: 30% for those with more than a year of tenure compared to 37% for those with less than three months of tenure.\n\nOver time, however, all workers increase their adherence, with more senior workers doing so faster so that the groups converge five months after deployment.\n\nIn Panel C, we show the same analysis by worker skill at AI deployment.\n\nHere, we see that compliance rates are similar across skill groups, and all groups increase their compliance over time.\n\nThe results in Figure 10 are consistent with agents–particularly those who are initially more skeptical—coming to value AI recommendations over time.\n\nAn alternative hypothesis, however, is that agents who dislike working with AI assistance exit the firm at higher rates.\n\nIn Appendix Figure A.6 we repeat the analysis above, focusing on within-agent changes in adherence (that is, adherence rates residualized by agent fixed effects).\n\nOur within-agent results follow a similar pattern: all workers increase adherence over time, and these increases appear largest for workers who were initially the least compliant and workers who were the most senior.\n\nThis suggests that increases in adherence over time are not driven exclusively by selection.",
    "# Worker Learning\n\nA key question raised by our findings so far is whether these improvements in productivity and changes in communication patterns reflect durable changes in the human capital of workers or simply their growing reliance on AI assistance.\n\nTo study this, we examine how workers perform during periods in which they are not able to access AI-recommendations due to technical issues at the AI firm.\n\nOutages occur occasionally in our data and can last anywhere from a few minutes to a few hours.\n\nDuring an outage, the system fails to provide recommendations to some, but not necessarily all, workers.\n\nFor example, outages may affect agents who log into their computers after the system crashes, but not agents working at the same time who had signed in earlier.\n\nThey may also affect workers using one physical server but not another.\n\nOur AI firm tracks the most significant outages in order to perform technical reviews of what went wrong.\n\nWe compile these system reports to identify periods in which a significant fraction of chats are impacted by outages.\n\nAppendix Figure A.7 shows an example of such an outage, which occurred on September 10, 2020.\n\nThe y-axis plots the share of post-treatment chats (e.g., those occurring after the AI system has been deployed for a given agent) for which the AI software does not provide any suggestions, aggregated to the hour level.\n\nThe x-axis tracks hours in days leading up to and following the outage event (hours with fewer than 15 post-treatment chats are plotted as zeros for figure clarity).\n\nDuring non-outage periods, the share of chats without AI recommendations is typically 30-40%.\n\nThis reflects the fact that the AI system does not generate recommendations in response to all messages, even when it is functioning properly.\n\nBecause many chats are short, it is common to see chats end without the AI system intervening.\n\nOn the morning of September 10th, however, we see a notable spike in the number of chats without recommendations, increasing to almost 100%.\n\nRecords from our AI firm indicate that this outage was caused by a software engineer running a load test that crashed the system.\n\nFigure 11 examines the impact of access to the AI system for chats that occur during and outside these outage periods.\n\nWhereas our main event study regressions are at the worker-month level, these are at the chat level, in order to more precisely compare conversations that occurred during outage periods, versus those that did not.\n\nPanel A considers the impact of the introduction of AI assistance on chat duration (shorter is more efficient), using only post-adoption periods in which no outages are reported.\n\nConsistent with our main results, we see an immediate decline in the duration of individual chats by approximately 10% to 15%.\n\nIn Panel B, we use the same pre-treatment observations, but now restrict to post-adoption periods that are impacted by large outages.\n\nWe first note that our estimates are noisy and their magnitude appears larger than for non-outage periods (15% to 25% declines in chat duration).\n\nBecause AI outages are rare and not necessarily random, this may reflect differences in the types of chats that are seen during outage periods than during non-outage periods.\n\nHowever, focusing on the size of estimated effects over time, an interesting pattern emerges.\n\nRather than declining immediately post-adoption and staying largely stable as we see in Panel A for non-outage periods, Panel B shows that the benefit of exposure to AI assistance increases with time during outage periods.\n\nThat is, if an outage occurs one month after AI adoption, workers do not handle the chat much more quickly than their pre-adoption baseline.\n\nYet, if an outage occurs after three months of exposure to AI recommendations, workers handle the chat faster—even though they are not receiving direct AI assistance.\n\nIn Figure 12, we split our main outage event studies by worker's initial AI adherence, as described in Section 5.1.\n\nPanel A shows that workers with high initial AI adherence see large and fast declines in chat processing times (relative to their pre-adoption baseline), even during outages.\n\nPanel B, in contrast, shows no such impact for workers who tend to deviate from AI recommendations: they see no improvement in chat times during outage periods, even after many months of AI access.\n\nThese findings suggest that workers learn more by actively using AI suggestions.\n\nTogether, these results suggest that generative AI tools can help workers develop durable skills.\n\nPrior to the deployment of AI-assistance, agents only received training from managers during brief weekly coaching sessions.\n\nDuring these sessions, managers would go through several conversations from the past week and advise the worker on how they might have handled certain conversations better.\n\nHowever, by necessity, managers can only provide feedback on a small fraction of the conversations that an agent conducts.",
    "However, by necessity, managers can only provide feedback on a small fraction of the conversations that an agent conducts.\n\nMoreover, because managers are often pressed for time and may lack training, they may simply point out weak metrics (“you need to lower your handle time”) rather than identifying strategies for how an agent could better approach a problem (“you need to ask more questions at the beginning to diagnose the issue better.”) This type of coaching is ineffective and can be counterproductive for employee engagement (Berg et al., 2018).\n\nIn contrast, AI assistance provides workers with specific, actionable suggestions in real-time.\n\nOur findings suggest that this can play a useful role in supplementing existing on-the-job training programs.",
    "# Conversational Change\n\nLastly, we consider how access to AI assistance influences how workers communicate.\n\nTo capture an overall sense of the content of conversations, we begin by creating textual embeddings of agent-customer conversations.\n\nTextual embeddings take a given body of text and transform it into a high-dimensional vector that represents its “coordinates” in linguistic space.\n\nTwo pieces of text will have more similar coordinates if they share a common meaning or style.\n\nThe specific embedding given to a body of text will depend on the embedding model that is used.\n\nWe form our text embeddings using all-MiniLM-L6-v2, an LLM that is specifically intended to capture and cluster semantic information to assess similarity across text (Hugging Face, 2023).\n\nOnce we create an embedding for each conversation, we can compare the similarity of conversations by looking at the cosine similarities of their associated vectors; this common approach yields a score of 0 if two pieces of text are semantically orthogonal and a score of 1 if they have the same meaning (Koroteev, 2021).\n\nFor context, the sentences “Can you help me with logging in?” and “Why is my login not working?” have a cosine similarity of 0.68 in our model.\n\nUsing this approach, we first show that AI assistance changes the content of what agents write to customers rather than just typing the same things faster.\n\nSecond, we explore how these patterns differ for high- and low-skill workers.\n\nWe are particularly interested in understanding whether AI models can disseminate the behaviors of high performers.\n\nIf this is the case, then we would expect AI assistance to lead lower-performing agents to write more like high performers.",
    "## Within-worker changes in communication\n\nWe begin by examining how an agent’s communication evolves over time, before and after access to AI assistance.\n\nWe begin by examining treated workers and comparing the similarity of their chats in each given event-time week to their chats from the month before AI deployment (week -4 to week -1).\n\nWe exclude messages from the customer and focus only on agent-generated language.\n\nPanel A of Figure 13 plots the cosine similarity associated with these comparisons.\n\nWe find that textual similarity to the pre-AI window is stable in the weeks leading up to the AI roll-out and drops immediately following AI deployment.\n\nThat is, conversations 12 to 5 weeks before deployment are quite similar to conversations 4 to 1 week before, but conversations 0 to 12 weeks after are all less similar.\n\nThis drop in similarity is broadly inconsistent with the idea that AI assistance merely leads workers to type the same things but faster.\n\nIf that were the case, we would expect call handle times to drop, but textual similarity to remain constant.\n\nNext, Panel B of Figure 13 compares the magnitude of this pre- versus post-deployment change in textual content varies by pre-AI worker skill.\n\nWe find that lower-skill agents (those in the bottom quintile of the pre-AI skill distribution) experience greater textual change after AI adoption, relative to top performers (those in the top quintile).\n\nOur results here control for firm-year-month fixed effects, which can account for seasonal changes in topics such as tax or payroll cycles or new product rollouts.\n\nWe also control for agent tenure fixed effects, which can account for the possibility that younger workers’ language may evolve more quickly independent of access to the AI model.\n\nAlthough we cannot control directly for the chat topic, we note that chat topics are randomly assigned to agents, so we would not expect differences in topics to vary systematically by agent skill.\n\nWe interpret these results as providing suggestive evidence that AI deployment shifts the communication patterns of low-skill workers more than high-skill workers.",
    "## Across worker comparisons\n\nFigure 14 considers whether individual-level changes in communication lead low- and high-skill workers to sound more alike.\n\nTo examine this, we plot the cosine similarity between high- and low-skill agents at specific moments in calendar time, separately for workers with (blue dots) and without (red diamonds) access to AI assistance.\n\nAmong agents without AI access, we define high- and low-skill agents as those who are in the top or bottom quintile of our skill index for that month.\n\nAmong agents with AI access, we define high- and low-skill agents based on whether they are in the top or bottom quintile of skill at the time of AI deployment.\n\nFocusing on the blue dots, we see that the average textual similarity between high- and low-productivity workers is 0.55 among workers who do not have access to AI assistance.\n\nThis figure is lower than our average within-person text similarity, which makes sense given that within-person changes are likely to be smaller than across-person differences.\n\nWe see, moreover, that this textual similarity is stable over time, indicating that high- and low-skill workers do not appear to be trending differently in the absence of AI assistance.\n\nTurning to the red diamonds, we see that, post-AI adoption, high- and low-skilled workers begin to use language that is more similar.\n\nThe magnitude of this change—moving from 0.55 similarity to 0.61 similarity—may appear small, but given that the average within-person similarity for high-skill workers is around 0.67, this result suggests that AI assistance is associated with a substantial narrowing of language gaps.\n\nTogether, the patterns in Figures 13 and 14 suggest that low-skill workers are converging toward high-skill workers, rather than the opposite.\n\nThis finding is consistent with AI models disseminating the behaviors of high-skilled workers to lower-skilled workers.\n\nIn such a scenario, we would expect low-skill workers to change their communication patterns more following AI deployment.\n\nTop performers, meanwhile, would change less because the AI model is more likely to suggest language they already use.",
    "# Effects on the Experience of Work\n\nQualitative studies suggest that working conditions for contact center agents can be unpleasant.\n\nThe repetitive nature of the job, coupled with regular exposure to challenging and emotionally charged conversations, can contribute to burnout and high turnover rates.\n\nAdditionally, contact center work for US-based businesses is frequently outsourced to lower-income countries such as India and the Philippines, meaning that agents often work difficult hours and may face cultural barriers or judgments when speaking with customers.\n\nIncreases in worker productivity may not necessarily lead workers to be happier with their jobs, especially if workers feel pressured to work faster and faster.\n\nIn this section, we examine the impact of generative AI on one key aspect of the workplace experience: how agents are treated by customers, as measured by customer sentiment and requests to speak with a manager.\n\nWe also examine the impact of AI assistance on worker turnover as an overall indicator of worker satisfaction.",
    "## Customer Sentiment\n\nCustomers often vent their frustrations on anonymous service agents and, in our data, we see regular instances of swearing, verbal abuse, and “yelling” (typing in all caps).\n\nService workers are called upon to absorb such customer frustrations while limiting one’s own emotional reaction (Hochschild, 2019).\n\nThe stress associated with this type of emotional labor is often cited as a key cause of burnout and attrition among customer service workers (Lee, 2015).\n\nAccess to AI-assistance may impact how customers treat agents, but the direction and magnitude of these impacts are ambiguous.\n\nAI assistance may improve the tenor of conversations by helping agents set customer expectations or resolve their problems more quickly.\n\nAlternatively, customers may become more frustrated if AI-suggested language feels “corporate” or insincere.\n\nTo assess this, we attempt to capture the affective nature of both agent and customer text, using sentiment analysis (Mejova, 2009).\n\nFor this analysis, we use SiEBERT, an LLM that is fine-tuned for sentiment analysis using a variety of datasets, including product reviews, and tweets (Hartmann et al., 2023).\n\nSentiment is measured on a scale from -1 to 1, where -1 indicates negative sentiment and 1 indicates positive.\n\nIn a given conversation, we compute separate sentiment scores for both agent and customer text.\n\nWe then aggregate these chat-level variables into a measure of average agent sentiment and average customer sentiment for each agent-year-month.\n\nPanel A of Figure 15 shows the distribution of customer sentiment scores.\n\nOn average, customer sentiments in our data are mildly positive and normally distributed around a mean of 0.14, except for a mass of very positive and very negative scores.\n\nPanel B shows the distribution of sentiments associated with agents: agents are unfailingly positive, with a mean sentiment score of 0.89.\n\nThis reflects the fact that agents are trained to be extremely polite and friendly, even prior to AI access.\n\nPanels C and D consider how sentiment scores respond following the roll-out of AI assistance.\n\nIn Panel C, we see an immediate and persistent improvement in customer sentiment.\n\nThis effect is economically large: according to Column 1 of Table 4, access to AI improves the mean customer sentiments (averaged over an agent-month) by 0.18 points, equivalent to half of a standard deviation.\n\nIn Panel D, we see no detectable effect for agents' sentiment, which is already is very high at baseline.\n\nColumn 2 of Table 4 indicates that agent sentiments increase by only 0.02 points or about 1% of a standard deviation.\n\nFocusing on customer sentiment, Appendix Figure A.8 examines whether access to AI has different impacts across agents.\n\nWe find that access to AI assistance significantly improves how customers treat agents of all skill and experience levels, with the largest effects for agents in the lower to lower-middle range of both the skill and tenure distributions.\n\nConsistent with our productivity results, the highest-performing and most-experienced agents see the smallest benefits of AI access.\n\nThese results suggest that AI recommendations, which were explicitly designed to prioritize more empathetic responses, may improve agents’ demonstrated social skills and have a positive emotional impact on customers.",
    "## Customer Confidence and Managerial Escalation\n\nChanges in individual worker-level productivity may have broader implications for organizational workflows (Garicano, 2000; Athey et al., 1994; Athey and Stern, 1998).\n\nIn most customer service settings, front-line agents attempt to resolve customer problems but can seek the help of supervisors when they are unsure of how to proceed.\n\nCustomers, knowing this, will sometimes attempt to escalate a conversation by asking to speak to a manager.\n\nThis type of request generally occurs when the customer feels that the current agent is not equipped to address their problem or becomes frustrated.\n\nIn Figure 16, consider the impact of access to AI-assistance on the frequency of chat escalation.\n\nThe outcome variable we focus on is the share of an agent’s chats in which a customer requests to speak to a manager or supervisor, aggregated to the year-month level.\n\nWe focus on requests for escalation rather than actual escalations both because we lack data on actual escalations and because requests are a better measure of customer confidence in an agent’s competence or authority.\n\nFollowing the introduction of AI assistance, we see a gradual decline in requests for escalation.\n\nRelative to a baseline rate of approximately 6 percentage points, these coefficients suggest that AI assistance generates an almost 25% decline in customer requests to speak to a manager.\n\nIn Appendix Figure A.9, we consider how these patterns change by the skill and experience of the worker.\n\nConsistent with our other results, we find that requests for escalation are disproportionately reduced for agents who were less skilled or less experienced at the time of AI adoption.",
    "## Attrition\n\nThe adoption of generative AI tools can have a variety of impacts on workers: their productivity, the amount of stress they encounter on the job, and how they are perceived by customers, to name a few.\n\nWhile we cannot observe all these factors, we can look at turnover patterns to provide one overarching measure of how workers are impacted by AI technology at work.\n\nFor this analysis, we compare attrition rates among treated agents to those of untreated agents with the same tenure.\n\nWe drop observations for treated agents before treatment because they do not experience attrition by construction (they must survive to be treated in the future).\n\nOur analysis also controls for location and time fixed effects.\n\nFigure 17 plots the impact of AI access on attrition: Panel A considers how this varies by agent tenure while Panel B considers heterogeneity by agent skill.\n\nConsistent with our findings so far, Panel A shows that access to AI assistance is associated with the strongest reductions in attrition among newer agents, those with less than 6 months of experience.\n\nThe magnitude of this coefficient, around 10 percentage points, translates into a 40% decrease relative to a baseline attrition rate in this group of 25%.\n\nIn Panel B, we examine attrition by worker skill.\n\nHere, we find a significant decrease in attrition for all skill groups, but no systematic gradient.\n\nFinally, we note that these results should be taken with more caution relative to our main results because we are unable to include agent fixed effects to control for unobservable differences between agents with and without access to AI assistance.\n\nThis is because attrition can only occur once for any given individual.\n\nOur results may overstate the impact of AI access on attrition if, for example, access to the AI tool is more likely to be given to agents whom the firm believes are more likely to stay.",
    "# Conclusion\n\nAdvancements in AI technologies open up a broad set of economic possibilities.\n\nOur paper provides the first empirical evidence of the effects of a generative AI tool in a real-world workplace.\n\nIn our setting, we find that access to AI-generated recommendations increases worker productivity, improves customer sentiment, and is associated with reductions in employee turnover.\n\nWe hypothesize that part of the effect we document is driven by the AI system’s ability to embody the best practices of high-skill workers in our firm and make it accessible to other workers.\n\nThese practices may have previously been difficult to disseminate because they involve tacit knowledge.\n\nConsistent with this, we see that AI assistance leads to substantial improvements in problem resolution and customer satisfaction for newer- and less-skilled workers but does not help the highest-skilled or most-experienced workers on these measures.\n\nFurthermore, agents who have used the system perform somewhat better even when the system is unexpectedly disabled.\n\nAnalyzing the text of agent conversations, we find suggestive evidence that AI recommendations lead low-skill workers to communicate more like high-skill workers.\n\nOur findings, and their limitations, point to a variety of directions for future research.\n\nMost importantly, our results do not capture the potential longer-term impacts of generative AI on skill demand, job design, wages, or customer demand.\n\nIt is unclear, for example, whether improvements in customer service productivity will lead to more or less demand for customer service workers.\n\nIf the demand for customer support is inelastic, then generative AI tools may reduce demand and wages in this sector in the long run.\n\nAlternatively, better product support could lead customers to seek out representatives for a wider range of questions; this, in turn, could increase demand for workers or give them new responsibilities, such as collecting customer feedback for the product development team (Berg et al., 2018; Korinek, 2022).\n\nOur findings also raise questions about the nature of worker productivity.\n\nTraditionally, a support agent’s productivity refers to their ability to help the customers they come in contact with.\n\nYet, in a setting where customer service conversations are fed into training datasets, a worker’s productivity also includes their ability to provide ML models with examples of successful behaviors that can be shared with others.\n\nIn our setting, top performers contribute many of the examples used to train the AI system we study, but they see relatively few improvements in their own productivity as a result.\n\nUnder our data firm’s current pay practices, these workers may even see a reduction in their pay because bonuses are calculated relative to other agents’ performance.\n\nOur results therefore raise questions about how workers, particularly top performers, should be compensated for the data that they provide to AI systems.\n\nFinally, as a potential general-purpose technology, generative AI can and will be deployed in a variety of ways, and the effects we find may not generalize across all firms and production processes (Eloundou et al., 2023).\n\nFor example, our setting has a relatively stable product and a set of technical support questions.\n\nIn areas where the product or environment is changing rapidly, the relative value of AI recommendations may be different: they may be better able to synthesize changing best practices, or they may actually impede learning by promoting outdated practices observed in historical training data.\n\nGiven the early stage of generative AI, these and other questions deserve further scrutiny.",
    "# References\n\nAcemoglu, Daron and David Autor, \"Skills, tasks and technologies: Implications for employment and earnings,\" in \"Handbook of labor economics,\" Vol.\n\n4, Elsevier, 2011, pp.\n\n1043–1171.\n\nAcemoglu, Daron and Pascual Restrepo, \"Low-Skill and High-Skill Automation,\" Journal of Human Capital, June 2018, 12 (2), 204–232.\n\nAcemoglu, Daron and Pascual Restrepo, \"Robots and Jobs: Evidence from US Labor Markets,\" Journal of Political Economy, 2020, 128 (6), 2188–2244.\n\n_eprint: \n\nAcemoglu, Daron, Gary Anderson, David Beede, Catherine Buffington, Eric Childress, Emin Dinlersoz, Lucia Foster, Nathan Goldschlag, John Haltiwanger, Zachary Kroff, Pascual Restrepo, and Nikolas Zolas, \"Automation and the Workforce: A Firm-Level View from the 2019 Annual Business Survey,\" 2022.\n\nAcemoglu, Daron, Philippe Aghion, Claire Lelarge, John Van Reenen, and Fabrizio Zilibotti, \"Technology, Information, and the Decentralization of the Firm*,\" The Quarterly Journal of Economics, November 2007, 122 (4), 1759–1799.\n\n_eprint: \n\nAkerman, Anders, Ingvil Gaarder, and Magne Mogstad, \"The Skill Complementarity of Broadband Internet *,\" The Quarterly Journal of Economics, July 2015, 130 (4), 1781–1824.\n\n_eprint: \n\nAthey, Susan and Scott Stern, \"An Empirical Framework for Testing Theories About Complementarity in Organizational Design,\" Working Paper 6600, National Bureau of Economic Research June 1998.\n\nAthey, Susan and Scott Stern, \"The Impact of Information Technology on Emergency Health Care Outcomes,\" RAND Journal of Economics, Autumn 2002, 33 (3), 399–432.\n\nAthey, Susan, Joshua Gans, Scott Schaefer, and Scott Stern, \"The Allocation of Decisions in Organizations,\" Stanford Graduate School of Business, 1994.\n\nAutor, David, \"Polanyi’s Paradox and the Shape of Employment Growth,\" Working Paper w20485, National Bureau of Economic Research September 2014.\n\nAutor, David H., Frank Levy, and Richard J. Murnane, \"The Skill Content of Recent Technological Change: An Empirical Exploration,\" The Quarterly Journal of Economics, 2003, 118 (4), 1279–1333.",
    "## Language Models as Few-Shot Learners\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei, “Language Models are Few-Shot Learners,” July 2020. arXiv:2005.14165 [cs].",
    "## Summary of ChatGPT/GPT-4 Research\n\nLiu, Yiheng, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and Bao Ge, “Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models,” April 2023. arXiv:2304.01852 [cs].",
    "## Training Language Models with Human Feedback\n\nOuyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe, “Training language models to follow instructions with human feedback,” March 2022. arXiv:2203.02155 [cs].",
    "## Figure 1: Sample AI Output\n\nSample Customer Issue\n\nSample AI-generated Suggested Response\n\n*Notes: This figure shows sample suggestions of output generated by the AI model.\n\nThe suggested responses are only visible to the agent.\n\nWorkers can choose to ignore, accept or somewhat incorporate the AI suggestions into their response to the customer.\n\n*",
    "## Figure 2: Raw Productivity Distributions, by AI Treatment\n\nResolutions Per Hour\n\n*Figure displays a series of bar graphs illustrating the distribution of various outcome measures, such as resolution rate and customer satisfaction.\n\n*\n\n*Notes: This figure shows the distribution of various outcome measures.\n\nWe split this sample into agent-month observations for agents who eventually receive access to the AI system before deployment (“Pre AI”), after deployment (“Post AI”), and for agent-months associated with agents who never receive access (“Never AI”).\n\nOur primary productivity measure is “resolutions per hour,” the number of customer issues the agent is able to successfully resolve per hour.\n\nWe also provide descriptives for “average handle time,” the average length of time an agent takes to finish a chat; “chats per hour,” the number of chats completed per hour incorporating multitasking; “resolution rate,” the share of conversations that the agent is able to resolve successfully; and “net promoter score” (NPS), which are calculated by randomly surveying customers after a chat and calculating the percentage of customers who would recommend an agent minus the percentage who would not.\n\nAll data come from the firm’s software systems.\n\n*",
    "## Figure 3: Event Studies, Resolutions Per Hour\n\nResolutions Per Hour\n\n*Notes: These figures plot the coefficients and 95% confidence intervals from event study regressions of AI model deployment using the Sun and Abraham (2021) interaction weighted estimator.\n\nSee text for additional details.\n\nPanel A plots the resolutions per hour and Panel B plots the natural log of the measure.\n\nAll specifications include agent and chat year-month, location, agent tenure, and company fixed effects.\n\nRobust standard errors are clustered at the agent level.\n\n*",
    "## Figure 4: Event Studies, Additional Outcomes\n\n*This figure contains four panels, each focusing on a different aspect related to AI deployment: average handle time, chats per hour, resolution rate, and customer satisfaction.\n\n*\n\n*Notes: These figures plot the coefficients and 95% confidence intervals from event study regressions of AI model deployment using the Sun and Abraham (2021) interaction weighted estimator.\n\nSee text for additional details.\n\nPanel A plots the average handle time or the average duration of each technical support chat.\n\nPanel B plots the number of chats an agent completes per hour, incorporating multitasking.\n\nPanel C plots the resolution rate, the share of chats successfully resolved, and Panel D plots net promoter score, which is an average of surveyed customer satisfaction.\n\nAll specifications include agent and chat year-month, location, agent tenure, and company fixed effects.\n\nRobust standard errors are clustered at the agent level.\n\n*",
    "## Figure 5: Heterogeneity of AI Impact, by Skill and Tenure\n\nImpact of AI on Resolutions Per Hour, by Skill at Deployment\n\n*Notes: These figures plot the impacts of AI model deployment on log(resolutions per hour) for different groups of agents.\n\nAgent skill is calculated as the agent’s trailing three month average of performance on average handle time, call resolution, and customer satisfaction, the three metrics our firm uses to assess agent performance.\n\nWithin each month and company, agents are grouped into quintiles, with the most productive agents in quintile 5 and the least productive in quintile 1.\n\nPre-AI worker tenure is the number of months an agent has been employed when they receive access to AI recommendations.\n\nAll specifications include agent and chat year-month, location, and company fixed effects and standard errors are clustered at the agent level.\n\nPanel A includes controls for agent tenure at deployment and Panel B includes controls for agent skill at deployment.\n\n*",
    "## Figure 6: Heterogeneity of AI Impact by pre-AI Worker Skill and Controlling for Tenure, Additional Outcomes\n\n*This figure showcases the heterogeneous impacts of AI model deployment by measuring average handle time, chats per hour, resolution rate, and customer satisfaction based on worker skill.\n\n*\n\n*Notes: These figures plot the impacts of AI model deployment on four measures of productivity and performance, by pre-deployment worker skill.\n\nAgent skill is calculated as the agent’s trailing three month average of performance on average handle time, call resolution, and customer satisfaction, the three metrics our firm uses for agent performance.\n\nWithin each month and company, agents are grouped into quintiles, with the most productive agents within each firm in quintile 5 and the least productive in quintile 1.\n\nPanel A plots the average handle time or the average duration of each technical support chat.\n\nPanel B graphs chats per hour, or the number of chats an agent can handle per hour.\n\nPanel C plots the resolution rate, and Pane...",
    "**A.\n\nSample AI-generated Technical Link**\n\nNotes: This figure shows a sample technical documentation suggestions made by the AI.\n\nOur data firm has an extensive set of documentation for their technical support agents, known as the knowledge base, which is like an internal company Wikipedia for product and process information.\n\nThe AI will attempt to surface the most helpful technical documentation page when triggered to do so during a customer interaction.\n\nThese links are only visible to the agent, and agents must review to see if the resource is helpful.\n\nWorkers can choose to read the suggested technical documentation or ignore the recommendation.",
    "## Figure A.2: Deployment Timeline\n\nMonth Onboarded\n\nNotes: This figure shows the share of agents deployed onto the AI system over the study period.\n\nAgents are deployed onto the AI system after a training session.\n\nThe firm ran a small randomized control trial in August and September of 2020.\n\nAll data are from the firm’s internal software systems.",
    "## Figure A.3: Event Studies, Resolutions Per Hour\n\nResolutions Per Hour\n\n1.2 1 .8 .6 .4 .2 0 -.2 -.4 -.6\n\n-10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 Months to/from AI Deployment\n\nSun-Abraham de Chaisemartin-D'Haultfoeuille Callaway-Sant'Anna Borusyak et al.\n\nTWFE OLS\n\nLog(Resolutions Per Hour)\n\n.6\n\n.4\n\n.2\n\n0\n\n-.2\n\n-10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 Months to/from AI Deployment\n\nSun-Abraham de Chaisemartin-D'Haultfoeuille Callaway-Sant'Anna Borusyak et al.\n\nTWFE OLS\n\nNotes: This table presents the effect of AI model deployment on our main productivity outcome, resolutions per hour, using a variety of robust dynamic difference-in-differences estimators introduced in Borusyak et al.\n\n(2022), Callaway and Sant’Anna (2021), de Chaisemartin and D’Haultfoeuille (2020) and Sun and Abraham (2021) and a standard two-way fixed effects regression model.\n\nAll regressions include agent level, chat-year fixed effects and controls for agent tenure.\n\nStandard errors are clustered at the agent level.\n\nBecause of the number of post-treatment periods and high turnover of agents in our sample, we can only estimate five months of preperiod using Borusyak et al.\n\n(2022) and de Chaisemartin and D’Haultfoeuille (2020).",
    "## Figure A.4: Experience Curves by Deployment Cohort, Additional Outcomes\n\nA.\n\nAverage Handle Time B. Chats Per Hour\n\n0 2 4 6 8 10 Agent Tenure, Months 0 2 4 6 8 10 Agent Tenure, Months\n\nResolution Rate D. Customer Satisfaction\n\n0 2 4 6 8 10 Agent Tenure, Months 0 2 4 6 8 10 Agent Tenure, Months\n\nNotes: These figures plot the experience curves of three groups of agents over their tenure, the x-axis, against five measures of productivity and performance.\n\nThe red lines plot the performance of always-treated agents, those who are started work in their first month with the AI and always have access to the AI suggestions.\n\nThe blue line plots agents who are never treated.\n\nThe green line plots agents who spend their first four months of work without the AI model and gain access to the AI during their fifth month on the job.\n\nAll panels include 95% confidence intervals.",
    "## Figure A.5: Heterogeneity of AI Impact by Initial AI Adherence, Additional Outcomes\n\nA. Log(Average Handle Time) B. Log(Chats Per Hour)\n\n0 .25\n\n-.05 .2\n\n-.1 .15\n\n-.15 .1\n\n-.2 .05\n\nInitial Adherence at AI Deployment (%) Initial Adherence at AI Deployment (%)\n\nLog(Resolution Rate) D. Log(Customer Satisfaction (NPS))\n\n.06 .06\n\n.04 .04\n\n.02\n\n.02\n\n0\n\n0 -.02\n\n-.02\n\n-.04\n\nInitial Adherence at AI Deployment (%) Initial Adherence at AI Deployment (%)\n\nNotes: These figures plot the impact of AI model deployment on additional measures of performance by quintile of initial adherence, the share of AI recommendations followed in the first month of treatment.\n\nPanel A plots the average handle time or the average duration of each technical support chat.\n\nPanel B graphs chats per hour, or the number of chats an agent can handle per hour (including working on multiple chats simultaneously).\n\nPanel C plots the resolution rate, the share of chats successfully resolved, and Panel D plots NPS, or net promoter score, is an average of surveyed customer satisfaction.\n\nAll specifications include agent and chat year-month, location, and company fixed effects and controls for agent tenure.\n\nAll data come from the firm’s internal software systems.",
    "### By Agent Skill at AI Model Deployment\n\n0 1 2 3 4 Months Since AI Deployment\n\nNotes: This figure plots the residualized percentage of AI suggestions followed by agents as a function of the number of months each agent has had access to the AI model, after controlling for agent level fixed effects.\n\nIn Panel A, we divide agents into terciles based on their adherence to AI suggestions in the first month.\n\nIn Panel B, we divide agents into groups based on their tenure at the firm at the time of AI model deployment.\n\nIn Panel C, we divide workers into terciles of pre-deployment productivity as defined by our skill index.\n\nAll data come from the firm’s internal software systems.",
    "### By Productivity at AI Model Deployment\n\n.25\n\n.2\n\n.15\n\n.1\n\n.05\n\nAgent Tenure at AI Deployment\n\nNotes: Each panel of this figure plots the impact of AI model deployment on the mean sentiment per conversation.\n\nSentiment refers to the emotion or attitude expressed in the text of the customer chat and ranges from -1 to 1 where -1 indicates very negative sentiment and 1 indicates very positive sentiment.\n\nPanel A plots the effects of AI model deployment on customer sentiment by agent tenure when AI deployed and Panel B plots the impacts by agent ex-ante productivity.\n\nAll data come from the firm’s internal software systems.\n\nAverage sentiment is measured using SiEBERT, a fine-tuned checkpoint of a RoBERTA, an English language transformer model.",
    "### Manager Assistance, by pre-AI tenure\n\n.01\n\n0\n\n-.01\n\n-.02\n\nAgent Tenure at AI Deployment\n\nNotes: Panels A and B show the effects of AI on customer requests for manager assistance, by pre-AI agent skill and in by pre-AI agent tenure.\n\nAll robust standard errors are clustered at the agent location level.\n\nAll data come from the firm’s internal software systems.",
    "## Table A.1: Main Effects: Productivity (Log(Resolutions per Hour)), Alternative Difference-in-Difference Estimators\n\nPoint Estimate\n\nStandard Error Lower Bound 95% Confidence Interval Upper Bound 95% Confidence Interval\n\nTWFE-OLS 0.137 0.014 0.108 0.165 Borusyak-Jaravel-Spiess 0.257 0.028 0.203 0.311 Callaway-Sant’Anna 0.239 0.025 0.189 0.289 DeChaisemartin-D’Haultfoeuille 0.116 0.021 0.075 0.156 Sun-Abraham 0.237 0.037 0.165 0.308\n\nNotes: This table shows the impact of AI model deployment on the log of our main productivity outcome, resolutions per hour, using robust difference-in-differences estimators introduced in Borusyak et al.\n\n(2022), Callaway and Sant’Anna (2021), de Chaisemartin and D’Haultfoeuille (2020) and Sun and Abraham (2021).\n\nAll regressions include agent level, chat-year fixed effects and controls for agent tenure.\n\nThe standard errors are clustered at the agent level.",
    "## Introduction:\n\nGenerative artificial intelligence (AI) language models can assist with various tasks from teaching and learning, to writing support, to data analysis.\n\nDistrict staff who have access to generative AI tools should understand underlying behaviors and the potential benefits and limitations associated with use.\n\nThis interim guidance outlines recommendations regarding the types of data that may and may not be entered into consumer or commercial generative AI products, with specific considerations for the safety and productivity of K-12 schools.\n\nIt also offers an overview of limitations to be aware of when using generative AI and offers some current best practices for working with these tools.\n\nThe creation of future AI usage policies will be handled at the local board level in collaboration with administrators and community stakeholders.\n\nThis sample guidance document is to support school districts and should be modified and reviewed with independent legal counsel as needed prior to implementation.",
    "## Recommended Practices:\n\nFor district users using generative AI tools that are not governed by a formal agreement with the district, we recommend the following practices.\n\n- **Consider Impact:** Assess the potential consequences of using AI, especially in terms of data privacy and integrity.\n\n- **Critical Evaluation:** Encourage cross-referencing AI information with reliable sources and consult with educational technology experts.\n\n- **Stay Informed:** Regularly update knowledge on AI advancements and adapt district guidelines accordingly.\n\n- **Alignment to Goals:** Where applicable, align AI integration to how it supports existing school improvement goals.",
    "## Risks and Limitations:\n\nUsers of generative AI should be aware of other risks and limitations related to the output generated by these products.\n\n- **Misinformation and Inaccuracies:** Actively verify the accuracy of AI-generated content and timeliness, especially for subjects prone to rapid changes.\n\n- **Bias:** Regularly evaluate AI outputs to identify and mitigate biases, ensuring fairness in information dissemination.\n\n- **Inappropriate Content:** Monitor AI tools for potential generation of or response to offensive or unsuitable content.",
    "## Disclosures:\n\nIt is important for educators to set exemplary standards in the use of AI technology.\n\nStaff who use AI are expected to:\n\n- **Clearly Disclose AI Involvement:** Whenever AI tools contribute to the development of higher profile educational materials, research, or work-related artifacts, or are being used to record/summarize others’ contributions.\n\n(e.g., “AI assisted X District staff in the creation of this resource,” OR “X% of this document was created with AI.”)\n- **Educate Through Example:** In demonstrating the use of AI, staff aim to educate.\n\nDescriptions of how AI was used should be informative, helping the audience understand AI’s role and capabilities.\n\n- **Continuously Update Citation Practices:** With the lack of identified current best practices, there is no formal language for citations.\n\nThe district is committed to staying informed about widely accepted AI citation practices and updating guidelines accordingly.\n\nWith or without AI, citation of primary sources for information is always a best practice.",
    "## Data Stewardship:\n\nAll data use must comply with all state and federal laws and organizational regulations and requirements, including the district's acceptable use and data policies.\n\nEthical considerations in alignment with the district's mission, vision, and values must also be considered.\n\nAlthough generative AI products may claim to have some privacy safeguards in place, users should assume that all consumer generative AI products make data publicly available unless otherwise indicated per explicit official agreement with the school district.\n\nIn addition to the expectations above, specific types of data should be handled in different ways when using a generative AI product:\n\n- **Compliance:** Ensure all AI interactions comply with state and federal laws, especially regarding student data under FERPA, IDEA, CIPA, and COPPA.\n\n- **Ethical Use:** Align AI tool usage with the district's mission, ensuring it supports strategic and educational objectives.\n\nStudent and staff Acceptable Use Policies should be reviewed and updated regularly.",
    "## Data Types:\n\n- **Public Data:** Use AI for general information and academic topics within the bounds of district policies.\n\n- **Confidential/Private Data:** Strictly prohibit entering sensitive student data, personally identifiable information (PII), or any information protected by privacy laws.\n\n- **Research Data:** Prioritize anonymization and legal compliance when using AI for research purposes in education.\n\n- **Intellectual Property:** Be cautious with proprietary or confidential information that is otherwise protected under contractual agreements.\n\nBy adhering to these disclosure principles, as educators, we not only enhance the credibility of our work but also position ourselves as responsible leaders in the educational application of AI technology.\n\nFurther guidance regarding more specific needs like handling generative AI in teaching and learning activities, selecting and adopting AI tools, creating sample syllabus language, and more should follow as the district continues to explore how to effectively leverage these new tools in a way that meets stakeholder needs while keeping data and users safe.\n\nThis document is adapted from Michigan State University’s Interim Guidance on Data Uses and Risks of Generative AI 2023.\n\nSupported by these members of the Michigan Virtual AI Statewide Workgroup:",
    "### Policy Statement\n\nThe University of North Texas promotes the integrity of learning and embraces the core values of trust and honesty.\n\nAcademic integrity is based on educational principles and procedures that protect the rights of all participants in the educational process and validate the legitimacy of degrees awarded by the University.\n\nIn the investigation and resolution of allegations of student academic dishonesty, the University’s actions are intended to be corrective, educationally sound, fundamentally fair, and based on reliable evidence.",
    "### Policy Definitions\n\n**Academic Misconduct**  \n“Academic Misconduct,” in this policy, means the intentional or unintentional action by a student to engage in behavior in the academic setting including, but not limited to: cheating, fabrication, facilitating academic misconduct, forgery, plagiarism, and sabotage.\n\n**Academic Integrity Database**  \n“Academic Integrity Database,” in this policy, means the electronic database maintained in the Dean of Student’s Office to manage confidential records of student academic penalties and misconduct sanctions and student academic educational status.\n\nRecords are protected by Federal Education Rights and Privacy Act (FERPA).\n\n**Academic Integrity Officer (AIO)**  \n“Academic Integrity Officer (AIO),” in this policy, means the faculty member appointed by the provost and vice president for academic affairs to promote academic integrity and administer the duties of the Office of Academic Integrity.\n\nThe AIO is charged with reviewing possible major, multiple, or repeat violations and imposed sanctions.\n\n**Academic Integrity Office Panel (AIOP)**  \n“Academic Integrity Office Panel (AIOP),” in this policy, means a panel convened for the purpose of providing a student the opportunity for impartial review of evidence underlying allegations of academic dishonesty, and of the academic penalties and misconduct sanctions imposed in matters involving allegations of major or multiple violations.\n\n**Appeal**  \n“Appeal,” in this policy, means a request made by a student to challenge an academic penalty imposed as a result of a finding that the student violated the Academic Integrity Policy.\n\n**Cheating**  \n“Cheating,” in this policy, means the use of unauthorized assistance in an academic exercise, including but not limited to:\n- use of any unauthorized assistance to take exams, tests, quizzes, or other assessments;\n- use of sources beyond those authorized by the instructor in writing papers, preparing reports, solving problems, or carrying out other assignments;\n- use, without permission, of tests, notes, or other academic materials belonging to instructors, staff members, or other students of the University;\n- dual submission of a paper or project, or resubmission of a paper or project to a different class without express permission from the instructor;\n- any other act designed to give a student an unfair advantage on an academic assignment.\n\n**Day**  \n“Day,” in this policy, means Monday through Friday during regular University business hours (8 a.m to 5:00 p.m.).\n\n**Fabrication**  \n“Fabrication,” in this policy, means falsifying or inventing any information, data, or research outside of a defined academic exercise.\n\n**Facilitating Academic Dishonesty**  \n“Facilitating academic dishonesty,” in this policy, means helping or assisting another in the commission of academic dishonesty.\n\n**Forgery**  \n“Forgery,” in this policy, means intentionally falsifying or altering a score, grade, or official academic University record or the signature of another.\n\n**Preponderance of the Evidence**  \n“Preponderance of the evidence,” in this policy, means standard of review in the student appeal process that evaluates whether allegations are more likely to be true than not true.\n\n**Instructor**  \n“Instructor,” in this policy, means any University employee who has been assigned instructional responsibilities including, but not limited to, tenure-track and professional faculty Instructors, librarians (or librarian appointments), adjunct instructors, and teaching fellows.\n\n**Minor Violation**  \n“Minor violation,” in this policy, means errors in judgment without clear intent by the student to violate the Academic Integrity Policy.\n\n**Major Violation**  \n“Major violation,” in this policy, means a serious act or acts of academic misconduct that supports evident substantial disregard for the academic integrity of the University as determined by the Office of Academic Integrity.\n\n**Plagiarism**  \n“Plagiarism,” in this policy, means use of another’s thoughts or words without proper attribution in any academic exercise, regardless of the student’s intent, including but not limited to:\n- the knowing or negligent use by paraphrase or direct quotation of the published or unpublished work of another person without full and clear acknowledgment or citation, or\n- the knowing or negligent unacknowledged use of materials prepared by another person or by an agency engaged in selling term papers or other academic materials.\n\n**Sabotage**  \n“Sabotage,” in this policy, means acting to prevent others from completing their work or willfully disrupting the academic work of others.",
    "**Sabotage**  \n“Sabotage,” in this policy, means acting to prevent others from completing their work or willfully disrupting the academic work of others.\n\n**Student**  \n“Student,” in this policy, means a person taking courses at the University, including individuals who withdraw after allegations of academic misconduct; those who are not currently enrolled in courses but who have a continuing academic relationship with the University; and those who have been admitted or readmitted to the university.",
    "**Responsibility for Single Violations of Academic Misconduct**\n\n**Instructor**  \nInstructors have primary responsibility for academic assessment.\n\nA finding by an instructor that academic dishonesty occurred may be considered grounds for academic penalties, up to and including failure in the course.\n\nDecisions about the degree of academic penalty to impose will be based on the seriousness of the violation.\n\nInstructors are expected to report all allegations, factual summary statements, and sanctions involving instances of academic misconduct.\n\n**AIO and Office of Academic Integrity**  \nWhen an instructor and student cannot reach agreement regarding the degree of academic penalty, the instructor may seek guidance and advisement from the Office of Academic Integrity.\n\nUnder the supervision of the AIO, the Office of Academic Integrity has the following responsibilities:\n- provide campus education and awareness training and academic integrity resources;\n- schedule appeals before the AIOP;\n- review single violation reports to determine whether the violation should be categorized as a major or minor violation;\n- administer the procedures and academic sanctions as set forth in this policy, including investigation of possible major violations and misconduct involving multiple or repeat violations of the policy; and\n- consult with instructors and students about procedures and rights, and inform students of impending investigations, misconduct findings, misconduct sanctions, and appeal rights as related to major, multiple, or repeat violations.\n\n**Department Chair**  \nThe department chair has final authority on appeal over academic penalties imposed for single violations.\n\n**Office of the Provost**  \nThe provost or designee reviews cases and holds final administrative authority for resolving appeals involving expulsion, suspension, and revocation of degree for undergraduate and graduate students.\n\nThe provost may request additional information from any person, as needed, to make a final decision on academic penalties.\n\n**Student**  \nStudents are expected to conduct themselves in a manner consistent with the University's status as an institution of higher education.\n\nA student is responsible for responding to an academic dishonesty report issued by an instructor or other University official.\n\nIf a student fails to respond after proper attempt at notification has been made, the University may take appropriate academic actions in the absence of the student.",
    "### Academic Penalties\n\nThe following academic penalties may be assessed upon determination that academic misconduct has occurred: Admonition, Educational Assignment, Partial or No Credit for an Assignment or Assessment, Lower Final Course Grade, Course Failure, Probation, Suspension, Expulsion, and Revocation of Degree.\n\nProbation, Suspension, Expulsion, and Revocation of Degree may only be handed out by the AIO with the final decision on appeal by the provost or designee, whereas the other penalties may be assigned by the instructor.\n\nAdmonitions and educational assignments are not appealable.\n\n**Admonition**  \nThe student may be issued a verbal or written warning.\n\n**Educational Assignment**  \nThe student may be required to perform additional coursework not required of other students in the specific course.\n\n**Partial or No Credit for an Assignment or Assessment**  \nThe instructor may award partial or no credit for the assignment or assessment on which the student engaged in academic misconduct.\n\n**Course Failure**  \nThe instructor may assign a failing grade for the course.\n\n**Lower Grade**   \nThe instructor may lower the student’s final grade by one letter grade.\n\n**Probation**  \nA student may be placed on probation for up to two (2) long semesters.\n\nStudents on probation may remain at the University but may be required to satisfy specific conditions or requirements, such as reporting regularly to the AIO, or being barred from holding any office or participating in any activity in which the student represents the University or University-recognized student organizations, either within or outside the University community.\n\nThe sanction of probation prohibits graduation until the period of probation has ended and the student has complied with all AIO requirements.\n\n**Suspension**  \nA student may be suspended from the University for up to one year, during which time the student is ineligible for the privileges associated with registration, including living in University housing.\n\nSuspension anticipates that the student may return once applicable conditions are satisfied.\n\n**Expulsion from the University**  \nThe student is removed from good standing with the Office of Academic Integrity and must leave the University permanently without an expectation of return to the University.\n\n**Revocation of Degree**  \nThe student’s official and unofficial transcript may reflect that revocation of degree assessed as an academic misconduct penalty.",
    "### Academic Penalties During an Appeal\n\nShould a student’s appeal of an academic misconduct penalty extend beyond the grade submission deadline for the semester of the incident, the student will be assigned a grade that reflects the recommended penalty.\n\nUpon conclusion of the appeal process, the student’s grade will be adjusted accordingly (penalty removed) based upon the outcome of the appeal.",
    "### Procedures for Single Violation of Academic Dishonesty\n\n**Instructor Actions**  \nAn instructor who suspects that a student has engaged in an act of academic misconduct must make a good faith effort to contact the student in writing as soon as possible after detecting the suspected academic dishonesty.\n\nThe instructor’s initial communication should be sent to the student’s University-assigned email address and should convey the details of the suspected academic misconduct in sufficient detail to allow the student to prepare a written response, and direct the student to schedule an in-person conference with the instructor to discuss the suspected misconduct.\n\n**No Student Response**  \nIf the student does not respond to the instructor’s written communication within five (5) days of the instructor sending the email, the instructor may assess academic penalties in-line with the suspected academic dishonesty.\n\n**Conference**  \nIf the student responds to the instructor’s written notice of suspected academic misconduct an in-person conference with the instructor should be scheduled.\n\nAs part of the conference, the instructor will review all the evidence or information relevant to the suspected act of academic dishonesty and provide the student a full opportunity to respond.\n\n**Instructor Finds No Misconduct**  \nIf, after the conference, the instructor determines that the student has not engaged in an act of academic dishonesty, no sanctions will be imposed and the instructor will notify the student immediately.\n\n**Instructor Finds Misconduct**  \nIf the instructor determines upon preponderance of the evidence that the student has engaged in an act of academic dishonesty, the instructor will notify the student of this determination in writing, as soon as possible, if not immediately upon the conclusion of the conference.\n\n**Single Violation Report Submission**  \nThe instructor will submit the online Academic Integrity Single Violation Report with the factual summary and any relevant documents to support the findings and the imposed academic penalty.\n\nThis will be filed in the Academic Integrity Database.",
    "### Student Appeal of Instructor’s Actions\n\n**Submission**  \nIf a student disagrees with an instructor’s determination of academic misconduct or with an academic penalty, the student may submit a written appeal to the instructor’s department chair within five (5) days of the date on the instructor’s written decision.\n\n**Contents**  \nThe student’s appeal must be submitted in writing using the online Appeal of Academic Integrity Single Violation form.\n\nThe appeal must detail:\n- whether the student is requesting appeal of the finding of academic misconduct or the instructor’s assigned academic penalty, or both;\n- the specific basis for the appeal; and\n- any factual information in support of the student’s case, including any specific evidence.\n\n**Department Chair Appeal Process for Review and Resolution of Appeal**  \nUpon receipt of the student’s written appeal the department chair must determine whether review of the appeal will be conducted by the department chair or a faculty committee.\n\nThe decision of the chair on this issue is final.\n\n**Timing**  \nWithin ten (10) days of receiving the student’s written appeal the department chair must complete the following actions.\n\n**Decision**  \nThe department chair will issue, based upon the preponderance of the evidence, one of the following findings and supporting rationale to the student and instructor:\n- No Academic Dishonesty: The student did not engage in an act of academic dishonesty as determined by the instructor; or\n- Academic Dishonesty: The student did engage in an act of academic misconduct as determined by the instructor and choose to:\n  - Uphold the assigned academic penalty;\n  - Deny the assigned academic penalty; or\n  - Modify the assigned penalty.\n\n**Information**  \nTo assist with the appeal review, the department chair may request a written statement from the instructor for review with the student’s appeal.\n\nThe department chair may request additional information, and as appropriate will meet with the student, the instructor, and other individuals with relevant information.\n\n**Academic Integrity Singe Violation Appeal Finding**  \nThe department chair will complete the online Academic Integrity Single Violation Appeal Finding.\n\nA copy of the Academic Integrity Single Violation Appeal Finding will be sent to the student’s University-assigned email address.\n\nStudents are responsible for regularly checking their University-assigned email.\n\n**Final Determination**  \nThe decision of the department chair is final.",
    "### Faculty Committee Review and Recommendation of Appeal to Department Chair\n\nDepartments may have a standing appeal committee comprised of faculty charged with reviewing academic misconduct appeals.\n\nIf no such committee exists, the department chair will appoint an ad hoc academic dishonesty appeal committee of three departmental faculty members: one selected by the student, one selected by the instructor, and one selected by the chair with agreement of the other two committee members.\n\nShould departmental faculty members be unavailable, committee members may include any person holding a faculty appointment in the college with similar subject matter expertise.\n\nThe chair’s appointee will chair the appeal committee.\n\nWithin fifteen (15) days of the receipt of the review assignment from the chair, the committee must complete the following actions.\n\n**Committee Decision**  \nThe committee will issue, based upon the preponderance of the evidence, one of the following recommendations with its rationale to the department chair:\n- The student did not engage in an act of academic misconduct as determined by the instructor; or\n- The student did engage in an act of academic misconduct as determined by the instructor, and made the determination to:\n  - Uphold the assigned academic penalty;\n  - Deny the assigned academic penalty; or\n  - Recommend a modified penalty.\n\n**Additional Information**  \nTo assist with the appeal review, the committee may request a written statement from the instructor for review along with the student’s appeal.\n\nThe committee may request additional information, and as appropriate, will meet with the student, the instructor, and other individuals with relevant information.\n\n**Chair Filing**  \nAs soon as possible, but no more than five (5) days after the receipt of the committee’s recommendations, the department chair will report the findings and rationale on the appeal by completing the online Academic Integrity Single Violation Appeal Finding.\n\nA copy of the Academic Integrity Single Violation Appeal Finding will be sent to the student’s University-assigned email address.\n\nStudents are responsible for checking their University-assigned email on a regular basis.\n\n**Final Determination**  \nThe decision of the department chair is final.",
    "### Procedures for Multiple or Major Violations of Academic Misconduct\n\n**AIO Assessment**  \nThe Office of Academic Integrity will assess reported violations of the Student Academic Integrity Policy for multiple or major offenses.\n\nThe AIO will review the associated evidence as soon as possible upon:\n- identification of multiple or major violations in the Academic Integrity Database, or\n- receipt of a recommendation for probation, suspension, expulsion, or revocation of degree.\n\n**Notification**  \nIf the AIO finds that the student’s academic misconduct rose to a level of a major violation, or that the student has multiple single violations the AIO will send written notice to the student’s University assigned e-mail address.\n\nThe notification will include:\n- a list of the academic misconduct violation(s), including the date(s) and reporting department(s), and corresponding section(s) of the Student Academic Integrity Policy that have been violated; and\n- the date by which the student must schedule a conference with the AIO to discuss the misconduct(s).\n\nDates may be extended at the discretion of AIO in the interest of fairness.\n\n**Conference with the AIO**  \nThe purpose of this conference is to determine if the student’s major misconduct or multiple single violations are sufficient to support a penalty of probation, suspension, expulsion, or revocation of degree.\n\n**Review of Misconduct**  \nThe AIO will hold a conference with the student to review the misconduct and provide an opportunity for the student to respond directly to the violation(s).\n\nThe AIO may invite others who can provide further information regarding the misconduct(s), such as DOS, instructor(s), or witnesses to a particular incident to participate in the conference at their discretion.\n\n**Information**  \nThe student may present relevant information regarding the misconduct, including witness statements, documents, or other information.\n\n**Accompanying the Student**  \nA student may be accompanied by an advisor, but the student must represent themselves at the conference.\n\nIf a student intends to be accompanied by an attorney, the student must notify the Office for Academic Integrity no later than two (2) days prior to the conference so that the University can have a representative from the Office of General Counsel present at the conference.",
    "### Notice of Findings and Sanctions\n\n**Previously Assigned Sanctions Sufficient**  \nIf, after review of the evidence and conferring with the student and other relevant involved parties, the AIO determines that the student’s previously assigned sanctions were sufficient, the AIO will notify the student in writing, informing them of the finding and no further sanctions will be assigned.\n\n**Previously Assigned Sanctions Insufficient**  \nIf the AIO determines that a student’s sanctions were not sufficient to address the multiple or major violations, the AIO will assign further sanctions to the student.\n\nThe AIO may assign any sanction listed in this policy.\n\nThe student will be notified of the modified sanctions in writing.\n\n**Notification**  \nThe written notification of findings and sanctions will be sent to the student’s University-assigned email address.\n\nStudents are responsible for regularly checking their University-assigned email.",
    "### Student Appeal of AIO’s Actions to the Office of the Provost\n\n**Written Appeal**  \nIf a student disagrees with the AIO’s determination of findings and sanctions, the student may submit a written appeal to the Office of the Provost within five (5) days of the date on the AIO’s written decision.\n\n**Online Form**  \nThe student’s appeal must be in writing using the online Appeal of Academic Integrity Multiple Violations form.\n\nThe appeal must detail:\n- whether the student is appealing the AIO’s finding that the violation is a major violation or that the student engaged in multiple violations, the assigned sanctions, or both;\n- the specific basis for the appeal; and\n- any factual information in support of the student’s case, including specific evidence.\n\n**Receipt of the Appeal**  \nUpon receipt of the student’s written appeal, the Office of the Provost must determine whether review of the appeal will be conducted by the provost, or designee, or a provost appeal committee.\n\nThe decision of the provost on this issue is final.",
    "### Provost or Designee Review and Resolution of Appeal\n\n**Decision**  \nWithin ten (10) days of receiving the student’s written appeal the provost or designee must complete the following actions.\n\nThe provost or designee will issue, based upon the preponderance of the evidence, one of the following findings and rationale:\n- The findings of the AIO are not upheld; or\n- The findings of the AIO are upheld, and a determination is made to:\n  - Uphold the assigned sanctions;\n  - Deny the assigned sanctions; or\n  - Modify the assigned sanctions.\n\n**Additional Information**  \nTo assist with the appeal review, the provost or designee may request a written statement from the AIO for review along with the student’s appeal.\n\nThe provost or designee may request additional information and, as appropriate, will meet with the student, the instructor(s), and other individuals with relevant information.\n\n**Notification**  \nThe Provost’s, or designee’s, written finding will be sent to the student’s University-assigned email address.\n\nStudents are responsible for regularly checking their University-assigned email.\n\n**Final Determination**  \nThe decision of the provost or designee is final.",
    "### Provost Appeal Committee Review and Recommendation of Appeal to Provost\n\nThe Provost or designee will appoint an ad hoc provost appeal committee comprised of three faculty members to act on the student’s appeal of the AIO decision.\n\nWithin fifteen (15) days of receiving the review from the provost, the appeal committed must complete the following actions.\n\n**Recommendations**  \nThe appeal committee will issue, based upon the preponderance of the evidence gathered, one of the following recommendations with its rationale to the provost or designee:\n- The findings of the AIO are not upheld; or\n- The findings of the AIO are upheld, and:\n  - Uphold the assigned sanctions;\n  - Deny the assigned sanctions; or\n  - Recommend modified sanctions.\n\n**Extension Additional Information**  \nTo assist with the appeal review, the committee may request a written statement from the AIO for review with the student’s appeal.\n\nThe committee may request additional information and where appropriate will meet with the student, the instructor(s), and other individuals with relevant information.\n\n**Notification**  \nAs soon as possible, but no more than five (5) days after the receipt of the appeal committee’s recommendations, the provost or designee will report the findings and rationale on the appeal.\n\nA copy of the provost’s or designee’s determination will be sent to the student’s University-assigned email address.\n\nStudents are responsible for regularly checking their University-assigned email.\n\n**Final Determination**  \nThe decision of the provost or designee is final.",
    "### APRIL 5, 2024\n\n- Prologue 2\n- Executive Summary 4\n- Recommended Policies 5\n- Principles for Guiding GenAI Policy Recommendations 5\n- Training and Support 7\n- Privacy and Security 8\n- Guidelines for Dealing with Inappropriate Use of GenAI 9\n- Administrative Recommendations 9\n- Background 11\n- GenAI Introduction 11\n- The Nature of GenAI Models 11\n- Ethics Concerns Surrounding GenAI Tools 14\n- GenAI in Research 15\n- GenAI Usage Survey 15\n- Benefits and Concerns Relating to GenAI in Research 15\n- Existing Guidelines 16\n- GenAI in Creating Artifacts: Writing, Arts, and Design 18\n- Best Practices for Incorporating GenAI in Creation of Artifacts 18\n- Concerns about Adopting GenAI in Creative Processes 19\n- Use Cases 20\n- GenAI in Teaching, Learning, and Student Professional Development 22\n- Course Policies 22\n- Faculty and Student Responsibilities 23\n- Administration Responsibilities 24\n- Appendices 26\n  - Appendix A: Transformers and the GPT Models 26\n  - Appendix B: Survey of Deans on GenAI in Research 26\n  - Appendix C: A Study of GenAI Text Detectors 28\n  - Appendix D: The GenAI Policy and Advice Landscape Project 31\n  - Appendix E: Examples of GenAI Use and for Research and Education 31\n- References 33\n\nThe Task Force on AI Use in Education and Research (AI Task Force) of Boston University (BU) was charged by the Provost with several pivotal responsibilities surrounding new and emerging Generative AI (GenAI) tools.\n\nThe task force was asked to assess the impact of GenAI tools on education and research at BU, and review existing policies adopted by BU schools, colleges, and other academic institutions.\n\nThe culmination of the task force’s work is a report detailing its findings, proposing new policies, and identifying resources and procedures that may help to implement such policies.\n\nThe AI task force met throughout the Fall 2023 semester and in the early part of the Spring 2024 semester.\n\nIt consulted with diverse experts, including BU faculty members, industry representatives, and external consultants specializing in AI and higher education.\n\nThe task force organized itself into subcommittees mapped to sections of this report.\n\nThe scope of the report is limited to GenAI tools and does not address the many other applications of AI in practically every field of research and business practice.\n\nThe purpose of the report is to inform and educate faculty and to advise the BU administration on issues related to the use of GenAI tools in education and research.\n\nIn addition to the report, the AI task force developed two further resources:\n- **Detector Reliability**: A preliminary analysis of the reliability (accuracy and consistency) of GenAI detectors (see Appendix C) was conducted by the teaching assistants of the Spring 2024 edition of DS380 “Data, Society, and Ethics,” offered by the Faculty of Computing and Data Sciences: Carmen Pelayo Fernandez and Jasmine Pham.\n\nThis analysis is a useful practical guide for faculty as they consider how to detect GenAI material in student submissions.\n\n- **Generative AI Policy and Advice Landscape App**: The Generative AI Policy and Advice Landscape project (see Appendix D) was conducted by the students of the Fall 2023 edition of DS380 “Data, Society, and Ethics,” offered by the Faculty of Computing & Data Sciences, led by teaching assistants Sai Krishna, Sashank Madipally, and Showndarya Madhavan, and implemented in a web app by undergraduate students Heather Davies and Jason Huang.\n\nThe app offers a searchable repository of GenAI policy and advice documents from institutions of higher education, along with content and network analyses.\n\nThe database and analyses may be updated from time to time.\n\nThe Spring 2024 update was performed by a SPARK!\n\nstudent group consisting of Haya Almajali, Jiasong Huang, Maxwell Malamut, and Samantha Rigor, with guidance from Seth Villegas and Michelle Voong.\n\nBU AI task force members were assisted by Deidre Fisher, project manager for strategic initiatives in the Office of the Provost.",
    "BU AI task force members were assisted by Deidre Fisher, project manager for strategic initiatives in the Office of the Provost.\n\nTask force members included:\n- Yannis Paschalidis, Distinguished Professor of Engineering, College of Engineering; Founding Professor of Computing & Data Sciences; Director, Rafik B. Hariri Institute for Computing and Computational Science & Engineering (Co-Chair)\n- Wesley J. Wildman, Professor of Philosophy, Theology and Ethics, School of Theology; Founding Professor of Computing & Data Sciences and Chair of Faculty Affairs, Faculty of Computing & Data Sciences (Co-Chair)\n- Mary Churchill, Associate Dean for Strategic Initiatives & Community Engagement; Professor of the Practice and Program Director, Higher Education Administration, Wheelock College of Education & Human Development\n- Mark Crovella, Professor of Computer Science, College of Arts & Sciences; Founding Professor of Computing & Data Sciences and Chair of Faculty Affairs, Faculty of Computing & Data Sciences\n- Anne Danehy, Associate Dean of Academic Affairs; Master Lecturer of Mass Communication, Advertising & Public Relations, College of Communication\n- Pary Fassihi, Senior Lecturer, Arts & Sciences Writing Program, College of Arts & Sciences\n- Juliet Floyd, Borden Parker Bowne Professor of Philosophy, College of Arts & Sciences; Director, BU Center for the Humanities\n- Priya Garg, Associate Dean, Office of Medical Education, Chobanian & Avedisian School of Medicine\n- Amie Grills, Associate Provost for Undergraduate Affairs; Professor of Counseling Psychology & Applied Human Development, Wheelock College of Education & Human Development\n- Tal Gross, Associate Professor of Markets, Public Policy & Law, Questrom School of Business\n- Wendy Heiger-Bernays, Clinical Professor of Environmental Health, School of Public Health\n- Keith Hylton, William Fairfield Warren Distinguished Professor; Professor of Law, School of Law\n- Daniel Kleinman, Associate Provost for Graduate Affairs; Professor of Sociology, College of Arts & Sciences\n- Gail McCausland, Assistant Dean of Academic Affairs; Clinical Professor of Periodontology, Henry M. Goldman School of Dental Medicine\n- Matt Parfitt, Associate Professor of Rhetoric, College of General Studies\n- Wilson Wong, Associate Professor of Biomedical Engineering, College of Engineering\n\nBoston University’s (BU) AI task force was established in Fall 2023 to analyze the impact of Generative AI (GenAI) technologies on education and research.\n\nThe task force consulted widely with experts and stakeholders.\n\nThis report surveys the landscape of GenAI technologies, assesses their potential to impact research and education, identifies limitations and concerns, offers guidelines on the best use of GenAI, and proposes policies and administrative support structures.\n\nThe task force’s key policy recommendations are as follows:\n- **Critical Embrace**: BU should not universally prohibit or restrict the use of GenAI tools.\n\nRather, BU should critically embrace the use of GenAI, support AI literacy among faculty and students, supply resources needed to maximize GenAI benefits in research and education, and exercise leadership in helping faculty and students craft adaptive responses.\n\n- **Pedagogical Clarity**: GenAI policies adopted by any college, school, or departmental unit should be consistent with the University’s policies and reflect the critical embrace of GenAI technology.\n\nConsistent with academic freedom, individual instructors should be free to define GenAI policies suited to the learning goals of their courses, and every syllabus needs to state the instructor's GenAI policy.\n\nConsistent with citation practices, instructors and students should acknowledge use of GenAI tools.\n\n- **Academic Misconduct**: BU should advise instructors to exercise caution when using GenAI detectors as evidence of GenAI use.\n\nGenAI detector output should be regarded as only one part of a wider evidence base in evaluating possible academic misconduct.\n\nIf used, GenAI detectors should be applied equally and fairly, and faculty should be aware of selection bias when applying GenAI detectors to specific suspected cases.\n\nAdvance notice should be given in syllabi, including naming the specific detectors employed, so that students have an opportunity to use them also.\n\nInstructors need to be informed and supported in using reliable and consistent detectors.\n\n- **Security and Privacy**: BU should adopt policies to prevent the inadvertent publicizing of sensitive or valuable information through uploading it to GenAI tools.\n\n- **Centralized Decisions**: BU should centralize decisions on GenAI tool acquisition and licensing, on resourcing and personnel for supporting GenAI literacy and pedagogical reflection, and on administrative structures to ensure ongoing adaptive response to rapidly developing GenAI technology.",
    "Further recommendations (Section 1) are organized in the following broad categories: (a) training and support for faculty and students in leveraging GenAI technology, (b) mitigating privacy and security concerns associated with the use of GenAI, and (c) administrative structures for streamlining BU processes and organizing resources associated with GenAI.\n\nThe report also offers basic background information (Section 2), as well as analysis of GenAI use in research (Section 3), GenAI use in creating artifacts for writing, arts, and design (Section 4), and teaching, learning, and student professional development (Section 5).\n\nAlongside policy recommendations, the report suggests best-practice guidelines for the appropriate use of GenAI tools, and for dealing with inappropriate use.\n\nIn addition to the report, the task force developed a set of resources to explore the landscape of academic GenAI policies, available at the BU AI task force website.\n\nGenerative Artificial Intelligence (GenAI) tools are evolving rapidly and so are policies and guidelines regulating their use.\n\nAcademic institutions are starting to adopt policies governing GenAI use and common policy themes have emerged (see Appendix D and BU AI task force website for a searchable repository and analysis of GenAI policies released by academic institutions).\n\nThe AI task force’s recommendations for (enforceable) policies at the University level, along with suggested guidelines for wise and productive use of GenAI tools, generally align with this emerging consensus.\n\nThe AI task force identified four important principles that should govern the use of GenAI: leadership, ownership, transparency, and excellence.\n\nThese principles have far-reaching implications, which will be considered in the sections of this document: research (Section 3), creating artifacts (Section 4), teaching, learning, and student professional development (Section 5).\n\n- **Leadership**: BU should recognize that GenAI tools are revolutionizing the economic landscape.\n\nThe University should lead the way in helping students and faculty adapt.\n\nFaculty and students should be supported to become literate about GenAI, competent in using GenAI tools, critically aware of the shifting balance of advantages and disadvantages of GenAI tools, and flexible in thinking about pedagogical and research aims and methods.\n\nBU should support ethical awareness in faculty, student, and institutional appraisal of GenAI tools, incorporating ethical dimensions in every educational and strategic resource.\n\n- **Ownership**: Faculty and students should assume full ownership and ultimate responsibility for their work, including when that work leverages GenAI.\n\nFaculty should emphasize the importance of human oversight over any GenAI created content and ensure students are not heavily or solely relying on GenAI for their work.\n\nGenAI tools may produce false information; the responsibility rests with the user to authenticate content using other verified sources.\n\nFaculty decisions about GenAI usage by students should be evidence-based, and caution should be used with GenAI detectors in any decision process.\n\nAuthorship of a document implies that the author is responsible for the work.\n\nSince a software tool cannot assume such responsibility, and consistent with policies put in place by academic publishers [1], [2], [3], GenAI tools should not be listed as co-authors of documents and creative works.\n\n- **Transparency**: If AI tools are used in research, class assignments, writing, design, or other forms of creative work, it is important to be transparent on how exactly the tool has been used.\n\nScholars should be mindful that documentation and transparency requirements may not be the same in all fields and journals, and that current requirements may change with the ever-evolving landscape.\n\nFaculty should be fully aware of and follow guidelines set forth by federal agencies and regulators (e.g., National Science Foundation guidelines [4]).\n\nResearchers should be aware of the individual AI practices and policies of those they collaborate with, as well as vendors and subcontractors who are involved in the research process.\n\nStudents should be transparent about their use of GenAI for course assignments and projects and be instructed to document how GenAI tools were used.\n\nInstructors need to be transparent about course policies and address GenAI in every syllabus, providing clear guidance to students on how GenAI may or may not be used.\n\nThey also need to provide advance notice of detection strategies and enforcement of GenAI rules.\n\nFor example, the CAS Writing Program, as part of their faculty AI literacy efforts, offers specific language on their syllabus building tool for faculty to incorporate in course syllabi.\n\nThe BU Center for Teaching & Learning (CTL) also offers guidance on referencing GenAI on course syllabi on their Communicate with Students about Generative AI resource.",
    "The BU Center for Teaching & Learning (CTL) also offers guidance on referencing GenAI on course syllabi on their Communicate with Students about Generative AI resource.\n\nTransparency requires that authors need to cite the specific GenAI tool used in any document or product.\n\nStudents should be instructed and reminded to credit both human and GenAI contributors, and that every source used must be cited.\n\nFaculty should familiarize themselves with the different approaches for citing GenAI tools and guide students on how to credit GenAI.\n\nBU’s CTL offers resources for “Citing with AI” on their Quick References for Teaching with AI resource page.\n\n- **Excellence**: BU’s commitment to excellence in research and teaching should be reasserted in connection with GenAI tools.\n\nEvery application of such tools, regardless of context, should deepen the quality of teaching, learning, and research.\n\nFaculty and students should be trained in creative engineering of prompts to extract the most profound outputs possible.\n\nGenAI outputs should be critically evaluated to improve the quality of research and teaching.\n\nGenAI use should be evaluated for conformity with University expectations about ethical and responsible computing.\n\nIn addition to the general policy recommendations mentioned in the Executive Summary, which are guided by these principles, we group specific additional recommendations in the following categories: (1.2) training and support, (1.3) privacy and security, (1.4) guidelines for appropriate use and for dealing with inappropriate use of GenAI tools, and (1.5) administrative recommendations.\n\nWhen appropriate, we discuss the need for resources to implement the proposed recommendations.\n\nGenAI tools will impact every field of study and most professions.\n\nIt is vital that we equip students and faculty to operate effectively in the emerging GenAI era.\n\nIt is likely that GenAI tools will accentuate disparities between those with the knowledge and resources to benefit from their use and the rest of the global population.\n\nBU, by properly educating its students, can help graduates maintain competitive advantage in their future careers, while remaining acutely aware of equity challenges.\n\nSimilarly, faculty can benefit by leveraging GenAI tools and BU should help educate them.\n\nRecommendations follow.\n\n- Incentivize the creation of new undergraduate and graduate courses or course modules.\n\nEveryone across the many disciplines represented at the University should have a basic understanding of GenAI technology so that they can better appreciate its benefits and limitations.\n\n- University units should add courses that meet Hub requirements by focusing on GenAI tools and related applications.\n\n- Support the development of GenAI literacy and education in best practices among faculty.\n\nGenAI, and AI more broadly, is likely to impact research in many domains, yet some faculty may lack the background or specialized training to benefit from these developments.\n\nThe more BU does to educate its faculty and support their work, the more it will benefit from their increased productivity and impact in their fields.\n\nThis leads to several specific recommendations.\n\n- For new research projects and collaborations, University-wide centers, such as the Hariri Institute, should play an important role, enabling and supporting new research collaborations through existing mechanisms (e.g., the Hariri Focused Research Programs).\n\n- Similar to facilitators for research computing and free allotments of computing resources at the Massachusetts Green High Performance Computing Center (MGHPCC) for all faculty, we recommend that BU consider funding positions of AI research and education consultants and facilitators.\n\nSuch consultants could be made available to faculty to facilitate the use of AI and GenAI in their research.\n\nIf the consultation service is centralized, the Hariri Software and Application Innovation Lab (SAIL) would be an appropriate place to house AI facilitators.\n\n- We recommend soliciting feedback from faculty about how GenAI is and is not assisting faculty efforts to provide excellent education, thereby helping all to learn about the limits of GenAI.\n\n- AI facilitators should be tasked to offer workshops to faculty and students on leveraging GenAI as research tools, introducing concepts such as prompt engineering, presenting case studies from faculty who leveraged GenAI in their research, and providing information about computing resources and software and how to access them.\n\n- Instructors should be provided with support to help them incorporate GenAI into their courses.\n\n- This leads to several specific recommendations.\n\n- CTL and the Office of Digital Learning & Innovation (DL&I) should be BU’s main vehicles for providing teaching and course support.\n\nTheir GenAI offerings should be vetted by the body of GenAI ambassadors described below.",
    "Their GenAI offerings should be vetted by the body of GenAI ambassadors described below.\n\n- CTL should continue to offer workshops on how to incorporate GenAI into classroom instruction for those who are interested.\n\n- CTL should continue to provide guidelines and resources in its AI in the Classroom and Teaching and Learning in an AI World web sites.\n\n- DL&I and CTL should produce self-paced online modules to educate faculty.\n\n- DL&I and CTL should establish GenAI faculty ambassadors across campus to act as school/college experts and to provide local advice.\n\nThese can be drawn from the ranks of the faculty in each corresponding unit.\n\nThis body of local ambassadors should exchange notes and identify best practices.\n\n- GenAI faculty ambassadors should act as a faculty body for CTL and DL&I in evaluating and approving their online offerings, similar to how a regular course is evaluated and approved by a department’s and a college’s faculty.\n\nCTL and DL&I should establish mechanisms to collect user feedback on their offerings and use it for continuous improvement.\n\nBU should consider establishing a seed-funding program to provide modest and time-limited grants to faculty who have innovative ideas on using or analyzing GenAI in their teaching, particularly if they are willing to make their innovations more broadly available to other faculty who want to test such ideas.\n\nOne potential mechanism is to leverage the AI and Education Initiative at the Hariri Institute which has introduced such a seed-funding program in collaboration with DL&I.\n\nThere exist serious privacy and security concerns associated with using GenAI tools running on servers that reside in corporate data centers.\n\nFor this reason, many scientific journals have introduced policies against the uploading of papers under review, and funding agencies, such as the National Science Foundation (NSF), have banned the uploading of proposals under review.\n\nThese concerns apply to GenAI tools ChatGPT, Claude, Gemini (formerly Bard), and Llama, among others, when one uses the company’s website and resources to run the model.\n\nThese companies can use the provided data from users to train the next generation of the models.\n\nThus, it becomes possible for information to be leaked through future model generations.\n\nIn contrast, some models (e.g., Llama, Mistral) can be downloaded and run locally.\n\nIn this case, all interactions with the model and content provided to the model are stored locally and the user maintains ownership and control.\n\nWe recommend:\n- BU should adopt policies against uploading documents such as personnel files, budget spreadsheets with identifying information, exams, homework papers and solutions, medical records, tenure letters, and any document that contains personal information that should not be made public through server-based GenAI tools.\n\n- University offices should implement policies to train all employees, including student employees, about this policy and their associated responsibilities.\n\n- Independent Review Board (IRB) outcome letters and Data Use Agreements (DUAs) should be modified to include a provision affirming non-disclosure to GenAI servers.\n\nThis policy can be reevaluated only when corporate assurances about not using prompts for training purposes are authoritatively audited.\n\nIn the past, plagiarism, unauthorized collaboration for assignments, and other violations of academic integrity have been relatively easy to detect (e.g., by comparing an assignment with the source).\n\nDetecting the illegitimate use of GenAI tools is challenging by comparison.\n\nGenAI detectors are not fully reliable and may provide different answers for the same query at different times (see Appendix C).\n\nThis is likely to remain an issue, particularly for cases where there is human-GenAI collaboration.\n\nThis leads to several recommendations.\n\n- Promote faculty literacy about GenAI detectors.\n\nFaculty and disciplinary committees should be informed of the unreliability and inconsistency of GenAI detectors and be advised that a likelihood score produced by GenAI detectors should not be considered sufficient evidence to accuse a student of unauthorized GenAI use.\n\nOn the other hand, taken together with other factors, it may be considered as a relevant piece of evidence to strengthen a case.\n\n- Promote faculty consistency in the use of GenAI detectors.\n\nIn view of the unreliability of GenAI detectors, faculty should adopt a uniform policy on the application of detection tools.\n\nIf a faculty member chooses to systematically employ a detection tool on submitted assignments, the specific tool should be applied to the assignments of all students.\n\nFaculty should be aware of selection bias that could be introduced by applying GenAI detectors to a specific targeted subset of students.\n\n- Promote faculty literacy about GenAI methods to hide plagiarism.",
    "- Promote faculty literacy about GenAI methods to hide plagiarism.\n\nGenAI tools may make it harder to detect plagiarism, as users can use GenAI to “paraphrase” from an original source.\n\nCTL offers guidance on AI and Academic Integrity and this guidance should be kept current with developments in the field.\n\nWe recommend that IS&T evaluates various options for tools to enhance plagiarism detection, including licensing tools that are successfully used by academic publishers (e.g., iThenticate [5], Grammarly [6]).\n\nThe task force heard from both faculty and students that the Blackboard solution (Turnitin) is ineffective at present.\n\nFinally, the AI task force offers the following administrative recommendations.\n\n- GenAI should be leveraged to improve, automate, and streamline a variety of BU processes.\n\nWhereas for business functions (e.g., Q&A for HR benefit options) commercial solutions are likely to emerge quickly, student advising may be a function where GenAI could help students conveniently discover information regarding courses, instructors, graduation requirements, and processes.\n\nWe recommend that the University explores the training of a BU native chatbot for these purposes, either by leveraging BU faculty expertise or partnering with an external provider.\n\nSuch a chatbot would help students quickly find answers to many administrative questions, reduce the burden on staff and faculty in large academic programs, and establish a productive collaboration between advising staff and chatbot assistants.\n\nThis is meant not to reduce faculty-student advising interactions, but rather to reduce the time devoted to procedural issues and increase the time for more meaningful exchanges.\n\n- BU should centralize decisions on recommending and licensing GenAI tools for faculty and students.\n\nSome existing tools (e.g., OpenAI’s ChatGPT) offer a subscription service for the most advanced version of the tool.\n\nA paid service can lead to a “digital divide” that intensifies inequities between students and groups with the resources to pay for subscriptions and those who lack such resources.\n\nGenAI tools are likely to become ubiquitous and necessary for work, so BU should implement a policy similar to what it is now using for commonly used software (e.g., office productivity, operating systems, Matlab, statistical, and other scientific and design software).\n\n- BU should create a centralized resource of GenAI information.\n\nBU should consider creating a centralized website to communicate the University’s policies and procedures regarding GenAI, and to provide links to various resources offered by BU offices, such as IS&T, CTL, DL&I, and any new resources that may be created as a result of these recommendations (e.g., AI research facilitators, AI ambassadors, the web-based app containing information on policies adopted elsewhere, etc.).\n\nThis website needs a manager to coordinate the various offices and resources and to ensure that content gets updated to reflect rapid developments in the GenAI landscape.\n\nAn initial web site was set up by the BU AI task force.\n\nGenerative Artificial Intelligence (GenAI) generally refers to foundational AI models that have been trained on vast amounts of data and can generate new content.\n\nAppendix A offers technical background on the evolution of the Natural Language Processing (NLP) field that gave rise to the recent Large Language Models (LLMs), such as OpenAI’s popular ChatGPT [7].\n\nThe latest GPT version (as of the writing of this report), GPT-4, has been trained on vast amounts of text (from the internet, books, news articles, scientific literature, etc.\n\n), yielding models with over 1.7 trillion parameters.\n\nBeyond text generation, models (such as OpenAI’s DALL-E [8]) can generate images based on a text description.\n\nThe GenAI landscape is rapidly changing.\n\nThere are hundreds of GenAI tools and more appear every week.\n\nPopular GenAI tools designed to generate text include: Microsoft’s Copilot (formerly BingChat) [9] (based on OpenAI’s GPT-4), Google’s Gemini (formerly Bard) [10], [11], Meta’s Llama [12], Mistral [13], and Anthropic’s Claude [14].\n\nText-based tools can be prompted to write many forms of text (including creative writing), edit text, write computer code in many programming languages, offer counseling services, function as tutors, automate call centers, and create interactive robotic companions.\n\nThere are countless other GenAI tools for text creation, many of them with no content moderation, which the better-known offerings do have.\n\nHundreds of text generators for specialized applications are now being offered by OpenAI, based on GPT-4, and many GenAI tools are being created based on the foundational GenAI models mentioned earlier.",
    "GenAI tools can also produce other forms of output, including images and visual art (e.g., at the time of this report, OpenAI’s DALL-E [8], Midjourney [15], Stable Diffusion [16], Adobe Firefly [17]), music (e.g., Soundraw [18], Bandlab [19]), video (e.g., Pictory [20], Synthesys [21], Sora",
    "# On the Process than on the End Product\n\nOne way to focus students more on the process than on the end product involves requiring students to submit iterative drafts and reflections on the evolution of their work in stages.\n\nAnother way involves requiring students to orally present and defend their work in class, in small groups, or in meetings with instructors whenever class size makes such an approach feasible.",
    "## Institutional Diversity\n\nBU is a large, diverse institution, and no single, “one-size-fits-all” GenAI course policy can comprehend the great diversity of learning outcomes, teaching methods, disciplinary cultures, and assessment practices across BU’s schools and colleges.\n\nAny single policy meant to apply to all courses across the University would have to be so vague that it would be of little practical use.\n\nThus, we recommend against a monolithic, University-wide policy on GenAI use in the classroom.",
    "## Explicit Guidance for Students\n\nGenAI tools can be powerful learning adjuncts, so faculty are encouraged to explore how they could be leveraged rather than banned.\n\nHowever, GenAI tools can also be used as shortcuts that inhibit or distort learning.\n\nThe precise risks and opportunities differ for each course, and students need clear, explicit guidance to understand how GenAI tools should and should not be used in each context.\n\nIdeally, faculty will clearly indicate if, when, and how GenAI tools can be used on class assignments.\n\nSample statements can be found on the BU Center for Teaching and Learning (CTL) website.",
    "## Faculty Adaptation\n\nFaculty need to recognize that many students are now entering the University as experienced users of GenAI tools, already in the habit of using GenAI as a learning tool, an assistant, and a collaborator.\n\nJust as the advent of the Internet in the 1990s and the smartphone in the 2000s forced faculty to reimagine their courses, change their teaching practices, and articulate clear policies for using the new technologies, so now faculty need to be supported to craft adaptive responses to the era of GenAI tools.",
    "## Faculty Need to Continually Adapt\n\nFaculty need to recognize that students today must be prepared for GenAI tool use in their future careers and become AI literate.\n\nBecause AI will impact every field and discipline, students in every school and college need to understand the basics of how GenAI works, where its strengths and weaknesses lie, and how to engineer effective prompts.\n\nThis is part of what is meant by the term AI literacy, but as the technology changes, as new tools become available, and as these tools become more deeply integrated into the workplace, the meaning of AI literacy will also change.",
    "## Faculty Can Make Use of GenAI as Student Coaches\n\nWe need to inform faculty that GenAI can provide students with a deeper understanding of course content by giving them formative feedback or clearer examples.\n\nFor example, BU students report that they are already using ChatGPT to provide alternative definitions or explanations for key concepts.\n\nSome students also report using ChatGPT to create practice questions to help them prepare for exams.\n\nFaculty need to know that students are using GenAI in this way, and they should consider how they can capitalize on this to further students’ mastery of course content.",
    "## Faculty Need to Understand Limitations of GenAI Detection\n\nAll faculty need to understand the limitations of GenAI-detection tools, which should be used with caution.\n\nWith the risk of false positives in mind, AI-detection tools could be used as only one element in a larger body of evidence for assessing the originality of student work.\n\nIn addition, faculty who choose to use detection tools should apply the tools consistently to all students in their classes, not to a subset chosen on the basis of gut-level suspicion or imperfect signals.\n\nFaculty should consider this in the information they provide to students about class assignments, and they should be transparent about their adopted GenAI detection and rule enforcement strategies.",
    "## Faculty, Not AI, Must Assign Grades\n\nGenAI tools should not determine grading, supply feedback, or stipulate learning outcomes in courses.\n\nSuch decisions – whether it is the grade breakdown on a syllabus or a specific grade for a particular student’s work – should be made by faculty, not outsourced to GenAI tools.\n\nGenAI is not yet reliable or subtle enough to manage such critical tasks without a “human in the loop.”",
    "## Faculty Should Not Assume that Students are Experts\n\nFaculty should not assume that students understand GenAI tools.\n\nStudents may have used GenAI tools, perhaps extensively.\n\nBut faculty should not assume that all students understand how GenAI works or what it can and cannot do.\n\nIn conversations with several student panels, the varying interest and expertise among our current students was clear.",
    "## Faculty Need to Teach Students about Limitations and Nuances\n\nTo use GenAI tools ethically and productively, students need to understand that their outputs are probabilistic predictions (informed guesses); that training datasets reflect a number of known biases; and that even when outputs incorporate web searching, GenAI tools can still hallucinate while appearing confident and authoritative.\n\nAt this point in their development, GenAI tools tend to generate writing that qualitatively tends toward the mean and is primarily declarative rather than critical or analogical.\n\nBoth faculty and students need to take responsibility for their own work.\n\nAll GenAI output needs to be revised, edited, and validated.",
    "# Preparing for Changing Careers\n\nBoth faculty and students need to be aware that GenAI will continue to transform the way that professionals in most industries work.\n\nBU must prepare students for this new method of working.\n\nSoon, GenAI will be incorporated in nearly every software product.\n\nAI will affect every discipline and it is becoming an important skill sought by employers.\n\nThe BU Center for Career Development offers a set of resources to students as they look to start their professional careers, and it would be helpful to consider adding workshops related to AI.\n\nThe Hariri Institute is running a student-led Data Science Mentoring Circles Program, offering BU alumni now in industry as mentors to graduate students seeking careers in data science.",
    "# Faculty Must Not Ignore GenAI Tools\n\nFinally, faculty are advised to not avoid discussing GenAI.\n\nGenAI is now a presence in our lives and our students’ lives, one that is changing the way we research, write, and learn.\n\nWe believe that ignoring it and refusing to address its implications is no longer an option.\n\nFaculty need university support to become prepared for discussions with students about GenAI, its role in student learning, its possible uses (ethical and unethical), and its limitations.",
    "## Professional Development\n\nBU needs to support ongoing professional development, including providing grants and resources to allow faculty to experiment and innovate with GenAI.\n\nBoth the risks and the potential benefits of GenAI tools for research, teaching, and student learning are considerable, but largely unexplored and untested.\n\nFaculty may need to reimagine and revise many of their existing teaching methods, course designs, syllabi, assignments, and classroom activities to capitalize on this technology and stay abreast of new developments.\n\nInvestments may also be needed to provide workshops, trainings, online resources, and consultations for faculty engaging in this work.\n\nThis may require a considerable investment of time for which faculty and staff may deserve compensation, depending on the nature of the investment.\n\nIn addition, some funding should perhaps be made available for more structured studies, designed to develop and promulgate new approaches to teaching and learning.",
    "## Centralized Resources\n\nA continuously maintained set of GenAI resources for faculty and students is imperative.\n\nFaculty need access to sample syllabi, course policies, advisory statements, use examples, and instructions.\n\nCTL, DL&I, and other offices can provide those resources.\n\nOpportunities should be available for faculty to discuss GenAI resources with CTL and DL&I staff and other experts.",
    "## Managing Inequities\n\nTeaching with GenAI tools should not lead to inequities among students.\n\nWhile we encourage faculty to develop new ways of using GenAI as an effective learning tool, we are also aware that some tools have paid tiers that may be difficult for some students to afford.\n\nFinancial inequity should not imply unequal opportunity to learn or complete work efficiently.\n\nThis concern may possibly be alleviated as prices change, or completely resolved through University-negotiated site licenses.",
    "# The Integration of GenAI Tools\n\nThe integration of GenAI tools in education presents both challenges and opportunities.\n\nBy understanding the capabilities and limitations of GenAI tools, incorporating thoughtful policies into syllabi, and fostering an environment of continuous learning and adaptation, educational institutions can effectively harness the potential of GenAI tools.\n\nThis approach will not only enhance the learning experience but also prepare students and faculty for a future where GenAI tools are an integral part of the professional and academic landscape.",
    "# GenAI Models\n\nGenAI models are based on deep learning approaches using neural networks, which are formed by arranging stacked layers of computational “neurons.” These neural networks perform basic linear and nonlinear transformations on a vector input to produce some other scalar or vector output.\n\nAmong the first generative models were so-called Generative Adversarial Networks (GANs) that were designed to generate images to be presented to an image classifier in an effort to improve the classifier.\n\nLater applications focused on Natural Language Processing (NLP), giving rise to Large Language Models (LLMs).\n\nA breakthrough in the field is considered to be the introduction of the transformer architecture, which provided a way for identifying which words in a sentence the model should pay attention to provide the proper context while mapping a word into a numerical vector (called embedding).\n\nThis architecture was first used for Natural Language Processing (NLP) purposes in the Bidirectional Encoder Representations from Transformers (BERT) model, which is able to encode each word (or smaller pieces called tokens) in its input text, taking into account an entire passage of text.",
    "# GPT Architecture\n\nThe so-called Generative Pre-Trained Transformer (GPT) architecture also uses transformers but only uses past words in a sentence to predict the next word, and thus functions as a sentence completion algorithm that can be made recursive.\n\nSubsequent GPT models became larger (GPT-2, GPT-3) and were shown to exhibit emergent properties (“few-shot” learning with no domain adaptation).\n\nThe latest version, GPT-4, powers OpenAI’s ChatGPT and has been trained on vast amounts of data (on the order of 45 GB), consisting of a collection of models with over 1.7 trillion parameters.\n\nIn addition to the ability of GPTs to predict the next word and their sheer size, another important element of their success is training with human feedback.\n\nThis involves humans rating the output of the model and ranking several model outputs for appropriateness.\n\nIn both cases, the human ratings are used to further improve the model and moderate its output.",
    "# Image Generation Using Transformers\n\nA transformer model can also be adapted to generate images from text.\n\nAt a high level, the model is trained on a vast collection of images with accompanying text descriptions.\n\nThis kind of model powers OpenAI’s DALL-E image-generation model.\n\nMany of the same principles can be applied to other forms of input and output, including audio and video.",
    "# Associate Deans for Research Responding to the Survey\n\n- John Byers, Senior Associate Dean of the Faculty for Mathematical & Computational Sciences, College of Arts & Sciences\n- Arianne Chernock, Associate Dean of the Faculty for Social Sciences, College of Arts & Sciences\n- Leslie Dietiker, Associate Dean for Research, Wheelock College of Education & Human Development\n- Kate Flewelling, Director, Library Services, BU Medical Campus\n- Maria Kukuruzinska, Associate Dean for Research, Henry M. Goldman School of Dental Medicine\n- Michael D. McClean, Associate Dean for Research and Faculty Advancement, School of Public Health\n- Sean Mullin, Associate Dean of the Faculty for Natural Sciences, College of Arts & Sciences\n- Andrew Taylor (conveying responses from multiple colleagues), Associate Dean of Research, Chobanian & Avedisian School of Medicine",
    "## Question 1: What value, if any, do you believe AI can bring to scholarly research?\n\n- Research and Methods Planner\n- Literature reviews\n- Generates new hypothesis\n- Content Generator and thought partner (e.g.\n\nbrainstorm research ideas)\n- Data Collection and Analysis (e.g.\n\naccelerate analysis)\n- Summarizes larger amounts of research data in shorter time\n- Make coding easier and faster\n- Edit, proofread, and help with creating graphics, reviewing manuscripts, and writing grant applications\n- Improve peer review times",
    "## Question 2: Do you have any concerns with the use of AI in research at BU?\n\nWhat are they?\n\n- Can lead to more reliance on pattern recognition without understanding\n- Bias or discrimination in data\n- Equitable access\n- Security threats\n- Potential of proliferation of misinformation\n- May make plagiarism and falsification of data easier to do and harder to detect\n- Issues around intellectual property\n- Image manipulation hard to detect\n- Concerns that putting confidential/prepublication information onto an AI server could prematurely release information into the public domain",
    "## Question 3: What kinds of central support do you believe faculty and other researchers would benefit from?\n\n- Tutorials/demonstrations/guiding principles\n- A searchable list of software providing ChatGPT and AI programming providing information as being open or closed, accuracy, and other problems\n- Repository of generated programming involving vetted routine administrative activities\n- Providing a framework for AI that defines infrastructure along with concerns\n- Information sessions and accessibility to machine learning algorithms, perhaps training sessions on deep-learning and other subsets of machine learning\n- Help researchers keep up to date on the latest opportunities, potential pitfalls, and best practices",
    "## Question 4: Do you believe that BU should provide guidelines for the use of AI in research?\n\n- 6 Yes (perhaps in partnership with other AAU institutions, or the AAU)\n- Not guidelines but BU researchers should have ways to stay up to date on the latest opportunities, potential pitfalls, and best practices as the field evolves",
    "# AI-detection Tools\n\nA number of AI-detection tools have been released since GPT-2 became available in 2019.\n\nUnfortunately, none of them can reliably and consistently detect AI outputs.\n\nThe fundamental problem is that GenAI outputs are usually unique and carry no “watermark” or signature that would confirm that they were produced by a machine.\n\nAlthough some of the tools boast high accuracy rates, independent studies indicate that all these tools are fallible, especially if the output has been modified by humans.\n\nEven a small chance of a false positive – that is, a result that falsely claims the writing was produced by GenAI – means that students could be unjustly accused of academic misconduct.\n\nThis is not an acceptable risk.",
    "# Reliability of GenAI Detectors\n\nThe AI task force arranged for a systematic analysis of the reliability (accuracy and consistency) of GenAI detectors.\n\nThis analysis was produced in association with the spring 2024 edition of the CDS DS380 “Data, Society, and Ethics” course.\n\nThis analysis is a useful practical guide for professors as they consider how to detect GenAI material in student submissions.\n\nThere are two reliability issues at stake in the small landscape of GenAI detectors.\n\nOn the one hand, while text detectors are generally well over 90% accurate, the probabilistic nature of detector judgments allows for the possibility that they can be badly mistaken, in both directions, producing false negatives (they mistakenly find human-generated text) and false positives (they mistakenly find GenAI text).\n\nThe latter can be devastating for a student who did nothing improper.\n\nOn the other hand, students may want to run a GenAI detector on an assignment to see what their grader will see, thereby protecting themselves from disciplinary charges based on a false positive detector reading.\n\nThis calls for reliability in the sense of consistency, but GenAI text have been reported as supplying quite different probability estimates on the very same document at different times.",
    "# GenAI Policies\n\nSome GenAI policies, such as the Generative AI Assistance Policy from the Faculty of Computing & Data Sciences, encourage students to employ GenAI detectors to protect themselves against false accusations that might lead to disciplinary action.\n\nBut such recommendations are only fully feasible if GenAI detectors give consistent readings.\n\nThe experiment involved seven GenAI text detectors, seven varieties of texts with various combinations of human-creation and GenAI creation to evaluate accuracy, and three time points one day and one week apart to evaluate consistency.\n\nThe seven texts used in this experiment had the following origins and characteristics:\n- AI-made with ChatGPT 4;\n- AI-made with Gemini (formerly Bard);\n- Made by a native English-speaking human;\n- Made by a non-native English-speaking human;\n- AI-made (ChatGPT 4) with intentional misspellings;\n- AI-made (Gemini) with intentional grammar errors;\n- AI-made (ChatGPT 4) with human-made sentences interposed.",
    "# AI Detectors Used\n\nThe seven AI Detectors were those that did not require paying a fee and delivered either a numeric probability estimate or a boolean guess (human or AI):\n- Copyleaks – detects GPT-2, GPT-3, ChatGPT, GPT-4, T5, Bert, and Jasper.\n\n- Originality – detects GPT-3, 3.5, and ChatGPT.\n\n- Crossplag.\n\n- Sapling – detects ChatGPT and Gemini; developed by former researchers at University of California Berkeley, Stanford University, Google, and Meta; free version allows up to 2,000 characters.\n\n- Winston AI – detects GPT-3, ChatGPT, and GPT-4; free version allows up to 2,000 words.\n\n- Content At Scale – detects GPT-2, GPT-3, 3.5, ChatGPT, and GPT-4.\n\n- GPTZero – to be carefully distinguished from ZeroGPT, which seems designed to benefit from GPTZero’s fame, and which we found to be unreliable.",
    "# Experimental Design\n\nThe experimental design involved the following steps:\n- Apply each AI detector to the test text and record detection results.\n\n- Repeat the testing procedure three times for each detector (Day 1: Feb 1, 2024, Day 2: Feb 2, 2024, Day 7: Feb 8, 2024) and use a different computer and a different IP address for Day 7.\n\n- Evaluate detectors' ability to consistently classify native, ESL, AI-generated, and modified texts.\n\n- Evaluation involved assessing:\n  - consistency of ratings;\n  - success rate of AI detectors in identifying human modifications aimed at deceiving them;\n  - appearance of paywalls that limit access.",
    "# Results\n\nResults (percentages are probability that text is AI-generated, orange shading means bad detector guess, P means a paywall was encountered and testing did not continue):\n- Conclusion: All seven detectors were largely consistent, though there were problems with Originality and Winston AI.\n\nCopyleaks and GPTZero were the most accurate and also largely consistent on this limited test, with Copyleaks handling intentional grammar errors better.\n\nThus, for the purposes of consistent ratings for both students and graders, on the basis of this limited evaluation, Copyleaks is the best option followed by GPTZero.\n\nThe others are too inaccurate or too inconsistent to use safely.",
    "# AI Task Force Web-Based App\n\nThe AI task force arranged for a web-based app that provides access to a searchable repository of over 100 GenAI policy and advice documents from institutions of higher education.\n\nThis app also includes semantic network analysis and topic modeling for those documents.\n\nThis app was produced by the members and TAs of the Fall 2023 edition of the CDS DS380 “Data, Society, and Ethics” course, with the help of two undergraduate designers and developers.\n\nDuring the spring 2024 semester, a machine-learning student group from SPARK!\n\nenhanced this site in several ways.\n\nThey expanded the dataset to include policy and advice documents from K-12 education, government, and business domains, as well as higher education.\n\nThey built a topic model and a semantic network analysis based on themes instead of documents.\n\nIn addition, they used GenAI to automate document tagging and they upgraded the interface.",
    "## Fine-Tuning LLMs for Specific Uses\n\nAs mentioned, LLMs can be used to analyze and extract useful information from a variety of documents, which can then be used to train various models.\n\nGeneric LLMs, such as ChatGPT, Gemini, Llama, have been trained on large text corpora but may lack an ability to process specialized text.\n\nOne prominent example is clinical text, which is rich in specialized medical terms and abbreviations.\n\nMotivated by applications in healthcare, researchers have been able to use available datasets of clinical reports (often the MIMIC dataset, consisting of ICU clinical reports and discharge notes) to fine-tune generic LLMs for analyzing and extracting information from clinical notes.\n\nOne such effort is based on the open-source Llama LLM.",
    "## Using LLMs to Streamline Literature Review\n\nThere is mention of using LLMs to automate literature review.\n\nAsking an LLM to write a literature review for a specific project is not the best way to leverage its abilities and, because of hallucinations, it may produce plausible but false references.\n\nAn example is given where two lawyers were sanctioned for submitting a legal brief that included six fictitious case citations generated by ChatGPT.\n\nIn work soon to be published, a group of researchers developed an approach where an LLM is fed with abstracts of published papers (extracted using a set of keywords from a public database such as PubMed).\n\nThe LLM is then asked whether a given abstract indicates that the paper contains a result that the researchers care about.\n\nThrough elaborate prompt engineering, it becomes possible to obtain accurate YES/NO responses from the LLM.\n\nSuch an approach can be used to narrow down a potential large number of papers one may recover through keyword search to a more manageable list that a human group can then study.",
    "## Brainstorming Counterarguments\n\nInstructors may ask students to use a Generative AI (GenAI) platform to list potential objections to or limitations of a claim they are making in their papers or projects.\n\nIn such an activity, students consider and evaluate the objections created by GenAI, and choose at least one objection to acknowledge and respond to in their paper.\n\nIf using GenAI-generated objections, students should be asked to: (1) acknowledge the use of GenAI in their paper (e.g., list in the Works Cited or acknowledge GenAI usage in a letter to the reader), and (2) express the objection in their own words, rather than copy the GenAI-produced text.",
    "## Practicing Debate with Generative AI Platforms\n\nOne can use GenAI platforms to help students prepare for a class debate.\n\nMany GenAI platforms now have a voice feature (e.g., ChatGPT-4 mobile app) where an instructor can have their students engage in a real-time voice conversation with the platform.\n\nStudents choose a position (affirmation or negation of a resolution), and interact with the GenAI platform.\n\nSuch an activity develops critical thinking and argumentation skills; helps students practice articulating and defending a position on a specific topic and encourages students to critically analyze Gen AI's responses and generate thoughtful counterarguments in return.",
    "## Using GenAI as a Peer Reviewer to Solicit Feedback\n\nOne can ask students to interact with GenAI as a peer reviewer.\n\nFirst, have students solicit feedback on their paper draft or an excerpt from their draft, listing two or three criteria they want GenAI to use in evaluating their draft.\n\nThese criteria should be directly connected to the student assignment.\n\nNext, encourage students to critically evaluate the feedback they received from GenAI and decide whether or not they agree with it and how they might respond to it.",
    "## Understanding an Author’s Frame of Reference\n\nMany writers have a particular frame of reference: for example, early essayists like Michel de Montaigne and Francis Bacon frequently quote Roman authors, Virginia Woolf often references 19th and 20th-century literature, and so on.\n\nUsing GenAI, students can get some quick information about a group of referenced authors or works, and also learn what those authors and works have in common, giving them a stronger sense of an author’s frame of reference.",
    "# Introduction\n\nm to identify their questions (perhaps by freewriting for a few minutes), then pose their questions to GenAI.\n\nSharing their questions and the GenAI’s answers in discussion can help students dispel confusion and feel better prepared to move forward.\n\nComparing human response to machine analysis to better understand pathos.\n\nGen AI can do a surprisingly good job of analyzing the emotional appeal of essays and stories, and even of particular passages if one uploads the text.\n\nStudents can learn about the nuances of emotional appeal by comparing GenAI’s analysis to their own.\n\nOften, GenAI gives an articulate analysis, and yet misses subtleties.",
    "# References\n\n“Publishing ethics | Elsevier policy,” .\n\nAccessed: Jan. 10, 2024.\n\n[Online].\n\nAvailable: \n\n“Tools such as ChatGPT threaten transparent science; here are our ground rules for their use,” Nature, vol.\n\n613, no.\n\n7945, pp.\n\n612–612, Jan. 2023, doi: 10.1038/d41586-023-00191-1.\n\n“IEEE CSS Publications | IEEE Control Systems Society.” Accessed: Jan. 10, 2024.\n\n[Online].\n\nAvailable: \n\n“Notice to research community: Use of generative artificial intelligence technology in the NSF merit review process | NSF - National Science Foundation.” Accessed: Jan. 10, 2024.\n\n[Online].\n\nAvailable: \n\n“Plagiarism Detection Software | iThenticate.” Accessed: Jan. 11, 2024.\n\n[Online].\n\nAvailable: \n\n“Plagiarism Checker | Grammarly.” Accessed: Feb. 17, 2024.\n\n[Online].\n\nAvailable: \n\nOpenAI, “GPT-4 Technical Report.” 2023.\n\n“DALL·E 2.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Microsoft Copilot: Your everyday AI companion,” Microsoft Copilot: Your everyday AI companion.\n\nAccessed: Apr.\n\n02, 2024.\n\n[Online].\n\nAvailable: \n\n“Gemini - Google DeepMind.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Gemini - chat to supercharge your ideas,” Gemini.\n\nAccessed: Feb. 17, 2024.\n\n[Online].\n\nAvailable: \n\n“Llama 2,” Meta AI.\n\nAccessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Mistral AI.” Accessed: Feb. 04, 2024.\n\n[Online].\n\nAvailable: \n\n“Claude.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Midjourney Documentation and User Guide.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: //docs.midjourney.com\n\n“Stable Diffusion Online.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Adobe Firefly - Free Generative AI for creatives.” Accessed: Feb. 11, 2024.\n\n[Online].\n\nAvailable: \n\n“AI Music Generator - SOUNDRAW.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“BandLab SongStarter | Generate royalty-free music,” BandLab.\n\nAccessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Pictory - Easy Video Creation For Content Marketers,” Pictory.ai.\n\nAccessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Unlock Generative AI Content At Scale | Synthesys.io.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Sora: Creating video from text.” Accessed: Feb. 17, 2024.\n\n[Online].\n\nAvailable: \n\n“Simplified: An Easy to Use All-In-One App For Modern Marketing Teams.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Create Presentation Slides With AI in seconds with SlidesAI.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\n“Gamma App: Generate AI Presentations, Webpages & Docs.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\nJ. Wei et al., “Chain-of-thought prompting elicits reasoning in large language models,” Adv.\n\nNeural Inf.\n\nProcess.\n\nSyst., vol.\n\n35, pp.\n\n24824–24837, 2022.\n\nM. M. Grynbaum and R. Mac, “The Times Sues OpenAI and Microsoft Over A.I.\n\nUse of Copyrighted Work,” The New York Times, Dec. 27, 2023.\n\nAccessed: Jan. 06, 2024.\n\n[Online].\n\nAvailable: \n\n“GPT-4,” Wikipedia.\n\nJan. 06, 2024.\n\nAccessed: Jan. 06, 2024.\n\n[Online].\n\nAvailable: \n\n“Introducing ChatGPT Team.” Accessed: Jan. 11, 2024.\n\n[Online].\n\nAvailable: \n\n“FACT SHEET: President Biden Issues Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence | The White House.” Accessed: Jan. 06, 2024.\n\n[Online].\n\nAvailable: \n\nM. Wong, “There Was Never Such a Thing as ‘Open’ AI,” The Atlantic.\n\nAccessed: Jan. 06, 2024.\n\n[Online].\n\nAvailable: \n\nX. Qi et al., “Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!” arXiv, Oct. 05, 2023. doi: 10.48550/arXiv.2310.03693.\n\n“Universal and Transferable Attacks on Aligned Language Models.” Accessed: Jan. 06, 2024.\n\n[Online].\n\nAvailable: \n\n“Researchers Say Guardrails Built Around A.I.\n\nSystems Are Not So Sturdy - The New York Times.” Accessed: Jan. 06, 2024.\n\n[Online].\n\nAvailable: \n\n“GPTZero | The Trusted AI Detector for ChatGPT, GPT-4, & More.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\nW. Liang, M. Yuksekgonul, Y. Mao, E. Wu, and J. Zou, “GPT detectors are biased against non-native English writers,” Patterns, vol.\n\n4, no.\n\n7, Jul.\n\n2023, doi: 10.1016/j.patter.2023.100779.\n\nN. Stiennon et al., “Learning to summarize with human feedback,” Adv.\n\nNeural Inf.\n\nProcess.\n\nSyst., vol.\n\n33, pp.\n\n3008–3021, 2020.\n\nL. Ouyang et al., “Training language models to follow instructions with human feedback,” Adv.\n\nNeural Inf.\n\nProcess.\n\nSyst., vol.\n\n35, pp.\n\n27730–27744, Dec. 2022.\n\n“What is ‘dark data’ and how is it adding to all of our carbon footprints?,” World Economic Forum.\n\nAccessed: Feb. 18, 2024.\n\n[Online].\n\nAvailable: \n\n“AI for Scientific Discovery - A Workshop | National Academies.” Accessed: Jan. 09, 2024.\n\n[Online].\n\nAvailable: \n\nA. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting, “Large language models in medicine,” Nat.\n\nMed., vol.\n\n29, no.\n\n8, pp.\n\n1930–1940, 2023.\n\nM. F. Romano, L. C. Shih, I. C. Paschalidis, R. Au, and V. B. Kolachalama, “Large Language Models in Neurology Research and Future Practice,” Neurology, vol.\n\n101, no.",
    "8, pp.\n\n1930–1940, 2023.\n\nM. F. Romano, L. C. Shih, I. C. Paschalidis, R. Au, and V. B. Kolachalama, “Large Language Models in Neurology Research and Future Practice,” Neurology, vol.\n\n101, no.\n\n23, pp.\n\n1058–1067, Dec. 2023, doi: 10.1212/WNL.0000000000207967.\n\nS. Amini et al., “Automated detection of mild cognitive impairment and dementia from voice recordings: A natural language processing approach,” Alzheimers Dement., vol.\n\n19, no.\n\n3, pp.\n\n946–955, 2023, doi: 10.1002/alz.12721.\n\nS. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “Chatgpt for robotics: Design principles and model abilities,” Microsoft Auton Syst Robot Res, vol.\n\n2, p. 20, 2023.\n\nI. Singh et al., “Progprompt: Generating situated robot task plans using large language models,” in 2023 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2023, pp.\n\n11523–11530.\n\nK. M. Jablonka et al., “14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon,” Digit.\n\nDiscov., vol.\n\n2, no.\n\n5, pp.\n\n1233–1250, 2023.\n\nV. Veselovsky, M. H. Ribeiro, A. Arora, M. Josifoski, A. Anderson, and R. West, “Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science,” ArXiv Prepr.\n\nArXiv230515041, 2023.\n\nT. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong, “Solving olympiad geometry without human demonstrations,” Nature, vol.\n\n625, no.\n\n7995, Art.\n\nno.\n\n7995, Jan. 2024, doi: 10.1038/s41586-023-06747-5.\n\nF. Dell’Acqua et al., “Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.” Rochester, NY, Sep. 15, 2023. doi: 10.2139/ssrn.4573321.\n\nJ. Carter, J. Feddema, D. Kothe, R. Neely, J. Pruet, and R. Stevens, “AI for Science, Energy, and Security Report 2023 | Argonne National Laboratory,” 2023.\n\nAccessed: Mar.\n\n31, 2024.\n\n[Online].\n\nAvailable: \n\n“NOT-OD-23-149: The Use of Generative Artificial Intelligence Technologies is Prohibited for the NIH Peer Review Process.” Accessed: Feb. 11, 2024.\n\n[Online].\n\nAvailable: \n\nI. Goodfellow et al., “Generative Adversarial Nets,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2014.\n\nAccessed: Jan. 06, 2024.\n\n[Online].\n\nAvailable: \n\nA. Vaswani et al., “Attention is all you need,” ArXiv Prepr.\n\nArXiv170603762, 2017.\n\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” ArXiv Prepr.\n\nArXiv181004805, 2018.\n\nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” 2018, [Online].\n\nAvailable: \n\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” OpenAI Blog, vol.\n\n1, no.\n\n8, p. 9, 2019.\n\nT. Brown et al., “Language models are few-shot learners,” Adv.\n\nNeural Inf.\n\nProcess.\n\nSyst., vol.\n\n33, pp.\n\n1877–1901, 2020.\n\nA. Pegoraro, K. Kumari, H. Fereidooni, and A.-R. Sadeghi, “To ChatGPT, or not to ChatGPT: That is the question!” arXiv, Apr.\n\n05, 2023. doi: 10.48550/arXiv.2304.01487.\n\nE. N. Crothers, N. Japkowicz, and H. L. Viktor, “Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods,” IEEE Access, vol.\n\n11, pp.\n\n70977–71002, 2023, doi: 10.1109/ACCESS.2023.3294090.\n\nA. E. Johnson et al., “MIMIC-III, a freely accessible critical care database,” Sci.\n\nData, vol.\n\n3, no.\n\n1, pp.\n\n1–9, 2016.\n\nA. E. Johnson et al., “MIMIC-IV, a freely accessible electronic health record dataset,” Sci.\n\nData, vol.\n\n10, no.\n\n1, p. 1, 2023.\n\nS. Kweon et al., “CAMEL | Clinically Adapted Model Enhanced from LLaMA,” CAMEL | Clinically Adapted Model Enhanced from LLaMA.\n\nAccessed: Mar.\n\n31, 2024.\n\n[Online].\n\nAvailable: \n\nS. Merken, “New York lawyers sanctioned for using fake ChatGPT cases in legal brief,” Reuters, Jun.\n\n26, 2023.\n\nAccessed: Mar.\n\n31, 2024.\n\n[Online].\n\nAvailable:",
    "# Generative Artificial Intelligence (AI) – Guidance on Use and Applicable Policies\n\nBoise State University has a number of policies that safeguard institutional data, which university faculty, staff, students, and affiliates must follow.\n\nThose using generative AI in their work should consider what data they are using and whether or not such data usage is prohibited by university policy or otherwise generally cautioned against.\n\nEntering data into a generative AI tool or service is like posting that data on a public website.\n\nAI tools collect and store data from users as part of their learning process.\n\nAny data you enter into an AI tool becomes part of its training data, which it may then share with other users outside the university.\n\nBoise State University does not have enterprise contracts or agreements with any generative AI tool or service provider.\n\nNo AI tool meets the university’s security, privacy, and compliance standards for handling anything besides public data.\n\nTherefore, you may not enter internal, sensitive, or restricted data into any generative AI tool or service.\n\nOnly publicly available information classified as level three data (data that has no legal or other requirement for confidentiality, integrity, or availability under the Freedom of Information Act) may be used in generative AI tools and services.\n\nQuestions about university data standards and security should be directed to the Chief Information Security Officer at  or (208) 426-5701.\n\nAs with everything you do at the university, you must follow Idaho State Board of Education and University policies when using generative AI tools and services.",
    "#### Prohibited Use Explanation Relevant Policy\n\nYou may not enter any sensitive, restricted or otherwise protected data into any generative AI tool or service.\n\nThis information includes, but is not limited to:\n\n- FERPA-protected information, such as:\n  - Bronco card ID photos\n  - University Directory data\n  - University non-directory data such as student ID numbers\n  - Work produced by students to satisfy course requirements\n  - Student names and grades\n  - Student disability-related information\n- Health information protected by HIPPA\n- Information related to employees and their performance\n- Intellectual property not publicly available\n\nThe university is obligated to protect sensitive information to comply with applicable state and federal privacy and security laws and regulations and with university and Idaho State Board of Education policies.\n\nAccess to protected institutional data must be authorized and managed to protect individual privacy, maintain promised confidentiality, and ensure appropriate access and use.\n\n- University Policy 1090 (Intellectual Property)\n- University Policy 1150 (HIPAA Hybrid Entity Designation)\n- University Policy 2080 (Equal Access for Students with Disabilities)\n- University Policy 5120 (Export Control and Controlled Data)\n- University Policy 7530 (Employee Files)\n- University Policy 8000 (Information Technology Resource Use)\n- University Policy 8060 (Information Privacy and Data Security)\n\nYou may not upload any data that could be used to help create or carry out malware, spam and phishing campaigns or other cyber scams.\n\nSystem IT resources may not be used to disseminate unauthorized email messages.\n\n- University Policy 8000 (Information Technology Resource Use)\n\nYou may not direct AI tools or services to generate or enable content that facilitates sexual harassment, stalking or sexual exploitation or that enables harassment, threats, defamation, hostile environments, stalking or illegal discrimination.\n\n- University Policy 1060 (Non-discrimination and Anti-harassment)\n- University Policy 1065 (Sexual Harassment, Sexual Misconduct, Dating Violence, Domestic Violence, and Stalking)\n- University Policy 1075 (Non-discrimination on the Basis of Disability)\n\nYou may not use AI tools or services to generate content that helps others break federal, state or local laws; institutional policies, rules or guidelines; or licensing agreements or contracts.\n\nSystem IT resources may not be used to violate laws, policies or contracts.\n\n- University Policy 8000 (Information Technology Resource Use)\n\nYou may not use AI tools or services to infringe copyright or other intellectual property rights.\n\nSystem IT resources may not be used to violate copyright or other intellectual property laws.\n\nEntering copyrighted material into a generative AI tool or service may effectively result in the creation of a digital copy, which is a copyright violation.\n\nFeeding copyrighted material into a generative AI tool or service could “train” the AI to output works that violate the intellectual property rights of the original creator.\n\nIn addition, entering research results into a generative AI tool or service could constitute premature disclosure, compromising invention patentability.\n\n- University Policy 1130 (Use of Copyrighted Works)\n- University Policy 1090 (Intellectual Property)\n- University Policy 8000 (Information Technology Resource Use)\n\nIn addition to violating university policies, many of the above uses also violate generative AI providers’ policies and terms.",
    "## Incident Reporting Policies\n\nAny member of the university community who learns of a potential breach of data protection or confidentiality—including through the use of generative AI—must report the incident.\n\n- University Policy 2020 (Student Code of Conduct) – Suspected plagiarism for failing to properly acknowledge or cite works or ideas produced through the use of generative AI may be reported through the Student Conduct Report form.\n\n- University Policy 2250 (Student Privacy and Release of Information) – Suspected FERPA violations should be reported to the Office of Institutional Compliance and Ethics at  or (208) 426-1258.\n\nReports may also be made through the university’s Compliance Reporting Hotline.\n\n- University Policy 8060 (Information Privacy and Data Security Policy) – Suspected security breaches as defined in University Policy 8060 should be reported to the Help Desk at (208) 426-4357.\n\nThe Information Technology Incident Response Procedure must also also be followed.",
    "## Walden’s Statement on the Use of AI Tools\n\nGenerative artificial intelligence (AI) tools, such as ChatGPT, Bard, Claude, and others, afford new opportunities to stimulate thinking and supplement learning and assessment in higher education.\n\nUsing AI tools responsibly for academic purposes and in the classroom is an emerging competency for both students and faculty.\n\nWhile we all continue to better understand and evaluate the evolving use of AI-assisted research and writing tools, students and faculty are expected to use them responsibly by ensuring transparency about the use of AI tools in the writing process.\n\nDoing so includes citing whole sentences or paragraphs written by an AI-assisted writing tool that are integrated into one's own writing, just as if using content from any other source.\n\nAdditionally, when citing or quoting the output of an AI tool, students should include the AI tool input/output transcript in an appendix at the end of the paper.\n\nMore information and examples are included on the Scholarly Research and Writing With AI Tools page.\n\nAs we work together to use these new innovative tools ethically and responsibly, setting our course for change does come with some challenges, which we will continue to navigate together.\n\nWalden students and faculty can expect ongoing support and training on the ethical and appropriate use of AI tools.\n\nBecause of the rapid and ongoing changes in the AI landscape, this guidance page will be updated as relevant.",
    "## Responsible Use\n\nLearning to use generative AI tools responsibly and ethically is an important skill.\n\nAs you develop your AI literacy, please be mindful of some of the known limitations of these tools:\n\n- Incomplete or inaccurate information\n- Bias and lack of diversity\n\nSome generative AI tools are trained on content from prior to a certain date in the past (e.g., prior to September 2021).\n\nTherefore, content may be outdated or incomplete.\n\nIt will be important to fact-check content to ensure its accuracy, especially if the content will be directly cited or used to inform further analysis in your scholarly work.\n\nUse your best judgment on how, if, and when to use these tools.",
    "### Output is Only as Good as Input\n\nThere is bias and lack of diversity in training data.\n\nBecause AI-assisted writing is machine-generated, the system will learn and reproduce any biases that may be inherent in the training content.\n\nAs such, AI tool outputs may assert biased information as factual.\n\nSimilarly, if the training data lacked diversity, the output may favor dominant voices.\n\nThis is one of the key ethical concerns with generative AI tools.",
    "### Not a Substitute for Critical Thinking\n\nWhat you get out is only as good as what you put in.\n\nThe quality of the outputs you receive from a generative AI tool depends on the prompt entered.\n\nIt will take time to learn and refine how to best provide prompts to get quality outputs.\n\nTry different ways of asking the same question to see how the outputs may vary.\n\nGive the tool a role or context to improve the output.",
    "### Not a Substitute for Reading\n\nAs scholars, our contribution in the classroom requires critical thinking, synthesis, and analysis.\n\nAI-assisted writing tools do not provide this type of thinking because they are not human.\n\nReferences to course materials should only be cited if you have read the actual source material.\n\nIn addition to citing the use of AI tools for ideas or writing that is not original to you, Walden also requires that students include an appendix to document any interaction with an AI tool in the development of their work product.",
    "## Citation Guidelines: How and When to Cite\n\nOn April 7, 2023, the American Psychological Association (APA) issued guidance on how to cite ChatGPT.\n\nWalden has adopted this method of citation.\n\nUse of the direct output of a generative AI tool (i.e., quoting excerpts from the tool’s output) in student work should be uncommon.\n\nAI tools, like ChatGPT, are not scholarly sources.\n\nThink of AI tool outputs as similar to Wikipedia; both are not scholarly sources, and therefore citation from them is not encouraged.\n\nHowever, if you have a reason to cite an AI tool, follow the APA guidelines below.\n\nAPA provides the following guidance for citing an AI-Assisted Writing tool:\n\nAuthor.\n\n(Date).\n\nTitle (Version) [Description].\n\nSource\n\nExample:\nOpenAI.\n\n(2023).\n\nChatGPT (May 24 version) [Large language model].",
    "## Ways to Use Generative AI Tools\n\nHere are some responsible ways to use generative AI tools for research and writing support:\n\n- Draft an outline to clarify your thoughts\n- Iterate on research questions or to find other research questions related to your topic\n- Brainstorm other topics or questions related to your prompt\n- Find an expression or different way to articulate an original thought\n- Help with sentence structure or content organization\n- Better understand complex or difficult concepts",
    "## Honor Code Implications of Generative AI Tools\n\nThe Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion.\n\nThese are novel tools, and both students and instructors have been experimenting with their use in academic settings.\n\nWhile these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives.\n\nTo give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework:\n\nAbsent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person.\n\nIn particular, using generative AI tools to substantially complete an assignment or exam (e.g., by entering exam or assignment questions) is not permitted.\n\nStudents should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt.\n\nIndividual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools.\n\nCourse instructors should set such policies in their course syllabi and clearly communicate such policies to students.\n\nStudents who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification.\n\nThe BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance.\n\nMembers of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy.\n\nNOTE: As part of the BCA’s guidance on clear communication of a course’s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.\n\nOther helpful information for faculty and course assistants can be found HERE.\n\nIf you are in doubt about whether a generative AI source (or any source) is permitted aid in the context of a particular assignment, talk with the instructor.",
    "# Generative AI Acceptable Use Policy\n\nJuly 2023\n\nThe exploration and use of Generative AI / Open AI / Chat GPT tooling (from this point forward referred to as Generative AI) by employees of Maergo is permitted and in fact encouraged for work-related purposes.\n\nGenerative AI is a new, exciting, and powerful tool that can assist in various aspects of work, including but not limited to writing, research, analysis, and customer service.\n\nAt Maergo, we embrace new technologies and encourage our teams to leverage them to improve efficiency and effectiveness, and enable the achievement of our objectives.\n\nHowever, it is vitally important that employees use Generative AI responsibly while remaining compliant with all existing company policies and all applicable laws.\n\nWe are committed to providing a safe and secure environment for our employees, partners, and customers and this includes our responsible use of Generative AI tooling.\n\nThis policy outlines the guidelines and principles that employees must follow when using Generative AI capabilities.\n\nThe company developed these guidelines so that we can experience the benefits of Generative AI tooling while ensuring its utilization is secure and aligned with our expectations for conduct and our internal data privacy controls.",
    "## Guiding Principles\n\nThere are two main principles that guide the Maergo approach to Generative AI and the entirety of this policy centers on these ideas:\n\nGenerative AI should be used to assist employees in their work, not perform their work.\n\nAll existing company policies apply to the utilization of Generative AI.\n\nGenerative AI should be used to assist employees in their work, not perform their work.\n\nEmployees are responsible for their work product and are expected to carefully review any output received through Generative AI.\n\nUtilizing Generative AI well includes writing prompts with care, reviewing output, verifying output, editing output, and providing feedback to the tool if it got something wrong.\n\nAll existing company policies apply to the utilization of Generative AI.\n\nThis means that the company has determined that all current policies apply to an employee’s use of Generative AI, including but not limited to the Employee Proprietary Information Agreement, as well as all company policies pertaining to confidential information; intellectual property; bias, harassment, and discrimination; fraud and other illegal activities.",
    "## Prohibited Use\n\nWhile Maergo allows and encourages utilization of Chatbots/Large Language Models (LLMs) such as ChatGPT, autonomous AI agents including but not limited to AutoGPT, Godmode AI, and AgentGPT are not by default approved for company use.\n\nDue to the functionality, failure modes, and risks associated with autonomous AI agents, utilization is prohibited unless a request is made and approved by the Vice President, Product and Engineering and the Executive Leadership Team.\n\nAs of the writing of this policy, Maergo does not permit the use of Generative AI in the candidate selection process in any way that would replace or substantially assist in the decision-making responsibilities of our hiring teams.\n\nProhibited use of Generative AI includes but is not limited to screening or comparing candidate data, profiles, or resumes.\n\nGenerative AI may be used to assist with various components of the recruitment process such as assisting with the drafting job description language and suggesting potential interview questions.\n\nAs of the writing of this policy, Maergo does not permit the use of Generative AI to directly communicate or interact with any other person through bots or any similar technology.\n\nGenerative AI may be used to assist with drafting or editing of communications but may not be used for direct communication with others internal or external to Maergo.\n\nAs previously stated in this document, all current policies apply to the utilization of Generative AI.\n\nFor the purposes of reinforcing this expectation, we are providing the following illustrative examples of activities that are strictly prohibited and constitute serious violations of company policy.\n\nThese following will result in disciplinary action, up to and including termination of employment.\n\nSharing company confidential and proprietary information in a Generative AI chat or by any other means sharing or entering this information into a Generative AI tool.\n\nSharing customer or partner confidential information in a Generative AI chat or by any other means sharing or entering this information into a Generative AI tool.\n\nSharing Personally Identifiable Information (PII) in a Generative AI chat or by any other means sharing or entering this information into a Generative AI tool.\n\nUsing Generative AI in a way that is not professional and respectful of others including using Generative AI to engage in any form of discrimination, harassment, or other inappropriate behavior.\n\nUsing Generative AI to engage in any activity that violates Maergo's policies.\n\nUsing Generative AI to engage in illegal activity, including but not limited to fraud, intellectual property theft, and copyright infringement.\n\nFor clarity, entering confidential information into the tool is prohibited without regard to how the information is shared.\n\nThe means of sharing may include, but is not limited to, direct entry, copying and pasting, uploading, video, voice, or any other means of sharing.\n\nFurther, entering confidential information is prohibited whether or not the entry is saved.\n\nOnly information that is generally publicly available may be entered into Generative AI tooling.\n\nIf employees are unsure about whether or not information is confidential, they are expected to ask a member of the People or Legal team prior to sharing such information in Generative AI.",
    "## Security Measures\n\nUtilization of Generative AI for any work-related purpose should always be done through an account associated with an employee’s maergo.com email address rather than a personal email address.\n\nAs with any system that is used by Maergo employees, those using Generative AI must ensure that their login credentials are kept confidential and not shared with anyone.\n\nIn case an employee suspects any breach of security or unauthorized use, they must report it immediately to a member of the People or Legal team.",
    "## Reporting\n\nEmployees must report any suspected violations of this policy, or any incidents related to the misuse of Generative AI tooling, to the Maergo People team.\n\nAll reports of suspected violations or incidents will be investigated promptly and thoroughly and as confidentially as possible.\n\nEmployees must cooperate fully with any investigations related to suspected violations or incidents where Generative AI has been applied.\n\nMaergo prohibits retaliation against any employee who reports or participates in an investigation of a possible violation of our code of conduct, our policies including this policy, or the law.\n\nIf you believe you are being retaliated against, please contact the Chief People Officer or General Counsel.\n\nMaergo reserves the right to review any communications sent through or information shared with Generative AI tooling for the purpose of investigating suspected violations or incidents.\n\nThis includes but is not limited to messages, prompts, attachments, and files.",
    "## Financials of Usage\n\nAt present, there are free Generative AI tools available that are sufficient to allow for the utilization outlined in this policy.\n\nThe company will not pay for or reimburse employees for premium subscriptions or other costs associated with Generative AI tooling.\n\nA business case for an exception request will require the approval of the Vice President, Product and Engineering and the Executive Leadership Team.",
    "## Review of this Policy\n\nMaergo will review this Acceptable Usage Policy as needed to ensure it remains relevant and effective, in line with our risk appetite for Generative AI use cases.\n\nAny changes or updates to this policy will be communicated to all employees.\n\nMaergo reserves the right to make changes to this policy at any time, without notice.\n\nEmployees are encouraged to ask questions or for clarification on this policy.\n\nEmployees are also encouraged to provide feedback on this policy and offer suggestions for changes or improvements.",
    "## Purpose\n\nThis policy guides NCAD staff in the use of Generative Artificial Intelligence (AI) when setting assignments on academic programmes, and NCAD students when undertaking those tasks.\n\nThis policy is designed to support the creativity, transparency and originality to which the teaching and learning at NCAD is committed while upholding the highest principles of academic integrity.\n\nThis policy should be understood as operating in conjunction with NCAD’s Plagiarism Policy.\n\nThis policy concerns the use of specific forms of AI, namely those which can be understood as Generative AI.\n\nThese are systems that are capable of generating texts, code, sounds, music and images in response to prompts or other instructions from the user.\n\nThese outputs will be new and unique.\n\nTypical examples of Generative AI tools include ChatGPT, Dall-E-2, MidJourney and Github Co-pilot.\n\nThis policy does not address the use of AI-powered tools designed to correct or improve visual or written expression (such as photo editors or grammar checkers) or to conduct online research (e.g.\n\nimage search engines).\n\nIn such cases, and as currently required, the student using such AI-powered tools should be able to make an unequivocal declaration of their authorship of any piece of work conducted during their studies.\n\nSimilarly, this policy does not address the training of a student’s own AI model through machine learning, data sets, etc.\n\nThis policy is guided by the understanding that Generative AI has the potential to act as a valuable aid to learning, research and creativity, and that, if used, students should do so in critical and informed ways.\n\nThis means that they should be guided to understand how Generative AI functions; what kind of data it draws on; and the implications of its use for copyright and Intellectual Property (both that of the student’s and of others’).\n\nIn particular, students need to be equipped to understand the limitations of any Generative AI that they use, as well as the tendency for these tools to exploit the creativity and research of human authors, artists and other creators without acknowledgment.\n\nOur approach to the use of Generative AI at NCAD requires transparency.\n\nStudents should make their own practices as learners evident to those who teach and assess them.\n\nIn this way, teaching staff are able to provide valuable and effective feedback.\n\nWhile there is much public discussion about the likely negative impact of Generative AI on academic integrity and creativity, it is clear that AI Generative tools will continue to develop and be a feature of many future applications of technology.\n\nThey will be employed in many workplace and professional settings.\n\nAt the same time, the core skills and understanding that we seek to develop in our students remain of central importance.\n\nAccordingly, our approach is to support NCAD teaching staff to set appropriate limits on the uses of Generative AI tools, balancing their utility against the learning needs of our students.\n\nWithout such declared limits, students will reasonably make use of freely available and highly purposeful AI Generative tools.\n\nIt is incumbent on staff to communicate clearly the limits and potential of Generative AI tools in relation to any task being set.",
    "## Policy\n\nTeaching staff have the right to prohibit the use of an AI Generator by a student when undertaking an assignment.\n\nTypically, this will be when the Learning Outcomes of a module require that the student develop and apply particular skills.\n\nThe development of research skills in the library or sketching skills are incompatible with the use of Generative AI, for instance.\n\nIn such cases, the teaching staff will make it clear in any published course documentation - such as a brief - that use of an AI Generator is not permitted.\n\nTeaching staff may allow limited use of an AI Generator to undertake a specified task which forms part of the work to be undertaken on the course.\n\nFor instance, an AI Generator might be used in an early phase of a project to support ideation or as a critical investigation of particular AI tools.\n\nLimited use may also include the use of Generative AI to check or improve content which has been created by the student.\n\nIn such cases, the teaching staff will specify the nature and extent of permitted usage of an AI tool.\n\nThis limited usage will be specified in any published documentation provided to students on the course.\n\nStaff may require students to declare in written form the nature and the extent of the use of an AI Generator when presenting / submitting work for assessment.\n\nThis may, for instance, take the form of a declaration on a cover sheet, a citation or a caption for images, as required by the teaching staff.\n\nTeaching staff may allow uninhibited use of an AI Generator to undertake any task which forms part of the work to be undertaken on a course.\n\nAgain, this should be declared in course documentation.\n\nAnd as above, the student will be required to make a full declaration about the way in which the AI Generator has been used.\n\nTeaching staff have the right to use AI detection tools to test or confirm their use in the production of work by NCAD students.\n\nStaff should not, however, rely on such tools while they have yet to prove their complete accuracy.\n\nIn the case where staff have reason to believe that an AI Generator has been used in ways that contravene the permitted uses outlined above, the following approach should be taken:\n- The student should be required to explain their work processes concerning the use of AI tools by the module coordinator or a nominee.\n\nThis will typically take the form of an in-person meeting with the student;\n- If the student admits to having breached this policy, a judgement should be made - by the module coordinator and the head of programme - about the scale and nature of the offence;\n- ‘First offences’ or minor breaches of the policy will typically result in the student re-submitting the work and a grade penalty can be considered at the point of assessment of the module.\n\nIf the timing of this assessment point does not permit the resubmission of work, first offices or minor breaches may result in a requirement that the student resits the assessment task;\n- More substantial breaches of the policy can be considered a fail and a recommendation be made to the Examination Board that the student be required to repeat the module;\n- If a student denies using an AI Generator despite evidence to the contrary or engages in an act of deception, the module coordinator and head of programme may decide that the student is guilty of serious academic misconduct (defined as ‘any action or attempted action that may result in creating an unfair academic advantage’).\n\nIn such cases they should request that the Head of Academic Affairs invoke the Student Discipline Procedure as outlined in the NCAD Student Code of Conduct.",
    "### Developing course policies around A.I.\n\nDeveloping course policies around A.I.\n\nYour syllabus is an essential document for conveying expectations to your students.\n\nYou likely already have policies around late work, attendance, and other aspects of your course, so it is highly recommended that you think about how you might incorporate a policy related to A.I.\n\nuse into your existing course structure.\n\nThere’s no one-size-ﬁts-all approach here since courses differ vastly from one another and there is wide variety in how instructors are incorporating A.I.\n\ninto their courses.\n\nBefore we present some speciﬁc examples, here are a few things for you to consider:\n\nHaving an A.I.\n\npolicy is important.\n\nSome instructors allow any use of A.I.\n\nin completion of coursework while others prohibit it entirely.\n\nIt is therefore very important for you to clearly convey when and how you allow use of A.I.\n\nin your courses, if at all.\n\nOtherwise, students may assume that the rules from their other courses also apply in yours, which can lead to inadvertent violations of academic integrity.\n\nYou may have different policies based on assignment type.\n\nYou likely already have different sets of expectations across the assessments in your course.\n\nFor example, textbook use is often encouraged on homework but prohibited on exams.\n\nYou may end up with similarly distinct policies regarding A.I.\n\nuse.\n\nFor example, it might be okay to use it for the brainstorming phase of a project but not production of the ﬁnal work.\n\nMaybe it could be useful for helping improve essays but prohibited on quizzes and exams.\n\nIf this is the case, be very clear about when and how you allow A.I.\n\nuse.\n\nExplain the ‘why’ behind your policy.\n\nGenerative A.I.\n\nis quickly becoming a commonplace feature in our world.\n\nIt is already being used in workplaces to speed up tasks, brainstorm ideas, organize concepts, and so many other things.\n\nBe clear about the value you place on your assignments in terms of learning value and explain why you place limits on A.I.\n\nuse in terms of how it would negatively impact learning.\n\nGive guidance for citation of A.I.\n\nIf you allow students to use A.I., you may want to ask them for formal citations of any text that was generated by A.I.\n\nas well as asking them to explicitly state how they used A.I.\n\nfor help.\n\nGuidance is becoming available for citation of A.I.\n\nusing formal styles like APA and MLA.\n\nAsk for help.\n\nIf you’re not sure how to get started or want to discuss your policy with someone, instructional designers at the CTT (contact-us) are available for consultation on this or any other teaching challenge you may be facing.",
    "### A.I.\n\nSyllabus Policy Examples\n\nAs noted above, there are a wide variety of different approaches you can take to developing a syllabus policy around A.I.\n\nConsider the different types of assignments you have in your course, how they would be impacted by student use of A.I., and what reasonable limits you can put in place.\n\nExample policies tend to fall into 3 main categories, as outlined below.\n\nThis page includes some examples of each type, but you can ﬁnd many more on this page of crowdsourced A.I.\n\npolicies.\n\n**Unrestricted use.\n\n**  \nThese policies allow students to use A.I.\n\nin any way they’d like to complete coursework.\n\nThese policies tend to include warning language about the likely pitfalls of turning in A.I.\n\ngenerated work without making any revisions.\n\n**Use allowed under specific circumstances.\n\n**  \nThese policies allow students to use A.I.\n\nin some situations but not others.\n\nFor example, it may be allowed in particular phases of a project but not others.\n\nIn other courses, it might be allowed on some types of assignments but not all of them.\n\nIf you choose to apply this type of policy, be sure that you cover all types of assignments in your course so your students know exactly what they can use when.\n\n**Use prohibited entirely**  \nThe set of policies put in place an outright ban on any use of generative A.I.. For policies like these, we recommend being very clear with students why you have the policy that you do.\n\nGiven how easy it is to use A.I.\n\nand how difficult it is for instructors to detect, these policies will be difficult to enforce making it particularly important to work on getting student buy in for the learning process.\n\n---",
    "### TEACHING AND GENERATIVE ARTIFICIAL INTELLIGENCE LIKE CHATGPT\n\n- AI Exchange Blog\n- What Does Generative Artificial Intelligence Mean for Higher Education?\n\n- What is Generative Artificial Intelligence?\n\n- How to Explore ChatGPT\n- Classroom Implications\n- Developing course policies around A.I.\n\n- Impact on Assessment Practices\n- Implications Outside Higher Ed and Future Development\n- FAQ\n\n---\n\nWas this page helpful?\n\nYes No",
    "## POLICIES & REPORTS\n\n- Accessibility Statement\n- Institutional Equity and Compliance\n- Notice of Nondiscrimination\n- Privacy Policy\n- Safety at Nebraska\n- Student Information Disclosures\n\n--- \n\nUNL web framework and quality assurance provided by the Web Developer Network · QA Test\n\nUNIVERSITY of NEBRASKA–LINCOLN  \nCopyright 1869 – 2024",
    "## PURPOSE\n\nTo support the ethical and responsible use of generative Artificial Intelligence for the purpose of teaching and learning at Holmesglen.\n\nGenerative Artificial Intelligence (GAI) is a typed or artificial intelligence (AI) technology that can produce various types of new content/outputs, including text, imagery, audio, synthetic data and more, based on the data on which they have been trained.\n\nIn the educational setting, GAI has the potential to dramatically change and shape the future of learning, content development and research practices.\n\nConsiderations such as ethical use, transparency, fairness, and accountability must be addressed to ensure GAI is used responsibly.",
    "## GUIDELINES FOR TEACHING EMPLOYEES\n\nEducate learners as to the impacts of generative AI use, including relevance to the Academic Integrity Policy.\n\nEducate learners as to potential impacts on the quality of their output resulting from use of generative AI, including systemic prejudices and biases in training data.\n\nEnsure that the use of AI tools is consistent with the requirements of relevant accrediting bodies (if any) and is consistent with relevant industry expectations.\n\nUnderstand that detection of use of generative AI is difficult and that existing academic integrity detection tools may not be reliable.\n\nConsider forms of assessment in the context of the availability of generative AI and the implications arising in relation to learning outcomes and authenticity of assessments.\n\nConsider alternate assessment methods, such as:\n\n- in person observation assessment\n- simulation / role play\n- oral assessment / viva voce\n- invigilated assessment including lockdown browser software\n- interactive activities\n- the inclusion of specific local context in assessments.\n\nDetermine, in relation to each subject, which of the following four options, or combination thereof, is best suited for assessment in that subject, based on the nature of the task and the extent to which it produces evidence of required learning:\n\n- prohibit all use of generative AI for an assessment task\n- restrict the types of generative AI tools which can be used for an assessment task\n- restrict how generative AI tools can be used for an assessment task\n- do not restrict use of generative AI for an assessment task.\n\nState clearly and consistently, on the Holmesglen Learning Management System and assessment instructions, where appropriate, the conditions applicable to each piece of assessment for use of generative AI, by reference to one of the above stated four categories.\n\nThe wording used should be consistent with the recommended wording set out in Appendix A to this Guideline.\n\nAttach a link to each piece of assessment to any applicable Holmesglen guides as to use of generative AI, including any guides as to how use of generative AI should be acknowledged for referencing purposes.\n\nEnsure that the declaration made by the learner for academic integrity purposes on each piece of assessment includes a declaration as to the following, in accordance with the wording set out in Appendix B:\n\n- a written acknowledgment as to the use of generative AI\n- details as to which AI technology was used\n- details as to how that information was generated\n- identification of the prompts used\n- an explanation as to how the output was used in the assessment task\n\nEnsure that, in addition to the above, learners reference material generated by AI is cited in the assessment item, in accordance with the citation methods required by Holmesglen.\n\nEnsure that, for assessment purposes:\n\n- final results are determined by a human\n- where AI has been employed in communication with learners and/or in the preparation of teaching material and assessments, teaching employees must verify the currency and accuracy of that documentation.\n\nConsideration should be given to where Generative Artificial Intelligence might be used in the preparatory steps taken prior to the actual writing of the assessment item.\n\nFor example, is it permissible for learners to use it as a writing prompter?\n\nIf so, this should be clearly stated on the guidelines for preparation of the assessment item, with an accompanying statement that the learners are required to build their own argument and employ their own skills of critical thinking.\n\nConsideration should be given on a department level, to permitting learners to use generative AI in their study.\n\nFor example, generative AI can be used to proofread final drafts, check for spelling and grammatical errors, sentence structure etc.\n\nIt should also be considered as a tool to explain study material.\n\nReport all unsanctioned or inappropriate use of generative Al in assessment in accordance with the Academic Integrity Policy and Academic Misconduct Procedure (Learners).",
    "## GUIDELINES FOR LEARNERS\n\nOnly use generative AI as permitted.\n\nCheck with teachers or subject coordinators as to permissible use of generative AI.\n\nWhere generative AI is used:\n\n- provide a statement in accordance with Appendix B\n- retain proof of their interactions with generative AI, in the event of a consequent misconduct case.\n\nWhere generative AI is used, learners must cite the source in the body of the text and reference list.\n\nUse of generative AI except as permitted in writing and, where use is permitted in full or in part, failure to clearly state the information required in Appendix B amounts to academic misconduct.\n\nIt will be dealt with in accordance with the Academic Misconduct Procedure (Learners).",
    "# Appendix A\n\nWhere all use of generative AI for an assessment task is prohibited:  \nIn this assessment, you must not use generative artificial intelligence in any way whatsoever.\n\nWhere use of certain types of generative AI tools are restricted:  \nIn this assessment, you are permitted to only use the following generative artificial intelligence (AI) (insert names or types of tools).\n\nWritten detail must be provided as to which tools were used, how output was generated and identification made as to the prompts used.\n\nIdeas and/or text obtained through this process must be cited.\n\nWhere certain uses for generative AI tools are restricted:  \nIn this assessment, you are permitted to use generative artificial intelligence (AI) for the following purposes (state purposes).\n\nWritten detail must be provided as to which tools were used, how output was generated and identification made as to the prompts used.\n\nIdeas and/or text obtained through this process must be cited.\n\nWhere use of generative AI tools are not restricted:  \nIn this assessment, you are permitted to use generative artificial intelligence (AI).\n\nWritten detail must be provided as to which tools were used, how output was generated and identification made as to the prompts used.\n\nIdeas and/or text obtained through this process must be cited.",
    "### Potential impact of AI on our academic mission\n\nAugust 31, 2023\n\nDear Members of the Brown Community,\n\nAmid ongoing national conversations across higher education regarding the use of AI tools, I am writing to provide information about University resources related to AI, guidelines for use, and upcoming community discussions.\n\nThere is no shortage of public analysis regarding the ways in which the use of generative artificial intelligence (AI) tools (such as ChatGPT)—which are open-access tools that can generate realistic text, computer code, and other content in response to prompts from the user—provides both challenges and opportunities in higher education.\n\nAccordingly, exploring the use of AI in ways that align with Brown’s values has been a topic of discussion among our senior academic leaders for several months.\n\nAs we identify the ways in which AI can enhance academic activities for faculty and student success and administrative activities for staff, we must also ensure these tools are understood and used appropriately and ethically.\n\nBelow is a list of resources where you can find guidance on the following:",
    "### AI Usage Guidance\n\nThere is a diversity of practice among faculty regarding AI, and instructors may choose whether or not to allow the use of AI tools and to what extent.\n\nWhile the University is not prescribing specific AI policies, faculty should offer clear, unambiguous information about what is, and is not, allowed in their courses.\n\nThe Sheridan Center for Teaching and Learning at Brown provides sample syllabus language as well as other tips for classroom integration.\n\nAgain, links to these and other resources in this message are provided at the bottom of this email.",
    "### Academic Integrity\n\nOne of the most common concerns about AI is the potential for plagiarism, or students using the system to do work that they then present as something they created without AI assistance.\n\nAny unapproved use of AI to complete assignments would be covered by Brown’s Academic Code and Academic Code, Graduate Student Edition.",
    "### Citation and Attribution\n\nIf you choose to use generative AI tools for course assignments, academic work, or other forms of published writing, you should give special attention to how you acknowledge and cite the output of those tools in your work.\n\nThe Library has compiled general guidelines for citing AI-generated content, as well as more specific guidance from the three major style guides.",
    "### AI as a Research Tool\n\nThe rapid emergence of AI technology offers great potential for researchers as they design, conduct, support, and present their research.\n\nHowever, there are also a number of challenges and risks to understand when using these tools in the research domain.\n\nOIT's Center for Computation and Visualization has published a reference guide to understanding and using generative AI as a research tool.\n\nThe guide also includes a helpful glossary of generative AI terms.",
    "### Protecting Intellectual Property\n\nWhile AI has great potential for university research, Brown investigators contemplating using generative AI tools in their own research should be cognizant of the various intellectual property issues related to its use.\n\nThe Office of the Vice President for Research (OVPR) has published Initial Guidance for Researchers, which offers important information for researchers about the intersection of generative AI and intellectual property, as well as considerations around public disclosure and patentability.",
    "### Next Steps\n\nAI tools will rapidly evolve in the coming months and years, and Brown is well-positioned to innovate in this space.\n\nExperts from across our campus—from engineering, public health, medicine, the social sciences, the humanities, policy leaders, and technologists—have already been incorporating AI into their work.\n\nI encourage all of you to experiment with AI to find ways it could be helpful to your academic experience.\n\nBeginning in the fall, the Data Science Institute will partner with the Office of the Provost for “Conversations on AI and our data-driven society,” a series of monthly campus-wide discussions about the impact of AI on how we live, work, and educate the next generation.\n\nThe Library has also planned weekly discussions and working groups to address various issues surrounding AI from different perspectives.\n\nI look forward to continued discussions with academic leadership as we monitor the evolution of these tools and discover innovative applications to improve learning outcomes and inform research directions.\n\nPlease feel free to send additional comments, questions, or concerns to .\n\nSincerely,\n\nFrank Doyle  \nProvost",
    "# GUIDELINES FOR AI INTEGRATION THROUGHOUT EDUCATION IN THE COMMONWEALTH OF VIRGINIA\n\nArtificial intelligence (AI) brings tremendous potential to transform education at all levels.\n\nBy enabling more efficient, universal, and deeper learning, AI can unlock new realms of knowledge that were previously unimaginable.\n\nThis emerging technology promises to catalyze business innovation and economic growth for the Commonwealth.\n\nHowever, AI also poses risks that must be carefully managed.\n\nAs we integrate these powerful tools into our schools and workplaces, we must urgently equip students and workers with the skills, knowledge, and competencies to harness AI responsibly and effectively.\n\nOur education system must adapt to prepare a workforce that can leverage AI to its full potential while safeguarding against its risks.\n\nWhile AI tools can analyze information and enhance certain aspects of learning with unmatched efficiency, we must remember that education is ultimately a human endeavor.\n\nThe true art of teaching involves wisdom, judgment and interpersonal skills that machines cannot replicate.\n\nAs such, AI should never fully replace teachers, who nurture students’ critical thinking, values and character development.\n\nVirginia is well-positioned for this new era.\n\nThe most critical national security and military intelligence institutions in the United States are headquartered in Virginia and the Commonwealth’s colleges and universities lead the nation in technology research and development.\n\nThese unique factors provide Virginia with a responsibility to spearhead a policy environment that strengthens trust, safety, and security in the development and deployment of AI technologies and the opportunity to assemble experts from the workplace and education to provide ongoing guidance and assistance in the responsible and effective implementation of AI in our education, training, and workplace learning.",
    "## Guiding Principles for the Use of AI in Education\n\nDo no harm: All integration of AI in education must be in accordance with the policy and IT standards that are set forth in the Executive Order and in other state and federal policies.\n\nThis includes ensuring the safeguarding of the privacy, security, and confidentiality of personally identifiable information, ensuring that algorithms are not based on inherent biases that lead to discriminatory outcomes, and that AI is only allowed when its use has the potential to contribute positive good/improvement to the status quo.\n\nPrioritize integrity: A core purpose of education is the development of responsible, ethical, and engaged citizens.\n\nTherefore, part of the integration of AI in education must be to teach about morality, ethics, honor, cheating, and how artificial intelligence can lead to perverse and destructive outcomes for individuals, relationships, and communities.\n\nAugment, not replace humans: AI cannot and should not ever replace human judgement.\n\nAlthough synthesis and analysis of information can be expedited through AI, it will never replace teachers who provide wisdom, context, feedback, empathy, nurturing and humanity in ways that a machine cannot.\n\nIt also should not overpower/over-ride the critical thinking, judgement, and morality of the learner.\n\nHarness AI to empower student success: AI presents transformative opportunities to enrich instruction and enable more adaptive, personalized learning.\n\nTo fully leverage these benefits, we must embrace an outlook of innovation and experimentation, while ensuring access for all learners.\n\nRather than just digitizing traditional practices, we can reimagine education to nurture each student’s unlimited potential with the assistive power of AI.\n\nOur vision should focus on possibility - AI as a gateway to expand human capacity - not conformity to conventions of the past.\n\nWork in partnership: AI researchers and experts in Virginia colleges and universities and employers are key partners and guides around building the skills and knowledge required to be successful in the new economy.\n\nThis includes how to think about evolving AI systems and the practices needed to use them responsibly.\n\nBe constantly discerning and responsive to the continuous expansion of AI capabilities and uses: This is not a one-and-done.\n\nThe guidelines, best practices, and tools available will constantly need to be revisited to keep pace with the changes brought by the exponential growth of AI and technology advancements in our world.\n\nNimbleness and urgency, balanced with careful considerations, will be critical to ensuring we are as prepared for the continuous change ahead in education.",
    "## Strategies for Successful Integration of AI in Education\n\nAt every level of education—from governing boards to individual classrooms—these strategies can improve the effective integration of AI into education.\n\nEncourage Exploration — To have informed discussions about integrating AI, educators first need hands-on experience to understand these technologies’ capabilities and limitations.\n\nSchool leaders should actively facilitate opportunities for teacher teams to directly explore various AI tools.\n\nThrough guided firsthand usage, educators can discover potential applications to enrich instruction, as well as critical risks and challenges to consider.\n\nRather than rely on rumors or superficial perceptions, we must empower teachers with deeper practical knowledge.\n\nBy allocating time and resources for structured experimentation, schools can tap into educators’ insights to uncover creative use cases while proactively addressing ethical concerns.\n\nThis ground-up understanding, straight from classroom trenches, will produce more thoughtful and responsible approaches to adopting AI.\n\nProvide Professional Development — As with any new tool or strategy, educators need professional development from experts in order to feel comfortable using it.\n\nOffering workshops or professional development courses, micro-credentials, or micro-badges on AI as well as meaningful follow-up implementation and application activities, discussions and opportunities to collaborate with colleagues and experts who can help teachers understand and experience its capabilities and limitations can help teachers understand and experience its capabilities and limitations.\n\nSpotlight Success — Spotlight interesting examples of AI that are already being used in schools.\n\nEnsure educators have meaningful outlets to share what they are trying and whether it is working or not.\n\nEncourage teachers to collaborate in their exploration of AI to learn from each other's experiences and builds a supportive community for this new educational environment.\n\nHost Stakeholder Conversations — Host regular conversations with business leaders, educators, governing members, leaders, and families about AI and how schools are preparing or will be preparing students with the skills to thrive in an AI-infused world.\n\nTopics could include, but not limited to, fostering ethical AI use, digital citizenship, collaboration and interdisciplinary learning, and integration of AI in the curriculum.\n\nSet The Right Conditions — When it comes to using AI or any other technologies in school, it is important to establish conditions for effective use.\n\nClearly outline the school or system’s policies and protocols around data privacy, honor code, student code of conduct, acceptable use, and ethical considerations when using AI, including those related to plagiarism and proper use of secondary sources.\n\nTeachers should know what is expected of them and the boundaries within which they should operate.\n\nThese conditions might include using technology to support teaching and learning.1",
    "## Roles & Responsibilities\n\nEducation stakeholders must understand and embrace their roles and responsibilities in an effort to integrate AI into our education institutions.\n\nThis is an initial list of those roles and responsibilities which will continue to be refined by the Advisory Group.\n\nState Education Agencies – Virginia Department of Education (VDOE) and State Council of Higher Education for Virginia (SCHEV) (including Virginia Community College System)\n\nGoverning Boards/Leadership – 131 K-12 school division boards of education and division leaderships and the boards of visitors/presidents/leadership teams at every public college and university\n\nHigher Education Programs and Faculty/School & Classroom Leaders\n\nAI/Technology Directors\n\nSpecific Roles of State Agencies: VDOE and SCHEV shall meet monthly with the SOE team to identify implementation hurdles, best practices, and areas of support needed from the field by the agencies to support implementation, accelerate what’s working well, and improve oversight of AI in education in the Commonwealth.\n\nEach agency shall on an ongoing basis:\n\nCreate advisory resources, including policy and protocols best practices, for generative AI2 that are continuously updated and modified.\n\nCreate processes for tiers and rules at which approval is given for use of AI tools.\n\nDevelop stakeholder training toolkits and workshops for governing bodies, educators, families, and students on AI, its capabilities, and its limitations.\n\nSpotlight success stories where AI is being used well in schools or higher education institutions.\n\nServe as a resource bank of what is working and provide a supportive learning community for interested educators or leaders.\n\nHost conversations with educators, institution leaders, and governing board members on how schools can best prepare students with the skills to thrive in an AI-infused world on topics such as fostering ethical AI use and digital citizenship; integration of AI in the curriculum; or establishing policy conditions for effective use.\n\nModify agency staffing structure to better provide expertise to the field, including on what AI tools are most applicable and encouraged to be used for K-12 or higher education institutions.\n\nThese agencies in partnership with the Virginia Community College System shall work in collaboration to design micro-badging and micro-credential training opportunities for the field to stay abreast of the latest AI technologies, ethics, and best governance leadership skills.\n\nOngoing Roles and Responsibilities:\n\nCapacity building/training at every agency and throughout the sector to the classroom level.\n\nSafeguarding privacy, security, and confidentiality of data.\n\nStrengthening governance of AI through supporting capacity building and technical assistance to the boards of visitors or school division governing boards.\n\nSpecific roles of Governing Boards/Leadership and Educators: Education happens at the local level.\n\nThe role of informed and engaged governing bodies, active leaders and informed and empowered faculty and staff at the institution or school/division level will determine how well AI is integrated and sued in education in Virginia.\n\nThe following roles and responsibilities will help ensure the local implementation is constantly monitored, actively managed, and continuously adjusted to address the ongoing changes AI will bring to education.",
    "These are the critical but not comprehensive roles and responsibilities at the point of education delivery:\n\nEstablish a culture of integrity\n- Define what this culture looks like (State Agencies and Governing Boards)\n- Build a culture of integrity by discussing the honor code often with students and parents, implementing an honor code system with real consequences (Faculty/Staff)\n\nFollow an Acceptable Use Policy inside and outside the classroom\n- Codify Acceptable Use Policy (State Agencies/Governing Boards)\n- Review and discuss the Acceptable Use Policy with students and parents (Faculty/Staff)\n- Implement and hold learning community accountable for living up to Acceptable Use Policy (Faculty/Staff)\n\nDesign assignments and assessments that encourage critical thinking and original thought and human judgement (Faculty/Staff)\n\nCreate opportunities for collaboration and peer review (Faculty/Staff)\n\nIntegrate digital citizenship with the following Standards of Learning strands (VDOE, Governing Boards, and Faculty/Staff)\n- Computer Science: The Impacts of Computing\n- Digital Learning Integration: Digital Citizen\n- English: Communication and Multimodal Literacies, Research\n- Fine Arts: Innovation in the Arts, History, Culture, and Citizenship\n- Health: Safety/Injury Prevention\n- Mathematics: Computation\n- Science: Scientific and Engineering Practices: Obtaining, evaluating, and communicating information\n- Social Studies and History: Skills\n- World Language: Interacting in School and Global Communities\n\nClarify expectations for use of AI in assignments (Governing Boards)\n- Communicate clearly and specifically when and how students can utilize Generative Artificial Intelligence in assignments (Faculty/Staff)",
    "# What are Generative AI Tools?\n\nGenerative artificial intelligence (AI) tools use machine-learning algorithms to create \"new\" digital content (text, images, audio, multimedia, computer code, etc.).\n\nThey are not currently capable of evaluating, critically combining, or verifying information—instead, they predict and rearrange the most likely outcome based on the requested output, feedback, and patterns in the data models they are trained on.\n\nNatural language processing (NLP) is the subset of generative AI tools, such as ChatGPT, that allows computers to analyze and then mimic human language.\n\nPaired with a user-friendly interface, these tools are commonly called \"chatbots\" and allow users of all skill levels to place requests and interact with the language models in a conversational manner.",
    "### Example\n\nA chatbot for a course could answer student questions related to the course subject matter, offer definitions, summarize or identify main ideas in documents or materials, generate review questions, or identify support resources.\n\nFor the model to learn, it needs to be exposed to relevant data.\n\nThe chatbots learn what the best answers are and how to deal with questions that don’t fit the framework of the questions asked.",
    "# Different Tools for Text and Image Generation\n\nChatbots such as ChatGPT, ChatSonic, LaMDA, BERT, Jasper, Jasper Art, Midjourney, Stable Diffusion, and Bing use NLP and are trained to understand language and respond in a meaningful way.\n\nSome apps such as ChatGPT are trained on large amounts of text data from various sources and then the model uses the prompt given, analyzes the data into components, processes the meaning of each part generated, and then produces a response.\n\nThis process is called \"transfer training,\" which involves the model being trained on new tasks and data to improve its performance and accuracy (Sanchez, 2023).\n\nThe tools differ in the data models they are trained on and the way in which they were designed for the user to interact with them; for example, the original version of ChatGPT is based on data up to September 2021 and uses a conversational format, ChatSonic is powered by Google Search and utilizes voice commands, Jasper remembers conversations, LaMDA is trained on dialogue and Replika is oriented to personal use.\n\nOther AI apps are conversation-focused and may use rule-based systems that rely on predetermined responses.\n\nAdditional apps are image-based and provide visual explanation such as Socratic or DALL-E 2, which can create realistic images from a given description.",
    "### Examples\n\n- **Bard (Google):** Powered by LaMDA (Language Model for Dialogue Applications); natural language input; can follow up on conversations; can generate responses to open-ended prompts.\n\n- **Bing:** Uses a conversational UI; delivers a summarized answer; provides sources to each data provided.\n\n- **Grammarly:** Suggests improvements to spelling, grammar, fluency, style, and tone.\n\n- **ChatSonic:** Conversational AI tool provides updates and factual information aided by Google’s information network; accesses the internet to generate accurate data on current events and remembers conversations; has a built-in image generator.\n\n- **Jasper:** AI writing tools for companies that want to generate high-quality content, templates for blogs and social media; great for short sentences and ad copy.\n\n- **Replika:** AI chatbot or companionship; can hold meaningful conversations; has video call options and avatars.",
    "### Examples of Good Practice\n\nHere are some examples of how teachers can rethink assessment of learning to focus on higher-order cognitive skills, such as creating, evaluating, and analyzing: if an assignment is easily done by an automated response system, is it worth asking students to do it?\n\n**Important!\n\n** When using generative AI tools for assignment creation, be sure to have students check facts, provide references, and add acknowledgements.",
    "## Ethics\n\nNew artificial intelligence (AI) tools and large language models (LLM) (e.g., ChatGPT, Google Bard, Bing Chat) interact in a conversational way and have many uses, but also present ethical challenges.\n\nSome key issues for educators related to AI-generated information are:\n\n- Explainability and transparency since sources are not identified and accuracy cannot be confirmed.\n\n- Intellectual property issues due to unclear data ownership, authorship, and citation information.\n\n- Low reliability as inaccurate or biased data may be sourced or fantasy responses provided.\n\n- Misinformation based on examples and data provided.\n\n- Lack of accountability since the source of data is unclear.\n\n- Privacy and security concerns as data could be misused or in breach of privacy requirements.\n\n- Unethical labor practices—moderations workers are employed in the global south for under $2/hour.",
    "## Privacy and Technology\n\nThe need to maintain student privacy is essential.\n\nCheck technology signup requirements and terms of use.\n\nTo maintain privacy (and meet FIPPA requirements) ensure tools do not collect student data (e.g., phone number, email address, age).\n\nTools that do not meet privacy requirements must be voluntary and alternatives provided.",
    "## Using AI in Your Courses\n\nWe are not preparing learners for the past but for the future.\n\nBeing able to use AI tools effectively and understand their value and limitations is important to be technically and conceptually prepared for future life and workplace demands.\n\nPlan for the use of artificial intelligence.\n\nPrepare by creating a policy, acknowledgment requirements, and an assignment.\n\nFurther consider how to make assessments AI-proof and help learners build new skills related to AI use.\n\nIt is impractical to try to ignore, ban or eradicate AI.\n\nInstead, the preferred path is to work on developing assessment, teaching, and learning in ways that promote responsible engagement with AI.\n\nWhen used thoughtfully, AI tools can be used to support academic practices.\n\nThe use of generative AI tools highlights existing questions about how we can refine assessment concerning what matters to staff, students, and society more broadly.\n\nSome course learning outcomes and related activities may benefit from using these tools, while others may not.\n\nYou can prepare by considering your learning outcomes and, where appropriate, how to make assessments AI-proof or help learners build new skills related to AI use.\n\nGenerative AI tools will continue to impact and influence our teaching and learning practices.\n\nWhat we know about these tools and how we use them will likely change as development and access shift in the coming months and years.",
    "## Policy\n\nBe clear about expectations.\n\nBe explicit about what cheating is, with AI tools or not.\n\nIf it appears a student is submitting AI work, talk with them one-on-one (e.g., Did you use any sources or tools you did not cite or acknowledge?).\n\nIf you allow AI use, then be clear about the limits of such tools and how to credit AI-generated information.",
    "### Sample citation and acknowledgment\n\nAn APA style citation might look like this:\n\nReference page:  \nOpenAI.\n\n(2023, January 17).\n\n[ChatGPT response to a prompt about three prominent themes in BCorp businesses].\n\nIn-text citation:  \n(ChatGPT, personal communication, February 11, 2023)\n\nAcknowledgments might look like this:  \nI acknowledge the use of ChatGPT for the generation of three key ideas that I then researched to ensure accuracy and further developed.",
    "## Sample assignment\n\nConsider an assignment in which AI (e.g., ChatGPT) acts as a starting point and the student critiques and improves it.\n\nAsk learners to complete an assignment using an AI tool (e.g., ChatGPT).\n\nNext steps:\n\n- Critique the assignment and explain reasoning (e.g., well done because..., poor because...)\n- Optimize the assignment (e.g., fact check, references, add content, improve writing and format)\n- Grade the work and explain the grade (Mollick and Mollick, 2023)\n\nFor additional ideas on using an AI tool like ChatGPT in a variety of subjects, see Trust and Maloy 2023.",
    "## Generative AI Tools\n\n- Bisen, V. S. (2019, July 19).\n\nWhat AI techniques are used in chatbots: Explained with examples.\n\nMedium.\n\n- Lund, B. D., & and Ting, W. (2023, February 14).\n\nChatting about ChatGPT: How may AI and GPT impact academia and libraries?\n\nPreprint of a manuscript that has been accepted for publication by Library Hi Tech News.\n\nRetrieved from: Chatting about ChatGPT: How May AI and GPT Impact Academia and Libraries?\n\n| Semantic Scholar\n- Montessi, C. (2023, February 1).\n\n20 ChatGPT alternatives for your business.\n\nInvgate.\n\nRetrieved from \n- Sanchez, V. (2023, Feb 24).\n\nThe origin and evolution of Chat GPT: The natural language model that is changing the game.\n\nRetrieved from",
    "## Enhancing Teaching and Learning\n\n- Ferlazzo, L. (2023, January 18).\n\n19 ways to use ChatGPT in your classroom.\n\nEducation Week.\n\n- Wiley, D. (2023, January 23).\n\nAI, Instructional Design, and OER – improving learning.\n\nOpencontent.org.\n\n- Herfteducator.\n\n(n.d.).\n\nA teacher’s prompt guide to ChatGPT aligned with \"what works best\" [Google Docs].",
    "## Academic Integrity\n\n- Baker, R. (2023).\n\nFoundation model class policy, v1.0.\n\nCC BY-SA 3.0\n- Mollick, E., & Mollick, L. (2023, February 8).\n\nWhy all our classes suddenly became AI classes.\n\nHarvard Business Publishing Education.\n\nHbsp.harvard.edu.\n\n- Simon Fraser University (SFU).\n\n(2023).\n\nSyllabus statements.\n\n.\n\n- Trust, T., & Maloy, R. (2023, February 24).\n\nGuide to teaching and learning critical analysis with ChatGPT.\n\nSmartBrief.\n\n- Trust, T., & Maloy, R. (n.d.).\n\nCritical media literacy guides.\n\nCritical Media Literacy and Civic Learning.\n\nmediaandciviclearning/cmlguides#h2_xqej.",
    "## Sustainability\n\n- Crawford, K. (2021).\n\nAtlas of AI.\n\nYale University Press.\n\n- Nast, C. (2023, October 2).\n\nThe generative AI race has a dirty secret.\n\nWired UK.\n\n- Perrigo, B.\n\n(2023, January 18).\n\nExclusive: The $2 per hour workers who made ChatGPT safer.\n\nTime.\n\n---\n\nThis work was created by BCIT / Education Support and Innovation and is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0).",
    "## Background\n\nGenerative Artificial Intelligence (Generative AI) is a computer-based technology that creates a variety of data, such as pictures, videos, music, or words, which look or sound like they were created by a person.\n\nIt has been predicted that in the future, there will be a greater number of AI systems designed to facilitate creative cooperation between humans and AI.\n\nAt the University of Alabama, faculty have expressed interests in exploring and applying generative AI tools (such as ChatGPT, DALL.E, Midjourney, Stable Diffusion, and Codex) to collaborative teaching and learning.\n\nThis document aims at providing UA faculty with understanding the limitations of these tools, offering guidelines to faculty on using these tools in the classroom as they continue to evolve.",
    "## What are ChatGPT and other GPT-based software and tools?\n\nChatGPT is a large language model (LLM) based chatbot that was made available to the public on November 30, 2022.\n\nGPT is an abbreviation of Generative Pretrained Transformer, derived from a specific architecture of neural networks for natural language processing.\n\nIt has been gaining a lot of attention and is evolving rapidly.\n\nIt has already been integrated into Microsoft Office apps and other software.\n\nThis type of generative AI analyzes language structure by mimicking how humans comprehend text and then uses this knowledge to automatically create text and other content.\n\nThe abilities of the generative AI have been expanded from text to images, videos, computer codes, and other data formats, such as DALL.E, Midjourney, Stable Diffusion, and Codex.",
    "## What are the functions of GPT-based software and tools?\n\nThe functions of GPT-based software and tools are continuously evolving and spanning from text, vision, music, coding, and mathematics.\n\nThe recent trends have demonstrated interactions with the world, humans, and other internet AI modules and systems, such as AutoGPT and AgentGPT.",
    "### Next-word-prediction paradigm\n\nThe model operates on a next-word-prediction paradigm, which means it only generates the next word, and currently, it has no mechanism to revise or modify its previous output.\n\nSome of these limitations could be solved by providing specific prompts, which are called “prompt engineering.” However, the inherent flaw is not solved yet.",
    "### It generates errors without warning\n\nErroneous references, content, and statements may be intertwined with correct information and presented in a persuasive and confident manner, making their identification difficult without close inspection and effortful fact-checking.\n\nTherefore, it still requires extensive efforts to search and fact-check the generated content.\n\nYou cannot rely on ChatGPT-generated content for research, learning, and education.",
    "### Experiment and learn these AI tools\n\nFaculty may experiment with generative AI tools in conjunction with their course materials and assignments.\n\nTo begin, you can create a free account on Open AI's platform with ChatGPT.\n\nOnce registered, you can input some of your assignment prompts and evaluate the accuracy of the results.\n\nSubsequently, you should consider how you can integrate the tool or develop alternative approaches that do not require it.\n\nAdditionally, involving students in the reflection process can provide a valuable learning experience, enabling them to comprehend the advantages and limitations of these tools.",
    "### Explain these AI tools explicitly in your class\n\nTalk with students about these generative AI tools explicitly.\n\nInvite them to collaboratively consider and establish learning goals and criteria for the task, with consideration for the role of AI software.\n\nIt would help students to evaluate and judge appropriate contexts in which AI can work as a learning tool.\n\nThis communication will further seize the opportunity to center the importance of critical thinking and digital literacy among students.\n\nDiscuss academic integrity with students.\n\nYou should clarify the principles and guidelines for using generative AI tools in your courses.\n\nIn addition to verbal explanations, we recommend a syllabus statement on using AI.\n\nDiscuss the ethical issues and limitations of AI with your students.\n\nAs you experiment with the course materials and assignments using generative AI tools, you may engage in conversations with your students about the effects of inaccurate or biased information generated by these tools, particularly as it pertains to the course materials.\n\nDespite the likelihood that students will continue to utilize ChatGPT and similar tools, it is essential that our community shares a common understanding of the associated risks and benefits.",
    "### Change your assignments so that they may not be easily completed using AI\n\nCreate assignments that are not easily completed using AI.\n\nIt is highly recommended that you provide clear instructions to students regarding the proper citation of generative AI tools in their assignments and requiring them to provide references for all submissions.\n\nStudents should be required to explain how they utilized these tools in their work.\n\nTo enhance students' oral communication skills, you may provide more opportunities for in-class presentations or incorporating them into discussion sections.",
    "### Move to more authentic assessments and include performance elements\n\nYou may incorporate a formative assessment practice where students are required to submit drafts of their work for review and feedback.\n\nThese reviews and feedback could be considered as grading subcomponents of the assignments or tasks.\n\nThis approach not only helps to detect plagiarism but also guides students in the development of their work, enabling them to improve their performance.\n\nTeachers, peers, or self-assessments can provide feedback, which can facilitate the critical evaluation of work in progress.\n\nEncouraging peer- and self-feedback can enhance the authenticity of the assessment process.",
    "## References\n\n- UCLA Guidance for the Use of Generative AI.\n\n- NTU Guidance for Use of Generative AI Tools for Teaching and Learning.\n\n- MontClair State University, Practical Responses to ChatGPT and other Generative AI.\n\n- The ethics of generative AI in the classroom.\n\n- Northwestern Generative AI tools and the Impact on Teaching and Learning.\n\n- Stanford Generative AI Policy Guidance.\n\n- SAGE journals policy on ChatGPT and Generative AI.\n\n- The Conversation: ChatGPT and cheating: 5 ways to change how students are graded.",
    "## September 26, 2023\n\nDear Students,\n\nAs you settle into the fall semester, we want to share important reminders and guidance about the use of generative Artificial Intelligence (AI) in your coursework and research.\n\nThe information below was put together by experts across campus to help you as you assess the use of this technology in your academic studies and research.",
    "## Generative AI Reminders and Guidance\n\nAsk before you use AI and check your syllabus and assignment guidelines.\n\nMany instructors are providing specific guidance about appropriate use of generative AI in their courses.\n\nIf you do not know whether your use of generative AI is appropriate to the outcomes in your course or research activities, tell your instructor or supervisor what you are considering and ask for guidance.\n\nDo this well in advance of your deadlines, not after you have submitted your work.\n\nRemember that academic and research integrity are a core part of your experience as a student.\n\nDisclosing your research methods, using proper citations when incorporating other’s work, and contributing original intellectual work as an author are central tenets for ethical scholarly work.\n\nAs an author, you take full responsibility for your writing and the evidence supporting it, including attribution of sources such as generative AI.\n\nIt should be used as a resource to facilitate your learning, not a replacement for your education.\n\nJust like personal items, keep your data safe - and beware of scams.\n\nDo not provide personal information about yourself or others that is not public, and always follow MSU’s data guidance.\n\nIf you use AI, be sure to use reputable, recognized sources, and never share your credentials with a third party, no matter how legitimate they may seem.\n\nKnow the potential risks and limitations of AI outputs.\n\nResults might include misinformation, inaccuracies, bias, or inappropriate or unintentionally harmful content.\n\nThroughout the year, we will be engaging in thoughtful conversations with the MSU community and experts about the ethical use of these new technologies in ways that align with the university’s mission and values.\n\nWe will continue to share additional guidance and reminders as those conversations progress.\n\nWe look forward to learning and using these new technologies in scholarly, equitable, and ethical ways.\n\nSincerely,\n\nThomas D. Jeitschko, Ph.D. (he/him)  \nInterim Provost and Executive Vice President for Academic Affairs\n\nMark A. Largent, Ph.D. (he/him)  \nVice Provost and Dean of Undergraduate Education\n\nPero G. Dagbovie, Ph.D. (he/him)",
    "# Proposal of Usage Guidelines for AI Generative Tools at CU\n\nThese guidelines were created and reviewed by College Unbound students (S. Fast, K. Linder Bey, Veronica Machado, Erica Maddox, Suleima, Lora Roy) in Spring 2023 with the support of Lance Eaton, Director of Faculty Development & Innovation.\n\nFor more information about this project, these guidelines, etc, please contact Lance Eaton at .",
    "## Introduction\n\nThe guidelines proposed here reflect the goal to support the responsible use of artificial intelligence (AI) generative tools in alignment with College Unbound's mission to reinvent the higher education experience for underserved adult learners, using a student-driven model of rigorous and engaged scholarship.\n\nCollege Unbound is committed to the value and recognition of human thought and recognizing the complexities that such tools as AI generative tools might augment, enhance, and more concerningly, interfere or misrepresent our thought processes in ways still not fully understood.\n\nWe wish to encourage and support faculty and students’ free expression while also creating opportunities for them to leverage technological tools that will likely be part of their future experiences.\n\nAlso, we recognize that these tools–at least currently–come with a range of complications such as concerns about bias, privacy, environmental harm, cultural privileging, and human exploitation that problematize our usage of them.\n\nThese guidelines aim to provide guidance for students and faculty about the usage of AI generative tools that attempt to balance the aforementioned tensions.\n\nWe also want to emphasize transparency and accountability for both faculty and students in their usage of AI generative tools.\n\nThis applies both in the legal expectations of those terms as they relate to institutional, local, state and federal laws as well as to the importance of these values in how CU cares for and supports students.\n\nIn this way, these guidelines are structured to make it clear when, where, and how the use of AI generative tools are being used to help students and faculty to understand the depth and breadth of usage, which will also further inform subsequent guideline development.\n\nAt this document’s center is the goal of helping students and faculty responsibly and transparently indicate the use of AI generative tools and its degree of use in the brainstorming, developing, drafting, and finalizing of content provided by students and faculty.",
    "## Definition\n\n**AI Generative Tools:** We define “AI generative tools” as including (but not necessarily limited to): the use of technologies that rely on machine learning, large language models (LLMs) and other advanced data-manipulation tools to produce distinct answers or outputs based upon prompts by the user.\n\n**Usage:** Usage of AI generative tools includes engaging with such tools to generate specific content that contributes to the submission of any activity or assignment or work to be evaluated in a course or requisite for graduation (e.g.\n\nBig 10, LIPS), including but not limited to papers, presentations, discussion posts, etc, by students or by faculty, including but not limited to learning content, presentations, assessments, feedback, etc.",
    "## Student Guidelines\n\nEach instructor at College Unbound may have student usage policies that are different in terms of expectation and approach to using AI generative tools.\n\nAn instructor’s syllabus policy supersedes these guidelines in terms of appropriate usage, however an instructor cannot require students to create accounts with unaffiliated organizations for the purposes of any assignment.\n\nIn the absence of a specific course policy, these guidelines stand as the default expectation.\n\nIf students choose to use these tools in some capacity that results in content from the generative AI tool making it into submitted student work, they must make clear and evident what portion of the work is generated by the AI tool and which AI tool they used.\n\nWhen available, use the appropriate citation format (e.g.\n\nMLA or APA) as indicated by the instructor or syllabus.\n\nIn absence of a particular citation format, students should use quotation marks around the AI generated-text and include a Works Cited that includes both the tool that was used (e.g.\n\nBard, ChatGPT) and the prompts used.\n\nFor visual materials or audio materials, consult your instructor on how they would want them particularly documented.\n\nIn situations where students use generative AI as part of the brainstorming or organizing process, they are not obligated to cite or reference.\n\nStudents are discouraged from copying entirely the content directly from an AI-generative text tool into their course work.\n\nStudents should edit and revise the AI-generative tool’s output, unless there are significant reasons not to (e.g.\n\nthe instructor’s guidelines say otherwise).\n\nUse AI tools as a supplement to your learning, not as a replacement for traditional learning methods.\n\nWhenever possible, make sure the AI tool is accurately guiding your learning.\n\nWith regard to LIPS, Big 10s, and other reflective practices, students may use such tools as a resource for insight and further understanding, but their reflective submissions should be still created by the student entirely.\n\nStudents who do not adhere to these guidelines (or ones specified within a given course syllabus), will be subject to the process highlighted in the Student Handbook’s policy (Academic Honesty item #6 on pg 107).\n\nStudents must get documented permission by faculty before putting original faculty content into any AI-generative tool.\n\nThis might include communications, feedback, learning content, and the like.\n\nIf students choose to use these tools in some capacity related to their work, the submitted work should be less than 50% generated by the AI tool unless otherwise stipulated by the instructor or assignment guideline.\n\nStudents are responsible for the possible negative outcomes of using AI-generative tools in the submission of their work as they relate to College Unbound and its community.\n\nThese negative outcomes include but may not be limited to:\n\n- The accuracy of the content of an AI-generative tool.\n\nFor example, ChatGPT has been known to provide sources that do not exist or links that do not work.\n\nIntegrating these into one’s work fails to meet the standards of appropriately identifying one’s sources of influence in a given work.\n\n- Usage that results in inappropriate harm to the wellbeing of others–individually or collectively.\n\nFor example, using material generated by generative AI that results in reinforcing stereotypes (in written, oral, and visual mediums) for assignments or materials related to one’s work at CU.\n\n- Usage that violates the privacy or security of other individuals.\n\nFor example, students should not enter names and personal information or writing of other people (students, faculty, staff, etc) to produce an output for any work or activity related to their role at CU.\n\n- Usage that undermines the academic integrity of assessments, exams, or others evaluations at College Unbound.\n\nFor example, putting students’ work into a generative AI tool for the purposes of checking for plagiarism or generating feedback (without students’ permission).\n\n- Any other usage that violates CU’s policies.\n\nIf you are looking to use generative AI in connection with your learning at CU, please consider reaching out to the Director of Digital Pedagogy, if you have questions or concerns about how you are using it and the policy above.",
    "## Faculty Guidelines\n\nFaculty may develop their own usage expectations within their courses that are different from this document.\n\nHowever, they would still need adhere to Item #3 in the Faculty Guidelines and follow the process highlighted in the Student Handbook’s policy (Academic Honesty item #6 on pg 107) for students that do not follow expectations.\n\nWhenever faculty use generative AI tools to produce anything related to teaching and assessment, faculty must make it evident how much of the content was created by them and how much was generated by the AI-generative tool.\n\nFor example, for written work, faculty should use the discipline’s preferred citation format (e.g.\n\nAPA, MLA) to cite AI generated text or some other method to distinguish it such as using boldface, italics, or highlighting the AI generated text and explaining that the text comes from a specific generative AI tool.\n\nFor visual materials or audio materials, faculty should include disclaimers about the role of AI-generative tools used to create such material–either as a preamble to the content or embedded within the content.\n\nFaculty cannot require students to get an account with any AI-generative tools at this time.\n\nIf looking for possible opportunities or practices for students to use AI-Generative Tools, please contact Lance Eaton, Director of Digital Pedagogy.\n\nFaculty must get documented permission by students before putting original student content into any AI-generative tool.\n\nFailure to do so may be subject to dismissal or other disciplinary actions.\n\nFaculty should be mindful of using these tools and keep a relational balance between what they ask of students in terms of how much AI-generative content can show up in student work and in their own work.\n\nFor example, if students are restricted from submitting work that includes more than 25% of AI-generative work in their submissions, then the faculty member’s work should also not include more than 25% AI-generated content.\n\nFaculty can use AI Generative Tools in the following ways:\n\n- **Learning Materials, Evaluation Materials, & Class Preparation:** Faculty can use these tools to help create content for their courses whether it is learning materials, assignment guidelines, slides, conversation questions, activities, etc.\n\n- **Classroom Demonstrations & Learning Activities:** Faculty can use these tools as part of classroom demonstrations and learning activities where the instructor and student can engage with the AI-Generative Tool for discovery and/or critique purposes.\n\n- **Feedback:** Faculty can use AI-Generative Tools to create effective feedback for students.\n\nHowever, they are not allowed to put student-created work into AI-Generative Tools in order to create effective feedback without explicit permission from students.\n\nFaculty are responsible for the possible negative outcomes of using AI-generative tools for any purpose related to their work at College Unbound and its community and may be subject to disciplinary action.\n\nThese negative outcomes include but may not be limited to:\n\n- The accuracy of the content of an AI-generative tool.\n\nFor example, ChatGPT has been known to provide sources that do not exist or links that do not work.\n\nIntegrating these into one’s work fails to meet the standards of appropriately identify one’s sources of influence in a given work.\n\n- Usage that results in inappropriate harm to the wellbeing of others–individually or collectively.\n\nFor example, using material generated by generative AI that results in reinforcing stereotypes (in written, oral, and visual mediums) for the purposes of teaching and learning.\n\n- Usage that violates the privacy or security of other individuals.\n\nFor example, faculty should not create a chat thread on ChatGPT for each student, where they update ChatGPT about how the student is doing and requests feedback/strategies about that student.\n\n- Usage that undermines the academic integrity of assessments, exams, or others evaluations at College Unbound.\n\nFor example, putting students’ work into a generative AI tool for the purposes of checking for plagiarism or generating feedback (without students’ permission).\n\n- Any other usage that violates CU’s policies.\n\nIf you are looking to use generative AI in connection with your course, it is recommended to reach out and talk with the Director of Digital Pedagogy to double check different assumptions about usage.",
    "## A Final Note\n\nA final note about these policies: Because this technology is both new and continuing to change, we recognize that these policies cannot and will not cover every situation.\n\nShould issues and edge cases arise which fit outside these guidelines, CU representatives will operate from a position of good faith in trying to address each case, which will also be used to help us refine these policies.",
    "## Guidance for Instructors\n\nLast modified: March 27, 2024\n\nDisclaimer: The field of generative AI and opinions about how to react to these tools in academia are evolving rapidly.\n\nAs an organization, LILE is working to keep up to date with this issue in order to provide robust and timely advice for faculty.\n\nWatch this page, as well as our events and blog, to learn more.",
    "### Quick Links\n\n- Looking for an AI policy?\n\nWe have written an expanded guide to writing artificial intelligence policies.\n\n- Ready to incorporate AI in your class?\n\nOur resource on the design of AI assignments has examples and considerations to get you started.\n\n- Missed one of our AI workshops?\n\nWatch the recordings here:\n  - AI Fundamentals: A Beginner’s Guide to Generative Artificial\n  Intelligence (February 23, 2024)\n  - Talking to Students about Generative AI: Leaning into AI Literacy (March 8, 2024)\n\nJump to:\n- Impact of AI on Education\n- Shortcomings of AI\n- Opportunities AI Might Provide\n- Speaking to Students About AI\n- Recommendations for Course Policies\n- AI Detection Software",
    "## Artificial Intelligence Tools\n\nArtificial Intelligence (AI) is any technology that attempts to solve problems and complete tasks that would usually require human intelligence.\n\nFor decades there have been increasing advances in AI software and hardware, including tools many of us use every day such as GPS, Alexa, or speech to text.\n\nThe most visible tools at the moment are generative AI models such as ChatGPT, Bing Copilot, Poe, and DALL-E. For some beginning guidance on trying these tools, watch our workshop on how to use ChatGPT and Midjourney (image generator).\n\nThese tools can generate written responses, images, and code to a wide variety of user prompts and questions.\n\nTheir power comes from machine learning models that are able to predict content to prompts based on a large amount of data they have been trained on.\n\nGenerative AI tools such as ChatGPT that produce written content utilize a natural language model that allows users to speak with the AI in a conversational manner.\n\nSome tasks that ChatGPT excels at are summarizing information, generating computer code, translating text from one language to another, analyzing data, and writing in specific genres and voice.\n\nExamples of prompts might be “Write a three-page paper on the effects of climate change” or “I am looking to buy a camera.\n\nDo you have suggestions?” or “Review my writing for errors.” The results of these prompts can be impressive, but it is important to note that generative AI is not technically thinking or emoting and lacks important human skills such as critical thinking and fact-checking.\n\nYou should not rely solely on AI to produce factual information.",
    "## Impact on Education\n\nGenerative AI will continue to grow in capabilities and it is already being incorporated into word processing and search engines.\n\nThe challenge for instructors is to discover how to incorporate Artificial Intelligence content generators as a tool in their teaching rather than view them solely as a threat.\n\nIn the past, other technology tools such as multi-function calculators, spelling and grammar checkers, and statistical analysis software shifted the ways we learn and teach.\n\nAs in those cases, educators will need to help students differentiate when AI can help with learning versus when it is a shortcut around learning.\n\nIt must be acknowledged that faculty’s bandwidth to address the emergence of AI is limited.\n\nLearning about AI and how it will impact teaching will take time, especially as it is a new trend.\n\nThis is likely a multi-year shift in education and as such does not need to be tackled all at once.\n\nSeveral steps instructors can take:\n\n- Try AI content generators and understand their capabilities.\n\nLearning the basics of how to write effective prompts is key to maximize the content produced by generative AI.\n\nYou may also want to take time to understand what happens on the back end of any AI tool and what data the tool is pulling from.\n\n- Update course policies to include considerations for using AI content.\n\nWhile you may not choose to integrate AI into your course, it is important not to ignore these technologies.\n\nHere are a few questions to ask yourself: First, do the values of the technologies align with your course values?\n\nSecond, what do you want to communicate about these technologies to your students?\n\nFinally, what are the intellectual gains and guidelines if you do allow students to use AI?\n\nReview our further considerations for developing AI policies for more guidance.\n\n- Learn about the ethical and legal questions surrounding generative AI.\n\nUnderstanding the risks will help you launch discussions in class that encourage students to understand their own values in relation to AI and mitigate ethical harm if you implement AI activities in your course.\n\n- Explore ways to change assignments to address AI concerns.\n\nConsider adding assignments that educate students about the strengths and limitations of AI and how it relates to your discipline.",
    "### Open Dialogue\n\nInitiating a direct conversation with students about the use of AI is an opportunity to explain its impact on their education.\n\nTogether, instructors and students can explore the ways in which AI can support their learning positively.\n\nThese conversations should also touch on what they may lose intellectually if they use AI to complete their assignments.\n\nStudents should understand that learning is difficult and challenging, but that is the point of education.",
    "### Limitations of AI\n\nStudents should be made aware of the ethical shortcomings of AI content to help them understand why passing it off as original content is not advisable.\n\nIf students are allowed to use AI, they need to understand how AI content must be reviewed and verified before incorporating it into their own writing.",
    "### Help Students Learn AI\n\nStudents need to understand how generative AI works and the data behind it.\n\nThey should also learn how to write effective prompts.\n\nThe reality is it will be a part of their careers and everyday life so they should have the skills to use it correctly.",
    "## Recommendations for Course Policies\n\nWe suggest that faculty clarify their expectations regarding the use of AI at the outset of their course.\n\nInstructors have discretion in setting specific AI policies to fit their course and individual assignments.\n\nThere is no one-size-fits-all policy.\n\nBelow are some general principles and we have written an expanded guide to writing artificial intelligence policies.",
    "### Plagiarism\n\nUpdated on October 30, 2023 for clarity.\n\nInstructors should update their academic integrity policy to include guidance on the use of generative AI content and plagiarism.\n\nSample syllabus language might be “Contributions from anyone or anything else in your writing— including AI sources—must be properly quoted and cited every time they are used.” Whether AI is banned outright or acceptable in some cases, define the consequences for plagiarism in your course.",
    "### Cheating\n\nUpdated on October 30, 2023 to reflect the changes to the Duke Community Standard.\n\nThe view of cheating will differ across classes.\n\nFor example, you might allow the use of AI for generating early ideas and drafts.\n\nOther instructors are comfortable with any use of AI with proper citations.\n\nSome instructors do not limit the use of AI at all.\n\nIf you intend to ban AI, sample language could be “Because this course requires you to use your own critical thinking to solve problems and practice skills, do not use generative AI.” To support instructors that limit or ban AI use, the Duke Community Standard has been updated to include the unauthorized use of generative AI as a form of cheating.",
    "### Attribution\n\nIf you allow AI, point students to proper citation rules.\n\nStudents should understand how to cite or give credit to AI content generators.\n\nThey can list ChatGPT as a reference work or quote it within their work.\n\nMLA, APA, and other style guides have developed specific guidelines.\n\nInstructors can also ask students to retain original conversations and prompts and append them to writing assignments.",
    "### Acceptable Use\n\nIf students may consult AI, it is important to give students specific guidelines of what is acceptable or not.\n\nSample language might be “Students are allowed to use AI to help revise this draft assignment, but they may not consult AI to write the final paper after I grade the draft.\n\nWhen submitting the draft, students must clearly identify any writing, text, or media generated by AI.”",
    "## AI Detection Software\n\nLearning Innovation does not endorse any software or programs that claim to determine if a student’s writing was produced by AI.\n\nIf instructors use detection software, it should not be considered a definitive measure of cheating, but rather the starting point of a conversation with the student about potential plagiarism.",
    "### False Positives and Negatives\n\nThese are early days for detection software and they are by no means foolproof when detecting AI text.\n\nIn the long term, relying on these tools will not be an effective strategy due to advances in AI.\n\nThe software can be biased; for example, a Stanford study showed that non-native speakers’ writing was flagged as AI-generated more than native speakers.",
    "## What’s Next?\n\nInterested in further exploration of the impact of AI on teaching?\n\nHere are a few in-depth articles to explore.\n\n- Consider this extensive guide to AI that explores AI’s institutional impact.\n\n- Review this research-based guide on the negative and positive impact of generative AI on teaching.",
    "## Footnotes\n\n1.\n\nFor an introduction to AI and its programming models, refer to this piece written specifically for educators.\n\n2.\n\nThis policy language, plus many other examples, are being collected as part of a collaborative project.\n\n3.\n\nThis policy wording, plus other student guidelines, come from Joel Gladd, College of Western Idaho, under creative commons licensing.",
    "## Recent Blog Posts on AI\n\n- Summit Reflection: Generative AI in Duke Courses\n- ‘Making AI Generative’ at Duke and Beyond with Ithaka S+R\n- Artificial Intelligence and Assignment Design\n- Embracing ChatGPT in the Financial Technology Classroom\n- Some Thoughts on AI, Plagiarism and Student Assessment\n- ChatGPT Answers Some Burning Questions About AI In Higher Ed",
    "## AI in Duke Classrooms\n\n- We’re playing the role of King Thamus (Mark DeLong | Nov. 2023)\n- Adapting to AI in the Classroom (Pratt, I/O Magazine | Oct. 2023)\n- Navigating AI at Duke (Duke Today | May 2023)\n- Embracing ChatGPT in the FinTech Classroom (Learning Innovation blog | May 2023)\n- Emerging Pedagogies Symposium speaker discusses impacts, limitations of AI in the classroom (Duke Chronicle | May 2023)",
    "## Introduction and background\n\nGenerative artificial intelligence (AI) is an AI model capable of generating text, images, code, video, and audio.\n\nLarge Language Models (LLMs) such as ChatGPT and Copilot produce text from large datasets in response to text prompts.\n\nRecent advances in AI models and user interfaces, and the need for greater public understanding of AI (Selwyn & Gallo Cordoba, 2021), have prompted a need for shared guidelines to assist higher education providers manage the practical, ethical, and policy implications of AI models.\n\nThe benefits of generative AI to education include automatic generation of outlines and summaries, support for personalised learning, and writing feedback.\n\nSome challenges posed by using generative AI in higher education include:\n- the authentication of individual attainment for accreditation purposes\n- potential challenges to the principles of academic integrity\n- the need for sustainable and adaptable responses to generative AI in learning, teaching and assessment and academic integrity policies and procedures\n- support for staff and students in understanding and using the technology.\n\nThis document provides guidance on the appropriate use of generative AI in higher education aligned with the Higher Education Standards Framework (Threshold Standards) 2021 (HESF).\n\nThe HESF states that providers assure the quality of teaching, learning, research, and research training, with respect to content and skills developed, assessment and determination of learning outcomes, and the mitigation of foreseeable risks to academic and research integrity.\n\nProviders should document their decisions and monitor their progress in addressing generative AI.\n\nIndividual institutions are developing their own resources aligned to their specific contexts.\n\nA working group drawn from the Australian Academic Integrity Network (AAIN) (Appendix 1) has established this set of guidelines.\n\nThe intention of the guidelines is to support existing and new guidance for students, teaching staff, professional staff, university administrators and decision-makers.\n\nNB: These Guidelines are a snapshot in time, created by a working group of the AAIN.\n\nAny derivative guidelines need updating as the area of generative AI continues to change and develop.",
    "## Guidelines for students\n\nAccording to advice to students from the Tertiary Education Quality and Standard Agency (TEQSA), ‘it’s important to understand that, depending on your university or college’s policies, using AI as part of your studies may be restricted or banned.\n\nAlternatively, there might be subjects or tasks where the use of AI is encouraged or even required’ (TEQSA, 2023a).\n\nStudents need to develop AI literacy skills, in addition to traditional information literacy skills (Bundy, 2004) and generic digital literacy skills.\n\nAI literacy skills enable ‘individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace’ (Long & Magerko, 2020:598).\n\nStudents should use AI models in ethical and responsible ways that are consistent with their institution’s learning, assessment and academic integrity policies and procedures, and the terms of use of the AI providers.\n\nEthical use of generative AI includes an obligation to follow institutional guidelines regarding the use of generative AI in any unit or course, and an understanding that it may not be appropriate to use generative AI in all circumstances.\n\nStudents should follow their institutional guidelines.\n\nStudents should check any output from generative AI against reliable sources of information and understand that they will be responsible for any errors or omissions in material generated by AI.\n\nStudents are required to identify AI models, tools and/or prompts that are appropriate for their discipline and acknowledge the use of AI in written assessments following any guidelines provided by their institution.\n\nIf it is not possible to identify and cite the original sources used in output from AI, this may result in plagiarism and academic misconduct.\n\nStudents also need to be aware of the possibility of “hallucinated references” or the tendency of generative AI language models to make up references from constituent parts of actual references.\n\nStudents should acknowledge the use of generative AI language models in assessment tasks, following any guidelines provided by their institution.\n\nThey should describe the way they have used the tool and integrated the results into their work, as appropriate to the specific guidelines within their discipline, unit or course.\n\nThe unauthorised use of AI language models or paraphrasing tools may be a form of cheating and may result in academic misconduct.\n\nWork submitted (including work generated by AI), and not cited or referenced, must be your own original work.\n\nStudents need to be aware that using the output from AI models without appropriate acknowledgement may constitute academic misconduct.\n\nIf unsure, students should confirm assessment requirements with teaching staff or seek advice on how to acknowledge the output from AI from academic support services, such as their library or other academic services.\n\nWhere appropriate, students should familiarise themselves with any relevant expectations of or constraints on the use of generative AI related to their future professional accreditation and be aware that these may be updated.",
    "## Guidelines for teaching staff\n\nAll students will need to develop capabilities in the ethical use of generative AI relevant to their discipline and future professional practice through ethical engagement with generative AI tools in learning and teaching activities and assessment.\n\nExisting and likely future uses of AI in professional contexts and in platforms such as Microsoft Office and search engines need to be considered when developing unit or course learning outcomes, activities, and assessment.\n\nExpectations regarding the appropriate use of generative AI in assessment tasks and learning activities should be consistent with institutional guidelines and require clear communication to students.\n\nThis includes clear instructions in student facing documentation, for example, in learning guides and through the learning management system.\n\nExpectations should align with AI provider terms of use and with curriculum requirements.\n\nTo ensure procedural fairness, it is important to communicate to students any inappropriate uses of generative AI that may result in academic misconduct.\n\nStudents should be aware of the potential for detection software (e.g., Turnitin) to detect generative AI use and that they are risking academic misconduct if using generative AI without appropriate acknowledgement by following the referencing guidelines provided by their institution.\n\nHaving conversations with students early in units and courses will improve a shared understanding of how and when they can use AI tools.\n\nStudents will benefit from examples of how and when generative AI have been used and acknowledged, and which tools to use.\n\nStudents should be aware of the limits of generative AI.\n\nLimitations of generative AI include biased or negative responses due to interaction with a “raw model”.\n\nAnother limitation is the potential for “AI hallucinations” which results when the system provides a response that is not factual.\n\nThis may be due to inadequate training of the model or the system’s inability to interpret specific data.\n\nA further limitation is that of currency; the responses given by the generative AI model will only be as up to date as the information in its training data.\n\nStudents should have opportunities to develop AI literacy.\n\nMany providers will be developing tools and resources to assist students to learn to use generative AI in ways that are appropriate to their institutional context.\n\nStudents should be aware of resources developed in their institution and across the sector.\n\nAs an example, the University of Queensland Library Digital Essentials module is free to use and adapt under Creative Commons (with attribution).\n\nAI tools may be used within institutional guidelines to support learning and assessment design, e.g., to generate assessments, feedback forms and exams.\n\nHowever, critical evaluation of generative AI output is required to ensure appropriateness against learning outcomes.\n\nUnit and course learning outcomes, assessment tasks and marking criteria may require review to incorporate the ethical use of generative AI, or to indicate when not permitted, following any institutional expectations or guidelines.\n\nAny revisions should align with institutional requirements and any accreditation requirements.\n\nIn designing formative and summative assessment tasks, teaching staff should consider the capabilities of generative AI.\n\nFor example, assessment tasks that award marks for summarising a topic area and online quizzes may no longer be useful measures of student achievement and new approaches may be required to promote creativity and originality.",
    "## Guidelines for academic support staff and misconduct teams\n\nLibrary staff and academic/learning advisors support students to develop academic skills and academic integrity by:\n- promoting the ethical and responsible use of AI tools in academic writing and research\n- providing guidance on correct attribution and acknowledgement conventions to be used when incorporating generative AI outputs\n- providing advice and training to enable and enhance clients’ effective use of AI tools\n- providing information and links to approved AI tools that are available to clients.\n\nGovernance offices need to update and maintain policies and procedures to facilitate the investigation of potential academic misconduct.\n\nThis includes but is not limited to ensuring definitions of types of misconduct are up to date and consider contemporary forms of misconduct and ensuring that policies are updated outside regular review cycles if needed (TEQSA, 2023b).\n\nTraining provided to staff investigating alleged misconduct to maintain current, relevant knowledge on changes in policies as well as trends in permitted use of AI tools.\n\nStaff involved in processing academic misconduct relating to the inappropriate use of generative AI models need to consider how they might include early educative interventions in institutional policies and procedures, prior to imposing punitive measures.",
    "## Guidelines for higher education providers\n\nPolicies and procedures should provide guidance on the ethical use of generative AI, accommodating diverse approaches across disciplines.\n\nChanges should be communicated to staff and students.\n\nGenerative AI policies and procedures should be treated as live documents, reviewed and updated regularly as the technology continues to shift and evolve.\n\nProviders need to consider and address any ongoing resource demands associated with adapting to generative AI.\n\nUniversity administrators and managers should engage with external partners, such as community members, members of relevant professions and professional bodies and other partners, to facilitate open communication on research, teaching and other engagement with generative AI.\n\nProviders need to learn from and address partner concerns and mitigate potential risks including risks to institutional reputation.\n\nAI applications that are required for units and courses should made available by providers at no additional cost to students, to ensure equitable access.\n\nLearning and teaching support units should develop, maintain, and update relevant professional learning resources to support academic staff in the use of generative AI in learning, teaching and assessment.\n\nThese programs need to include implications of existing and future likely uses of AI in professional contexts for learning, teaching and assessment.",
    "## Glossary\n\nAI hallucination is the result when a generative AI system provides a response that is not factual.\n\nArtificial intelligence (AI) is ‘the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings’ (Copeland, 2023).\n\nArtificial intelligence literacy is ‘a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace’ (Long & Magerko, 2020, p.2).\n\nChatGPT (Generative Pre-trained Transformer) is a large language model developed by OpenAI, designed to engage in natural language conversations with users.\n\nIt uses deep learning algorithms to understand and respond to a wide variety of questions and prompts in a conversational manner (OpenAI ChatGPT, personal communication, 9 March 2023).\n\nHallucinated references are fake references that are generated by generative AI language models, and that are made up of constituent parts (e.g., authors, journal titles, etc.)\n\ntaken from actual references.\n\nLarge Language Models (LLMs) produce text from large datasets in response to text prompts.",
    "## References\n\nBundy, A.\n\n(2004).\n\nAustralian and New Zealand Information Literacy Framework: Principles, Standards and Practice (Second Edition).\n\nAustralian and New Zealand Institute for Information Literacy (ANZIIL) and Council of Australian University Librarians (CAUL).\n\nCopeland, B.J.\n\n(2023).\n\nArtificial intelligence.\n\nIn Encyclopedia Britannica, \n\nLong, D. & Magerko, B.\n\n(2020).\n\nWhat is AI literacy?\n\nCompetencies and design considerations.\n\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems, Honolulu, Hi.\n\nUSA.\n\nSelwyn, N., Gallo Cordoba, B.\n\n(2021) Australian public understandings of artificial intelligence.\n\nAI & Society, 37, 1645–1662 (2022).\n\nTEQSA (28 Feb 2023a).\n\nArtificial intelligence: advice for students.\n\nTertiary Education and Quality Standards Agency.\n\nTEQSA (2023b).\n\nSector update: Maintaining up to date academic integrity policies and procedures.\n\nTertiary Education and Quality Standards Agency.",
    "## Introduction\n\nThe pace of development of artificial intelligence tools is rapid.\n\nAs a business, we want to make sure that all those working for us have a clear understanding of what these tools are and when and how their use will be permitted in the workplace.\n\nOur aim, in putting together this policy, is to set out clear guidelines and rules for the use of artificial intelligence tools in the business.\n\nWe aim to maximize the benefits of Generative Artificial Intelligence (Generative AI) while reducing risks and ethical concerns.\n\nThis policy does not form part of any contract with us.\n\nWe reserve the right to amend or remove it.\n\nThis policy applies to anyone who works for us.\n\nThis includes employees, workers, contractors, agency workers, casual workers, interns, and volunteers.",
    "## What is Generative AI?\n\nGenerative AI applications are algorithms (such as ChatGPT) that can be used to create new content, including audio, code, images, text, simulations, and videos.\n\nArtificial intelligence (AI) is pretty much just what it sounds like—the practice of getting machines to mimic human intelligence to perform tasks.\n\nVoice assistants like Siri and Alexa are founded on AI technology, as are customer service chatbots.\n\nGenerative AI is a type of artificial intelligence.\n\nThrough machine learning, artificial intelligence models can ‘learn’ from data patterns without human direction.\n\nGenerative AI in its current form generally uses prompts to create content.",
    "## Generative AI applications which are allowed at work\n\nA list of the Generative AI applications which are currently permitted to be accessed and used for work-related purposes (in accordance with the terms of this policy) can be found [insert location eg on the intranet].\n\nThis list is amended regularly, and applications may be removed at any time.\n\nIt is your responsibility to check in advance of using any Generative AI in the workplace that your chosen application remains on the permitted list.\n\nReferences to Generative AI in the remainder of this policy should be treated as being limited to those applications contained, from time to time, on the permitted list.\n\nPlease contact [the IT department] with any concerns or comments you may have regarding the use of any of the listed Generative AI applications.\n\nIf you become aware of an application which you may wish to use but which is not on the current permitted list, then please contact [the IT department] to discuss.",
    "## Use which is permitted\n\nEmployees and contractors are allowed to use Generative AI for the following tasks:\n- Drafting and editing documents\n- Producing attractive PowerPoint presentations, slides and charts\n- Brainstorming ideas\n- Research assistance\n- Analyzing data\n- Automating repetitive tasks\n\nprovided that any information gathered or content generated is only generated, reviewed, and used in accordance with the rules as set out in this policy.\n\nUse of Generative AI should at all times be in accordance with our [Computer, Email and Internet Policy] [insert any other relevant policies] [and] [Data Protection Policy].",
    "## Use which is not permitted\n\nDo not use Generative AI for non-work purposes when using our IT and communications systems.\n\nDo not use Generative AI to write responses to emails or communications, other than as a very preliminary draft which will need to be carefully tone- and fact-checked.\n\nDo not use Generative AI for internal company reports, except as a contributing source to your research.\n\nGenerative AI must not be used to create or modify content produced at any stage of the appraisal process.\n\nGenerative AI must not be used to create or modify any written content applicable to recruitment, including job or promotion applications.",
    "## Risks and Dangers\n\nThis policy aims to reduce the risks associated with the use of Generative AI applications at work.\n\nYou should be aware of the following potential risk areas:\n\nGenerative AI tools might generate content based on biased or outdated data.\n\nOver-reliance on Generative AI tools might result in impersonal or insensitive communications, which could harm relationships with recipients (including colleagues and clients).\n\nThere is a risk that use of Generative AI tools may restrict workplace learning – if users do not take the time to make sure that they have understood the generated content before using it or passing it on.\n\nUsing Generative AI tools to process sensitive or personal information may increase the risk of data breaches or privacy violations.\n\nGenerative AI-generated content might inadvertently infringe on copyrights or trademarks.\n\nWhere Generative AI applications have been used to create content this could raise ethical questions about transparency, fairness, and accountability.",
    "## Controlling the quality and appropriateness of generated content\n\nGenerative AI creates content by using large amounts of data available to it.\n\nHowever, it does not fact-check and has been shown to be vulnerable both to factual errors and discrimination/bias.\n\nIf you use Generative AI at work to create any form of content then it remains your responsibility to fact-check that content to make sure that it is accurate.\n\nYou can do this by cross-checking with trusted sources and seeking input from colleagues.\n\nYou should at all times make sure that you fully understand any content which has been created using Generative AI tools.\n\nYou need to be in a position to take full ownership and responsibility for the content before it is used.\n\nYou are at all times responsible for verifying the appropriateness of content before using it.\n\nYou should identify any biased or discriminatory content and remove it.\n\nIt is important that communications drafted using Generative AI applications are carefully tone-checked to make sure that they are pitched appropriately and reflect the correct tone and degree of formality.\n\nYou should maintain a balance between Generative AI-assisted work and human input to preserve the personal touch in communications and decision-making.\n\nFailure to carry out these necessary quality control steps could lead to harm to the business and may result in disciplinary action being taken against you.",
    "## Intellectual Property issues\n\nThere are intellectual property issues associated with Generative AI.\n\nThe pace at which development is occurring in this area means that many are not yet resolved.\n\nFor this reason, we require you to take a cautious approach in relation to the use of any content created using Generative AI.\n\nIf you are producing content for external publication (including sending to customers, clients, or any other third party) and are using Generative AI to assist with its creation then you should seek the approval of [your line manager] OR [insert relevant person] before any publication or transmission so that any intellectual property risks can be properly assessed.\n\nWe may, on occasion, require you to identify Generative AI content when such content is made public or disclosed to clients or other third parties.",
    "## Monitoring and record-keeping\n\nWe reserve the right to monitor your use of Generative AI applications on our IT and Communications systems.\n\nAny monitoring will at all times be in accordance with our [Data Protection Policy].\n\nYou must keep appropriate records whenever you create content using Generative AI.\n\nThis should include the date when the content was generated and the prompt used to generate it.",
    "## Confidentiality and Privacy\n\nGenerative AI applications are external resources.\n\nThis means that our control over information we input is limited.\n\nFor this reason, you must avoid inputting sensitive, confidential, or proprietary information, as well as personal information of employees, clients, or associates into any Generative AI.\n\nIt should be used for general, anonymized purposes only.\n\nWe are not able to guarantee the privacy or data security of information inputted into Generative AI.\n\nYou should act on the basis that anything that you input could potentially be seen by others, far outside our organization.",
    "## Useful links and contacts\n\nThe following internal policies are referred to in this policy and contain additional information and guidance [amend as appropriate]:\n\n- [Disciplinary Policy]\n- [Computer, Email and Internet Policy]\n- [Data Protection Policy] [and]\n- [insert details of any other relevant policies here]\n\n[Our IT Department can be contacted at any time in relation to the matters detailed in this policy on [insert email address]].",
    "#### Policy\n\nACCEPTABLE AND UNACCEPTABLE USE OF AI  \n[This syllabus statement is useful when you are allowing the use of AI tools for certain purposes, but not for others.\n\nAdjust this statement to reflect your particular parameters of acceptable use.\n\nThe following is an example.]\n\nThe use of generative AI tools (e.g.\n\nChatGPT, Dall-e, etc.)\n\nis permitted in this course for the following activities:  \n- Brainstorming and refining your ideas  \n- Fine tuning your research questions  \n- Finding information on your topic  \n- Drafting an outline to organize your thoughts  \n- Checking grammar and style  \n\nThe use of generative AI tools is not permitted in this course for the following activities:  \n- Impersonating you in classroom contexts, such as by using the tool to compose discussion board prompts assigned to you or content that you put into a Zoom chat  \n- Completing group work that your group has assigned to you, unless it is mutually agreed upon that you may utilize the tool  \n- Writing a draft of a writing assignment  \n- Writing entire sentences, paragraphs, or papers to complete class assignments  \n\nYou are responsible for the information you submit based on an AI query (for instance, that it does not violate intellectual property laws, or contain misinformation or unethical content).\n\nYour use of AI tools must be properly documented and cited in order to stay within university policies on academic honesty.\n\nFor example, [Insert citation style for your discipline.\n\nSee these resources for APA guidance, and for other citation formats.].\n\nAny assignment that is found to have used generative AI tools in unauthorized ways [insert the penalty here*].\n\nWhen in doubt about permitted usage, please ask for clarification.\n\nUSE ENCOURAGED AND PERMITTED  \n[This syllabus statement is useful when you are allowing, and perhaps encouraging, broad use of generative AI tools.]\n\nYou are welcome/expected to use generative AI tools (e.g.\n\nChatGPT, Dall-e, etc.)\n\nin this class as doing so aligns with the course learning goal [insert the course learning goal use of AI aligns with].\n\nYou are responsible for the information you submit based on an AI query (for instance, that it does not violate intellectual property laws, or contain misinformation or unethical content).\n\nYour use of AI tools must be properly documented and cited in order to stay within university policies on academic honesty.\n\nFor example, [Insert citation style for your discipline.\n\nSee these resources for APA guidance, and for other citation formats.]\n\nUSE PROHIBITED  \n[This syllabus statement is useful when you are forbidding all use of generative AI tools for any purpose in your class.]\n\nThe use of generative AI tools (such as ChatGPT, DALL-E, etc.)\n\nare not permitted in this class; therefore, any use of AI tools for work in this class may be considered a violation of Temple University’s Academic Honesty policy and Student Conduct Code, since the work is not your own.\n\nThe use of unauthorized AI tools will result in [insert the penalty here*].\n\n* We highly recommend that you have conversations in your department about the appropriate penalties for unauthorized use of an AI.\n\nIt is important to think about the appropriate level of penalty for first-time offenders and those who repeatedly violate your policies on the use of AI.\n\nSource / Other Info  \nCenter for the Advancement of Teaching | Sample Syllabus Statements (PDF)  \n\n---",
    "#### Policy\n\nThe beta release of Dall-E-Mini in July 2022 and ChatGPT in November 2022 are among many tools using artificial intelligence.\n\nThere is a good possibility that using tools like these are going to become an important skill for careers in the not distant future.\n\nIn the meantime though, it's going to take a while for society to figure out when using these tools is/isn't acceptable.\n\nThere are three reasons why:  \n- Work created by AI tools may not be considered original work and instead, considered automated plagiarism.\n\nIt is derived from previously created texts from other sources that the models were trained on, yet doesn't cite sources.\n\n- AI models have built-in biases (i.e.\n\nthey are trained on limited underlying sources; they reproduce, rather than challenge, errors in the sources).\n\n- AI tools have limitations (i.e.\n\nthey lack critical thinking to evaluate and reflect on criteria; they lack abductive reasoning to make judgments with incomplete information at hand).\n\nGiven these (important) ethical caveats, some scholars in computational sciences debate if the hype over AI-based tools-- especially as \"automated plagiarism\" tools-- should be heeded at all.\n\nFor the time being, I'm tentatively, pragmatically augmenting my academic integrity policy with a policy regarding a responsible use of AI-based tools in my class.\n\nThis policy was developed from a response by ChatGPT-3 (2023) and edited on critical reflection by me:  \n\nAcademic integrity is a core principle at UMass Lowell and it's vital that all students uphold this principle-- whether using AI-based tools or otherwise.\n\nFor my class, a responsible use of AI-based tools in completing coursework or assessments must be done in accordance with the following:  \n- You must clearly identify the use of AI-based tools in your work.\n\nAny work that utilizes AI-based tools must be clearly marked as such, including the specific tool(s) used.\n\nFor example, if you use ChatGPT-3, you must cite \"ChatGPT-3.\n\n(YYYY, Month DD of query).\n\n\"Text of your query.\"\n\nGenerated using OpenAI.\n\n- You must be transparent in how you used the AI-based tool, including what work is your original contribution.\n\nAn AI detector such as GPTZero may be used to detect AI-driven work.\n\n- You must ensure your use of AI-based tools does not violate any copyright or intellectual property laws.\n\n- You must not use AI-based tools to cheat on assessments.\n\n- You must not use AI-based tools to plagiarize without citation.\n\nViolations of this policy will be dealt with in accordance with UMass Lowell's academic integrity policy.\n\nIf you are found in violation of this policy, you may face penalties such as a reduction in grade, failure of the assignment or assessment, or even failure of the course.\n\nFinally, it's your responsibility to be aware of the academic integrity policy and take the necessary steps to ensure that your use of AI-based tools is in compliance with this policy.\n\nIf you have questions, please speak with me first, as we navigate together how best to responsibly use these tools.\n\nChatGPT-3.\n\n(2023, January 10).\n\n\"Write a syllabus policy about the academic integrity of students using ai-based tools.\"\n\nGenerated using OpenAI.\n\n---",
    "#### Policy\n\nArtificial Intelligence Policy: Are all of our classes now AI classes?\n\nI expect you to use AI (e.g., ChatGPT, Dall-e-2) in this class.\n\nIn fact, some assignments will require it.\n\nLearning to use AI is an emerging skill, and I will provide basic tutorials about how to leverage it for our work.\n\nHowever, be aware of the limits of these software systems.\n\nAI is vulnerable to discrimination because it can inadvertently (or intentionally) perpetuate existing biases present in the data it is trained on.\n\nFor example, if an AI system is trained on data that contains a bias against a certain group of people, the system may make decisions that are unfair or discriminatory towards that group.\n\nThere are several reasons why AI systems can perpetuate discrimination:  \n- **Bias in the training data:** If the training data contains biases, the AI system may learn and replicate those biases in its decision-making.\n\n- **Lack of diversity in the training data:** If the training data does not include a diverse range of examples, the AI system may not perform well on diverse inputs, which may lead to discrimination.\n\n- **Lack of transparency:** Some AI systems can be difficult to understand and interpret, making it challenging to detect and correct for biases.\n\n- **Lack of accountability:** Without proper oversight and accountability, it can be difficult to identify and address discrimination in AI systems.\n\nIt is important to keep in mind that these biases can be unconscious, unintended, and hard to detect, but they can have serious consequences if they are not addressed.\n\nAI can be a valuable tool for augmenting human decision-making and critical thinking, but it is not a replacement.\n\nAI is a tool, just like a pencil or a computer.\n\nHowever, unlike most tools you need to acknowledge using it.\n\nPay close attention to whatever information you use in your own work that is produced from AI, and explain how/what you used at the end of assignments.\n\nMy recommendation is to screenshot and save everything (i.e., what prompts you used, what answers were produced, where, why, and how).\n\nThis is new territory, but basic attribution rules still apply.\n\nCite everything, otherwise you are likely violating academic integrity policies.\n\nIf you provide minimum effort prompts, you will get low quality results.\n\nYou will need to refine your prompts to get better outcomes.\n\nThis will take time and practice.\n\nDon't trust anything the systems say.\n\nAssume it is wrong unless you already know the answer and can verify with trusted sources.\n\nIt works best for topics you deeply understand.\n\nUse your best judgement to determine if/where/when to use these tools.\n\nThey don't always make products easier and/or better.\n\nLarge language models and chatbots are \"look back\" machines.\n\nThey don't advance knowledge (yet).\n\nChatGPT-3 uses data from 2021 and earlier (a lot has changed since 2021).\n\n*Note... some of this was written with AI; OpenAI.\n\n(2021).\n\nGPT-3 API.\n\nRetrieved from   \nSource / Other Info  \nRyan Gagnon  \n\n---",
    "#### Policy\n\nA Few Words about Generative AI (e.g.\n\nChatGPT)  \n\nWriting is integral to thinking.\n\nIt is also hard.\n\nNatural language processing (NLP) applications like ChatGPT or Sudowrite are useful tools for helping us improve our writing and stimulate our thinking.\n\nHowever, they should never serve as a substitute for either.\n\nAnd, in this course, they cannot.\n\nThink of the help you get from NLP apps as a much less sophisticated version of the assistance you can receive (for free!)\n\nfrom a Bentley Writing Center tutor.\n\nThat person might legitimately ask you a question to jump-start your imagination, steer you away from the passive voice, or identify a poorly organized paragraph, but should never do the writing for you.\n\nA major difference here, of course, is that an NLP app is not a person.\n\nIt’s a machine that is adept at recognizing patterns and reflecting those patterns back at us.\n\nIt cannot think for itself.\n\nAnd it cannot think for you.\n\nWith that analogy in mind, you will need to adhere to the following guidelines in our class.\n\n**Appropriate use of AI when writing essays or discussion board entries**  \n- You are free to use spell check, grammar check, and synonym identification tools (e.g., Grammarly and MS Word).\n\n- You are free to use app recommendations when it comes to rephrasing sentences or reorganizing paragraphs you have drafted yourself.\n\n- You are free to use app recommendations when it comes to tweaking outlines you have drafted yourself.\n\n**Inappropriate use of AI when writing essays or discussion board entries**  \n- You may not use entire sentences or paragraphs suggested by an app without providing quotation marks and a citation, just as you would to any other source.\n\nCitations should take this form: OpenAI, chatGPT.\n\nResponse to prompt: “Explain what is meant by the term ‘Triple Bottom Line’” (February 15, 2023,   \n- You may not have an app write a draft (either rough or final) of an assignment for you.\n\nEvidence of inappropriate AI use will be grounds for submission of an Academic Integrity report.\n\nSanctions will range from a zero for the assignment to an F for the course.\n\n---",
    "#### Policy\n\nTBA  \n\nStudents shall:  \n- Give credit to AI tools whenever used, even if only to generate ideas rather than usable text or illustrations.\n\n- When using AI tools on assignments, add an appendix showing (a) the entire exchange, highlighting the most relevant sections; (b) a description of precisely which AI tools were used (e.g., ChatGPT private subscription version or DALL-E free version); (c) an explanation of how the AI tools were used (e.g., to generate ideas, turns of phrase, elements of text, long stretches of text, lines of argument, pieces of evidence, maps of conceptual territory, illustrations of key concepts, etc.\n\n); (d) an account of why AI tools were used (e.g., to save time, to surmount writer's block, to stimulate thinking, to handle mounting stress, to clarify prose, to translate text, to experiment for fun, etc.).\n\n- Not use AI tools during in-class examinations, or assignments, unless explicitly permitted and instructed.\n\n- Employ AI detection tools and originality checks prior to submission, ensuring that their submitted work is not mistakenly flagged.\n\n- Use AI tools wisely and intelligently, aiming to deepen understanding of subject matter and to support learning.\n\nFaculty shall:  \n- Seek to understand how AI tools work, including their strengths and weaknesses, to optimize their value for student learning.\n\n- Treat work by students who declare no use of AI tools as the baseline for grading.\n\n- Use a lower baseline for students who declare use of AI tools, depending on how extensive the usage, while rewarding creativity, critical nuance, and the correction of inaccuracies or superficial interpretations in response to suggestions made by AI tools.\n\n- Employ AI detection tools to evaluate the degree to which AI tools have likely been employed.\n\n- Impose a significant penalty for low-energy or unreflective reuse of material generated by AI tools and assigning zero points for merely reproducing the output from AI tools.\n\nThis policy recognizes that:  \n- This policy depends on goodwill, a sense of fairness, and honorable character.\n\n- Some instructors may prefer stronger restrictions on the use of AI tools, and they are free to impose them so long as care is taken to maintain transparency and fairness in grading.\n\n- This policy takes account of the existence of subscription versions of AI tools, which are not affordable for some students; the policy may need to be revised as the differences between subscription and free versions become better understood.\n\n- This policy may be revised in light of other policies and novel technological developments in AI tools.\n\nSource / Other Info  \nBoston University Faculty of Computing & Data Sciences | Using Generative AI in Coursework - Boston University Computing & Data Sciences  \n\n---\n\nDeveloped and shared by Lance Eaton and modified by the NIU Center for Innovative Teaching and Learning.\n\n© 2024 Board of Trustees of Northern Illinois University.\n\nAll rights reserved.\n\nPrivacy Notice 1425 W. Lincoln Hwy., DeKalb, IL 60115",
    "## Table of Contents\n\n4 Message from Washington State Superintendent Chris Reykdal  \n5 Executive Summary  \n5 In This Guidance  \n5 How to Use This Guidance  \n6 Understanding Artificial Intelligence  \n6 AI in Education: A Human-Centered Approach  \n8 Definitions: What AI Is and What It Is Not  \n9 Potential Opportunities for Using AI in Education  \n9 Potential Risks That Need to Be Mitigated When Using AI in Education  \nPrinciples and Values  \nGuidance  \n11 A Human-Centered Approach to AI  \n11 Implementing AI in Student Learning  \n12 Sensitive and Confidential Data  \n13 Policy  \n13 Building Human-Centered AI Policies  \n14 Updates for Existing Policies  \n14 Academic Integrity  \nProfessional Development  \nConclusion and Next Steps  \nAcknowledgements  \nReferences  \nLegal Notice",
    "## Message from State Superintendent Chris Reykdal\n\nIn the last year, Washington’s teachers delivered over a billion hours of instruction to our students – and technology held an important role in that delivery, as it has every year.\n\nParticularly during the pandemic, our schools took on the massive effort of establishing a technological infrastructure that allowed for each and every student and teacher to have their own device.\n\nArtificial Intelligence (AI) is emerging rapidly into the various aspects of teaching, learning, and school district operations.\n\nWashington state is remarkably positioned to integrate AI in our classrooms and campuses across our state.\n\nIt is with great excitement and appropriate caution that we distribute guidance to schools and districts now.\n\nLike many of the innovations in technology that came before it, the world of AI is evolving at lightning speed.\n\nAlso like many of the technology innovations that came before it, young people are accessing these tools and wanting to use them in their daily lives.\n\nIn other words, AI is here and slowing down isn’t an option.\n\nStudents and educators are already engaging with AI, but the key question remains: How will we use it in a way that empowers critical thinking?\n\nAs this technology revolutionizes industries, communities, sciences, and workplaces, our responsibility is to prepare students and educators to use these tools in ways that are responsible, ethical, and safe.\n\nSchools across Washington are already pioneering efforts to integrate AI into classrooms.\n\nWith a full embrace of AI, Washington’s public education system will be at the forefront of innovation and excellence.\n\nThis initiative is not just about staying current with technology—it’s about enriching the learning journey of every student and empowering our educators with the most effective tools available.\n\nI encourage all stakeholders—caregivers, families, teachers, education partners, and community members—to join us in this groundbreaking journey.\n\nYour insights and participation are invaluable as we chart this path and learn together.\n\nOur state leads by example, setting a standard for how technology and human ingenuity can work hand in hand to prepare the next generation of leaders for success in careers, jobs, and communities that don’t yet exist.\n\nOur commitment is not just to integrate AI into the classroom; it’s to do so with a vision that places our educators and students at the center of this digital revolution with a priority for human inquiry that uses AI for production, but never as the final thought, product, or paper.\n\nAI is a powerful tool, but it only enhances learning if students and educators embrace an “H AI H” approach.\n\nStart with human inquiry, see what AI produces, and always close with human reflection, human edits, and human understanding of what was produced.\n\nIt is imperative that we empower our teachers to utilize AI as a responsible and transformative tool.\n\nThis means providing educators with the necessary resources, training, and support to incorporate these technologies in ways that enhance their instruction and, more importantly, nurture our students’ critical thinking.\n\nTogether, we will create an educational environment where technology supports, but where human control and inquiry lead to boundless learning, and where our children are ready to lead in a world augmented by artificial intelligence.\n\nChris Reykdal Superintendent of Public Instruction",
    "## Executive Summary\n\nArtificial Intelligence (AI) is emerging rapidly across industries—including K–12 education.\n\nTo support educators and education leaders in equitable and inclusive uses of AI in classrooms across Washington, the Office of Superintendent of Public Instruction (OSPI) presents this initial guidance, which emphasizes a human-centered approach to using this ever-evolving tool.",
    "### In This Guidance\n\n- **Understanding AI:** Provides context about the emergence of AI and a framework for public education systems to address it.\n\n- **Definitions:** Explains what generative AI (or “Gen AI”) is and is not, and identifies potential opportunities and risks for using AI in public education.\n\n- **Principles and Values:** Lists the guiding principles that OSPI supports, including that uses of AI in public education should be equitable and inclusive, safe and secure, understandable, and purposeful and beneficial.\n\n- **Guidance:** Provides considerations for creating human-centered AI learning environments, implementing AI in student learning, and protecting sensitive and confidential data.\n\n- **Policy:** Details key components of AI usage policies, including acceptable use policies and requiring both human input and review of AI outputs.\n\n- **Academic Integrity and AI Assistance:** Indicates academically honest ways to use AI, how to cite AI, and when using AI could be classified as plagiarism.\n\n- **Professional Development:** Lists the topics and subjects on which local education agencies (LEAs) should provide training.",
    "### How to Use This Guidance\n\nOSPI’s guidance on integrating AI into Washington classrooms is designed to be used by stakeholders across public education:\n\n- **School district administrators** should integrate this guidance into their district and school policies on AI use.\n\n- **Educators** should use this guidance for recommendations on implementing AI in their classrooms.\n\n- **Students and families** can refer to this guidance for suggested uses of AI and to gain a deeper understanding of Washington’s statewide approach to and understanding of AI.\n\nThis guidance is designed to evolve with advances in AI and adapt to the unique needs of school communities across Washington.\n\nWith the transformative power that AI presents, the overarching goal of this guidance is to empower Washington’s students to lead in the world they are helping to create.",
    "## Understanding Artificial Intelligence\n\nThe integration of Artificial Intelligence (AI) in education starts with the fundamental understanding that AI is not a replacement for human intelligence or humanitarian presence in education.\n\nAccording to UNESCO, AI in education is expected to be a $6 billion worldwide industry in 2024 with estimates of growth reaching $19.9 billion by 2028.\n\nLinkedIn’s Economic Graph Research Institute estimates that, by 2030, the skill sets needed for jobs will change by 65%, affecting not just tech, but all industries.\n\nCorporate entities are moving fast to meet the need and demand for products that streamline the delivery of education, but not all products are the same.\n\nIt is the responsibility of the education community to carefully and strategically understand how these products work, what data is collected, and where information is sourced.\n\nWhile AI is an emerging innovation in education, Local Education Agencies (LEAs) can utilize and build on existing policies that are based on educational integrity, student safety, and proven instructional practices.\n\nIn conversation, AI tools are often discussed as a holistic, outside influence on education, yet policy regarding AI should not be separately written.\n\nKey facets of these tools already apply to concerns such as student data privacy, plagiarism, cyberbullying, and digital literacy, and can be called out within existing and corresponding policies.\n\nWhether updating policies, adopting software, delivering instruction, or analyzing outputs, the vital role each of us plays in understanding what AI is and is not cannot be overemphasized.\n\nThe first step in AI integration is realizing the opportunity for AI to transform the way we access and use information as we learn and work in our world today, as well as into the future.\n\nThe next step is to ensure all guidance, policies, systems, and instructional practices are grounded on ethical, equitable, and inclusive uses of AI throughout our Washington communities.\n\nLEAs must ensure educational and technology standards are applied to meet the needs of all students so that AI tools enhance education.\n\nFinally, state and local policymakers and education leaders must act to build an ethical framework of funding to support policies around every student that embraces each student’s unique abilities and allows them to use existing and emerging AI tools in a safe learning environment.",
    "## AI in Education: A Human-Centered Approach\n\nA human-centered AI learning environment is one that prioritizes the needs, abilities, and experiences of students, educators, and administrators.\n\nAn educational leader can support a human-centered learning environment by considering the following:\n\n- Developing students’ AI literacy by helping them understand the concepts, applications, and implications of AI in various domains, and empowering them to use AI as a tool for learning and problem-solving.\n\n- Ensuring ethical, equitable, and safe use of AI by protecting the privacy and security of data, addressing potential biases and harms, and promoting digital citizenship and responsibility.\n\n- Providing professional development and support for educators by helping them integrate AI into their pedagogy, curriculum, and assessment, and by facilitating their collaboration and innovation with AI.\n\n- Applying human-centered design principles to the development and implementation of AI solutions, such as involving stakeholders in the design process, testing and iterating the solutions, and evaluating the impact and outcomes.\n\n- Aligning AI solutions with the best practices and principles of learning, such as supporting learner agency, fostering collaboration, enhancing feedback, and promoting critical thinking.",
    "## Definitions: What AI Is and What It Is Not\n\nGenerative Artificial Intelligence (also abbreviated as Gen AI) refers to software tools modeled on large amounts of data to produce text, images, videos, or other digital artifacts.\n\nCommon, general-purpose examples include a wide range of software known for text generation through Large Language Models (LLMs) and image generation.\n\nTools more tailored to an educational setting include personal AI tutoring programs, virtual assistants, and software with the capability to generate lesson plans within seconds or grade tasks in real time.\n\nAs with all new tools and technologies, it is important to understand the tool itself in order to use it responsibly.\n\nGenerative AI has a place in the classroom when educators and students remain at the center of instruction and learning.",
    "### Generative AI is…\n\n- a means to augment teaching and learning.\n\n- already embedded into many technologies, as is AI more broadly speaking.\n\n- permeated with flaws such as algorithmic bias that must be considered when utilized.\n\n- a product of companies led by teams of humans with their own values, agendas, limitations, bias, and organizational needs.\n\n- an algorithm that enables users to generate new content based on a pre-trained Large Language Model.",
    "### Generative AI is not…\n\n- a replacement for student development.\n\n- something that can plausibly be avoided or “turned off.”\n- a source of unquestionable, factual information.\n\n- produced in a vacuum free of societal influence.\n\n- a sentient being with untethered superhuman capabilities.\n\n- a replacement for highly qualified educators.",
    "## Potential Opportunities for Using AI in Education\n\n- Personalize learning and feedback in real time\n- Lesson plan and assessment design with customized planning for differentiation\n- Translation between languages\n- Develop critical thinking through human input, data output, and elevated human analysis\n- Aid in creativity, simulation, and skill development\n- Streamline operational and administrative functions",
    "## Potential Risks That Need to Be Mitigated When Using AI in Education\n\n- Increasing and/or creating inequitable learning environments\n- Unauthorized access to protected user information and unauthorized data collection\n- Perpetuating institutional and systemic biases\n- Plagiarism and academic dishonesty\n- Over-relying on technology and undermining the importance of human intelligence in education\n\nArtificial Intelligence tools provide opportunities, benefits, and potential risks.\n\nIt is the responsibility of every parent/guardian, policymaker, teacher, administrator, and support staff member to ensure that the use of this transformative new technology and its future is regularly reviewed to ensure equity of access, data privacy, and safe and ethical usage are maintained at all levels.\n\nIt is equally critical that LEAs embrace and teach students what AI is and isn’t and how to use AI technologies to enhance learning – not prevent students from developing critical skills needed to graduate with technological literacy.",
    "## Principles and Values\n\nThe National Institute of Standards and Technology (NIST) AI Risk Management Framework outlines guiding principles regarding the trustworthiness of systems that use AI, referenced in Washington Technology Solutions’ (WaTech’s) own guidelines.\n\nSimilarly, the TeachAI Toolkit outlines principles for the use of AI in education.\n\nWith these resources as foundations, OSPI supports the following principles and values for the use of AI in education in Washington:\n\n- **New Level of Education:** Artificial Intelligence brings a new level of hope and opportunity to the delivery of education in Washington.\n\nThe onset of technological tools using AI provides a new way for educators to think about how we, as humans, learn, create, and process information.\n\n- **Equitable and Inclusive:** All students should have access to AI tools which are inclusively designed with all students in mind.\n\n- **Safe and Secure:** Students should not be put at further risk by using AI nor should their personally identifying information be unlawfully shared.\n\n- **Consistent and Compliant:** AI tools should be consistent in their expected capabilities and meet the needs of the education system.\n\n- **Accountable and Transparent:** The companies partnering with schools to provide AI tools should be accountable for their products, just as schools and LEAs should be accountable for student use of AI.\n\n- **Interpretable and Understood:** Though many AI systems are black boxes (too mechanically complex to be human interpretable), at a practical level students and educators should be literate in how AI works and why it produces the results that it does.\n\n- **Purposeful and Beneficial:** All use of AI should ultimately provide a positive experience with intentional use in teaching and learning.\n\n- **Human and Machine:** AI should aid in (not replace) decision-making, creativity, learning, development, growth, and productivity.\n\n- **Continuous Learning and Development:** AI has permeated society faster than technologies of the past and it is essential that Washington educators, technology specialists, and partners in school services provide access to AI tools, while continuing to engage in conversations to learn how to effectively manage and develop the role AI plays in shaping the future of education.",
    "### A Human-Centered Approach to AI\n\nA human-centered AI learning environment always starts with human inputs and inquiry, and always concludes with human reflection and edits.\n\nIt should prioritize the needs, abilities, and experiences of students, teachers, and administrators.\n\nAn education leader can support a human-centered learning environment by considering the following:\n\n- Developing students’ AI literacy by helping them understand the concepts, applications, and implications of AI in various domains, and empowering them to use AI as a tool for learning and problem-solving.\n\n- Ensuring ethical, equitable, and safe use of AI by protecting the privacy and security of data, addressing potential biases and harms, and promoting digital citizenship and responsibility.\n\n- Providing professional development and support for teachers by helping them integrate AI into their pedagogy, curriculum, and assessment, and by facilitating their collaboration and innovation with AI.\n\n- Applying human-centered design principles to the development and implementation of AI solutions, such as involving stakeholders in the design process, testing and iterating the solutions, and evaluating the impact and outcomes.\n\n- Aligning AI solutions with the best practices and principles of learning, such as supporting student agency, fostering collaboration, enhancing feedback, and promoting critical thinking.\n\n- Avoiding sole reliance on the use of “AI detection” tools in checking for student plagiarism.\n\nThese tools often use data that is biased against students who are multilingual/English language learners.",
    "### Implementing AI in Student Learning\n\nWhen integrating AI into student learning, it’s important to empower students in how and to what degree AI is utilized in their learning journey.\n\nBy doing so, students can actively participate in shaping their educational experience with AI.\n\n- Co-create and share an AI decision-making rubric with students (example rubric).\n\n- Support students in taking a human-centered approach to using AI.\n\n- Empower students in leveraging AI in scaffolding understanding, feedback, and reflection.\n\n- Support students in critically thinking about the role of AI within their learning journey and within their preparation for college, career, and life.\n\n- Empower students receiving special education services to use AI to personalize and increase their access to learning.\n\n- Integrate ethics and critical thinking activities that align with grade-level and subject-level instruction.\n\n- Use AI for differentiation and assessment, including intelligent tutoring systems that allow text to speech, translation, personalized learning, and inquiry-based learning.\n\n- Prepare students for jobs of the future, including those in career and technical education (CTE) programs, by partnering with industry to update and integrate learning standards.",
    "### Sensitive and Confidential Data\n\nEnsure that your LEA AI use complies with student/personal privacy and data protection laws.\n\nBe aware of and follow any age restrictions for the use of all AI tools and resources.\n\nBefore sharing private data, ensure that the AI tool meets the following requirements:\n\n- Family Education Rights and Privacy Act (FERPA) requires that schools not disclose personal identifying information of a minor or eligible student without express written consent of the parent or eligible student.\n\n- Children’s Online Privacy Protection Act (COPPA) requires schools to obtain parental consent before allowing students under 13 to use online services that collect, use, or disclose personal information.\n\n- Children’s Internet Protection Act (CIPA) requires that schools implement an internet safety policy that includes protective measures to block or filter access to obscene or harmful content.\n\nHave a clear understanding of your data collection processes.\n\nUpdate policies to include the use of and considerations for using AI.\n\nGive users options to opt-out or delete their data if they want.",
    "### Policy\n\nAI policies must promote equitable and inclusive access to AI.\n\nEducation policymakers must focus on ensuring the use of AI increases the public good, with emphasis on empowering students who are members of communities that have been historically underserved.\n\nIt is important that policies, by design, enhance a human-centered approach to pedagogy and learning, and respects ethical norms and standards.\n\nAI policy and use should be geared to improving learning for every student, empowering teachers, and strengthening learning management systems.",
    "### Building Human-Centered AI Policies\n\n- Incorporate the need for human intelligence and responsibility into AI usage policies.\n\nAll AI use should start and end, with human insight.\n\n- **Responsible Use Policy:** Known as an acceptable use policy (AUP) or technology use policy, this describes what any person authorized to utilize the district’s technology system may do and not do.\n\nIt describes the terms and conditions for educational institutions and should be updated to include the safe and appropriate use of AI tools.\n\n(A separate AI AUP is not needed.)\n\n- **AI Inquiry and Input Review:** Require human input of data with clear mandates that staff and students should never input personal, sensitive, or confidential data, including any data related to student education records, into any AI system without first ensuring that the system meets FERPA, COPPA, and CIPA requirements.\n\nEmphasize the need for review prior to finalizing any information into a system that learns from data entered.\n\n- **Embrace the Use of Data and the Evaluation of AI Output:** Invest in systems that create streamlined opportunities for staff and students to enter information efficiently and safely, allow them to improve instruction, and draw connections to better understand student thinking and learning.\n\n- **AI Output Review:** AI users should review and critically assess outputs from AI tools before sharing or publicizing results, including in the classroom.\n\nStaff and students should not rely exclusively on AI-generated content without fact-checking and evaluating results.\n\nUltimately, it is up to human users to determine how AI information is shared and used.\n\n- **Bias and Misinformation:** AI-generated content is based on datasets or data models that may contain biases, false information, or other inaccuracies.\n\nAI systems do not have the ability to think or verify accuracy.\n\nTherefore, verifying AI results to ensure the source is credible must occur before considering an AI output in academic work.\n\n- **Safety and Respect:** Users must never use AI tools to create misleading or inappropriate content, take someone’s likeness without permission, or harm humans or the community at large.\n\n(Note: This may also be added to a student code of conduct or bullying/cyberbullying/harassment policy.)",
    "### Updates for Existing Policies\n\n- **Data Collection:** Parents, guardians, and students must be informed of specific data collection activities or potential risk, where applicable, with consent required.\n\nAll AI-driven data collection must adhere to local data protection regulations, best practices, and community standards.\n\n- **Third-Party AI Tools:** The district’s approved list of AI software and tools should always be reviewed and updated.\n\nUnauthorized AI software and tools (including updates) might not adhere to the district’s data privacy standards and practices.\n\n- **Personal Information:** Staff and students should never input personal, sensitive, or confidential data into any AI system without prior parental or guardian authorization, including any data related to student education records.\n\nPersonnel must adhere to the FERPA, COPPA, and CIPA when entering information.",
    "### Academic Integrity and AI Assistance\n\n- **Assessments:** Teachers are encouraged to allow students to use AI tools as a personal tutor or studying buddy to prepare for assessments.\n\nWhen students are completing exams or quizzes, it should be at the teacher’s discretion to determine whether an AI tool is used, and it should be explicitly stated whether this is allowed.\n\n- **Assignments:** Teachers should have the responsibility of clarifying appropriate or inappropriate uses of AI tools when students are completing assignments.\n\nTeachers might allow the limited use of generative AI on entire assignments, parts of assignments, or as a brainstorming tool.\n\nTeachers should be encouraged to articulate why they do or do not allow its use in other assignments or parts of assignments.\n\n- **Bias and Critical Thinking:** All users of AI software should review and evaluate AI output for accuracy and potential bias.\n\nStudents must develop the critical thinking skills needed to successfully use and navigate the world of AI.\n\nLimitations of AI systems and the data used to develop AI content can, and sometimes do, amplify human bias.\n\nUse caution and human review prior to using AI outputs in classrooms, publications, data analysis, etc.\n\n- **Citations, Disclosures, and Attributions:** AI-generated content is considered plagiarism, unless appropriately cited in assignments or publications; any use must be referenced.\n\nInstruction should emphasize the importance of ethical use of AI and the role of using sources in assignments.\n\nThe following resources provide a sample of how to appropriately cite the use of AI in any publication or assignment:\n- MLA Style - Generative AI\n- APA Style - ChatGPT\n- Chicago Style - Generative AI\n- **Plagiarism:** Staff and students should be encouraged to use AI tools for brainstorming, creative stimulations, or preliminary research.\n\nUsing AI tools to generate answers, stories, essays, or other publications, and/or complete assignments without proper citation, is plagiarism.\n\n- **Use of AI Detection Tools:** Software companies that claim products can detect content developed by another AI tool, or its own AI tool, are currently not reliable and should not be used as the sole way to determine whether cheating and plagiarism have occurred.",
    "## Professional Development\n\nAs Gen AI continues to evolve and impact all aspects of industry, LEAs need to provide training and understanding of Gen AI for all educational stakeholders.\n\nThe appropriate use of AI always begins with human inquiry and ends with human engagement with the AI output.\n\nLEAs must ensure users of Gen AI understand the safe, responsible ways to utilize these tools in a human-centered approach.\n\nLEA leadership should prioritize staff understanding of how to utilize the technology in the following areas:\n- Improve organizational awareness, productivity, and effective use of AI tools\n- Understand the pedagogical changes that Gen AI has for learning\n- Promote student empowerment in the use of AI in work and assessments\n- Establish a shared understanding about the importance and equity concerns when using AI\n- Promote access to appropriate AI tools for learning\n- Create a shared understanding of academic integrity in the era of AI\n- Emphasize ethical use of AI\n- Promote understanding of AI and AI tools across the wider educational community\n- Empower teachers to generate curriculum using open educational resources provided by OSPI (Washington OER Hub)",
    "## Conclusion and Next Steps\n\nOSPI is committed to leading Washington state in AI education and preparing the next generation of learners and leaders for a future that is built with and alongside AI.\n\nThis guidance is intended to be a living document; OSPI recognizes that AI tools are constantly evolving and expanding and will regularly review this guidance to determine if and when updates are appropriate.\n\nOSPI, at the guidance of the AI Advisory Group and in partnership with stakeholders across Washington, will also continue to develop tools and resources for use by educators, education leaders, and students and families.\n\nOver the coming months, the following will be made available:\n- A list of Frequently Asked Questions (FAQs)\n- Guidance and resources focused on classroom and student implications\n- Guidance on using AI to improve internal district operations, such as data management, resource allocation, and decision-making\n- A checklist for LEA leaders responsible for developing and implementing AI policies\n- Resources on how to promote ethical, responsible, and inclusive use of AI in K–12 education\n- Resources that address the challenges of privacy, security, bias, and equity\n- High-quality, equity-based professional learning opportunities about AI integration for educators and administrators",
    "**AI Advisory Group:**\n\n- Adam Aguilera, Teacher at Evergreen Public Schools\n- Cindy Cromwell, Principal in Kelso School District\n- Dr. Trevor Greene, Superintendent of Yakima School District\n- Kris Hagel, Executive Director of Digital Learning for Peninsula School District\n- Travis Rush, Technical Education Lead for the Association of Educational Service Districts (AESD)\n- Dr. Min Sun, Associate Professor at the University of Washington’s College of Education\n- A student representative from the Bainbridge Island School District",
    "**OSPI Staff:**\n\n- Mikhail Cherniske, Senior Policy Analyst at OSPI\n- Terron Ishihara, Computer Science Program Supervisor at OSPI\n- Holli Kalaleh, Business and Marketing Program Supervisor at OSPI\n- Dr. Jon Ram Mishra, Assistant Superintendent of Elementary, Early Learning, and Federal Programs at OSPI\n- Zac Murphy, Director of Multimedia and Information Strategy at OSPI\n- Bre Urness-Straight, Director of Educational Technology at OSPI\n- Rebecca Wallace, Assistant Superintendent of Secondary Education and Postsecondary Pathways at OSPI\n\nOSPI also extends recognition to Tammie Schrader and Jeff Utecht for their contributions as facilitators.\n\n**AI Use Statement**  \nIn the spirit of fostering an educational environment where technology amplifies human potential and capability, this guidance is anchored in the “Human  AI  Human” model.\n\nIt is designed to direct the responsible, ethical, and safe engagement with AI technologies.\n\nEducators are encouraged to use AI as a tool to augment, not replace, human insight and creativity in the learning process.\n\nThis guidance provides a framework to integrate AI into student learning, ensuring that it is used to enhance educational outcomes while upholding principles of equity and inclusion.\n\nAs AI continues to evolve, this document will serve as a living reference, evolving to meet the needs of Washington’s diverse and dynamic educational landscape.",
    "## References\n\nChen, C. (2023).\n\nAI Will Transform Teaching and Learning.\n\nLet’s Get it Right.\n\nStanford University Human-Centered Artificial Intelligence.\n\nDimitriadis, Y., Martínez-Maldonado, R., Wiley, K. (2021).\n\nHuman-Centered Design Principles for Actionable Learning Analytics.\n\nIn: Tsiatsos, T., Demetriadis, S., Mikropoulos, A., Dagdilelis, V. (eds) Research on E-Learning and ICT in Education.\n\nSpringer, Cham.\n\nInternational Market Analysis Research and Consulting Group.\n\n(2023).\n\nSize, Share and Trends.\n\nLinkedIn Economic Graph.\n\n(2023).\n\nPreparing the Workforce for Generative AI: Insights and Implications.\n\nMichigan Virtual.\n\nPlanning Guide for AI: A Framework for School Districts.\n\nNational Institute of Standards and Technology.\n\n(2023).\n\nArtificial Intelligence Risk Management Framework (AI RMF 1.0).\n\nU.S. Department of Commerce.\n\nUnited Nations Educational, Scientific and Cultural Organization.\n\n(2021).\n\nAI and education: guidance for policy-makers.\n\nTeachAI.\n\n(2023).\n\nAI Guidance for Schools Toolkit.\n\nThe University of Melbourne.\n\n(2023).\n\nKeep education human-centered in an AI, digital teaching world: report.\n\nWashington Technical Solutions.\n\n(2023).\n\nInterim Guidelines for Purposeful and Responsible Use of Generative Artificial Intelligence.",
    "## Legal Notice\n\nThis resource may contain links to websites operated by third parties.\n\nThese links are provided for your convenience only and do not constitute or imply any endorsement or monitoring by OSPI.\n\nIf this work is adapted, note the substantive changes and re-title, removing any Washington Office of Superintendent of Public Instruction logos.\n\nProvide the following attribution:  \n“This resource was adapted from original materials provided by the Office of Superintendent of Public Instruction.\n\nOriginal materials may be accessed at   \nOSPI provides equal access to all programs and services without discrimination based on sex, race, creed, religion, color, national origin, age, honorably discharged veteran or military status, sexual orientation including gender expression or identity, the presence of any sensory, mental, or physical disability, or the use of a trained dog guide or service animal by a person with a disability.\n\nQuestions and complaints of alleged discrimination should be directed to the Equity and Civil Rights Director at 360-725-6162 or P.O.\n\nBox 47200 Olympia, WA 98504-7200.\n\nThis material is available in alternative format upon request.\n\nContact the Resource Center at 888-595-3276, TTY 360-664-3631.",
    "# Appropriate Use of ChatGPT and Similar AI Tools\n\nWith the emergence of ChatGPT and other AI tools, many members of our community are eager to explore their use in the university context.\n\nThis advisory provides guidance on how to use these tools safely, without putting institutional, personal, or proprietary information at risk.\n\nAdditional guidance may be forthcoming as circumstances evolve.",
    "## Prohibited Use\n\nAt present, any use of ChatGPT should be with the assumption that no personal, confidential, proprietary, or otherwise sensitive information may be used with it.\n\nIn general, student records subject to FERPA, and any other information classified as Protection Level P2, P3, or P4 should not be used.\n\nSimilarly, ChatGPT should not be used to generate output that would be considered non-public.\n\nExamples include, but are not limited to, proprietary or unpublished research; legal analysis or advice; recruitment, personnel or disciplinary decision making; completion of academic work in a manner not allowed by the instructor; creation of non-public instructional materials; and grading.\n\nPlease also note that OpenAI explicitly forbids the use of ChatGPT and their other products for certain categories of activity, including fraud and illegal activities.\n\nThis list of items can be found in their usage policy document.",
    "## Rationale for the Above Guidance\n\nUPDATE 7/1/2023: The University of California recently renegotiated the UC systemwide agreement with Microsoft to include Microsoft Azure OpenAI.\n\nUC Berkeley is currently working with Microsoft to establish how to use this service under UC’s agreement.\n\nAs of September 2023, the University of California's agreements with the parent companies of ChatGPT and other generative AI services, which include our terms and conditions, Appendix Data Security and other privacy protections, do not cover the use of ChatGPT or other similar generative AI services.\n\nThe UC Office of the President is working on this issue.\n\nWe hope to see this addressed in the near future and will update this guidance when additional information is available.",
    "## Additional Readings\n\n- Governor Newsom Signs Executive Order to Prepare California for the Progress of Artificial Intelligence\n- Understanding AI Writing Tools and their Uses for Teaching and Learning at UC Berkeley, from UC Berkeley Research, Teaching & Learning\n- Quantamagazine: The Unpredictable Abilities Emerging From Large AI Models\n- The Atlantic: Don’t Be Misled by GPT-4’s Gift of Gab\n- University of California Presidential Working Group on Artificial Intelligence\n- Inclusive Intelligence: Artificial Intelligence in the Service of Science, Work, and the Public Good, from UC Berkeley Research\n\n(\n\nPowered by Open Berkeley Accessibility Nondiscrimination Privacy Copyright © 2024 UC Regents; all rights reserved\n\nBack to Top",
    "### ChatGPT Syllabus Statements\n\nThe following syllabus statements were developed by the Senate Committee on Educational Policy as a resource for faculty and instructors, with options ranging from “no restrictions” to “ChatGPT and other AI tools may not be used under any circumstances.” These statements may be adopted or adapted to reflect a given instructor's course learning objectives and instructional / assessment style.",
    "### For instructors who wish to embrace ChatGPT\n\nArtificial intelligence (AI) language models, such as ChatGPT, may be used for any assignment with appropriate citation.\n\nExamples of citing AI language models are available at:\n\nlibguides.umn.edu/chatgpt [or provide an alternative reference appropriate for your class].\n\nYou are responsible for fact checking statements composed by AI language models.",
    "### For instructors who wish to allow limited usage of ChatGPT\n\nArtificial intelligence (AI) language models, such as ChatGPT, may be used for [assignment types A, B & C] with appropriate citation, but not for [assignment types D, E & F].\n\nIf you are in doubt as to whether you are using AI language models appropriately in this course, I encourage you to discuss your situation with me.\n\nExamples of citing AI language models are available at: libguides.umn.edu/chatgpt [or provide an alternative reference appropriate for your class].\n\nYou are responsible for fact checking statements composed by AI language models.",
    "### For instructors who wish to prohibit the usage of ChatGPT\n\nThe Board of Regents Student Conduct Code states the following in Section IV, Subd.1: Scholastic Dishonesty:\n\n\"Scholastic dishonesty means plagiarism; cheating on assignments or examinations, including the unauthorized use of online learning support and testing platforms; engaging in unauthorized collaboration on academic work, including the posting of student-generated coursework on online learning support and testing platforms not approved for the specific course in question; taking, acquiring, or using course materials without faculty permission, including the posting of faculty-provided course materials on online learning and testing platforms; ...\"\n\nArtificial intelligence (AI) language models, such as ChatGPT, and online assignment help tools, such as Chegg®, are examples of online learning support platforms: they can not be used for course assignments except as explicitly authorized by the instructor.\n\nThe following actions are prohibited in this course [remove bullets as necessary]:\n- Submitting all or any part of an assignment statement to an online learning support platform;\n- Incorporating any part of an AI generated response in an assignment;\n- Using AI to brainstorm, formulate arguments, or template ideas for assignments;\n- Using AI to summarize or contextualize source materials;\n- Submitting your own work for this class to an online learning support platform for iteration or improvement.\n\nIf you are in doubt as to whether you are using an online learning support platform appropriately in this course, I encourage you to discuss your situation with me.\n\nAny assignment content composed by any resource other than you, regardless of whether that resource is human or digital, must be attributed to the source through proper citation.\n\n(Examples of citing content composed by digital tools are presented in: libguides.umn.edu/chatgpt [or provide an alternative reference appropriate for your class].)\n\nUnattributed use of online learning support platforms and unauthorized sharing of instructional property are forms of scholastic dishonesty and will be treated as such.",
    "### Additional Information\n\n- Site Map\n- COVID-19 Response\n- Initiatives\n- Academic Oversight\n- Office Directory\n- Request an Appearance\n\n© 2023 Regents of the University of Minnesota.\n\nAll rights reserved.\n\nThe University of Minnesota is an equal opportunity educator and employer.\n\nPrivacy Statement  \nReport Web Disability-Related Issue  \nSystem Campuses: System Crookston, Duluth, Morris, Rochester, Twin Cities",
    "## Executive Summary\n\nThis report offers perspectives and practical guidelines to the Cornell community, specifically on the use of Generative Artificial Intelligence (GenAI) in the practice and dissemination of academic research.\n\nAs emphasized in the charge to a Cornell task force representing input across all campuses, the report aims to establish the initial set of perspectives and cultural norms for Cornell researchers, research team leaders, and research administration staff.\n\nIt is meant as internal advice rather than a set of binding rules.\n\nAs GenAI policies and guardrails are rapidly evolving, we stress the importance of staying current with the latest developments, and updating procedures and rules governing the use of GenAI tools in research thoughtfully over time.\n\nThis report was developed within the same 12-month period that GenAI became available to a much wider number of researchers (and citizens) than AI specialists who help create such tools.\n\nWhile the Cornell community is the intended audience, this report is publicly available as a resource for other research communities to use or adapt.\n\nNo endorsement of specific tools is implied, but specific examples are referenced to illustrate concepts.\n\nRecognizing many potential benefits and risks of GenAI tools, we address the use of GenAI at four stages of the research process: (i) research conception and execution stage, (ii) research dissemination stage, (iii) research translation stage, and (iv) research funding and funding agreement compliance stage.\n\nWe further outline coordinating duties by researchers that apply across these stages: duty of discretion, duty of verification, and duty of disclosure; identify categories of GenAI use in research; and illustrate how these duties apply to specific categories and situations in the research process.\n\nWe emphasize the importance of clearly defining individual and collective/communal responsibilities for meeting these duties throughout the research process.\n\nWe conclude by offering a set of guidelines for Cornell researchers in varied faculty, staff, and student roles, as well as considerations for Cornell leadership.\n\nIt is important that Cornell offers its research community access to appropriate GenAI tools and resources, particularly to improve our “AI literacy” regarding the limits of the appropriate use of specific public and commercial GenAI tools and the risks involved in their use for academic research.\n\nIt is equally important that researchers have Cornell-facilitated access to licensed GenAI tools with privacy/confidentiality provisions, and thus important that Cornell researchers from varied communities understand the value, limitations, and trade-offs of using such tools in research.\n\nThe report also contains responses to anticipated questions about best practices and use cases for each of the four stages of research (Appendix 0) that may serve as discussion starters for research communities.\n\nFinally, we offer a summary of existing community publication policies regarding the use of GenAI in research from funders, journals, professional societies, and peers, which we surveyed as part of the preparation of this report (Appendix 1); references consulted and cited that include a list of recommended resources (Appendix 2); and task force charge (Appendix 3).\n\nNotably, the task force included Cornell faculty and staff quite familiar with GenAI tools and uses, and the task force elected to not use GenAI in drafting the structure, text, or figures of this report.",
    "## Introduction\n\nGenerative Artificial Intelligence (GenAI) offers transformative capabilities, but we must strike a balance between exploring the potential of these tools and ensuring that research meets standards of veracity, validity, originality, and reproducibility.\n\nBriefly, GenAI has the capacity to generate new content (new text or images or audio), typically by computer-generated pattern recognition gleaned from access to large volumes of prior examples.\n\nThese prior examples, data collectively called training sets or training data, can be provided by the GenAI user or provided by others, with or without their explicit awareness or consent.\n\nThis exciting capability to spark new ideas from prior knowledge, perhaps now connected in unexpected new ways, can now be accessed by the masses via online tools and for-fee apps.\n\nThese users include the masses of academic researchers with a shared sense of research integrity, but with widely varied experience in computer programming and in cultural norms for creation, authorship, and invention.\n\nMany of these tools, whether “free to the user” or fee-based, have been released by for-profit companies that maintain the model details as proprietary (i.e., do not disclose details of the trained models or the training data sets that serve as the basis for the GenAI output).\n\nOpen-source approaches for GenAI development can be a counterpoint that provides a more transparent toolset, but are not an automatic panacea to responsible development or use of such tools in academic research.\n\nThus, we need to develop common ground and guardrails that prioritize research integrity, accelerate innovation, address obvious issues like data privacy and security, and reflect on non-obvious issues like how practices and expertise of research communities will evolve for better and worse.\n\nNo one policy can cover the range of research carried out at a university, from archives to surveys to experimental labs to pure math and the visual arts.\n\nGenAI capabilities and affordances are also changing from month to month, only a year into publicly available and initially free versions, but for the cost of providing your own email address or mobile phone number to a for-profit company.\n\nExternal to Cornell, GenAI policies and guardrails are rapidly evolving, and procedures and rules governing the use of GenAI tools in the research enterprise should be regularly updated to stay current with the latest developments.\n\nInternal to Cornell, aligning practices for GenAI use with existing policies on research data and with our institutional values will also remain a work in progress for years to come.\n\nGenAI has many potential benefits for researchers at all stages of their research career, for administrative staff who provide key support to but do not participate in research and translation activities directly, and for other users in the course of conducting and administering research.\n\nThese include:\n\n- **Abstractions**: Many systems for data analysis and document retrieval have been available only to those with substantial programming experience.\n\nLikewise, systems for creative audiovisual generation have been limited to specialists or those with years of content production experience.\n\nGenAI tools can provide powerful results with interfaces accessible to anyone.\n\n- **Efficiency**: Even when researchers, administrative staff, and other users have the capability to perform a task, GenAI may be able to produce comparable output in dramatically less time, allowing users to focus on more difficult, human interaction-intensive, and/or rewarding tasks.\n\n- **Scale**: GenAI may allow users to perform a task such as coding/annotating documents or generating infographic images on a larger scale than would be possible with manual effort.\n\nThis ability may allow users to explore larger or broader data sets that were previously limited.\n\nAt the same time, we have identified several concerns that apply across the wide range of research disciplines at Cornell and represented internationally.\n\nThese include:\n\n- **Blind spots and potential bias**: All current generative language models are entirely defined by their training data, and thus perpetuate the omissions and biases of that training data.\n\nThe model behind ChatGPT has no access to information that was not presented during training, and can only access that information through learned combinations of parameters.\n\nThere is no explicit and verifiable representation of data or text encoded in the model.\n\nOther GenAI systems, such as GPT-4, may increasingly have access to web searches and databases to retrieve verifiable sources, and the underlying language model may be able to interact with text returned from those searches (in the same way it interacts with user queries), but the model itself still has no \"knowledge\" beyond the statistical regularities of its training data.",
    "- **Validation and responsibility**: There is a risk that systems are good enough that users will become trusting and complacent, but bad enough that there are serious problems that have profound consequences.\n\nA system that produces seemingly plausible answers, yet is prone to false and biased information, can cause researchers to lower their guard.\n\nTherefore, we emphasize the crucial role of researcher validation of research outputs produced with the help of GenAI tools.\n\nResponsibility is an area that is particularly sensitive to discipline-specific variation.\n\nMost fields have tacit understandings of roles and responsibilities (e.g., principal investigator, Ph.D. student, corresponding author), which may differ substantially from those in even closely related fields.\n\n- **Transparency and documentation**: Guidelines for the use of GenAI in research vary greatly across journals, funding agencies, and professional associations, from blanket bans to restrictions on certain outputs to permissions with disclosure (most with the emphasis that AI-generated outputs should be original, i.e., reflecting the authors’ actual work).\n\nLaws and regulations for patents, copyright, and data use are evolving and vary among countries.\n\nAs the policies regarding the use of GenAI continue to evolve, maintaining documentation and reporting transparency will remain critical to ensure the reproducibility and replicability of research findings produced with the help of GenAI tools.\n\n- **Data privacy and protection**: GenAI tools should not be assumed a priori to be private or secure.\n\nUsers must understand the potential risks associated with inputting sensitive, private, confidential, or proprietary data into these tools, and that doing so may violate legal or contractual requirements, or expectations for privacy.\n\n- **Resource utilization tradeoffs**: Because GenAI users perceive the output of such tools to be a “free good” or at least generated “in the cloud” even when user fee-based, as with research travel impacts the resource utilization of this computational output can be out of sight and out of mind.\n\nHowever, the magnitude of computational resources operating on Earth to create models based on large volumes of training data, and the electricity use and potential cooling water use associated with such computational processing, can be in tension with values associated with sustainability.\n\nA recent study posted on arXiv and currently under peer review attempts to quantify this resource tradeoff in terms of carbon dioxide emissions, equating the carbon cost for you to generate a single image using a specific energy-intensive GenAI model to that required to fully charge your mobile phone (Luccioni et al., 2023 preprint).\n\nCertainly, other research-related activities may contribute more significantly on a per-instance basis (e.g., a research group flying to present at an overseas conference).\n\nHowever, Cornell’s public commitment to climate action and our individual sense of responsibility for our own resource use choices benefit from our shared awareness that the use phase (or inference phase) of GenAI can be estimated, is non-zero and will remain so without concerted effort, and naturally scales with access to computational resources.",
    "## Framework for Using Generative AI in Research\n\nThe epochal developments of the past five years in GenAI have enabled systems to generate complex, recognizable outputs such as text, programming code, images, and voices.\n\nAI as a field has been around for decades, but the output of systems has often been narrow, binary predictions: whether an email is spam, or whether a transaction is likely to be fraudulent.\n\nGenAI offers dramatic new capabilities, generating output in response to prompts (i.e., questions, requests, instructions) from the user.\n\nGenAI provides the user a sense of power in its apparent intellectual assistance on demand, which unsurprisingly also vests the user with a need to take responsibility.\n\nAcademic research groups and projects often include multiple users with different stages of contribution, different degrees of experience and leadership, and different responsibilities to research integrity and translation of research results to societal impact.\n\nThus, we begin with the following general framework describing categories of uses of GenAI in research and categories of duties that researchers may have.\n\nThere are many levels of potential uses in research, ranging from surface level adjustments to applications that blur the boundary of authorship.\n\nAt one extreme, we might consider systems that simulate a copyeditor, correcting spelling or grammar, which are already integrated in many word processing systems.\n\nAt the other extreme might be a system that acts as a ghost writer, converting vague descriptions and specifications into polished final presentations.\n\nIn between, systems might act more like research assistants, collecting and collating information, writing short computer programs, or filling in paragraph bodies from thesis statements.\n\nOther uses might be more like reviewers or editors, enabling researchers to \"bounce ideas\" or summarize a passage of text to ensure that it reads correctly.\n\nThese uses imply corresponding duties by researchers.\n\nMost high-performance GenAI systems are currently available as third-party (i.e., company product, not university-managed resource) cloud (i.e., using remotely located computers) applications, so there is a researcher duty of discretion in what data should be uploaded.\n\nGenAI, while usually convincing and fluent (at least in English), is often factually incorrect or lacking in attribution, so verification is another key duty to ensure accuracy and validity of research outputs.\n\nResearchers may also have a duty to provide transparency and disclosure to identify how and where GenAI contributed.\n\nFinally, we need clear lines of individual and collective responsibility to ensure that the other duties are actually executed.\n\nFor the remainder of this report we will identify specific situations in the research process, and describe how they relate to these categories of use and what duties we believe apply in academic research – and are consistent with Cornell shared values.\n\nIn all of these research stages, we consider GenAI to be a useful research tool that can and should be explored and used to great scholarly advantage.\n\nAs with all tools, the user is responsible for understanding how to use such tools wisely.\n\nAs with all academic research, the responsibilities are shared, but the research leader – called principal investigator in some fields and contexts, and lead author, corresponding author, or lead inventor in others – is considered responsible for communicating expectations to their research colleagues and students, and ultimately bearing consequences of intentional or incidental errors in tool use.",
    "## Generative AI Use across Research Stages\n\nWe considered four stages of research, each of which may receive different emphasis among Cornell’s impressive breadth of research and scholarship areas.\n\nFigure 1 illustrates these four stages where GenAI can be used to great advantage, with appropriate sense of duty by the researcher(s).",
    "#### GenAI for Literature Review\n\nThe volume of published research literature and data have been expanding exponentially, accelerating with technology advances such as movable type and publishing changes such as electronic journal proliferation and public databases.\n\nIt is widely assumed that there are pockets of information in distinct fields that, if combined, could lead to breakthroughs.\n\nBut despite the volume of published information, those serendipitous connections are infrequent because fields are mutually inaccessible due to technical language, and no one from either field knows to look for the other (e.g., epidemiologists and aerosol physicists).\n\nIn fact, interdisciplinary research often espouses the mixing of existing information in new ways, implying that GenAI systems which can keep track of vastly more information than any person may find connections that might be missed entirely by humans.\n\nActing as “state-of-the-art information retrieval” systems (Extance, 2018), they go beyond conventional databases, such as Google Scholar and PubMed, by being able to retrieve, synthesize, visualize, and summarize massive amounts of existing knowledge (e.g., Semantic Scholar, Scopus AI, Microsoft Academic, Iris.ai, Scite, Consensus).\n\nAs such, they have the potential to help overcome the problem of “undiscovered public knowledge” (Swanson, 1986; Davies, 1989), which may exist within the published literature, and break through disciplinary silos, facilitating the discovery of relevant research across diverse academic disciplines.\n\nIn this context, suggested practices for using GenAI in the literature review phase of research conception are that:\n- GenAI can be used to triage, organize, summarize, and quickly get directionally oriented, in the context of an exponentially growing base of reported claims and established knowledge.\n\n- GenAI can be used to assist with drafting literature reviews, although researchers should fact-check and be aware of incomplete, biased, or even false GenAI outputs.\n\nIn some cases, it can help to provide GenAI with explicit prompt text to try to guard against the use of fake references, although it still does not guarantee accurate results.\n\n- Subject to authorship, citation, and fact-checking considerations, it can be helpful to use GenAI to ideate and iterate on the quality of a literature review.\n\nExamples include refining the review to include both prior research and its connection to the new research idea, rewriting the style of the literature review, and refining the literature review to emphasize the contribution of the new research, such as is relevant to other gaps in literature, uncertainties, or even market sentiments.\n\n**Duty of verification**: The reliability and quality of AI-powered literature review tools are limited by the databases they search, which can affect the comprehensiveness and accuracy of the results.\n\nTherefore, it is advisable to use these tools in conjunction with other methods.\n\nAnother major concern when using these tools is plagiarism, as they can produce verbatim copies of existing work without proper attribution or introduce ideas and results from actual published work but provide incorrect or missing citations.\n\nTo minimize the risk of unintentional plagiarism, it is best to start with original text and then use GenAI assistance to refine it, in line with the distinction between AI-assisted and AI-written text (van Dis, Bollen, Zuidema et al., 2023).\n\nThis will help ensure that AI-generated text is original and reflects the authors’ own work, as also emphasized in journals and professional societies that permit the use of GenAI tools (but note that some journals prohibit the use of GenAI for any part of the research process entirely; see Appendix 1 for a summary of existing community publication policies, noting that such policies are subject to change by those communities and publishers).\n\nFinally, depending on the extent of GenAI assistance with information search and literature review production (specifically, when it is used beyond grammatical polishing of author-written text), researchers may have a duty of disclosure for this research stage.",
    "#### GenAI for Research Infrastructure\n\nOne of the more benign possible uses of AI is in improving workflows and research processes.\n\nCollecting and processing data often involve custom software, using complicated APIs (application programming interface, or software with a specific function) that may be poorly documented.\n\nCode generation tools such as Copilot have become powerful and successful, leading to significant improvements in users' ability to create software to collate and analyze data.\n\nOther ways might involve using GenAI to help construct or critique survey questions or interview templates.\n\nIn each case, the AI is not involved in producing or recording data, but in building the infrastructure that is itself used to produce data.\n\nA second category of infrastructure might include code or language generation for presentation of research results.\n\nAPIs for generating figures, such as matplotlib or ggplot2, are notoriously complicated, with innumerable options for modifying the appearance and layout of graphics.\n\nCode generation may help in producing programs to generate graphics from data sets, without being directly involved in the construction of data sets themselves.\n\nSimilarly, language models might assist in generating alt-text for image accessibility.\n\n**Duty of verification**: As with any other use of GenAI, infrastructure-building uses require careful checking to ensure that outputs are correct.\n\nThere should be clear responsibility for who will do this verification.\n\nWe see less need to disclose the use of GenAI in these \"back office\" contexts relative to other uses, though still with care for potential implications at later research stages.",
    "#### GenAI for Data Collection and Generation\n\nA subtle but important distinction is when we move from using GenAI to help develop tools that we use in research to using GenAI as a tool for research, specifically for data collection and generation.\n\nIn principle, the potential for data collection is enormous.\n\nGenAI can be used to help construct data sets from unstructured data, such as descriptions of patents, job vacancies, SEC (US Security and Exchange Commission) filings, banker speeches, etc.\n\nGenAI tools can also be used to synthesize information coming from text, or images.\n\nThey can be employed to self-assess (predict accuracy) and augment which coding tasks are conducted by human iteration.\n\nAdvantages for data collection and generation using GenAI as a tool for research include:\n- Collecting and organizing data.\n\nConsider the Cornell Lab of Ornithology’s example of eBird as one data-rich source: Through this global application platform, birdwatchers have submitted a large amount of bird observations that have already informed development of species distribution models (Sullivan et al., 2014).\n\n- Generating data out of unstructured information.\n\n- Summarizing data coming from various sources.\n\nData related to human clinical trials or patient outcomes hold different and important data privacy concerns, but the collection and organization/cleaning of such data is a key step in inference for patient-centered health outcomes (Waitman et al., 2023).\n\n- Scaling up data collection with GenAI by conducting faster and less resource-intensive experiments.\n\nThe challenges of using GenAI tools for data collection and generation primarily relate to the duties of verification and disclosure:\n- **Issues with performance and accuracy**: Large language models like ChatGPT are currently not fundamentally trained to speak accurately or stay faithful to some ground truth.\n\n- **The reliance on large amounts of data may be challenging**, and the needed data may not always be available.\n\n- **Bias** (King and Zenil, 2023): AI is traditionally trained on data that has been processed by humans.\n\nExample: In using ML to categorize different types of astronomical images, humans might need to feed the system with a series of images they have already categorized and labeled.\n\nThis would allow the system to learn the differences between the images.\n\nHowever, those doing the labeling might have different levels of competence, make mistakes and so on.\n\nGenAI could be used to detect and to some extent redress such biases.\n\n- **Attribution**: Data sources may not always be tracked.\n\nThere is a need for ensuring correct attribution of data sources.\n\nGiven these challenges, the use of GenAI tools for data generation and collection must be carefully documented and disclosed to facilitate research assessment, transparency, and reproducibility.",
    "#### GenAI for Ideation & Hypothesis Generation\n\nWhile the use of GenAI for idea generation is under early consideration by most academic researchers, it is important to weigh its strengths and weaknesses in the early phases of research.\n\nIf we think of the idea generation process as a creative process (as opposed to fact-checking or verification), then complementing ideation with GenAI can potentially offset human weaknesses, such as comparatively poorer memory recall versus recognition processes and narrower breadth of knowledge bases.\n\nIn this sense, GenAI can complement individual researchers during the idea-generation process and democratize access to research assistants.\n\nOn the other hand, scientific knowledge relies on the ability to reason rationally, do abstract modeling and make logical inferences.\n\nHowever, these abilities are handled poorly by statistical machine learning (ML).\n\nHumans do not need a large amount of data or observations to generate a hypothesis, while statistical ML relies on vast amounts of data.\n\nAs a consequence, computers are still unable to formulate impactful research questions, design proper experiments, and understand and describe their limitations.\n\nFurthermore, assessing the scientific value of a hypothesis requires in-depth, domain-specific, and subject-matter expertise.\n\nAn example is the potential of “Language Based Discovery” (LBD) as the possibility to create entirely new, plausible and scientifically non-trivial hypotheses by combining findings or assertions across multiple documents.\n\nIf one article asserts that “A affects B” and another that “B affects C,” then “A affects C” is a natural hypothesis.\n\nThe challenge is for LBD to identify which assertions of the type “A affects C” are novel, scientifically plausible, non-trivial and sufficiently interesting that a scientist would find them worthy of study (Smalheiser et al.\n\n2023).\n\nWhereas GenAI does well in identifying and retrieving potential data constructs, researcher domain expertise remains critical for determining the quality of output (Dowling and Lucey, 2023).\n\nWhile considering the possibilities of GenAI-human collaboration for research ideation, it is essential to emphasize the duty of discretion to prevent the leakage of proprietary, sensitive, and confidential information into public information space.\n\nFurthermore, since using GenAI tools is an evolving space, academics should learn more about GenAI technologies and stay abreast of potentially useful ways for hypothesis generation.\n\nFor example, as food for thought, one of the GenAI prompts used by Dowling and Lucey (2023) for idea generation: \"You [the GenAI tool] created this research idea, and I’d like you to improve it.\n\nCould you see if there is an additional article that can be added, to improve the research idea.\n\nCan you also talk about the novel contribution of the idea.\n\nPlease keep it to about 100 words.” Moreover, as we humans gain experience and familiarity with new tools, we do well to be observant to the expectation that they can also change how we conduct research and interact with researchers at this ideation and hypothesis generation stage – in ways that are not always easy to identify a priori.\n\nFinally, we note that research execution includes expectations of responsible conduct of research, which for some studies and disciplines includes prior approval of data use and management, animal welfare and procedures, and human subjects.\n\nUse of GenAI in research will likely augment considerations of these approvals per expectations of sponsors or federal agencies through research integrity review processes of the university.\n\nThose considerations related to research compliance are expanded in Section D. Next, we consider the stage where research of any type is disseminated through public disclosure including peer-reviewed publications.",
    "### Research Dissemination Stage\n\nGenAI offers new affordances that support both positive and negative outcomes for research dissemination (Nordling, 2003).\n\nOn the positive side there is the potential to level the playing field for non-native speakers of English; to provide writing assistance resulting in improved clarity; and for new tools that aid in more equitable discovery of related work (improving on common practices of searching for well-known authors, for example).\n\nOn the negative side there are serious and reasonable concerns such as erroneous information being disseminated because of inadequate verification; easier plagiarism (either intentional or accidental); lack of appropriate attribution because current LLM-based tools are unable to indicate the source of information; bias and ideological influence; and inappropriate use of GenAI as a lazy peer review tool.\n\nAdditionally, we must be aware that careless use of GenAI may entrench biases in scholarly communication and dissemination, such as reinforcing the positions of prominent scholars and preferring sources in English as the dominant language of the initial trained models available to the general public.\n\nHowever, future GenAI tools may also provide new interventions to oppose existing biases that are entrenched in current practice.\n\nAs such, the following is less focused on specific GenAI tools available today, but more on general recommendations for the responsible use of GenAI tools in research dissemination that upholds research integrity as a principal value at Cornell.\n\nIn this section and research stage, we do not discuss questions of copyright, confidentiality or intellectual property (see Section C), but instead focus on the conceptual impact that GenAI can have on producing research output.\n\nFollowing from the definition of GenAI from above, a key distinction is whether the tool produces output for dissemination that contains concepts and interpretations that the author did not supply.\n\nFrom this perspective, GenAI tools that fill in concepts or interpretations for the authors can fundamentally change the research methodology, they can provide authors with “shortcuts” that lead to a breakdown of rigor, and they can introduce bias.\n\nThis makes it imperative that users of GenAI tools are well aware of the limitations of these tools, and that it is clear who is responsible for the integrity of the research and the output that is produced.\n\nBelow we discuss these issues in more detail and provide a minimal set of norms that we recommend across all disciplines.\n\nHowever, we recognize that the methodology and standards are differentially impacted by GenAI across disciplines (e.g., humanities as well as engineering), and that community norms around the use of GenAI may be stricter than what is outlined below.\n\n- **Authorship**: We posit that GenAI tools do not deserve author credit and cannot take on author responsibility.\n\nThis means that authors of research outputs, not any GenAI tools used in the process, carry the responsibility for checking the correctness of any statements.\n\nAuthors must be aware that GenAI tools can and do produce erroneous results including “hallucinated” citations.\n\nThe content will be viewed as statements made by the authors.\n\nIndeed, there are emerging concerns on impact to scientific publishing with which publishers and AI ethicists are now grappling (Conroy 2023), but the responsibility of authentic authorship is a component of research integrity that will continue to rest with the human authors.\n\n- **Impact on Concepts and Interpretations**: Researchers need to be aware that GenAI tools can have a substantial impact on the research output depending on how they are used to fill in concepts and add interpretations.\n\nIf the impact is substantial, we recommend that the use of GenAI is disclosed and detailed so that readers are aware of its potential impact.\n\nWhat constitutes substantial impact depends on the type of publication (e.g., journal articles, books, talks, reports, reviews, research proposals) and community norms in the respective discipline.\n\nAn example that is probably considered to have a substantial impact in any discipline is the use of GenAI to draft a related work section.\n\n- **Impact on Methodology**: Writing and other dissemination activities typically cannot be separated from conducting the research, and the act of writing is explicitly part of the research methodology in some disciplines.\n\nA key concern is that the use of GenAI as a “shortcut” can lead to a degradation of methodological rigor.\n\nIf the use of GenAI tools can be viewed as part of the research methodology, then we recommend disclosure so that readers can assess the rigor of the methodology.\n\nIndeed, there may be collective impact on methodology at the scale of the research community’s practices.",
    "Indeed, there may be collective impact on methodology at the scale of the research community’s practices.\n\nWhether GenAI becomes a tool that sharpens our minds or a blunt instrument that dulls them is a question that Cornell (and other communities of research scholars) must address actively over time.\n\nHistorically, human imagination sees most tools as helpful implements to move on to harder problems and more creative discovery and analysis, if one masters the tool instead of the other way around.\n\nBut we can also recognize from past experiences that zeal for rapid development of exciting new research-enabling capabilities – especially when these provide competitive advantage over peers that can relate to economic competition or even national security – can shift even the best intentioned individuals to start to behave collectively as a group that focuses sharply on the benefits without openly discussing the costs and trade-offs.\n\n- **Potential for Bias**: Just as authors need to be aware of human biases in their sources, authors using GenAI tools need to be aware that these tools have biases and may reflect specific ideologies.\n\nIt is the authors’ responsibility to counteract or avoid these biases, and authors cannot offload responsibility for bias in their work on the AI system.\n\nFor example, use of a GenAI tool to create a hospital scene might result in an image in which the nurses are female and the doctors are male.\n\nChanging the prompt could address this bias.\n\nAnother issue is that GenAI tools may reflect a particular ideology, or they may perpetuate historical biases because GenAI tools are trained on historical data.\n\nThis may be compounded by particular algorithms such as citation analysis which has an inherent time lag, and might further bias recommendations back in time or towards a dominant group or language.\n\n- **Acceptable Use**: There are many different forms and venues of research dissemination: journal articles, books, talks, reports, reviews, research proposals, etc.\n\nWhat is acceptable use of GenAI in one form of communication is not necessarily acceptable in other forms, and authors must adhere to community standards.\n\nTo take an extreme example, having a GenAI tool draft a peer review from scratch runs counter to the idea of peer review and has an extremely high impact on the review, even if the author checks and edits the review.\n\nThis is likely unacceptable in most communities.\n\nEven within communities, different publication venues (e.g., journal, conference) may have different policies, and authors must check and follow these if more stringent than what is outlined here.\n\n- **AI Literacy to support Research Integrity**: Rigorous and ambitious use of GenAI tools requires a good understanding of the strengths and weaknesses of these tools.\n\nFurthermore, as GenAI has become part of the integrity of research and its dissemination, then research leaders such as principal investigators and faculty supervising student research should now make the appropriate use of GenAI part of their mentoring.\n\nIn particular, part of their mentoring is to communicate the standards and the norms in their specific fields to the researchers and students they lead – just as they mentor on other topics of research conduct (e.g., plagiarism, co-authorship, privacy regulations).\n\n- **Regulations**: Any use of GenAI tools needs to be compliant with regulations (e.g., copyright law, privacy regulations such as HIPAA and FERPA, confidentiality agreements, and intellectual property).\n\nIn particular, users must be aware that use of GenAI tools may disclose sensitive information to a third party, which may be in violation of regulations and confidentiality norms (Lauer et al.\n\n2023; Conroy 2023).\n\nThis extension to implications for subsequent research translation to policies, processes, and products of all types is discussed further in Section C.",
    "### Research Translation Stage\n\nThe use of GenAI in any stage of the research process may impact the translation, protection and licensing of intellectual property (IP), commercialization of technology, open-source release of software and other uses of the research output downstream.\n\nInterpretations of laws and new regulations regarding GenAI are major topics for governments in many countries including the US.\n\nThere may be new government agencies and international organizations created for AI regulation and coordination in the near future.\n\nIn fact, while the European Union recently announced new regulations on artificial intelligence, current understanding is that most of this EU policy effort to create these “first rules” preceded widespread use of GenAI (European Union Commission 2023).\n\nWe can draw no immediate conclusions on how the EU’s risk-based approach may impact GenAI development and uses specifically.\n\nThe nature of the impact of GenAI is still evolving and may change in coming years, with legislation and guidelines expected to lag the use of GenAI and its shorter term implications, which may be inadequately addressed under current laws and regulations.\n\nThe following are important areas for researchers to consider for translation when they use GenAI in their research process:\n\n- **Inventorship and Patentability**: Recent US case law has held that inventors of patents must be human beings under the US Patent Act.\n\nDocumentation of human contribution and disclosure of the nature of GenAI utilization are essential for patent eligibility.\n\nKey information needs to be carefully documented, such as:\n  - Specific GenAI tools used and rationale for their use;\n  - Detailed input into and output of the GenAI tool;\n  - Whether the outputs lead to any aspects of the conception of the invention;\n  - Contributions of individual inventors in the inventive idea, and how they directed and refined the GenAI output; and\n  - For research done in teams, delineate the role of GenAI for each inventor.\n\n- **Copyright and Authorship**: Under current US copyright law, copyright can protect only material that is the product of human creativity and authors of copyrighted materials must be human beings.\n\nWhen incorporating GenAI-generated content, the authors should:\n  - Clearly document the boundary between human-created and GenAI-created content with clear annotations.\n\n- According to guidance published by the US Copyright Office, if copyright registration is sought, the nature and extent of the use of GenAI, if containing more than de minimis AI-generated material, must be disclosed with clarifications of what part of the work was created by the researchers and what part of the work was created by the GenAI.\n\n- Specifically for computational algorithms and code, where research code can be further translated to wider use through copyright and various licensing types including open-source licensing, considerations attach at a time of active discussion.\n\nWe note emerging considerations of copyright infringement, not only for creative works such as songs but also for computational code.\n\nFor example, it is possible that code generated by a LLM reflects code reproduced verbatim from the LLM training set unbeknownst to the user.\n\nWhen such code is part of a research outcome that may be made available to licensees (even open-source licensees), it is possible but not yet well understood how use of such code, even when unintentionally plagiarized from other original sources, may violate copyright or invalidate licenses.\n\n- **Commercialization and Fair Use**: For research that leads to commercialization and publications with financial benefits, to mitigate risks of potential infringement claims, the inventors and authors should:\n  - Prioritize the use of GenAI tools that are trained using only public domain works, which is a small but growing area of development.\n\nFor example, the recently announced AI Alliance coalition that includes Cornell as a founding member and anchored by two for-profit companies, IBM and Meta, advocates for development of open-source software tools including those enabling GenAI ( Lin, B., 2023).\n\n- Understand that the commercial intent can significantly impact fair use considerations.\n\nConsult with relevant university offices, such as the Center for Technology Licensing or General Counsel’s office, when there are questions.\n\n- Stay informed of ongoing litigation that may influence the use of copyrighted materials in GenAI training data set.\n\nThere are pending class action copyright suits by authors against entities owning GenAI tools for training without compensation to the authors.\n\n- **Data Privacy and Protection**: For data that researchers enter into GenAI themselves, it is important that researchers follow Cornell Policy 4.21 on Research Data Retention.\n\nNote that this is an existing policy and practice, simply extended to GenAI.",
    "Note that this is an existing policy and practice, simply extended to GenAI.\n\nPrivate, confidential, or proprietary data owned or controlled by Cornell may have certain contractual or legal restrictions or limitations, such as those to sponsors or collaborators, that would preclude their use in GenAI research projects, and it is a researcher’s responsibility to verify/determine whether any such data sets have restrictions on such use before inputting them into public-facing GenAI and ensuring compliance with any restrictions mandated by contract, law, or governing body (e.g., IRB, IAUCC).\n\nAny use of patient or human-derived data should be disclosed to such governing body during the approval process and any such data set should only be used in research projects upon the explicit approval of the relevant governing body on campus.\n\n- **Training**: Specific to this stage of research translation, it is recommended that the university provide ongoing workshops on campus or through online platforms, and offer training materials through websites and other distribution channels, on topics related to the use of GenAI and its impact on patent rights, copyrighted materials, commercialization, open-source release and other uses to aid the researchers in understanding their rights, obligations, best practices and landscapes of relevant laws and regulations.\n\nIndeed, Cornell includes faculty and staff experts that can facilitate and co-develop such resources as part of their scholarly practice.",
    "### Research Funding & Funding Agreement Compliance Stage\n\nDuring the research funding and funding-agreement compliance stage, there are many potential applications of GenAI.\n\nFor example, these tools can be leveraged to assist in the writing of technical, science-related information for a proposal to a sponsor or a donor, such as the technical scope and anticipated impact.\n\nOn the non-technical side, they can also be used to draw appropriate data from multiple data sources to develop information for a biosketch, a report of Current and Pending Support, and other documentation relevant at this stage of the research process.\n\nWork conducted during the Research Funding and Funding Agreement Compliance stage is poised to benefit from the use of GenAI tools, for example, due to efficiency improvements and reductions in the time taken to produce previously time-consuming work.\n\nHowever, the use of these tools also comes with risks.\n\nGenAI may produce outputs that include incorrect or incomplete information.\n\nThese tools also may lack sufficient security and privacy protections, both in the tools themselves, and in any plug-ins and add-ons to the tools.\n\nNote that we and many federal agency sponsors refer to the person of primary responsibility in research as the PI, or principal investigator, and pronounced pee-I, for shorthand.\n\nWe acknowledge that this term is common for research in the sciences and engineering with cultures of team-based research and that other fields have a tradition of independent scholarship and authorship even when enrolled as graduate students.\n\n- **Responsibility**: As with the earlier stages of research, users of GenAI hold some burden of responsibility (or duty) in the Research Funding and Funding Agreement Compliance stage.\n\nIn this stage, however, it is common to attach the primary responsibility of compliance to the leader of the research effort.\n\nFor example, the accuracy of any information contained in a proposal for funding is ultimately the responsibility of the PI, and so if the PI uses GenAI in the development of materials for that proposal, they must review the information in those materials and correct any omissions, errors, or otherwise inaccurate information.\n\nThe PI must also understand that although resources (for example, research administration staff professionals in Cornell departments, colleges/schools, or research & innovation office units) are available to help them during this stage of the research process, these resources cannot certify to the accuracy of much of the information provided to them, and therefore cannot be expected to identify mistakes in that information, such as those generated by GenAI.\n\nThe PI must also understand that they are responsible for the activities of students and research personnel working on funded projects under their supervision or mentorship, and for ensuring the appropriate use of GenAI tools by those individuals.\n\nDuring this stage, individuals may desire to input information into GenAI to assist in the production of their research proposals, reports to sponsors, or even the public dissemination and translation stage documents that may have specific restrictions placed by the sponsors.\n\nBecause some of this information will be highly sensitive, such as unpublished technical information or private funding data, users of GenAI tools must understand their responsibility for protecting the privacy and security of any information they input into these tools, and must seek approval to do so from the owner (such as the PI) of any such information.\n\nIn fact, even in the peer review of sponsored research proposals (e.g., faculty serving on review panels for NSF or study sections for NIH), the use of GenAI may not be allowable by the sponsor (NSF Notice to Research Community, Dec 2023).\n\nIn this stage of the research process, it is also important for those who are responsible for making decisions regarding the use of sponsored funds to consider whether, and under what circumstances, it is appropriate to charge the use of GenAI tools to a research account, and to ensure their awareness with each sponsor’s requirements.\n\nAlthough some sponsors are clear on whether and how funds may be applied to the use of GenAI, others are not.\n\n- **Guidance and Training**: The nature of these tools, their potential applications, and the associated benefits and pitfalls will continue to develop and change over time, and thus, so will the appropriate guidance on how to use them.\n\nAlthough information and guidance should be shared with users about the risks of the use of GenAI and about which tools to avoid, it is also important to share information and training on how users can make use of these tools, how to navigate security and privacy concerns with confidence, and to provide access to tools that have been vetted and found to be aligned with the university’s expectations for security and privacy.",
    "In this context, we suggest the following considerations as resources developed by and for the research community, including staff professionals experienced in research integrity, information systems, and user experience.\n\n- Broad communications and outreach about GenAI in responsible conduct of research.\n\nThese communications should include guidance and resources on the use of GenAI, as well as information about training, what tools to use or avoid, and references to offices and units that are available to provide support.\n\nWhen appropriate, this outreach should be shared by central offices and posted to central web pages – such as the recently developed Artificial Intelligence website hosted by Cornell Information Technology (CIT) that links to Cornell’s GenAI in Education report and other resources – rather than from individual units or departments, to create consistent understanding and information access across campus.\n\nProviding this type of outreach from central offices can help ensure that the university as a whole is looking to the same resources; that inquiries and concerns come to the appropriate offices; that approaches, advice, and guidance given are consistent across units; and that gaps in accessibility of information and learning are minimized.\n\n- As with training on the use of, for example, animals, human participants, or biological agents in research, centralized training should be provided on the use of GenAI.\n\nThis training should not only focus on risks and concerns, but on how to get the most out of these types of tools, and how to use them better.\n\n“Hackathons as Training” should make it enjoyable for researchers to gain new skills, while also contributing to the safe and responsible use of GenAI.\n\n- Guidance on navigating mistakes made and security breaches should be communicated university-wide.\n\nIt is important to acknowledge that with these new tools comes some anxiety about making mistakes in using them appropriately or even safely.\n\nTo an extent, inadvertent mistakes present opportunities for education and training.\n\nHowever, it is also important that any mistakes that lead to security, privacy, or other concerns are handled correctly and in a timely manner.\n\nInformation should be shared university-wide about Cornell’s expectations and processes with regard to what to do in the case of a potential security or other risk related to the use of GenAI, and which responsible offices should be notified.\n\n- Additional tools and resources should be developed to provide guidance.\n\nIt would be beneficial to researchers and administrative staff alike to develop a GenAI-enabled tool (e.g., a form of a chatbot) that would respond to common inquiries about the use of GenAI in research.\n\nFor example, “Can I use GenAI to edit my scope of work?” This tool could be populated with responses to common questions, so that consistent answers could be communicated broadly – even while appreciating that perspectives and cultural norms and even sponsor requirements and expectations may be changing fluidly in the coming years.\n\nBecause such a tool would be automated and would provide immediate access to answers to these types of common questions, it would both reduce wait times and delays associated with other means of gathering this information, and reduce administrative workload in responding to these types of common requests.\n\nSimilarly, resources that facilitate awareness of resource use (e.g., estimated carbon dioxide emissions associated with tool use; see Section A) can be made available at the Artificial Intelligence website and/or developed by Cornellians whose research and translation focus includes sustainability practices (e.g., Cornell Atkinson Center for Sustainability).",
    "## Perspectives and Cultural Norms\n\nHaving framed the use of GenAI in research across the stages of research above, we here summarize the perspectives that can inform our cultural norms.\n\nThe widespread availability of GenAI tools offers new opportunities of creativity and efficiency, and as with any new tool depends on humans for responsible and ethical deployment in research and society.\n\nThus, it is important that Cornell anticipates that researchers can and should use such tools appropriately, facilitates researcher access to appropriate GenAI tools and to resources to improve researchers’ “AI literacy.” It is also important that we develop shared understanding of the limits of appropriate use of specific publicly available and commercial GenAI tools, as well as the tradeoffs or risks involved in their use.\n\nWhile these perspectives and cultural norms will vary reasonably among different research communities, and likely vary over time in the coming years, we offer the following summary considerations.\n\nThese are considerations of both opportunity (ambitious use that may create new knowledge, insights, and impact for the world) and accountability or responsibility (duty grounded in research integrity of individuals, research teams, and institutions including Cornell).\n\nWe consider these to be peer-to-peer guidelines, not a suggestion of any formalized university policy.\n\nHowever, we remind our fellow Cornellians that two existing policies naturally extend to use of GenAI tools in research:\n- As noted in the University Privacy Statement, Cornell strives to honor the Privacy Principles: Notice, Choice, Accountability for Onward Transfer, Security, Data Integrity and Purpose Limitation, Access, and Recourse.\n\nThis is noted on Cornell’s Artificial Intelligence website, along with preliminary guidelines of accountability that are discussed in this report in the context of researcher duties and research integrity.\n\n- Cornell Policy 4.21 on Research Data Retention.\n\nPrivate, confidential, or proprietary data owned or controlled by Cornell may have certain contractual or legal restrictions or limitations, such as those to sponsors or collaborators, that would preclude their use in GenAI research projects.\n\nIt is the researcher’s responsibility to verify/determine whether any such data sets have restrictions on such use before inputting them into public-facing GenAI and ensuring governing body compliance (e.g., IRB, IAUCC).\n\nRelatedly, any use of patient or human derived data should be disclosed to such governing body during the approval process, and any such data set should only be used in research projects upon the explicit approval of the relevant governing body on campus.",
    "## Considerations for the Cornell Research Community\n\nWe as colleagues encourage faculty, research and administrative staff, and students to help develop the norms, technology, and public reflection on GenAI use in research, to both shape and stay current on these uses and scholarly practices.\n\nThese five areas of consideration for the Cornell research community are summarized below, as part of responsible experimentation.\n\n1.\n\n**HELP DEVELOP THE NORMS, TECHNOLOGY, and PUBLIC LITERACY around GenAI.\n\n**\n   - Actively develop the norms and best practices around the use of GenAI in their disciplines.\n\n- Develop GenAI technology that is particularly suited for research (e.g., improved attribution).\n\nGenAI development for academic use should not be left solely to for-profit companies.\n\n- Engage in GenAI public literacy efforts to foster responsible and ethical use of GenAI tools.\n\nUsing at least one of these tools is enormously helpful to being part of that conversation and process, and many are freely and publicly available with associated caveats on risks of use.\n\nTable 1 provides examples of currently available GenAI tools that can be accessed (denoted as “free” to indicate no financial charge to the user).\n\nWe emphasize user awareness and appropriate caution: only publicly available data should be included, and the user should assume that any entry of information by the user can be absorbed into that tool’s training set and potentially exposed to others.\n\n2.\n\n**STAY UP-TO-DATE with GenAI Uses and Practices**\n   - Each research subcommunity (whether a faculty member’s research group, a department, interdisciplinary research center or institute, or college/school as those researchers see fit) gather relevant information on relevant policies by professional associations, journals and funding institutions to stay up-to-date with evolving policies, practices, and requirements in your field.\n\nAppendix 0 may serve as a discussion starter.\n\n- Train in how to use GenAI tools in a safe, effective, and productive manner in research and innovation.\n\nDevelop expertise in the potential limitations and risks of GenAI tools.\n\nTable 1: A sample of free GenAI tools as of December 2023, where *free indicates no financial fee charged to the user.\n\nNone of these tools are endorsed by Cornell, and those using these tools should note that these tools are only for use with public data.\n\nFurther, when acting as well-informed academic researchers with access to this research tool among others, consider the individual and shared duties of verification, disclosure, and discretion across the stages of research ideation and execution, public disclosure, translation, and funding expectations:\n\n- **Duty OF VERIFICATION**\n  - DO verify the accuracy and validity of GenAI outputs.\n\nThe responsibility for research accuracy remains with researchers.\n\n- DO check for unintentional plagiarism.\n\nGenAI can produce verbatim copies of existing work, or more subtly, introduce ideas and results from other sources but provide incorrect or missing citations.\n\n- **Duty OF DISCLOSURE**\n  - DO keep documentation and provide disclosure of GenAI use in all aspects of a research process, in accordance with the principles of research reproducibility, research transparency, authorship and inventorship.\n\n- **Duty of DISCRETION**\n  - X DO NOT assume that GenAI is private.\n\nGenAI systems run on training examples, and user input and behavior are a prime source.\n\nEven if organizations that provide GenAI tools do not currently claim to use data in this way, there is no guarantee that they may not in the future.\n\n- X DO NOT share confidential, sensitive, proprietary, and export-controlled information with publicly available GenAI tools.\n\n- X DO NOT assume that GenAI output is already considered part of the public domain (e.g., not legally encumbered by copyright).\n\nGenAI tools can \"memorize\" their training data and repeat it with a level of verbatim accuracy that violates copyright.\n\nEven material that is not copyrighted may produce liability for corporate partners in sponsored research, if it is derived from data generated by a competitor.",
    "## Considerations for Cornell Leadership\n\nWe also provide considerations for Cornell leadership, particularly for aspects of GenAI preparedness and facilitated use in research and innovation that can be implemented collectively across Cornell’s colleges, schools, and campuses.\n\n1.\n\nDevelop a knowledge base module, perhaps as part of responsible research conduct training resources, for rigorous, ethical and responsible use of GenAI in research and related activities.\n\nUsers of GenAI tools need to understand their strengths and weaknesses, as well as regulations around privacy, special data considerations such as personally identifiable, human subject, or proprietary commercial data, and confidentiality and commercialization.\n\n2.\n\nConsider procurement of Cornell-licensed GenAI tools with data and privacy protection as facilities for research, as well as for administrative and teaching uses.\n\nText generation and chat, program code generation, streamlined processes, and image generation would likely all be of value.\n\n3.\n\nConsider development or co-development of GenAI tools that are particularly suited for academic research use cases, including use cases in research administration services.\n\n4.\n\nIdentify relevant central offices responsible for providing university-wide communications, guidance, outreach, and training to all GenAI users on various aspects of uses.\n\nTo the extent that it is possible and relevant, information on the use of GenAI should be shared from central locations to encourage consistent access and understanding across the university, and to avoid siloed, inconsistent, or incorrect information.\n\n5.\n\nIn support of Cornell's public engagement mission, recognize Cornell efforts that improve GenAI public literacy beyond the university-affiliated community.\n\n6.\n\nConsider periodic updates to Cornell guidance, through a task force or other appropriate mechanisms, given the rapidly changing landscape of generative AI tools, uses, and considerations in academic research and translation of research outcomes.",
    "## Appendix 0.\n\nPrompts on GenAI in Research (Discussion Starters or Frequently Asked Questions)\n\nWe further consider best practices and use cases in response to questions (prompts) for each of the research stages.\n\nDuring the task force, we simply used these prompts to stimulate early conversations and perspectives among Cornell colleagues from different fields of research and at different stages of research.\n\nThe responses to such questions provided below are not prescriptive or complete, but share our initial, collective responses to such prompts as a diverse group of faculty and staff began shared discussion of this topic.\n\nThese same questions could be used within Cornell research group discussions or at department faculty meetings.\n\nFor researchers to generate familiarity or insights into how these tools work to generate text or images or audio, these same prompts could be entered into multiple GenAI-enabled programs, or entered multiple times in the same GenAI-enabled program, or variations on these prompts.",
    "## Research Ideation and Execution Stage\n\n- When using a tool such as ChatGPT to generate research ideas for a research project sponsored by NSF, how do the researcher and principal investigator decide on which information and ideas to enter and “share” with ChatGPT?\n\nAny information entered into public versions of ChatGPT involves sending that data to a third-party that is under limited confidentiality and privacy restrictions (if any) with end users and not party to agreements with the NSF via data entry by other users, and the information that is entered can eventually become public.\n\nAs such, where the use of ChatGPT for research idea generation does not currently violate any known general NSF policies, users should also be sure not to violate any other agreements that may exist relative to their funding sponsorship agreements, such as confidentiality, intellectual property, and entity identification clauses (e.g., mentioning NSF in any input data may be discouraged to the extent that it is in conflict with an agreement).\n\n- When using a tool such as ChatGPT to brainstorm solutions for research sponsored by a company (e.g., Samsung, Boeing, Johnson and Johnson, Google), how do the researcher and principal investigator decide what information about the project can be entered and shared?\n\nAgain, any information entered into public versions of ChatGPT involves sending that data to a third-party that is under limited confidentiality and privacy restrictions (if any) with end users and not party to agreements with corporate sponsors via data entry by other users.\n\nFurthermore, any information that is entered can eventually become public.\n\nAs such, where the use of ChatGPT for brainstorming solutions does not currently violate any known general policies, users should also be sure not to violate any other agreements that may exist relative to their funding sponsorship agreements, especially including but not limited to, confidentiality and intellectual property clauses.\n\n- When using generative AI tools to summarize the literature for the introduction or discussion of a peer-reviewed article, how should the researcher and corresponding author attribute or disclose this section of a manuscript or thesis?\n\nIn general, authors and/or principal investigators have ultimate responsibility for works (including their accuracy), and furthermore, summaries should not violate plagiarism rules and regulations.\n\nCitation style guides and support websites (e.g., for APA, Chicago, Harvard) are being updated to reflect proper citations for verbatim output from generative AI and other uses.\n\nAs a general practice, authors should be transparent and fully disclose uses of generative AI technologies, consistent with publication outlet, department, or area policies.\n\n- What are the conditions, if any, that a researcher should not use GenAI to generate research ideas?\n\nExamples may vary among research fields, sources of information included in a prompt including FERPA or HIPAA data, and collaborating or sponsoring organizations.\n\nThere are no general rules that prohibit the user of generative AI to generate research ideas.\n\nHowever, because inputs into public generative AI platforms are not confidential and data can also become public, sensitive information and individual data should never be entered for any phase of a research project, pursuant to personal data identification, re-identification, and/or chain of custody FERPA and HIPAA requirements.\n\n- In the process of research publication development, can tools such as Bard or ChatGPT be used: To summarize responses to online surveys of income level of state residents?\n\nTo summarize preclinical research animal model histology?\n\nTo summarize patients’ blood oxygen levels in a registered NIH study?\n\nHow can the differences among these use cases be distinguished in the responsible conduct of research?\n\nThe summary of online survey data and/or other data sets can be assisted by Bard or ChatGPT to the extent that the researcher 1) does not enter confidential or data protected by other laws (e.g., HIPAA), 2) does not violate a broader agreement (e.g., between researcher, institution, host, and/or funder), and 3) has responsibility for the accuracy of the summary.",
    "## Research Dissemination Stage\n\n- When a figure for a publication or presentation or patent disclosure is generated by AI tools (e.g., Midjourney; DeepAI), how should the principal investigator (who is typically the corresponding author or communicating inventor) verify the accuracy and intellectual ownership over the data or content of that image?\n\nWe see a distinction between cases where the author supplies all semantics and uses a tool for layout and rendering, and when a tool is used to introduce semantics such as organization of ideas or the generation of structure.\n\nWhere the author supplies all semantics the use is akin to PowerPoint style suggestions or an automated layout tool, and acknowledgement is generally unnecessary unless publication policies require it.\n\nHowever, when GenAI introduces new semantics then we advise acknowledging its use, and checking the output carefully for accuracy.\n\nNorms around intellectual ownership are in flux at this time, and we caution authors that this poses a substantial risk.\n\n- Can images used in publications and theses be created wholly by generative AI?\n\nHow does this expectation change if the GenAI-drafted images are edited by the authors?\n\nHow does that vary among research disciplines?\n\nUses include generation of a cover image for a book or presentation, or images similar to clipart.\n\nWe advise that authors should generally acknowledge the use of GenAI in this case, and they should carefully check images for bias and accuracy.\n\nAuthors should be aware that, in the US and many other jurisdictions, it is not possible to claim copyright in non-human created works even if any human additions/edits may be copyrightable.\n\nThere are unresolved legal questions regarding possible copyright infringement both as a result of the training of GenAI programs on works under copyright, and as a result of output that might closely resemble works under copyright.\n\nThere is also significant variability in the acceptability of GenAI-generated images based on the publication venue.\n\n- When and how should the corresponding author inform a journal of manuscript elements created by GenAI, if not explicitly required to disclose by the publisher and when not obviously using or studying GenAI?\n\nExamples may be a proposed cover image, a graphical depiction of a new method, a graph containing research-generated data, a set of test data, generation of derivative data, etc.\n\nGenAI technology is increasingly being built into services that provide grammar checks, polish language, and proofreading.\n\nGeneral purpose tools such as ChatGPT are also effective for these tasks.\n\nIt is not usual to acknowledge the use of checking and suggestion tools.\n\nGenAI tools, like human proofreaders who may not understand the subject matter in detail, can suggest changes that change the intended meaning so authors must still verify suggestions with care.\n\nCommercial checking and suggestion tools are being extended with GenAI features to draft entire sections or articles, or summarize texts, so authors should consider when their use crosses the line into generative use as defined above.\n\n- When and how should research group leaders (e.g., faculty) communicate these expectations of appropriate/ethical/responsible use of GenAI in research to researchers who are undergraduate students?\n\nGraduate students?\n\nPostdoctoral researchers?\n\nOther research staff?\n\nEducating about the responsible use of GenAI should become part of the regular training on research methodology and norms of the respective discipline.\n\nThis includes training that research leaders provide, but it also is a responsibility of Cornell to educate faculty and students on the affordances and pitfalls of GenAI tools.",
    "## Research Translation Stage\n\n- If an invention is reduced to practice in part by use of generative AI, how should the inventors document and inform others when considering a disclosure of invention or copyright?\n\nAny use of GenAI in the conception and reduction to practice of an invention or generation of copyrighted materials should be carefully documented and disclosed to the Center For Technology Licensing by the inventors/authors.\n\nFor example, as to the conception and reduction to practice:\n  - What was the GenAI tool used?\n\n- What were the inputs to the GenAI?\n\nDo you have rights to the data used for input?\n\n- What were the outputs of the GenAI?\n\n- How did the outputs of the GenAI, if at all, lead to the conception of any aspect of the invention?\n\n- Were there any difficulties encountered in using the GenAI to yield the outputs desired and, if so, which researchers updated use of the GenAI, model and/or data to yield the desired outputs?\n\n- Which researchers substantively contributed to/controlled the development of the input and output corresponding to the invention?\n\n- If the research outcome is open-source licensable and/or posted on an open-source repository (e.g., code or algorithm or app), should and how should the researcher disclose use of GenAI in creation of the “open source” item?\n\nDisclosure of the use of a specific GenAI tool and possibly even the origins of, and the rights to use, the input data used will likely be viewed as the standard for ethical behavior over time.\n\nCurrently there are no hard and fast rules.\n\n- If the research outcome is a creative work (e.g., book, play, sculpture, musical score, multimedia exhibit) that used GenAI in the creation of that work, how should the researcher disclose that contribution in discussions of copyright?\n\nAccording to Guidance published by USCO, if copyright registration is sought, the nature and extent of the use of GenAI, if containing more than de minimis AI-generated material, must be disclosed, and what part of the work was created by the researchers and what part of the work was created by the GenAI.\n\n- How should the researcher inform themselves of the uncompensated contributions of others to the GenAI output used in their own creative and/or copyrighted work or invention?\n\nHow does this responsibility depend on whether the researcher derives personal financial benefit (e.g., royalties on published book) from the research outcome?\n\nThere are pending class action copyright suits by authors against entities owning GenAI tools.\n\nIn those suits, GenAI tools are alleged to utilize existing copyrighted works for training without compensation to the authors.\n\nCommercial purpose is an important factor in the determination of fair use.\n\nFor research that leads to commercialization and publications with financial benefits, it will be safer to use GenAI tools that are trained using only public domain works.\n\nFor data that the researchers put into GenAI themselves, it’s important that they make sure they have the rights to do so regardless of whether they expect financial benefits from the output of GenAI.",
    "## Research Funding and Funding Agreement Compliance Stage\n\n- If grant proposal information related to science (technical scope) and non-science (biosketch, current & pending funding) components are generated by generative AI, who is responsible for editing them before submission to a potential sponsor?\n\nWho is responsible if there are omissions or errors in those work products?\n\nThe PI is responsible for the accuracy of information related to the science, as well as for omissions and errors in that information.\n\nOn the non-science side, the PI is, again, primarily responsible for the information contained in their proposal.\n\nThere are resources available to help them (such as research administration staff), but those resources cannot certify the accuracy of the information provided to them, or identify mistakes in information provided to them that occurred as a result of the use of GenAI.\n\nIt is also important for the PI and their less experienced collaborators (mentees, supervised students) to discuss concerns about inputting information into GenAI tools.\n\nThis information can be highly sensitive (unpublished technical information, for example) or personal to an individual (Current & Pending funding that must be disclosed to employers and sponsors but not to peers or the general public).\n\nTo an extent, whether information is considered sensitive may depend on the context of the use of GenAI or of the research field itself.\n\nConsensus of this task force was that the PI is responsible for the security of his or her research information, but that anyone who intended to input information into GenAI would need to seek approval to do so from the owner of that information (such as the PI).\n\n- Should the costs of Generative AI be charged to a research account, assuming this is not disallowed by the corresponding funding agreement (i.e., not disallowed by a research sponsor)?\n\nThe appropriate source of funds for GenAI in research may depend on how the GenAI is being used.\n\nIf such use is categorized in such a way that other things falling under the same type of use could be charged to a research account (e.g., software services), then it is plausible that the use of GenAI may be acceptable.\n\nIn some cases, sponsors note definitively whether such charges to sponsored project accounts are allowed, but this is not always the case.\n\n- If a principal investigator becomes aware that her graduate student queried a generative AI tool (e.g., ChatGPT) with proprietary data obtained appropriately from a company when summarizing research team meeting notes, what should her next steps be?\n\nWho is responsible for notifying the company?\n\nWho is responsible for remedying the action if the company has grounds to sue for breach of the data use agreement?\n\nThe PI is responsible for what their students do in the course of their Cornell work, and is therefore responsible for ensuring that these individuals use GenAI resources appropriately.\n\nThat said, mistakes are bound to happen, and they present great opportunities for education and training of both the faculty and of the students.\n\nFurther, in these situations when proprietary information is input into GenAI inappropriately, it is reasonable that the PI may feel compelled to directly report this issue to their technical contact at the company, but doing so may not align with Cornell’s processes for resolution.\n\nTherefore, we should educate faculty about the appropriate way to resolve something like this, which Cornell resources are available to them, and what offices – such as OSP or Counsel’s Office – are available to help.\n\n- What tools or approaches might Cornell researchers find useful for shared awareness of responsible GenAI use?\n\nThe use of GenAI comes with significant privacy and security concerns, and it may be important for the university to gain an understanding of the privacy policies of GenAI companies in order to determine whether they are safe to use.\n\nAlso of concern are plug-ins to GenAI programs, which may come with their own privacy and security issues.\n\nAlthough Cornell should provide guidance on risks and tools to avoid, it would also be very useful to provide researchers with information about what tools and resources they can or should use, as well as access to those tools, and confirmation that they’ve been vetted and found to be secure.\n\nThe university could also provide information about the use of GenAI through other means:\n  - Creation of a tool – “Asking for a Friend” – which could be used to answer questions researchers may have (ex.\n\n: “Can I use GenAI to edit my scope of work?”).\n\n- Training should not only focus on risks and concerns, we should also provide training on how to get the most out of these types of tools, and how to use them better.\n\n“Hackathons as Training” – could make it fun for researchers to gain new skills, while also contributing to the safe and responsible use of GenAI.",
    "“Hackathons as Training” – could make it fun for researchers to gain new skills, while also contributing to the safe and responsible use of GenAI.\n\n- The IT@Cornell web page is a centralized location that can be used to post preliminary guidelines, general information about GenAI, and what researchers need to know about it.\n\n- In order to educate researchers on the use of GenAI, communication and outreach are key.\n\nWe should educate researchers about the central offices that issue training, guidance, etc.\n\nthat can help them, rather than leaving them to rely on potentially siloed offices in the units that may not provide consistent advice.\n\nIf the university as a whole is looking to the same resources, and inquiries consistently come to the same/appropriate offices, approaches/advice/guidance given is more likely to be consistent university-wide.\n\n- Finally, much like training on the use of other things in research (animals, human participants, biological agents, etc.\n\n), education and training should be provided on how to use GenAI safely.",
    "## Appendix 1.\n\nExisting Community Publication Policies\n\nWe surveyed current policies regarding the use of GenAI in research from funders, journals, professional societies, and peers.\n\nWe found that most of these examples were stated by journals, professional societies, and research funders, and centered around the research dissemination phase.\n\nThese include the authorship and review of publications.\n\nAs of fall 2023, we found relatively little policy about the \"private\" phases of research, such as ideation or data analysis in what Fig.\n\n1 of our report describes as research stage A, the ideation and execution phase.\n\nIn these policies, institutions tend to be cautious rather than eager to embrace the possibilities of AI.\n\nCurrent policies on AI use are often cited in the context of publication, through journals, funding agencies, or professional societies that run peer-reviewed conferences.\n\nMany express an openness to the use of GenAI as a tool for writing and editing, especially in so far as it \"levels the playing field\" for researchers who are not native English speakers.\n\nBut many also express serious concerns about generation of text beyond grammatical polishing of author-written text.\n\nPotential harms usually fall into two categories.\n\nFirst, AI can produce plausible-sounding information that is not, in fact, correct.\n\nThe published record could become increasingly polluted by unfounded information that is extremely difficult to detect.\n\nSecond, AI can produce verbatim copies of existing work, possibly causing unintentional plagiarism.\n\nMore subtly, AI could introduce ideas and findings from actual published work but omit or provide inaccurate citations.\n\nThere is significant concern about responsibility.\n\nWe can find no example of a journal that allows non-human authors, and several that explicitly ban such a practice, as it cannot meet the authorship criteria of accountability for the work.\n\nBut there are also more subtle questions of duty.\n\nGiven that the use of generative AI provides substantial risk of inappropriate output (either false or inadequately cited) and may require substantial work to fact-check, who should carry out that work, and who should be punished if it is not done adequately?\n\nThere is unlikely to be a single policy throughout academia as there are many distinct cultures of collaboration and responsibility.\n\nSome fields make strong distinctions between a principal investigator (PI)/advisor's work and PhD student work.\n\nIn this case a PI may have comparatively little responsibility to check an advisee's use of GenAI.\n\nIn other fields PIs and students work collaboratively on multi-author publications, where a senior or last author may be expected to have a more supervisory (and therefore responsible) role.\n\nSome agencies also raise the issue of sensitive data.\n\nThe best current language models are beyond the capabilities of typical laptop hardware, so they are often available as a cloud-based service.\n\nWhile there have been differing statements about what OpenAI or Google might do with information uploaded to the systems, the bottom line is that using such tools exposes potentially sensitive information to third parties.\n\nTherefore, institutions explicitly ban entering confidential, sensitive, proprietary, and export-controlled information into publicly available GenAI tools.\n\nSimilarly, grant agencies and publications prohibit using AI tools in the peer review process to avoid the confidentiality breach.\n\nWhen AI tools are permitted, there is a consensus about documenting their use in research conception and execution for reporting transparency and reproducibility/replicability purposes.\n\nMost publications require disclosure of GenAI use in the Materials and Methods section of a submitted manuscript as well as in a disclosure to editors, except when AI is used as an editorial assistant for author-written text.\n\nLiving guidelines for generative AI published in Nature, \n\n“For Researchers, reviewers and editors of scientific journals \nBecause the veracity of generative AI-generated output cannot be guaranteed, and sources cannot be reliably traced and credited, we always need human actors to take on the final responsibility for scientific output.\n\nThis means that we need human verification for at least the following steps in the research process:\n- Interpretation of data analysis;\n- Writing of manuscripts;\n- Evaluating manuscripts (journal editors);\n- Peer review;\n- Identifying research gaps;\n- Formulating research aims;\n- Developing hypotheses.\n\nResearchers should always acknowledge and specify for which tasks they have used generative AI in (scientific) research publications or presentations.\n\nResearchers should acknowledge which generative AI tools (including which versions) they used in their work.",
    "Researchers should acknowledge which generative AI tools (including which versions) they used in their work.\n\nTo adhere to open-science principles, researchers should preregister the use of generative AI in scientific research (such as which prompts they will use) and make the input and output of generative AI tools available with the publication.\n\nResearchers who have extensively used a generative AI tool in their work are recommended to replicate their findings with a different generative AI tool (if applicable).\n\nScientific journals should acknowledge their use of generative AI for peer review or selection purposes.\n\nScientific journals should ask reviewers to what extent they used generative AI for their review.”",
    "## Appendix 2.\n\nReferences Consulted or Cited\n\n- Conroy, G. (2023).\n\nHow ChatGPT and other AI tools could disrupt scientific publishing, Nature 622, 234-236.\n\n- Current Cornell guidance on Gen AI for use in research, education, and administration (2023): \n\n- Current Cornell guidance on Gen AI in teaching (Nov 2023): Available for download at  and accessible online at \n\n- Bockting, C. L., van Dis, E. A. M., van Rooij, R., Zuidema, W., & Bollen, J.\n\n(2023).\n\nLiving guidelines for generative AI—why scientists must oversee its use.\n\nNature, 622(7984), 693-696.\n\n- Davies, R. (1989).\n\nThe creation of new knowledge by information retrieval and classification.\n\nJournal of documentation, 45(4), 273-301.\n\n- Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., ... & Wright, R. (2023).\n\n“So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy.\n\nInternational Journal of Information Management, 71, 102642.\n\n- Dowling, M., & Lucey, B.\n\n(2023).\n\nChatGPT for (finance) research: The Bananarama conjecture.\n\nFinance Research Letters, 53, 103662.\n\n- European Union Commission (2023).\n\nPress release, Dec 9, 2023: Commission welcomes political agreement on Artificial Intelligence Act.\n\n- Figure 1: Subfigure component sources were obtained from three sources: Microsoft Powerpoint; Freepix free license (patent certificate icon):  \"title=\"certificate icons.\n\nCertificate icons created by Freepik – Flaticon; and iStock (istockphoto.com) standard license with subscription (top hat icon).\n\n- Free GenAI to write your research paper with/for you: \n\n- Free GenAI to assist with literature review:  (among many available AI-powered tools for literature review, such as Iris.ai, Microsoft Academic, Scopus AI, Elicit, Scite, and Consensus).\n\n- Glossary of GenAI-related terms.\n\nSteven Rosenbush, Isabelle Bousquette and Belle Lin, “Learn these AI basics,” the Wall Street Journal \n\n- Institutional Review Board considerations: Some research centers consider these implications as part of",
    "### About this guide\n\nStudents and instructors are embracing ChatGPT (GPT-4 as of March 2023) and similar artificial intelligence (AI) technologies across disciplines for different learning goals.\n\nThere is no one-size-fits-all best practice for their use.\n\nThis document is meant as a guideline for instructors on what to consider as these tools evolve.\n\nWe will provide strategies for adopting AI technologies in a responsible, ethical manner, and innovating within each discipline, major, and course.\n\nExploring and communicating about the opportunities and limitations to using these tools will allow instructors and students to critically think about how knowledge is created.",
    "## About ai@cam\n\nThe University of Cambridge aspires to be a global leader in AI research, an innovator in AI education, and a hub that connects research with business, civil society, and policy, supporting the deployment of AI technologies for wider social and economic benefit.\n\nIts vision is of AI-enabled innovations that benefit society, created through interdisciplinary research that is deeply connected to real-world needs.\n\nai@cam is the University of Cambridge’s flagship mission to deliver this vision, driving a new wave of AI innovation that serves science, citizens, and society.\n\nMore information: ai.cam.ac.uk",
    "## About the Bennett Institute for Public Policy\n\nThe Bennett Institute for Public Policy is one of the UK’s leading public policy institutes, achieving significant impact through its high-quality research.\n\nOur goal is to rethink public policy in an era of turbulence and inequality.\n\nOur research connects the world-leading work in technology and science at the University of Cambridge with the economic and political dimensions of policymaking.\n\nWe are committed to outstanding teaching, policy engagement, and to devising sustainable and long-lasting solutions.\n\nMore information:",
    "### FOREWORD\n\nWhich path should the UK take to build national capability for generative AI?\n\nThe rapid rollout of generative AI models, and public attention to Open AI’s ChatGPT, has raised concerns about AI’s impact on the economy and society.\n\nIn the UK, policy makers are looking to large language models and other so-called foundation models as ways to potentially improve economic productivity.\n\nThis policy brief from Dr Ann Kristin Glenster and Sam Gilbert outlines which policy levers could support those goals.\n\nThey argue that the UK should pursue becoming a global leader in applying generative AI to the economy.\n\nRather than use public support for building new foundation models, the UK could support the growing ecosystem of startups that develop new applications for these models, creating new products and services.\n\nA UK approach to generative AI could leverage the existing national strengths in safe, responsible, and ethical AI to put human safety and flourishing at the forefront of innovation.\n\nA national approach could achieve these goals by increasing understanding of and access to generative AI tools throughout the economy and society.\n\nThis policy brief answers three key questions:\n- What policy infrastructure and social capacity does the UK need to lead and manage deployment of responsible generative AI (over the long term)?\n\n- What national capability does the UK need for large-scale AI systems in the short- and medium-term?\n\n- What governance capacity does the UK need to deal with fast moving technologies, in which large uncertainties are a feature, not a bug?\n\nThanks to Ann Kristin and Sam’s extensive research, this policy brief maps out an ethical framework for the governance of generative AI, through the creation of an AI Bill.\n\nWe hope that this policy brief will be useful to a wide range of stakeholders and address how we can use regulatory and legislative power today, to ensure that the British public can trust how this technology is used.\n\nWe are also excited that this policy brief brings together expertise from three groups at the University of Cambridge: the Bennett Institute for Public Policy, Minderoo Centre for Technology and Democracy and ai@cam.\n\nEvidenced-based, science-informed research like this brief is what our three organisations do best, and we hope that our insights can help decision-makers navigate public debates and policy choices with more clarity.\n\nProfessor Dame Diane Coyle  \nBennett Professor of Public Policy, Bennett Institute for Public Policy, University of Cambridge  \nProfessor Gina Neff  \nExecutive Director, Minderoo Centre for Technology and Democracy, University of Cambridge  \nProfessor Neil Lawrence  \nDeepMind Professor of Machine Learning, University of Cambridge",
    "## EXECUTIVE SUMMARY\n\nThis policy brief aims to give the policy community an overview of the generative artificial intelligence (AI) field and highlight the key policy issues raised by its rapid development and adoption.\n\nOur main findings and recommendations are as follows:\n\n- Generative AI represents a significant technological advance, of comparable importance to the web, and offers a material opportunity for the United Kingdom (UK) to improve economic productivity.\n\n- The aspiration for the UK to become a global leader in the development of the foundation models that support generative AI products and services is unrealistic given the capital investment and compute capacity required.\n\n- The UK should focus on being a leader in applying foundation models in the real world, to change how things are produced, responsibly, safely, and fairly.\n\n- Expanding understanding of and access to generative AI tools throughout the economy and society is the most important way that the UK can build capacity in responsible AI implementation.\n\n- Innovation and skills policy levers can be applied to this challenge, including lobbying major cloud computing infrastructure providers to establish GPU-clusters in the UK, and introducing tax incentives for businesses to apply generative AI technologies to their existing operations.\n\n- There are potential legal, regulatory, cultural, and societal impediments to the adoption of generative AI which need to be addressed, including uncertainty over the applicability of data protection, intellectual property, and product safety laws.\n\n- The sectoral approach to regulation based on value-based principles rather than enforceable legislation means there is a risk that regulators will lack the capacity to enforce their regulatory frameworks, or that sectoral regulatory frameworks will develop with contradictory and incoherent rules.\n\n- Currently, the UK’s approach to regulating generative AI combines value-based sectoral regulation with efforts to shape international agreements.\n\nAs a result, businesses lack incentives to comply with Responsible AI principles, with negative consequences for public trust in organisations’ use of generative AI.\n\n- We believe this can be addressed through an AI Bill and sectoral legislation designed to embed an ethical framework for the governance of generative AI in domestic law, along with investment in strengthening regulatory capacity.",
    "## INTRODUCTION\n\nThis policy brief aims to give the policy community an overview of the generative AI field.\n\nIt highlights the key policy issues raised by its rapid development and adoption (section 2).\n\nWe focus in particular on the questions of what is needed for the UK to unlock the productivity improvements promised by generative AI (section 3), and what impediments will need to be addressed to reconcile generative AI with emerging legal and ethical frameworks (section 4).\n\nFinally, we make a set of recommendations for building capabilities to augment productivity through generative AI (section 5).\n\nExplanatory infographics, case studies, and a glossary of generative AI terminology (denoted by italics) are interspersed throughout.\n\nWe note that AI is a contested term.\n\nFor the purposes of this brief, we assume a narrow definition of AI, taking it to mean computer systems which can improve themselves without explicit instructions, by making inferences from patterns in data.\n\nThe ‘AI’ we are concerned with is the kind that (among other things) organises social media newsfeeds, determines the sentiment of online comments, decides which adverts should be displayed on a webpage, classifies medical images, or recommends music, films, or books people might enjoy based on what they have previously consumed.\n\nWe recognise the debate about the potential for developments in AI research to engender machine superintelligence that poses an existential risk to humanity, but do not enter into it here – not least because AI Safety is already addressed extensively elsewhere as the focus of the UK’s Frontier AI Taskforce.\n\nWe likewise acknowledge important critiques which have drawn attention to the ways AI systems can reproduce bias and injustice, taking as a given that all AI should be responsible AI.\n\n[Department for Science, and Technology.\n\n2023.\n\n“Tech Entrepreneur Ian Hogarth to Lead UK’s AI Foundation Model Taskforce,” Gov.uk < model-taskforce> accessed 21 September 2023]  \n[“Responsible AI UK.” n.d.\n\nResponsible AI UK < accessed 21 September 2023]",
    "### What is generative AI capable of and how does it work?\n\nGenerative AI involves running the kind of pattern-matching that machine learning systems do, only in reverse.\n\nRather than looking at data and finding existing examples that fit a particular pattern, it draws on data to ‘generate’ new examples of that pattern.\n\nGenerative AI systems can therefore output original high-quality text, images, audio, or video at mindboggling speed and scale.\n\nMuch of the excitement about generative AI has been driven by the runaway popularity of ChatGPT, a consumer facing app developed by OpenAI, which reached 100 million users even faster than the app TikTok.\n\nChatGPT is underpinned by a type of generative AI system called a large language model (LLM).\n\nLLMs take instructions (or prompts) from users in natural language, and then output text in response—from stump speeches to Shakespearean sonnets and everything in between.\n\nThey work by predicting what word (or, strictly, token) ought to come next in a sequence, based on inferences from the vast corpus of data on which they have been trained, together with the user’s instructions.\n\nWhile OpenAI’s GPT-4 is the best-known LLM, there are many other examples (see Figure 2).\n\nAlthough image-generation models like Midjourney use a different process called diffusion, from the perspective of the user they work in the same way as LLMs.\n\nNatural language text prompts can yield Van Gogh-inspired cover art for ‘Stairway to Heaven’, Pope Francis in a Balenciaga puffer jacket, or more or less anything else than can be imagined and articulated.\n\n[Evans, Benedict.\n\n2022.\n\n“ChatGPT and the Imagenet Moment,” Benedict Evans < accessed 21 September 2023]  \n[Tung, Liam.\n\n2023.\n\n“ChatGPT Just Became the Fastest-Growing ‘app’ of All Time,” ZDNET < accessed 21 September 2023]  \n[{N.d.} Prompthero.com < accessed 21 September 2023]; “Reddit - Dive into Anything.” [n.d.].\n\nReddit.com < accessed 21 September 2023]",
    "### Generative AI’s limitations\n\nAt first sight these capabilities can seem miraculous, but it is important to be aware of their limitations.\n\nDiffusion models are not underpinned by an understanding of the physical world; they don’t ‘know’ what text symbols mean, or that human hands usually have five fingers.\n\nThe results can be comical, nightmarish, or simply wrong.\n\nSimilarly, LLMs do not function like search engines, reliably retrieving information from a database.\n\nRather, LLMs generate new text probabilistically, meaning that they often invent facts and refer to seemingly plausible but non-existent academic studies and URLs (a phenomenon known as ‘hallucination’).\n\nOvercoming these limitations requires a combination of fine-tuning, prompt engineering, and plugins.",
    "### Foundation models vs applications\n\nBoth LLMs and diffusion models are types of foundation model—a term describing models that others could ‘build on top of’ for many different purposes.\n\nThis is enabled by giving third-party developers API access, allowing them to incorporate foundation model capabilities into their applications.\n\nNew startups have been able to develop software-as-a-service (SaaS) products that apply foundation models in specific contexts.\n\nFor example, Harvey AI uses OpenAI’s GPT models in products designed to assist lawyers with research, contract drafting, and document review.\n\nEstablished tech companies have enhanced their products with generative AI features.\n\nFor example, the graphic design platform Canva introduced a text to image feature powered by the DALL-E 2 model, and Microsoft added LLM-powered writing and editing features to its Office 365 products.",
    "### The economics of generative AI\n\nProviders of foundation models earn revenue by charging a small fee for each API request.\n\nAs a result, their business model depends on the volume of API requests from applications being sufficient to offset the massive compute costs involved in developing and operating foundation models.\n\nThese costs are partly a function of the vast size of training datasets.\n\nFor example, the text used to train OpenAI’s GPT-3 model included a 45 terabyte archive of the web, 11,000 books, and the entirety of Wikipedia.\n\nProcessing such large quantities of data requires Graphics Processing Units (GPUs).\n\nA single GPU designed by market-leader Nvidia costs $10,000; and thousands of GPUs are needed to train a single foundation model.\n\nFurther compute costs accrue once models are released and begin processing prompts from users.\n\n[Mirjalili, Seyedali.\n\n2023.\n\n“If AI Image Generators Are so Smart, Why Do They Struggle to Write and Count?,” The Conversation < accessed 21 September 2023]  \n[“Harvey.” n.d. Harvey.Ai < accessed 21 September 2023]  \n[{N.d.-b} Canva.com < accessed 21 September 2023]; Stallbaumer, Colette.\n\n2023.\n\n“Introducing Microsoft 365 Copilot—A Whole New Way to Work,” Microsoft 365 Blog < accessed 21 September 2023]  \n[Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, and others.\n\n{n.d.} “Language Models Are Few-Shot Learners,” Arxiv.org < accessed 21 September 2023]\n\nAnalysts estimate that ChatGPT costs $40 million per month to run, and that Microsoft would need $4 billion of compute if its GPT-powered Bing Chat product responded to all queries from Bing’s users.\n\nA final nuance to note is that it is not necessary for foundation model developers to own GPUs themselves— they can rent GPU time from cloud providers as a service.\n\nCurrently, most real-world end users of generative AI systems are paying nothing for the privilege, meaning foundation model providers’ revenues are negligible – OpenAI projects just $200 million for 2023.\n\nBoth development and usage of generative AI is therefore currently being funded by venture capital and the balance sheets of big tech companies – a situation which will clearly not last forever.\n\nIt seems likely that a small number of dominant foundation model providers will emerge and then increase prices to a level that produces attractive shareholder returns.\n\nIn the interim, the biggest beneficiaries are likely to be compute providers – Nvidia’s share price, for example, is +150% year-on-year.\n\nEconomics are more benign for application developers, as their foundation model API costs rise and fall in proportion to usage of their products, and they can switch between different model providers easily.\n\n[Leswing, Kif.\n\n2023.\n\n“Meet the $10,000 Nvidia Chip Powering the Race for A.I,” CNBC < accessed 21 September 2023]  \n[All market cap figures are correct as of 1 August 2023]",
    "### Figure 2: Selected Generative AI Companies, United States (US) and UK\n\n[Singh, Jagmeet, and Ingrid Lunden.\n\n2023.\n\n“OpenAI Closes 300M Share Sale at 27B-29B Valuation,” TechCrunch < accessed 21 September 2023]; “OpenAI.” {n.d.} Crunchbase < accessed 21 September 2023]  \n[Wiggers, Kyle.\n\n2023.\n\n“Anthropic Raises $450M to Build Next-Gen AI Assistants,” TechCrunch < accessed 21 September 2023]  \n[“Microsoft-Backed AI Startup Inflection Raises $1.3 Billion from Nvidia and Others.” 2023.\n\nReuters < accessed 21 September 2023]  \n[Rogenmoser, Dave.\n\n{n.d.} “Jasper Announces 125M Series A Funding Round, Bringing Total Valuation to 1.5B and Launches New Browser Extension,” Jasper.Ai < accessed 21 September 2023]",
    "### Figure 2: Selected Generative AI Companies, United States (US) and UK (continued)\n\n[Sharma, Shubham.\n\n2023.\n\n“DeepMind Unveils RT-2, a New AI That Makes Robots Smarter,” VentureBeat < accessed 21 September 2023]; Knight, Will.\n\n2023.\n\n“Google DeepMind’s CEO Says Its next Algorithm Will Eclipse ChatGPT,” Wired < accessed 21 September 2023]  \n[Shu, Catherine.\n\n2014.\n\n“Google Acquires Artificial Intelligence Startup DeepMind for More than $500M,” TechCrunch < accessed 21 September 2023]  \n[Wiggers, Kyle.\n\n2022.\n\n“Stability AI, the Startup behind Stable Diffusion, Raises $101M,” TechCrunch < accessed 21 September 2023]  \n[Browne, Ryan.\n\n2023.\n\n“Nvidia-Backed Platform That Turns Text into A.I.-Generated Avatars Boosts Valuation to $1 Billion,” CNBC < accessed 21 September 2023]  \n[Arm Ltd. {n.d.} “New Arm Total Compute Solutions Enable a Mobile Future Built on Arm,” Arm | The Architecture for the Digital World < accessed 21 September 2023]  \n[“Blow for Tech Unicorn Graphcore as Sequoia Writes off Stake.” {n.d.} Times (London, England: 1788) (The Sunday Times) < accessed 21 September 2023]",
    "## PRODUCTIVITY AND GENERATIVE AI\n\nWe take the view that generative AI is a very significant technology, of comparable importance to the web.\n\nHowever, it cannot be taken for granted that the adoption of generative AI will inevitably lead to whole-economy productivity growth—indeed, the digital innovations of the last 15 years have had no discernible impact on measured UK productivity.\n\nIt must also be acknowledged that there is still a lot of uncertainty about how generative AI will become economically useful.\n\nGoogle search data suggests the predominant use-cases for ChatGPT are currently job applications and homework, which have little relevance to the economy.\n\nMeanwhile, most capital investments in generative AI companies to date have been at the foundation model and infrastructure layers; at the application layer, the majority of venture-backed companies are developing chatbots, virtual customer services assistants, writing tools, and features for video games.\n\nWhile these may reduce operating costs in contact centres and increase copywriters’ output and gamers’ play-time, they are unlikely to have a transformative economic impact.\n\nIf the UK is to benefit from generative AI, it needs to encourage direct application of the technologies to the productive economy, across multiple sectors.\n\n[{N.d.-c} Parliament.uk < accessed 21 September 2023]  \n[Gilbert, Sam.\n\n2023.\n\n“I Find That Homework Is Actually the #2 Application of ChatGPT (as Measured by US Google Search).\n\nThere Is a Higher Volume of Searches Relating to Job Applications (Eg ‘Chatgpt Resume’, ‘Chatgpt Cover Letter’), & a Comparable Volume for Code (per  Https://T.Co/Xu0dUnxWtQ Pic.twitter.com/fcxqckaci3,” Twitter < accessed 21 September 2023]  \n[“No Title.” {n.d.} Dealroom.Co < accessed 21 September 2023]",
    "### Foundation model leadership\n\nUnlike the US, the UK has no big tech companies with balance sheets large enough to invest meaningfully in foundation model developers, and the UK venture capital market is far smaller ($31bn vs $235bn in 2022).\n\nTraditional startup funding models where companies raise seed capital (~£1m) to develop a minimum viable product, followed by larger and larger amounts of investment once they have gained traction with customers will not work at the scale needed for foundation model development.\n\nPolicy discussion has focused on how the UK could become a world leader in the development of novel commercial foundation models.\n\nWe doubt that this is realistic, despite the UK benefitting from a world-leading research base in underpinning technologies.\n\nTraining foundation models requires vast amounts of compute, and little compute capacity is available in the UK.\n\nThe £900m supercomputer announced by the chancellor in March 2023 will not be online until 2026, and neither Amazon Web Services, Microsoft Azure, nor Google Cloud have UK-located GPU clusters.\n\nStability AI trains its foundation models on clusters in the US.\n\nHowever, the idea of sending sensitive data offshore is very unpalatable for all organisations concerned with privacy (including, say, the NHS), and such data transfers are not reconcilable with UK law.\n\nA related barrier is the limited availability of investment capital to fund compute.\n\nModest government support for the UK chip industry—which has strategic importance well beyond generative AI—speaks to constraints on state spending relative to China and the US.\n\nThe upfront capital requirements to develop foundation models are of a different order of magnitude, making them unsuitable for UK-style startup investing.\n\nThe foundation model layer is also not the most economically attractive part of the generative AI ‘stack’.\n\nMost models have been trained on the same openly-available data, rather than proprietary sources, meaning there is limited scope for competitive differentiation and defensible market leadership.\n\nIt is at least plausible that competition between the likes of OpenAI, Google, Anthropic, and Inflection will drive down prices, leading to foundation models becoming increasingly commoditised.\n\nMeta’s open-sourcing of Llama 2 means that a powerful LLM is now available for commercial use, without the upfront capital costs associated with building these models, undermining the business model of the closed-source foundation model developers.\n\nThere remain, however, significant compute costs associated with their use.\n\nThere are also indications that the performance of open source models is progressing at pace.\n\nGiven these market conditions, it is unclear how foundation model leadership would contribute to economic productivity, even if it could be attained.",
    "### Real-world applications of foundation models\n\nRather than building publicly or privately funded competitors to the likes of OpenAI and Google, we see greater opportunity for the UK in becoming a leader in how foundation model are applied in the real world.\n\nWith smaller funding requirements, application layer products which customise foundation model capabilities to specific use-cases are a better fit for the UK venture capital market, and can build on existing strengths in sectors like fintech, healthtech and cybersecurity.\n\nA further opportunity could be leveraging the UK’s research capabilities to drive progress in underpinning technologies and to develop products which address specific major challenges at the foundation model and infrastructure layers of generative AI, such as the detection of AI-generated content and cooling of data centres, as well as AI safety solutions.\n\n[Facebook company.\n\n2023.\n\n“Meta and Microsoft Introduce the next Generation of Llama,” Meta < accessed 22 September 2023]; “Alpaca Eval Leaderboard.” {n.d.} Github.Io < accessed 22 September 2023]  \n[“AI Foundation Models: Initial Report.” 2023.\n\nGov.uk < accessed 22 September 2023]  \n[Cooke, Elizabeth.\n\n2023.\n\n“AI Model Collapse Could Spell Disaster for AI Development, Say New Studies,” Verdict < accessed 22 September 2023]; {N.d.-e} Datacentremagazine.com < accessed 22 September 2023]",
    "#### CASE STUDY: SOFTWARE DEVELOPMENT WITH GENERATIVE AI\n\nAnkur Shah is a London-based technology entrepreneur, whose previous exits include footwear brand Mahabis and adtech platform Techlightenment.\n\n“I trained as a barrister and my coding skills are adequate for simple proofs of concept, but I’ve always relied on outsourced developers when building new projects, which is time-consuming and costly.\n\nBut in the last 12 months generative AI has changed everything.\n\nFor people like me who want to build websites, apps, and workflow automations it’s akin to a superpower.\n\n“One simple example I really like is Meoweler – a light-hearted travel site, ostensibly for cats.\n\nIt’s beautifully executed and provides a nice snapshot of thousands of cities around the world.\n\nBut what’s significant is that it cost only $140 to build, and the guy who made it is a designer with no formal training in software development.\n\nHe found a freely available database of cities, then wrote GPT and Midjourney prompts to generate the content and images for each city in a consistent format and style.\n\nThen he used the Svelte web framework to create URLs, page components, and site search.\n\nIt’s a similar approach to the one we’ve taken to programmatically reviewing insurance products*, albeit we use a different web framework and are more focused on data quality.\n\n\"Features that used to take months and cost tens of thousands of pounds, I can now build myself in an afternoon with ChatGPT.\n\nIt’s insane.\n\nBut sites and apps only scratch the surface.\n\nWhat’s exciting me at the moment is systems that use LLM capabilities recursively.\n\nI love the idea of ‘teams’ of AI agents that can take a request like ‘get me some quotes to have a heat-pump installed’ and then automate the whole series of linked tasks needed to fulfil it – background web research, shortlisting and prioritising suppliers, contacting them for quotes, and so on.\n\nMy intuition is, it will be scrappy, bedroom-hacker types – not computer science graduates or corporate IT departments – who get there first.”\n\n* Author disclosure: Sam Gilbert is involved with this project.\n\nGenerative AI’s primary contribution to productivity will be in changing how things are produced.\n\nThe biggest benefits to productivity will not come from a small number of technologically-sophisticated companies using generative AI to invent new products, or cut their costs.\n\nRather, generative AI’s promise lies with changing production itself, just as occurred with interchangeable parts (19th century), assembly lines (1910s), just-in-time production (1980s), and globalised supply chains (2000s).\n\nThe best example is software.\n\n[Coyle, Diane.\n\n2023.\n\n“The Promise and Peril of Generative AI,” Social Europe (SE) < accessed 22 September 2023]\n\nThe code interpreter plugin for ChatGPT and LLM-powered tools like GitHub Copilot are already enabling developers to write code up to 55% faster than before, presenting a potential solution to the UK’s chronic developer labour shortage.\n\nEven more significant is how generative AI expands the scope of no-code, enabling people without programming knowledge to build increasingly sophisticated software applications.\n\nIn the past, which systems and automations could be developed was constrained by the availability of workers with skills in programming languages.\n\nGenerative AI tools effectively remove this constraint for some types of development, meaning that the capability to imagine what a system might do and to articulate how it ought to function becomes more valuable than formal computer science training—a paradigm brought to life by the case study on the previous page.\n\nWhen it comes to productivity, in our view the most important national capability is a means of widely disseminating understanding of and access to generative AI tools through the economy and society.\n\nThere is good evidence that only a minority of firms are adopting existing digital tools in ways that enhance their productivity and commercial success, pulling ever further ahead of the pack.\n\nThe gap could grow with the powerful new capabilities afforded by generative AI.\n\nThe national economic challenge is to spread know-how among businesses and employees.\n\nThere is a role for government and AI experts to encourage learning about the potential of generative AI, not only through sharing techniques and examples but also through the range of business support tools available.\n\nIn some ways this runs counter to prevailing trends: many organisations have banned their employees from using generative AI applications, reasonably fearing it could lead to data leaks and/or the loss of intellectual property.\n\nWhile understandable, such practices inhibit the bottom-up emergence of productivity opportunities inside organisations.\n\nThere is also some anecdotal evidence that productivity gains from generative AI are already being realised, but lost to forms of arbitrage.",
    "There is also some anecdotal evidence that productivity gains from generative AI are already being realised, but lost to forms of arbitrage.\n\nRemote workers secretly use ChatGPT to get more free time or impress their superiors; marketing agencies outsource content writing to LLMs while leaving their client fees and service-level agreements unchanged.\n\nIncentives must be created for expert users of generative AI tools to share their techniques.\n\n[“ChatGPT Plugins.” {n.d.} Openai.com < accessed 21 September 2023]  \n[Dohmke, Thomas.\n\n2023.\n\n“GitHub Copilot for Business Is Now Available,” The GitHub Blog < accessed 22 September 2023]  \n[McDonald, Clare.\n\n2022.\n\n“Around 750 New Software Developer Jobs Advertised Every Day,” Computerweekly.com < accessed 22 September 2023]  \n[Milmo, Cahal.\n\n2023.\n\n“ChatGPT Limited by Amazon and Other Companies as Workers Paste Confidential Data into AI Chatbot,” INews < accessed 22 September 2023]  \n[Ito, Aki.\n\n2023.\n\n“Employees Are Secretly Using ChatGPT to Get Ahead at Work,” Business Insider < accessed 22 September 2023]",
    "## IMPEDIMENTS TO DEVELOPING THE UK’S NATIONAL CAPABILITIES IN GENERATIVE AI\n\nSeveral impediments may hamper efforts to unlock the full potential of the UK’s capabilities for generative AI.\n\nThere are economic impediments in terms of lack of investment, and impediments from the challenges of scale of the technical infrastructure, as explained in previous sections.\n\nWhile the UK has not adopted specific legislation to regulate generative AI, there are some restrictions in existing laws, notably concerning personal data protection and intellectual property.\n\nFurther, there is an impediment to the uptake of national capabilities in generative AI in that these technologies are considered unethical and untrustworthy by some.\n\nThus, national capability will depend on generative AI tools which are reliable, safe, responsible, and trustworthy.\n\nWe have identified how generative AI can unlock the UK’s potential for augmented productivity by changing the ways things are produced.\n\nHowever, there are several impediments to UK businesses’ access to and use of generative AI.\n\nFigure 3 sets out some of the chief legal, regulatory, economic, cultural, and societal impediments to the adoption of generative AI in the UK.\n\nThis section gives an overview of impediments to the uptake of generative AI in the UK.\n\nIt specifically addresses risks associated with generative AI and what is meant by ethical and responsible AI.\n\nThe section also addresses the concerns regarding, personal data, privacy, and data governance, particularly in relation to copyright, that arise from the development and use of generative AI tools.\n\n['Trustworthy AI' is a contested term.\n\nThe European Commission’s Independent High-Level Expert Group on Artificial Intelligence identifies three components of Trustworthy AI: (1) it should be lawful, complying with all applicable laws and regulations; (2) it should be ethical, ensuring adherence to ethical principles and values; and (3) it should be robust, both from a technical and social perspective.]",
    "### Figure 3: Impediments to the uptake of generative AI in the UK (continued)\n\nIn its interim report published on July 2023, the House of Commons Science, Innovation and Technology Committee summarised the barriers to implementing safe and effective AI as 12 AI challenges:\n1.\n\nThe Bias challenge\n2.\n\nThe Privacy challenge\n3.\n\nThe Misrepresentation challenge\n4.\n\nThe Access to Data challenge\n5.\n\nThe Access to Compute challenge\n6.\n\nThe Black Box challenge\n7.\n\nThe Open Source challenge\n8.\n\nThe Intellectual Property and Copyright challenge\n9.\n\nThe Liability challenge\n10.\n\nThe Employment challenge\n11.\n\nThe International Coordination challenge\n12.\n\nThe Existential challenge\n\nWhile not negating the importance of AI safety, this policy brief narrowly focuses on how to build UK’s capabilities for productivity using generative AI.\n\nWe therefore only consider risks that pose impediments to that goal.\n\nThere are three chief impediments to building the UK’s capabilities in this regard.\n\nFirst, there is the risk that a lack of trust in generative AI becomes so pervasive that the deployment of these technologies is rejected by businesses and the public.\n\nSecond, there is the risk that generative AI will be subjected to legal and ethical regimes which will be overly restrictive and thus hamper its full potential.\n\nThird, the issue with AI hallucinations, whereby the generative AI tool makes up information, alongside other technical limitations, poses a challenge to their reliability which again is an impediment to their uptake nationally.",
    "### Risks with generative AI\n\nThis section briefly examines risks associated with generative AI and the legal and ethical frameworks that are emerging to address these.\n\nFundamentally, the British public must be able to trust the use of generative AI.\n\nThere are many conceptualisations of risks associated with generative AI.\n\nThe list below is not intended to be read as a complete overview, but rather a list of some of the most prominent concerns related to AI.\n\nNumerous risks are associated with generative AI, including risks to personal data, privacy, and intellectual property.\n\nThere are risks that due to lack of transparency or accountability, generative AI may produce unreliable outcomes, or be used for hidden or unacceptable outcomes.\n\nKey concerns with generative AI applications are the reliability or veracity of the outputs, especially as the capacity of non-technical users to produce deepfake images, audio, and video abounds.\n\nScholars have also identified risks of negative environmental consequences, the overrepresentation of hegemonic viewpoints and value-lock in training data, the risk of propagating toxic stereotypes and racist, sexist, and ableist ideologies, marginalising communities, violating personal data, and subjecting people to abusive language, hate speech, micro-aggressions, derogating language, dehumanising and denigrating content and framing, which could lead to psychological harm.\n\n[Emily M. Bender et al., On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\n\nFaccT’21, March 3-10, ACM <\n\nThere are risks that data scraping for training foundation models violates copyright laws, or that the foundation models will reproduce bias, which may produce illegal outcomes, especially when generative AI is used in the context of social services, policing, and education.\n\nThe cumulative effect of these risks is the erosion of trust in the technology, and of societal trust overall.\n\nAccording to the Ada Lovelace Institute: “It is also unlikely that international agreements will be effective in making AI safer and preventing harm, unless they are underpinned by robust domestic regulatory frameworks that can shape corporate incentives and developer behaviour in particular.” (Ada Lovelace Institute, Regulating AI in the UK, p. 5).",
    "### Ethical and responsible generative AI\n\nResponsible AI means demonstrating how the ethical principles are adhered to throughout all the stages of the generative AI lifecycle.\n\nTo do so, there must be appropriate accountability, risk mitigation, and liability.\n\nIn terms of building national capabilities for the workforce, there are particular concerns regarding automated decision-making and the role of humans in the loop.\n\nNumerous voices have expressed concern that generative AI is not responsible or ethical.\n\nTo meet these concerns about the use of AI more broadly, the Government has proposed a guiding principle-based framework.\n\nThe principles are drawn from the work of the OECD and as such build on the emerging international consensus for ethical and responsible AI.\n\nThis principle-based approach is dependent on regulatory capacity to be effective.\n\nThe UK’s government’s value-based principles are:\n1.\n\nSafety, security, and robustness\n2.\n\nAppropriate transparency and explainability\n3.\n\nFairness\n4.\n\nAccountability and governance\n5.\n\nContestability and redress\n\nThe principles are designed to be future-oriented and flexible, with the intention of promoting growth and innovation.\n\nWhile not legally binding, the Government envisions that sector-specific regulators will adopt the principles as fit to their sectors and industries.\n\n[AI ethics is a growing academic field with numerous different interpretations of the term.\n\nSome relevant scholarly articles are Robert Ganna and Emre Kazim, Philosophical foundations for digital ethics and AI ethics: a dignitarian approach, AI and Ethics (2021) 1: 405-423; Samuele lo Piano, Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward, Humanities and Social Sciences Communications (2020) 7-9; Jessica Fjeld, Nele Achten, Hannah Hilligoss, Adam Christopher Nagy, Madhulika Srikumar, Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI, Berkman Klein Center For Internet & Society at Harvard University, 2020, Ibo van de Poel, Embedding Values in Artificial Intelligence (AI) Systems, Minds and Machine 2020 30:385-409]  \n[Australian Government Department of Industry, Science and Resources, Safe and responsible AI in Australia (Discussion paper June 2023), pp.\n\n8-9.]\n\nHowever, this approach may pose challenges in ensuring that regulators have the incentives, resources, or mandate to do so, especially as many regulators’ remits are constrained by statutory language.\n\nThus, the approach has been challenged by leading academics, pointing to the need for more holistic thinking.\n\nIt is also problematic that the Government’s principles are so vague as to be nearly vacuous.\n\nIt is, for example, difficult to discern with any certainty whether the principles are focused on outcomes or how those outcomes are to be achieved.\n\nHowever, elsewhere, for example in data protection, the Government has suggested that regulation should be based on outcomes; an approach that could potentially be taken for the five value-based principles as well.\n\n(Department for Digital, Culture, Media & Sport, Data: A new direction, 10 September 2021, updated 23 June 2023, p.\n\n7.)\n\nAs the principles are not legally binding, it is unlikely that businesses will have an adequate incentive to adopt all the principles unless there are compelling competitive advantages to doing so.\n\nWhile the Government has provided tools such as the Algorithmic Transparency Recording Standard, which aims to support the implementation of ethical AI principles, the extent to which such tools are being implemented in practice is not clear.\n\nThus, the Ada Lovelace Institute has noted that: “The principles will not – initially – be placed on a statutory footing, and so regulators will have no legal obligation to take them into account, although the Government has said it will consider introducing a ‘duty to have regard’ to the principles.” (Ada Lovelace, Regulating AI in the UK, p.\n\n16.)\n\nThe House of Commons Science, Innovation and Technology Committee has criticised the Government’s unwillingness to consider AI-specific legislation, noting that: “[t]here is a growing imperative to ensure governance and regulatory frameworks are not left irretrievably behind the pace of technological innovation.” (The governance of artificial intelligence; interim report, Ninth Report of Session 2022-23, p.\n\n3.).\n\nThus, rather than see legislation as an impediment to the development of the UK’s competitiveness in generative AI, we echo the sentiment of the review of the digital technologies, led by Sir Patrick Vallance, that: “Well-designed regulation and standards can have a powerful effect on driving growth and shaping a thriving digital economy.” (HM Government, Pro-innovation Regulation of Technologies Review: Digital Technologies (March 2023), p.\n\n3.)",
    "3.)\n\nWhile calls for legislation are mounting, it does not mean that the content of AI legislation is self-evident.\n\nLegal rules that are too specific risk being quickly outdated while principles that are too broad or vague risks being meaningless.\n\nThe challenge is therefore how to find the regulatory approach that will be robust and future-proof.\n\nLegislation would also clarify the chain of liability throughout the value-chain and lifecycle of generative AI.\n\nFor example, the All-Party Parliamentary Group on Data Analytics (APGDA) has noted that: “there are issues around transparency, explainability, and accountability in relation to third party/outsourced AI system development.\n\n[The Central Digital and Data Office expounded on these principles in its Data Ethics Framework for the use of digital technologies, including AI, in 2020.\n\nHowever, this guide is only for the public sector and does not have the force of law.]\n\nFor example, attention was drawn to the difficulty of testing for bias in third party systems.” (Policy Connect, An Ethical AI Future: Guardrails & catalysts to make artificial intelligence a force for good, 19 June 2023, p.\n\n10.)\n\nLegislation could clarify the standards and responsibility of testing that would befall UK businesses using third-party generative AI systems.\n\nA key issue with applying the law or ethical principles to generative AI is that the outcome is personalised or bespoke, therefore making predictability or comparison difficult.\n\n“Generated content is probabilistically and randomly generated based on certain input (or ‘prompts’), which are usually written by a human.\n\nTherefore, the output of any given generative AI model is likely to be different for each person prompting the model and may both resemble patterns in the training data or appear to be something completely new.” (Forbrukerrådet, Ghost in the Machine: Addressing the consumer harms of generative AI, June 2023, p.\n\n8.)\n\nA recent review of 10 foundation models found that none met the compliance requirements set out in the EU’s draft AI Act.\n\nThere is a question of whether generative AI should go through an approval or vetting process before being used, or if redress and contestability should be used as a deterrent for unacceptable practices.\n\nAccountable principles also means that there must be ways to audit the generative AI systems, which will require access to data for researchers and for regulators.\n\nA right to access to data for researchers in relation to the processing of personal data has been proposed included in the Online Safety Bill in relation to the online information environment, but this has yet to be adopted by Parliament.\n\nFurther there are no legal stipulations for data access in the legislative pipeline with regard to generative AI in the UK.",
    "### Personal data and privacy concerns\n\nMany of the ethical concerns regarding generative AI are linked to the use of personal data and privacy.\n\nThese concerns span personal data that is being inputted into generative AI systems, personal data generated by these systems, and uses of generative AI systems for surveillance.\n\nSome of these fears should be alloyed with the Data Protection Act 2018 (DPA) and its forthcoming replacement, the Data Protection and Digital Information (No.\n\n2) Bill.\n\nThe UK’s data protection framework is based on the EU’s General Data Protection Regulation (GDPR), which includes the stipulation that all processing of personal data must adhere to the data processing principles.\n\nThe data processing principles are: (1) lawfulness, fairness, and transparency; (2) purpose limitation; (3) data minimisation; (4) accuracy; (5) storage limitation; (6) integrity and confidentiality; and (7) accountability.\n\nThat means that all uses of personal data by generative AI must respect these principles as a matter of law.\n\nAs the legislation covers all forms of personal data its remit is broader than processing that concerns privacy.\n\nThe legal definition of personal data is technologically neutral and comprehensive to ensure that all forms of personal data falls under its scope.\n\nThe broadness of the definition has implications for the use of generative AI and may pose a considerable impediment from the uptake of these technologies by UK companies.\n\nThe Court of Justice of the European Union (CJEU) has for example ruled that an IP address can be personal data when combined with other factors held by third parties.\n\nRegardless of where in the process personal data is generated or the sources from which it is harvested, including public domain sources, or provided directly (and voluntarily) by an individual, all the data processing principles still apply in full.\n\nThere are further restrictions on the use of sensitive data, which poses challenges for companies using generative AI as sensitivity may first become apparent once the system has generated output data.\n\nThe use of generative AI poses several challenges when it comes to compliance with data protection law.\n\nIt may not be apparent whether data is personal or not, or the system may generate personal data unbeknown or unintended by the creator of the AI system.\n\nHowever, it must be noted that the DPA is not a privacy statute, and that the objective of the legislation is not to preclude the processing of personal data, but instead to ensure that the processing is legal.\n\nThus, the DPA does not automatically prevent the generation and use of personal data in generative AI.\n\nThe UK Government clearly recognises the role personal data has in innovation and AI.\n\nIn its White Paper on data, the Department for Digital, Culture, Media & Sport writes that: “Innovative uses of personal data are at the forefront of driving scientific discovery, and enabling cutting-edge technology, like artificial intelligence (AI)…This means maintaining a clear legal framework overseen by a regulator that takes account of the benefits of data use, while protecting against the harms that can come from using personal data irresponsibly.” (p. 6).\n\nThe objective of the White Paper on data is to use “personal data responsibly” (p.\n\n6.\n\n), which necessitates an ethical framework.\n\nOne of the drawbacks in the context of generative AI of the current data protection regime is that it is focussed on the right to data protection of the individuals and as such does not address the potential for systemic risk of bias, discrimination, and inequality arising from the use of personal data at scale.\n\nHowever, several European data protection authorities are examining whether generative AI tools comply with the GDPR.\n\nNotably, the Italian data protection authority has placed a temporary ban on an OpenAI generative chatbot for failing to provide information as required under the GDPR.\n\nThe Spanish Data Protection Authority is also investigating ChatGPT for breaches of GDPR.\n\nFurthermore, the European Data Protection Board (EDPB) has set up a taskforce to examine whether generative AI is compatible with the GDPR.\n\nIt must, however, also be noted that these concerns regard whether generative AI comport with the data-processing principles for the processing of personal data, not whether they should be banned outright as illegal.",
    "### Data governance\n\nGenerative AI needs data and there is therefore considerable concern and interest in the data that goes into the training of foundation models and the data that is input into generative AI systems.\n\nThere is growing concern that generative AI violates intellectual property rights.\n\nLegal challenges have been mounted in the US concerning the use of data scraping for training data which could violate copyright.\n\nGenerative AI has been front and centre of the recent labour dispute and strike by the SAG-AFRA trade unions representing actors in the US.\n\nWhether any legal dispute will be successful is highly uncertain, however, the broader point is that the labour force of the creative industries is under threat from generative AI, which will have a direct effect on the UK economy as these industries represent 5.6% of GDP.\n\nAs mentioned above, the UK’s Intellectual Property Office is currently drafting a code for AI and copyright in an attempt to answer some of these questions.\n\nIn the meantime, there are signals of a broader debate about the societal value and risk associated with absorbing a large portion of human knowledge into large AI models, potentially impinging on fundamental human rights, such as access to culture.\n\nAs a countermeasure to copyright concerns, there is a chance that companies will hold data in so-called walled gardens.\n\nThat would give the public even less access to open data and would stifle innovation and productivity.\n\nThere is still room for clarification of the legal framework in this regard.",
    "### Regulatory capacity\n\nThe government’s approach to AI regulation would support individual regulators to develop sector-specific frameworks for the adoption of the value-based principles by UK industry.\n\nIn many ways, this is a more concrete and pragmatic approach than the approach taken by other jurisdictions, notably the EU, where centralised, overarching principles have been adopted in comprehensive legislation.\n\nAs such, the UK is showing more willingness to operationalise the principles in ways that will have a direct impact on the development and uptake of generative AI.\n\nFor example, the Competition and Markets Authority has proposed a set of principles to regulate the development of AI models.\n\n[Competition & Markets Authority, AI Foundation Models: Initial Report, 18 September 2023.]\n\nThe principles are:\n1. ensuring that foundation model developers have access to data and computing power, and that early AI developers do not gain an entrenched advantage;\n2. that both closed and open source models are allowed to develop;\n3. that businesses have a range of options to access AI models – including developing their own;\n4. that consumers should be able to use multiple AI providers;\n5. that no anticompetitive conduct like 'bundling' AI models into other services take place;\n6. that consumers and businesses are given clear information about use and limitations of AI models.\n\nThe effectiveness of the approach taken by the Government will depend on regulatory capacity and there is a risk that efforts will be unnecessarily duplicated, or that regulatory frameworks will promote contradictory rules.\n\nThe Sir Patrick Vallance Review of digital technologies, published in March 2023, found more than 10 different regulators of digital technologies.\n\nWe concur with others who have observed a need for centralised regulatory oversight to coordinate the efforts of the many departments and regulators.\n\nThis is necessary to ensure that the UK’s value-based principled framework for the governance of generative AI is adopted in a consistent manner across the UK’s industrial sectors.\n\nThese functions are today met by the Office for Artificial Intelligence and Centre for Data Ethics and Innovation under the Department of Science, Innovation and Technology (DSIT), and the Digital Regulation Cooperation Forum, which was formed as a membership organisation consisting of four key regulators: the Competition & Markets Authority, OFCOM, the Information Commissioner’s Office, and the Financial Conduct Authority.\n\nAlongside recent internal changes to Government’s policy delivery infrastructure, through the establishment of DSIT, further changes to the Government’s interactions with the external expert community are also expected, which may influence the ability to rapidly identify and respond to emerging technological changes with regulatory implications.",
    "### International leadership\n\nThe UK Government has repeatedly set forth an ambition of international leadership in AI, both in terms of development and regulation.\n\nIn March 2023, the Sir Patrick Vallance Review asserted that the UK had a window of no more than 24 months to realise that ambition.\n\nIn relation to the development of regulatory frameworks, the UK is struggling to keep pace, as suggested by Figure 3 in an earlier section.\n\nWhile the UK’s Government has resisted calls for legislation to allow for growth and innovation in the sector, the lack of AI-specific legal regulation opens the possibility that the safe and responsible deployment of AI solutions and products will depend on the enforcement of rules devised and overseen by other jurisdictions or the international community.\n\nThe absence of robust legislation poses a serious risk to the safety and trustworthiness of generative AI solutions, especially when these are devised wholly or in part by foreign companies.\n\nBeing first-to-the-post in adopting legislation to regulate AI is not necessarily a desirable objective if that legislation is not robust, balanced, and feasible.\n\nHowever, the UK’s lack of binding regulation means that despite any ambition of the Government, the UK is failing to reach its ambition of international leadership in this regard.\n\n[The AI Council and Centre for Data Ethics and Innovation have recently come to the end of their term or been disbanded, with plans for an alternative approach to external engagement in development.]",
    "## RECOMMENDATIONS TO BUILD CAPABILITY IN GENERATIVE AI\n\nAlthough the National AI Strategy is concerned with the broader AI field and pre-dates the latest developments in generative AI, many of the key actions it sets out retain their relevance and do not need to be repeated here.\n\nWe focus instead on innovation and skills policy levers that both support the goal of making the UK a global leader in applying generative AI to the economy, and are not discussed in detail in the National AI Strategy.\n\nIt is worth noting that exactly how these policy levers are used depends on whether the UK pursues AI Nationalism or a more open approach.\n\nAs noted by the National AI Strategy, increased compute capacity is a dependency for the development of most generative AI capabilities.\n\nAn efficient way of mitigating the UK’s compute deficit would be lobbying hyperscalers to establish GPU-clusters in the UK.\n\nThis would allow organisations like the NHS to run fine-tuned foundation models with fewer concerns about data security and privacy.\n\nIn parallel, subsidies could be increased for companies developing capital-intensive proprietary and/or strategically important generative AI capabilities (e.g.\n\nchips; cybersecurity and defence applications).\n\nTax incentives like the Seed Enterprise Investment Scheme (SEIS) could be enhanced to increase the supply of early-stage capital to generative AI startups at the application layer.\n\nTax credits could be introduced for all businesses to incentivise them to apply generative AI technologies to their existing operations and/or to develop new generative AI-powered products and services.\n\nChallenge prizes could be launched to identify and disseminate effective bottom-up uses of generative AI by teams and individuals inside organisations operating in industries where productivity gaps have been identified.\n\nThey can also be used to motivate innovation in industries identified as potential growth areas for the UK economy.\n\nAn AI Nationalist approach would imply government acting assertively to steer market outcomes.\n\nPublic sector procurement of generative AI capabilities could positively favour UK suppliers—for example, public funding for supercomputers could be made contingent on the use of chips designed by UK companies like Graphcore.\n\nAcquisitions of major UK generative AI companies by foreign rivals—comparable to the past acquisitions of Deepmind by Google, Arm by Softbank, or Instadeep by BioNTech—could be challenged.\n\n[“National AI Strategy - HTML Version.” {n.d.} Gov.uk < accessed 22 September 2023]\n\nBy contrast, an open approach might involve designing a regulatory regime encouraging foreign generative AI entrepreneurs to set up in, or relocate their companies to, the UK.\n\nIn addition to the National AI Strategy’s plans to make visas easier to obtain, this might include corporation tax and entrepreneurs’ relief incentives.\n\nIt would not, however, be compatible with the kind of controls on mergers and acquisitions described above.\n\nFor generative AI to pervade the economy, school and higher education curriculums would need to be developed to increase both understanding of the technologies and critical thinking about how they are used in practice.\n\nComputer science education may need to be reformed, or a new discipline established, to teach software development using no-code and LLMs.\n\nThese could also be the subject for new Skills Bootcamps, and/or upskilling programmes co-designed with employers and workers.\n\nRegardless of whether an AI Nationalist or open strategy is pursued, our view is that legislation and regulation will be needed to remove impediments to the adoption of generative AI and ensure that the British public can trust organisations’ use of the technology.\n\nWe favour government adopting a principled approach to introducing legislation that would embed an ethical framework for the governance of generative AI in domestic law in multiple sectors.\n\nIt should forbid high-risk uses of generative AI, for example in the operation of critical infrastructure, where it could pose a significant threat to human safety or violate fundamental ethical rules.\n\nLegislation takes a long time to pass.\n\nIn the interim we recommend the adoption of soft governance models, such as the IEEE7001 Standard on Transparency, together with moves to strengthen regulatory capacity.\n\nInternational standards may also be used as frameworks for legislative proposals.\n\nWe therefore support the All-Parliamentary Group on Data Analytics’ (APGDA) recommendation for a centralised AI office with a renewed strategic focus to not only oversee and coordinate AI regulation across regulators, as set out in the Government’s White Paper, but also to ensure that regulators enforce regulation.",
    "This could be achieved by, for example, bolstering the remit to the Office of Artificial Intelligence with a strategic focus on work programmes that identify regulatory gaps and empower existing regulators to deliver responsive regulatory interventions in their domains.\n\nThere continues to be a need for capacity building among regulators.\n\nAlthough this is well under way in some domains, as seen from the framework being developed by the Competition & Markets Authority, others will need further support to deliver the Government’s current AI White Paper proposals.\n\nIn addition, as is already recognised, regulators need to enhance their existing co-operation to ensure clarity about responsibilities, as the technology will cut across all sectors.\n\nThis coordinating function may need additional or more active guidance and support than is currently proposed.\n\nIt is crucial that the regulatory oversight mechanism has sufficient resources and expertise to test and oversee the use of generative AI to build national capabilities for productivity, and be transparent about the oversight in order to inspire public confidence.\n\n[“Find a Skills Bootcamp.” 2022.\n\nGov.uk < accessed 22 September 2023]",
    "## ABOUT THE AUTHORS\n\nDr Ann Kristin Glenster is Senior Policy Advisor on Technology Governance and Law at the Minderoo Centre for Technology and Democracy.\n\nThe Executive Director of the Glenlead Centre, she is a legal expert on information technology law and regulation in the UK, US, and EU.\n\nShe holds a UK qualifying law degree, has been a doctoral visiting scholar at the Harvard Law School, and holds a PhD in Law from the University of Cambridge.\n\nSam Gilbert is an affiliated researcher at the Bennett Institute for Public Policy.\n\nHe is the author of Good Data: An Optimist’s Guide to Our Digital Future (Welbeck Publishing, 2021) as well as influential reports on data ethics, crypto, web3, the metaverse, and online safety.\n\nPreviously, he was Employee No.\n\n1 and Chief Marketing Officer at the fintech unicorn ManyPets, and held senior roles at Experian and Santander.",
    "## APPENDIX\n\nThe table below briefly summarises some of the other policy areas generative AI bears on—all of which deserve more thorough exploration than is possible here.\n\nThe table is included to demonstrate how generative AI will have an impact across society, and also to show that while we are aware of that impact, this brief has too narrow a focus to allow a full investigation of these implications.",
    "## SELECTED BIBLIOGRAPHY\n\n“AI Foundation Models: Initial Report.” 2023.\n\nGov.uk < accessed 22 September 2023]  \n“Blow for Tech Unicorn Graphcore as Sequoia Writes off Stake.” {n.d.} Times (London, England: 1788) (The Sunday Times) < accessed 21 September 2023]  \n“Microsoft-Backed AI Startup Inflection Raises $1.3 Billion from Nvidia and Others.” 2023.\n\nReuters < accessed 21 September 2023]  \n“National AI Strategy - HTML Version.” {n.d.} Gov.uk < accessed 22 September 2023]  \n“Responsible AI UK.” {n.d.} Responsible AI UK < accessed 21 September 2023]\n\nAndre Charlesworth, Kit Fotheringham, Colin Gavaghan, Albert Sanches-Graells, and Clare Torrible, Response to the UK’s March 2023 White Paper “A pro-innovation approach to AI regulation”, Centre for Global Law and Innovation, University of Bristol Law School, 19 June 2023, <  \nAndrew Dalton and the Associated Press, Writers Strike: Why A.I.\n\nis such a hot button in Hollywood’s labor battle with SAG-AFTRA, Fortune, 24 July 2023,<  \nArm Ltd. {n.d.} “New Arm Total Compute Solutions Enable a Mobile Future Built on Arm,” Arm | The Architecture for the Digital World < accessed 21 September 2023]  \nArtificial Intelligence and Data Act, <  \nAustralian Government Department of Industry, Science and Resources, Safe and responsible AI in Australia (Discussion paper June 2023)  \nBradshaw, Tim, and Anna Gross.\n\n2023.\n\n“UK Government Unveils Long-Awaited £1bn Semiconductor Strategy,” Financial Times < accessed 21 September 2023]  \nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, and others.\n\n{n.d.} “Language Models Are Few-Shot Learners,” Arxiv.org < accessed 21 September 2023]  \nBrowne, Ryan.\n\n2023.\n\n“Nvidia-Backed Platform That Turns Text into A.I.-Generated Avatars Boosts Valuation to $1 Billion,” CNBC < accessed 21 September 2023]  \nC-582/14 Breyer, Judgement of the Court (Second Chamber on 19 October 2016), ECLI:EU:2016:779, <  \nCentral Digital & Data Office, Data Sharing Governance Framework (Guidance), 23 May 2022, <  \nCentral Digital and Data Office, Data Ethics Framework (Guidance), 13 June 2018, updated 16 September 2022, <  \nComputer & Markets Authority, AI Foundation Models: Initial Report, 18 September 2023, <  \nCooke, Elizabeth.\n\n2023.\n\n“AI Model Collapse Could Spell Disaster for AI Development, Say New Studies,” Verdict < accessed 22 September 2023]  \nCoyle, Diane.\n\n2023.\n\n“The Promise and Peril of Generative AI,” Social Europe (SE) < accessed 22 September 2023]\n\nData Protection Act 2018, <\n\nData Protection and Digital Information (No.\n\n2) Bill, <  \nDelacroix, Sylvie, Data Rivers: Re-balancing the data ecosystem that makes Generative AI possible (March 14, 2023).\n\nAvailable at SSRN: < or <  \nDepartment for Digital, Culture, Media & Sport, Data: A new direction, 10 September 2021, updated 23 June 2023, <  \nDepartment for Science, and Technology.\n\n2023a.\n\n“Initial £100 Million for Expert Taskforce to Help UK Build and Adopt next Generation of Safe AI,” Gov.uk < accessed 21 September 2023]  \nDepartment for Science, Innovation and Technology and Department for Digital, Culture, Media & Sport, National Data Strategy (Guidance), 8 July 2019, updated 5 December 2022, <  \nDepartment for Science, Innovation and Technology and Office for Artificial Intelligence, A pro-innovation approach to AI regulation (policy paper), 23 March 2023, <  \nDepartment for Science, Innovation and Technology, Office for Artificial Intelligence, Department for Digital, Culture, Media & Sport, and Department for Business, Energy & Industrial Strategy, National AI Strategy (Guidance), 22 September 2021, updated 18 December 2022, <  \nDigital Markets, Competition and Consumer Bill, <  \nDigital Regulation Cooperation Forum, <  \nDohmke, Thomas.\n\n2023.\n\n“GitHub Copilot for Business Is Now Available,” The GitHub Blog < accessed 22 September 2023]  \nEmily M. Bender et al, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, FaccT’21, March 3-10, ACM <  \nEuronews and AFP, Spain opens an investigation into OpenAI’s ChatGPT over a potential data breach, 14 April 2023, <  \nEuropean Commission, European Centre for Algorithmic Transparency, <  \nEuropean Data Protection Board, EDPB resolves disputes on transfers by Meta and creates task force on ChatGPT, 13 April 2023,  <  \nEvans, Benedict.\n\n2022.",
    "2022.\n\n“ChatGPT and the Imagenet Moment,” Benedict Evans < accessed 21 September 2023]  \nExecutive Department State of California, Executive Order N-12-23, <  \nForbrukerrådet, Ghost in the Machine: Addressing the consumer harms of generative AI, June 2023, <  \nFreshfields Bruckhaus Deringer, Italian ban on AI chatbot lifted: Updates on data protection investigation, Lexology, 30 March 2023, <  \nHeather Stewart, ‘The challenges are real’: TUC taskforce to examine AI threat to workers’ rights, The Guardian, 3 September 2023, <  \nHM Government, Pro-innovation Regulation of Technologies Review: Digital Technologies (Sir Patrick Vallance Review) (March 2023), <  \nHouse of Commons Science, Innovation and Technology Committee, The governance of artificial intelligence: interim report, Ninth Report of Session 2022-23, 31 August 2023, <  \nHouse of Lords Library, Arts and creative industries: The case for a strategy, 21 December 2022, <  \nIbo van de Poel, Embedding Values in Artificial Intelligence (AI) Systems, Minds and Machine 2020 30:385-409\n\nIntellectual Property Office, The governments code of practice on copyright and AI (Guidance), 29 June 2023), <  \nIto, Aki.\n\n2023.\n\n“Employees Are Secretly Using ChatGPT to Get Ahead at Work,” Business Insider < accessed 22 September 2023]  \nJessica Fjeld, Nele Achten, Hannah Hilligoss, Adam Christopher Nagy, Madhulika Srikumar, Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI, Berkman Klein Center For Internet & Society at Harvard University, 2020  \nKatyanna Quach, Judge lets art trio take another crack at suing AI devs over copyright, The Register, 21 July 2023, <  \nKnight, Will.\n\n2023.\n\n“Google DeepMind’s CEO Says Its next Algorithm Will Eclipse ChatGPT,” Wired < accessed 21 September 2023]  \nLeswing, Kif.\n\n2023.\n\n“Meet the $10,000 Nvidia Chip Powering the Race for A.I,” CNBC < accessed 21 September 2023]  \nMatt Davies and Michael Birthwistle, Regulating AI in the UK, Ada Lovelace Institute, 18 July 2023, <\n\nMcDonald, Clare.\n\n2022.\n\n“Around 750 New Software Developer Jobs Advertised Every Day,” Computerweekly.com < accessed 22 September 2023]  \nMichael Atleson, Keep your AI claims in Check (blog, Federal Trade Commission 27 February 2023), <\n\nMilmo, Cahal.\n\n2023.\n\n“ChatGPT Limited by Amazon and Other Companies as Workers Paste Confidential Data into AI Chatbot,” INews < accessed 22 September 2023]  \nMirjalili, Seyedali.\n\n2023.\n\n“If AI Image Generators Are so Smart, Why Do They Struggle to Write and Count?,” The Conversation < accessed 21 September 2023]  \nNIST, AI Risk Management Framework, <  \nOECD AI Policy Observatory, OECD AI Principles Overview, <  \nOffice of Artificial Intelligence, <  \nOnline Safety Bill, <  \nPolicy Connect, An Ethical AI Future: Guardrails & catalysts to make artificial intelligence a force for good, 19 June 2023, <  \nProposal for a Directive of the European Parliament and of the Council on adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive), COM/2022/496 final, <  \nProposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislation Acts, COM/2021/206 final, <  \nProposal for a Regulation of the European Parliament and of the Council on European data governance (Data Governance Act), COM/2020/767 final, <",
    "### Guidance on AI\n\nGenerative Artificial Intelligence (AI) holds promise in improving higher education, offering both opportunities and challenges for universities.\n\nAI-powered tools can provide new learning modes, personalize learning experiences, and enable students to receive tailored support and feedback.\n\nAdditionally, AI can streamline administrative tasks, enhancing efficiency and potentially reducing operational costs.\n\nHowever, the integration of AI also raises important questions about data privacy, ethics, algorithmic bias, and the need for faculty and staff preparation.\n\nBalancing AI’s potential and addressing these challenges is important, as the advent of these tools is already reshaping aspects of higher education.",
    "### AI and Academic Integrity\n\nThe UHM student code of conduct (IV.B.1.a) addresses “Cheating, plagiarism, or other forms of academic dishonesty.” It gives the instructor authority over defining unauthorized assistance, authorized sources, and specifically prohibited behavior in classes.\n\nFor this reason, instructors are strongly encouraged to:\n\n- Be specific about expectations and limitations on student use of AI in assignments.\n\n- Hold students responsible for the accuracy of facts and sources used in assignments, and to\n- Talk through scenarios with classes to provide clarity on expectations.\n\nSyllabi and class discussions should make instructor expectations clear with respect to use of AI tools.\n\nUHOIC maintains sample statements for adaptation in a syllabus or specific assignments.",
    "### Assignment and Assessment Redesign\n\nThe widespread availability of AI tools creates both challenges and opportunities for instructors.\n\nSimilar to the reframing required at the onset of digital calculators (at least for math and science instruction) and internet search engines, assignments and assessments may need redesign to achieve our learning objectives.\n\nSome useful redesign strategies are:\n\n- Chunk assignments with due dates for individual elements that precede final submission: an outline, notes on research articles, and drafts.\n\nInstructors might consider Google Docs version history and/or draftback, a tool to record the writing process, to review students’ writing progression.\n\nVersions of a document can be renamed to show stages of an assignment.\n\n- Replace a writing assignment with an audio file, podcast, video, speech, drawing, diagram, or multimedia project.\n\n(Make running to AI more work than it’s worth)\n- Incorporate AI.\n\nAsk students to generate a ChatGPT response to a question of their own choosing, then write an analysis of the strengths and weaknesses of the response.\n\n- Reference class materials or sources that are not available on the internet.\n\n- Include visuals — images or videos that students need to respond to.\n\n- Connect to current events or conversations in your field.\n\n- Ask for application of personal knowledge/experience (Khan Academy’s, AI for Education)\n\nAdditional strategies include:\n\n- Employ authentic and contextualized assessments.\n\nDesign to require critical thinking, analysis, and application of knowledge in real-world contexts.\n\nBy tailoring assessments to specific scenarios or case studies, it is challenging for students to rely solely on AI for complete solutions.\n\n- Use open-ended questions that prompt students to demonstrate their understanding, creativity, and ability to articulate ideas.\n\nThese are less likely to have direct answers generated by AI tools, encouraging engagement in original thinking and reflection.\n\n- Incorporate elements that evaluate the process students followed to arrive at solutions.\n\nThis could include written explanations, justifications, or reflections alongside final submissions, demonstrating the individual thought processes and learning journeys (UH Recommendations on Assessment).",
    "#### Learning About Generative AI\n\nExperts agree that generative AI tools are here to stay.\n\nTeaching and learning about AI is one piece of a need to assist students in learning appropriate 21st-century digital skills.\n\nLike other new technologies, there are challenges and concerns that come along.\n\nFaculty and students are encouraged to experiment with AI tools while staying within legal and ethical parameters to the best of their ability.\n\nProliferation of AI-integrated tools is predicted, and university personnel should be aware of policies and procedures that relate to the adoption of new technologies, including:\n\n- EP 2.219 Student Online Data Protection Requirements for Third Party Vendors",
    "### NTU Position on the Use of Generative Artificial Intelligence in Research\n\nNanyang Technological University (NTU) recognizes that Generative Artificial Intelligence (GAI) – such as OpenAI’s ChatGPT, Microsoft Bing, and Google Bard – provides new tools to support research practices and scholarly activities, including the preparation and drafting of research proposals, manuscripts, and other scholarly work.\n\nNTU acknowledges that GAI has the potential to enhance the quality and efficiency of research, and provide new modes of inquiry.\n\nHowever, it is important to recognize that its use in research can raise ethical, research integrity and other concerns.\n\nResearchers must therefore carefully evaluate these issues, and have a thorough understanding of the technology – including its potential risks and limitations (e.g., current AI- generated outputs can contain bias, errors, inaccuracies, or falsehoods) – before considering its use.\n\nResearchers who use AI tools in their scholarly work must use GAI in a responsible and accountable manner, and be transparent on the extent and nature of the involvement of GAI in their work.\n\nThis would include acknowledging the use of any AI tools in their research proposals, manuscripts, and scholarly works.\n\nCurrently, NTU does not impose restrictions on the use of GAI in research, except in the scenarios outlined in points 3 and 4 below.",
    "### Acknowledging/Declaring the Use of GAI\n\nIn the interests of transparency and integrity, the use of GAI of beyond basic spelling and grammar checks should be appropriately acknowledged and cited.\n\nAdditionally, any use of GAI to generate images for illustrations or figures should be clearly stated in the caption and/or document, in a manner easily understandable to the reader.",
    "### Authorship\n\nAuthorship requires the acceptance of responsibility for the work described in any manuscript.\n\nResearchers should recognise that GAI cannot be held responsible as an author for the accuracy, integrity, and content of such work.\n\nTherefore, any GAI (e.g.\n\nChatGPT) will not be listed as an author of any paper with an affiliation to NTU; or listed as a Principal Investigator (PI), Co-PI, or collaborator in any research proposals.\n\nAuthors and/or PIs are fully responsible for the content of their scholarly materials (e.g., research proposals, grant applications, manuscripts for publication) in which GAI was used in the preparation and/or development of such outputs.\n\nResearchers must therefore exercise caution and judgement when using GAI, and be ready to verify the accuracy and validity of their work.",
    "### Data Privacy & Confidentiality\n\nThe use of GAI to process or analyse research data must comply with all relevant data privacy and protection laws, regulations, and institutional policies - e.g., the Personal Data Protection Act (PDPA), NTU’s Data Governance Policy.\n\nAny confidential or sensitive information, and/or personal data are not to be uploaded to any GAI software, system, or platform unless:\n\n- Access to the GAI is controlled and restricted to only authorized study members involved in the research;\n- The data is not retained in or by the GAI; and\n- The activity does not contravene any applicable laws, regulations, or institutional policies in the process.\n\nResearchers must prioritize and safeguard the privacy and confidentiality of research data when using GAI and will be held responsible for any leakage of data.",
    "### Data Owned by Non-NTU Entities\n\nData owned by external parties, including but not limited to businesses, organisations, and government ministries and agencies, are also not to be uploaded to any GAI software, system, or platform unless:\n\n- All requirements for confidential, sensitive data, and/or personal data listed under “Data Privacy & Confidentiality” are satisfied;\n- Valid written permission has been explicitly provided by the data owner/custodian; or\n- The use of such GAI has been agreed upon in the Research Collaboration Agreement (RCA).",
    "## Conclusion\n\nGenerative AI in research must be used in an ethical, responsible, transparent, and accountable manner that benefits society.\n\nNTU faculty, staff, and students should adhere to the following when employing GAI in their research:\n\n- Appropriately acknowledge and cite the GAI used;\n- Do not attribute or list any GAI as an author;\n- Take full responsibility for the use of any GAI-generated content in their work; and\n- Safeguard the privacy and confidentiality of confidential and/or sensitive research data, including Personal Data.",
    "## Generative AI Policy\n\n(portions adapted from Middle TN State University and University of Kansas)  \nThe rapid development and widespread availability of AI raises ethical concerns for academic integrity.\n\nIt is imperative that all work submitted should be your own.\n\nUse of generative AI, including but not limited to, Chat GPT, Google Bard, Midjourney, DALL-E, etc., is explicitly prohibited unless otherwise noted by the instructor.\n\nThe information derived from these tools is based on previously published materials.\n\nTherefore, using these tools without proper citation constitutes plagiarism.\n\nAdditionally, be aware that the information derived from these tools is often inaccurate or incomplete.\n\nAny assignment that is found to have been plagiarized or to have used unauthorized AI tools may be subject to disciplinary action as an academic integrity violation.\n\nThe instructor maintains the right to investigate whether the submitted work is original.",
    "## Syllabus language\n\n(portions adapted from University of Delaware)  \n**ACADEMIC INTEGRITY:** Academic integrity is to be maintained at all times to ensure genuine educational growth.\n\nStudents are expected to generate their own original work.\n\nCheating and plagiarism in all forms, including unauthorized use of generative AI, will be subject to disciplinary action.\n\nSerious infractions will be addressed by the Dean of the College.\n\nAppropriate sanctions will be imposed.",
    "### Options for instructor (choose one):\n\n- **Use prohibited:** Students are not allowed to use generative AI tools on assignments in this course.\n\nStudents are expected to complete each assignment without substantive assistance from others, including automated tools.\n\n- **Use only with prior permission:** Students are allowed to use generative AI tools on assignments in this course only if instructor permission is obtained in advance.\n\nUnless permission is given to use these tools, each student is expected to complete each assignment without substantive assistance from others, including automated tools.\n\n- **Use only with acknowledgement:** Students are allowed to use generative AI tools on assignments in this course if that use is properly documented and credited.\n\nFor example, text generated using ChatGPT-3 should include a citation such as “ChatGPT-3.\n\n(YYYY, Month DD of query).\n\n‘Text of query.’” Material generated using other AI tools should follow a similar citation convention.",
    "### ChatGPT and other AI-based tools\n\nGiven the speed and scale at which artificial intelligence-powered tools can analyze data and identify patterns, artificial intelligence (AI) is already advancing human understanding in fields across the disciplinary spectrum.\n\nTools that use AI to generate text or images are also supporting individuals, particularly those who struggle with executive functional skills, at various stages of the research, writing, or creative process.\n\nInside and beyond the classroom, AI-based tools are creating efficiencies that help humans focus on deeper thinking.\n\nThese tools present instructors with both opportunities and challenges when it comes to teaching and learning.",
    "### Talking with students about AI\n\nMany instructors are already integrating AI into their teaching.\n\nOthers would prefer that students not use AI to complete assignments.\n\nBecause AI tools are constantly evolving, it is very difficult to develop technology that can reliably identify when a student uses AI to complete an assignment.\n\nThus, a policing approach to student use of AI has the potential to be both time-consuming and unsuccessful.\n\nInstead, it is important to help students understand the issues associated with AI and its relationship to learning in general and your class in particular.\n\nThe following strategies can help instructors think about how to communicate with students, set expectations, and design assignments that increase students’ motivation to develop their own skills and ideas.\n\n- **Set expectations** – Establish a policy for your course around the use of AI-based tools (e.g., ChatGPT) and communicate this with students through the syllabus and/or assignment prompts.\n\nDiscuss how you will proceed if you discover that a student has turned in AI-generated work.\n\nHere are some sample syllabus statements you can use or adapt to help articulate your expectations for student use of AI in your course.\n\n- **Communicate the importance of college learning** – Many students are focused only on learning that seems related to their intended career track.\n\nHowever, the vast majority of them will change careers at least once in their lives.\n\nTalk with students about how the relevance of your course may only become apparent years from now.\n\nThe skills they are learning will likely transfer to other careers – even careers that do not yet exist!\n\n- **Acknowledge that struggle is part of learning** – Talk with students about how intellectual struggle is an inherent part of learning.\n\nLearning happens only when we move outside what we already know.\n\nSeeking a shortcut or workaround through AI tools only prevents them from learning.\n\nThe short-term consequence is that they pay for a benefit they never receive.\n\nThe long-term consequence is that they miss the opportunity to become better, more effective thinkers, writers, researchers, and creators.\n\n- **Discuss the social, ethical, and practical issues surrounding AI** – The processes that support the development and functionality of AI-based tools raise issues related to privacy, disinformation, environmental impact, bias, exploitation, and academic integrity, among other things.\n\nIn addition, although AI-generated output appears authoritative and factual, it is frequently riddled with inaccuracy.\n\nDiscussing the ethical and social concerns related to AI with students can help them see the social context of AI and can position them to make thoughtful decisions about their own use of AI-based tools.\n\n- **Assess process as much as (or more than) product** – Lowering the stakes of individual assignments reduces students’ motivation for cheating and encourages them to build their own skills and competencies.\n\nLow- or no-stakes formative assessments reinforce the notion that learning is a process and demonstrates to students that what’s valuable is the learning, not the grade.\n\nDesign assignments that ask students to connect course content, class discussion, and lived experience.\n\nIt’s harder for AI-based tools to effectively connect the dots between these sources of knowledge.\n\nConsider teaching through AI-based tools.\n\nThink about how using AI-based tools might facilitate students’ learning and prepare them to thoughtfully engage these tools in their personal and professional lives.\n\nHow can students use AI-generated output to think critically and analytically?\n\nHow can these tools help them ask questions about digital literacy and information accuracy?\n\nFurther down this page we’ve provided some examples of how to integrate AI into assignments.",
    "#### Using AI to enrich your teaching\n\nThe list below contains just a few of the ways you might use AI to enhance your teaching.\n\nNote how often the word “draft” appears in this list.\n\nIt’s important to remember that AI is merely an assistive technology.\n\nBecause AI cannot distinguish fact from fiction, you should always refine AI-generated output.\n\n- Draft learning outcomes\n- Draft module/unit descriptions\n- Draft slides or other teaching materials\n- Draft discussion prompts\n- Draft rubrics\n- Brainstorm authentic assessment ideas\n- Identifying and analyzing themes in student feedback (e.g., mid-quarter check-in survey)",
    "#### Using AI to enhance student learning and engagement\n\nBelow are some examples of how instructors might use AI to facilitate learning.\n\nMany of these examples familiarize students with AI-based tools, but also prompt critical examination of their value, accuracy, strengths, and shortcomings.\n\n- **Think-pair-AI-share.\n\n** Students think (as individuals) about a question/concept, then pair up with a peer to discuss.\n\nThe pair then plugs the question/concept into an AI tool (e.g., ChatGPT, GPT-4, Bing Chat) and discusses or analyzes the output.\n\n- **Evaluating AI output.\n\n** Co-develop a rubric with students that describes the components of an effective essay, lab report, précis, technical manual, blog post, etc.\n\nStudents prompt an AI tool to generate three versions of the assignment on a given topic and then use the rubric to evaluate the quality of the AI-generated versions.\n\n- **Improving upon/adapting AI-generated output.\n\n** Students use an AI tool to draft text or code in response to a prompt.\n\nStudents must then improve upon the AI-generated output.\n\nWhen students turn in their assignment, they must include both the AI-generated text and their improved version.\n\n- **Explaining the steps in an AI-generated solution.\n\n** Students use AI to solve a math problem.\n\nWorking from the AI-generated solution, they then work in groups to explain or analyze the steps that the AI tool used to arrive at the solution.\n\n- **Visualizing concepts with AI.\n\n** Students select a concept covered in lecture or course readings.\n\nStudents then prompt an AI image generator to create an image that represents the connection between the concept and daily life.\n\nThey must then explain how the AI-generated image conveys the concept and its relationship to daily life.\n\nStudents might also analyze the strengths and shortcomings of AI image generators.\n\n- **Exploring AI in your field.\n\n** Students explore current applications of AI in the discipline of the course or in their major.\n\nWithin the context of the discipline (or their major), students examine both AI’s advantages and limitations.",
    "### What to do if you suspect academic misconduct\n\nStudents are expected to practice high standards of academic and professional honesty and integrity.\n\nThe University communicates with students about the importance of knowing and understanding the expectations of both the University and specific instructors regarding academic standards.\n\nIf you have prohibited the use of AI-based tools and suspect that a student has engaged in academic misconduct, you can make a report to your campus Student Conduct office.\n\n- UW Bothell Student Affairs\n- UW Seattle Community Standards and Student Conduct\n- UW Tacoma Student Conduct\n\nInformation that is communicated to students regarding academic standards and the Student Conduct Code is available on the Office of Community Standards and Student Conduct Academic misconduct page.\n\nAdapted from Finley-Croswhite, 2023, and Wong, 2023.\n\nAdapted from Michael McCreary, Goucher College.\n\nAdapted from assignments created by Andrea Otañez, UW Communications; Carly Gray, UW Psychology; Richard Ross, University of Virginia; and materials in Laquintano, et al, 2023.\n\nAdapted from Finley-Croswhite, 2023.\n\nAdapted from an assignment created by Christine Savolainen, UW Biology.\n\nAdapted from UW Bothell, Office of Student Academic Success.\n\n---",
    "## STUDENT GENERATIVE AI USAGE\n\nReturn to Employee Resource page\n\nTo the Instructor:\nAs you are aware, ChatGPT and other generative AI have created many questions on the use of these tools in academic situations by students.\n\nTo address these and other issues created by the emergence of this technology, the Provost and Deans\n\nThe UNC AI Committee has developed the following recommended language for student guidelines for their use of generative AI.\n\nThey are designed to provide a framework for the ethical and responsible use of Generative AI tools, such as ChatGPT, by students in their academic work.\n\nThe view of the committee is that society’s acceptance and use of generative AI is inevitable.\n\nOrganizations will expect our students to have generative AI skills, and students, faculty and staff will find them useful tools for learning, productivity, and creativity.\n\nHowever, we must ensure they are understood and used appropriately and ethically.\n\nTherefore, the philosophy guiding this recommended language is that humans are responsible for the use of generative AI and that “AI should help you think.\n\nNot think for you.” Please review and incorporate the recommended language in your syllabi, adapting them as you choose to fit your course.\n\nIt is also recommended that you cover them on the first day of class.\n\nLastly, as you will note in the document,\n\nGiven the speed at which generative AI is changing and being used, we expect these guidelines to continue to evolve.\n\nTherefore, if you have any questions or feedback, please contact Mark McNeilly at  or your academic unit representative.",
    "## Syllabus Guidelines for Generative AI\n\nGenerative AI is extremely useful; however, it has the following limitations:\n- How output is arrived at is not clear as the internal processes used to produce a particular output within the generative AI cannot be determined.\n\n- The output is based on existing data (often scraped from online sources) and may reflect biases that should be acknowledged; it may also be inaccurate or entirely fabricated, even if it appears reliable or factual.\n\n- AI evokes a range of intellectual property concerns; sourcing and ownership of information is unclear, and the status of\n\nThe following sections provide the philosophy and specific guidelines for using these tools and features (increasingly, generative AI capabilities will be integrated with everyday applications).\n\nUnless provided other guidelines for an assignment or exam, you should follow these guidelines.",
    "### Usage Philosophy\n\nUse of generative AI in your coursework is based on the following principles:\n- AI should help you think.\n\nNot think for you.\n\n- Use these tools to give you ideas, perform research (in compliance with point 2 below), and analyze problems.\n\nDo not use them to do your work for you, e.g., do not enter an assignment question into ChatGPT and copy & paste the response as your answer.\n\n- Engage with AI Responsibly and Ethically: Engage with AI technologies responsibly, critically evaluating AI-generated outputs and considering potential biases, limitations, and ethical\n\nthe data used for AI applications is obtained and shared responsibly and in compliance with relevant regulations.\n\nYou are 100% responsible for your final product.\n\nYou are the user.\n\nIf the AI makes a mistake, and you use it, it’s your mistake.\n\nIf you don’t know whether a statement about any item in the output is true, then your responsibility is to research it.\n\nIf you cannot verify it as factual, you should delete it.\n\nYou hold full responsibility for AI-generated content as if you had produced the materials yourself.\n\nThis means ideas must be attributed, facts are true, and sources must be verified.\n\n- The use of AI must be open and documented.\n\n- The use of any AI in the creation of your work must be declared in your submission and explained.\n\nDetails on how to source your AI usage are explained below.\n\nThese guidelines are in effect unless given specific guidelines for an assignment or exam.\n\nIt is your responsibility to ensure you are following the correct guidelines.\n\nData that are confidential or personal\n\nexposes you and others to the loss of important information.\n\nTherefore, do not do so.\n\nThe following sections provide the philosophy and specific guidelines for using these tools and features (increasingly, generative AI capabilities will be integrated with everyday applications).\n\nUnless provided other guidelines for an assignment or exam, you should follow these guidelines.",
    "#### Assignments\n\n- Writing and Presentation: In principle, you may submit material that contains AI-generated content, or is based on or derived from it, if this use is properly documented.\n\nThis may include drafting an outline, preparing individual sections, combining elements, removing redundant parts, and compiling and annotating references.\n\nYour\n\n- Multimedia Assignments: In principle, you may submit material that contains AI-generated content, or is based on or derived from it, if this use is properly documented.\n\nThis may include the generation of images, audio, music, video, etc.\n\nYour documentation must make the process transparent – the submission itself must meet the relevant standards of attribution and validation.\n\n- Mathematical and Statistical Analysis, Data Analysis, Data Interpretation, Coding of Data, generalizing data to a problem set or any other forms of quantification of language or concepts, etc.\n\n: Generative AI can be used for these purposes; however, the output must be verified via your own mathematical calculations and proof of work provided in your assignment.\n\n- Readings and Discussions: Generative AI can be used to analyze readings.\n\nHowever, you must also do the readings.\n\nGenerative AI analysis is not a substitute for reading the works themselves.\n\nSimilarly, participating in online discussions of readings requires that you provide your own contributions.\n\nUnless\n\ndocument your use.\n\nPossibilities include topic brainstorming, search assistance, source evaluation, and summaries and source documentation.\n\nTrack your use of AI throughout these stages, and then document this assistance as you submit the project.\n\nAny material generated through AI in your projects should also be documented in your citations.\n\n- Simulations: In principle, you may use AI tools for advice or brainstorming.\n\nIt should not, however, be used to find cheats or other unfair advantages.\n\nIf a report is part of the assignment, your documentation of how you used AI in completing the simulation must make the process transparent.\n\n- Group Work: Group work guidelines are based on the type of assignment above (e.g., a group written assignment will use the guidelines for written assignments).\n\nIn-Class Activities: Instructions on the appropriate use of AI for in-class activities will be provided by me.\n\nWritten & Oral Exams: Unless explicitly granted permission, the utilization of AI tools is prohibited and could potentially constitute a reportable violation to the\n\ncitation, verification, and clarity as outlined below.",
    "#### Sourcing Use of AI\n\n- Accuracy: Generative AI may invent both facts and sources for those facts.\n\nVerification is your responsibility, whether the source of the error is you or the AI makes no difference.\n\nYou need to check the facts, the quotes, the arguments, and the logic, and document what you did to validate your material.\n\n- Attribution: All ideas that are not originally your own have a source and that source must be attributed.\n\nPlease be aware that generative AI tends to invent sources.\n\nYou have a two-fold obligation with respect to attribution:\n  - If a source is identified, find and attribute the original source of the idea, identify the location of the text within the source, and provide a working link to the location (if the source is available online).\n\nIf you are not able to locate the source, delete that content.\n\n- Document the process by explaining how you used generative AI\n\nproject, develop, and include an appropriate version of the below statements:\n“I attest that this project did not use AI at any stage in its development or in the creation of any of its components.”\n“I attest that this project made use of AI in the following ways:” You must then use the following form to document your usage.\n\n*Note that such attribution is not a valid source for facts, only for the output itself.\n\n[1] ChatGPT was used in the development of these guidelines – more specifically, it was employed to generate suggestions for student use policies and to rephrase and consolidate certain sections of the text.\n\nAlso, Sentient Syllabus was a resource for a number of the ideas within this document.\n\nReturn to Employee Resource page\n\n© 2017 The University of North Carolina at Chapel Hill\n\nThis website uses cookies and similar technologies to understand visitor experiences.\n\nBy using this website, you consent to UNC-Chapel Hill's cookie usage in accordance with their Privacy Notice.",
    "# Communicating Your ChatGPT/AI Policies\n\nThe new generation of generative AI tools, including ChatGPT, makes it easy to quickly generate content.\n\nStudents will be tempted to use this content to complete their assignments.\n\nWhen this is done without faculty permission, this can lead both to academic honesty violations as well as robbing students of the learning process.\n\nChamplain College’s academic honesty policy states clearly that:\n\nAs faculty, it is important that we be clear as to our intended purpose as well as providing clear instructions around citations and AI-generated content.\n\nThis better informs students and avoids confusion when there is a suspected academic honesty violation.\n\nThe syllabus is the proper place to initially capture this information.\n\nIt is important that you also review it both at the beginning of the semester and reiterate it when framing or describing relevant assignments.\n\nThe language included here is meant to be examples only.\n\nAdjust it as needed for your own courses.\n\nA simple catch-all statement that will work for many faculty is (provided by Amanda Crispel, CCM):\n\nFor your own courses, you may find that you need something more specific.\n\nHere are some examples for different circumstances.",
    "## Full Use of AI Tools\n\nExample Citation:\n\nNone of these statements is required.\n\nAs a faculty member, you may choose to ban AI content generation, use it for specific purposes, or allow it generally.\n\nThe College does not have a policy banning all use of AI.\n\nHowever, you are encouraged to consider what policy is the best fit for your outcomes, students’ learning and information literacy, and academic integrity.\n\nWe highly recommend that your syllabus clearly reflect your stance.\n\nWas this article helpful?\n\nYes No",
    "## Related Articles\n\n- Getting Started with Canvas Studio \n- Getting Started with InSpace \n- Advanced Features of Google Sites \n- Digital Polling Tools\n- Setting Up Your Recording Environment \n- Make Recording Lectures Easier on Yourself\n\nBurlington, Vermont\n\nAcademics Admissions Student Life Career Success Champlain College Online\n\nABOUT CHAMPLAIN COLLEGE CENTERS OF EXPERIENCE GIVING\n\nNEWS MEDIA INQUIRIES VISIT US LIBRARY CONTACT CHAMPLAIN\n\nMAPS & DIRECTIONS\n\nPRIVACY LEGAL CONSUMER INFORMATION SITEMAP\n\n© Copyright 2024 | Champlain College Incorporated, Burlington, VT | (802) 860-2700 or (800) 570-5858",
    "### The Infinite Potential of Generative AI\n\nChatGPT has become a topic of conversation for nearly everyone.\n\nAs a generative artificial intelligence based on large language models (LLMs), it is not the only one available today, but it is certainly the best-known and most widely used compared to its counterparts: DALL-E, Bard, Stable Diffusion, Midjourney, and others.\n\nIn the history of technologies, it currently holds the record for penetration, with 100 million new users in just two months.\n\nThis record will be broken, as previous ones have been, and it will be fascinating to see who accomplishes this achievement and when.\n\nChatGPT is both fascinating and intimidating.\n\nThe ability for anyone to interact with an interface capable of answering nearly any question, providing millisecond responses in impeccable language, is undeniably exciting.\n\nHowever, despite its remarkable competence, critics caution that it can make errors and even engage in what is technically referred to as “hallucinations”.\n\nRegardless, Bill Gates considers it an epochal step, equal to, if not surpassing, the Internet revolution.\n\nYet, there is also a worrisome aspect.\n\nThe creators of ChatGPT, such as Sam Altman of OpenAI and others, harbor concerns about their own creation.\n\nWhile they are certainly not contemplating putting the genie back in the bottle, they emphasize the need for caution, the establishment of rules, and the definition of an ethical framework for the creation and utilization of AI.\n\nIt is unlikely that they wish to stifle the goose that lays the golden eggs; more plausibly, they seek to safeguard themselves against the potential adverse outcomes of their creation.\n\nThis book utilizes ChatGPT not for recreational purposes or generic research and discovery, but to conduct an experiment exploring the tool’s potential in the professional and cognitive domains.\n\nThe authors of the experiment—Paolo Cervini, Elisa Farri, and Gabriele Rosani—have immersed ChatGPT in the realm of corporate strategy and culture, investigating how generative AI can enhance the application of key strategic and organizational theories and practices, both in terms of incremental improvements and radical rethinks.\n\nThe experiment encompassed ten management theories and practices, categorized into two main groups: business strategy and innovation, and organization and culture.\n\nA dialogue with AI was constructed using generated and regenerated prompts to obtain meaningful answers pertaining to well-known management theories and practices, such as the Blue Ocean Strategy and Agile.\n\nThese answers were then compared with the professional experiences of the authors, who are seasoned consultants and have recently been included in the world ranking of Thinkers50.\n\nThe path taken was meticulously prepared and rigorous, aiming to acquire reliable and qualified answers for further scrutiny.\n\nIn fact, ChatGPT was asked to rank the top three elements for each response based on established criteria: business value, speed of execution, cost of execution, and feasibility.\n\nThe ranking provided by the generative AI was not subsequently challenged, although there were instances where the team held differing opinions.\n\nThe outcome of this concentrated experiment, conducted over a handful of days, is of significant interest, not only because it may be the first of its kind, but also because it allows anyone interested to form their own opinion regarding the tool’s potential.\n\nHarvard Business Review Italia considers this with great attention, as shown by the recent publication of the book “The Revolution of Artificial Intelligence”.\n\nWe are merely at the beginning, and the research continues.",
    "## Approach\n\nBetween May 29th and June 5th, 2023, we conducted a series of experiments with ChatGPT Plus (GPT-3,5 and GPT-4) to investigate how GenAI can improve the application of leading strategic and organizational theories/practices, both in terms of incremental enhancements and radical rethinking.\n\nThe scope of experiments included the following 10 theories and practices, grouped in two main categories:\n\n- **Business Strategy & Innovation Management:** value innovation; growth planning & experimentation; platforms & business ecosystems; multi-stakeholder co-creation; open innovation.\n\n- **Organization & Innovative Culture:** leadership behaviors; fluid & flexible organization; agile way of working; empowerment & decision making; employee motivation.\n\nThe selection was based on two main criteria: \n\n1.\n\nTheories/practices that are popular and globally recognized (such as Blue Ocean Strategy, Open Innovation, or Agile).\n\n2.\n\nOn which we are domain experts (because of our extensive application, research, and publications over the last 10-15 years).\n\nHere below we provide more detailed information about our experimental approach:\n\n- **Format of interaction:** human-ChatGPT dialogue (like a Q&A with an expert).\n\n- **Role of ChatGPT:** we asked ChatGPT to act as an expert both in GenAI and management theories/practices.\n\n- **Context:** For each theory/practice, we input ChatGPT with references in terms of: ideator of each theory/practice; articles or books explaining the related approach; examples; case studies (mainly featured in media or academic publications).\n\n- **Prompting:** After testing various options, we selected a sequence of 8 main prompts (i.e., text query/question) because it delivered the most accurate, creative, and detailed answers:\n\n\n1.\n\nPrompt about valuable enhancements that GenAI can bring to the application of a selected theory/practice.\n\n2.\n\nPrompt about common pitfalls.\n\n3.\n\nPrompt about how GenAI can help overcoming the common pitfalls.\n\n4.\n\nAgain prompt #1.\n\n5.\n\nPrompt about the main shifts in human role and skills.\n\n6.\n\nPrompt about how GenAI can disrupt and radically rethink the selected theory/practice.\n\n7.\n\nAgain prompt #5.\n\n8.\n\nPrompt about the ranking of top 3 enhancements and top 3 areas of disruptions.\n\n- **Regeneration:** we often preferred to rephrase the questions to ChatGPT, varying the prompting with additional information and feedback, rather than simply using the “regenerate” button.\n\n- **Ranking:** once the entire prompting sequence was implemented, we asked ChatGPT to consider only the improvement ideas and radical transformations that we selected and reviewed.\n\nThen we provided ChatGPT with four evaluation criteria: business value, execution speed, execution cost, and feasibility.\n\nBased on these specific criteria, ChatGPT identified the “Top 3 improvements” and “Top 3 radical transformations” for each theory/practice examined in their respective chapters.\n\nThe remaining ideas are presented without a defined ranking.\n\nWe did not question the ranking determined by ChatGPT, even though we had different opinions in some cases.\n\nChatGPT also provided detailed explanations for each criterion to support its evaluation.\n\nThis further confirms the future possibilities of interaction and collaboration with ChatGPT.",
    "## Experiment Rules & Guidelines:\n\n1.\n\nWe created a new ChatGPT Plus account for this experiment.\n\n2.\n\nWe created separate chats (‘verticals’) for each theory and practice.\n\nWe did not want to mix the topics, but most importantly we wanted to train the language model on selected theories and practice, laying the foundations for an ongoing experimentation.\n\n3.\n\nWe assigned the theories and practices to the person with the strongest domain expertise (on average, each of us covered 3 areas).\n\n4.\n\nWe asked ChatGPT to provide output in the form of max 8 items for each prompt.\n\n5.\n\nFor Prompt #6 we asked ChatGPT to be creative when thinking of potential ideas for disruption.",
    "## Human Curation:\n\n- We challenged ChatGPT answers based on our hands-on experience (e.g., “based on our experience, we’d add another pitfall… or we’d rephrase the last pitfall because of…”).\n\n- We offered additional input, insights, examples (especially when ChatGPT delivered answers we felt too generic).\n\n- We commented responses to help ChatGPT fine-tune and improve the quality of each answer.\n\nBut also, to make it as much similar as possible to a human-to-human conversation.\n\n- We peer-reviewed ChatGPT’s responses to judge the quality of the generated answers: elimination of least convincing items, sequencing and prioritization of the most insightful items, refinement of the labeling and the text (e.g., cutting some redundant parts of the generated text, or highlighting important parts), identification of criteria for ranking.\n\n- We identified the criteria for evaluating and ranking the list of ideas proposed by ChatGPT.",
    "### An Enriching Experience\n\nOn one hand, it provided us with valuable on-the-job training.\n\nEach interaction served as a learning opportunity, helping us refine our prompting techniques and foster our critical thinking skills.\n\nOn the other hand, engaging with ChatGPT proved to be an intense mental exercise.\n\nIt demanded a significant yet gratifying and stimulating effort.\n\nWe found ChatGPT to be an excellent resource, serving as a reliable partner in simulating exceptional, high-quality brainstorming sessions.",
    "#### Challenges and Future Approach\n\nConcentrating our testing activities within a few days presented its own set of challenges.\n\nTo address this, we plan to utilize the tool for max 2 hours a day in the future.\n\nThis approach aligns with the principles of brainstorming among human participants, where the process is often spread out over multiple sessions.\n\nAdditionally, we aim to continue training the language model from a perspective of continuous learning on each specific theory and practice.",
    "### PROMPTING\n\nInitially, ChatGPT may provide generic and superficial responses, but subsequent prompts can be used to improve its accuracy and quality.\n\n- When utilizing multiple sources such as books and articles, it is more effective to present them as a context and then query ChatGPT “source by source” before requesting consolidation.\n\n- Certain topics consistently appear throughout various theories/practices, and ChatGPT incorporates these recurring themes as part of its default model (e.g., integrating data-driven insights, simulations, visualization, ethics).\n\n- Occasionally, ChatGPT may mix significant insights with less valuable information, so human intervention is necessary to curate and select the most appropriate responses.\n\nIf unsatisfactory answers are provided, asking for better alternatives can be helpful.\n\nTo provide updates to ChatGPT, specific excerpts from texts such as chapters, articles, or books can be entered.\n\nWe found ChatGPT to be an excellent resource, a partner to stimulate high-quality brainstorming sessions",
    "### REPROMPTING / REGENERATION\n\nWhen interacting with ChatGPT, it is important to request specific responses linked to the context of the chapter, avoiding overly generic answers, and encouraging the use of keywords related to the specific topic.\n\nThere are instances where it is necessary to guide ChatGPT to consider certain aspects more thoroughly or refer to specific articles, parts, or more granular details within a broader theory.\n\n- ChatGPT tends to perform better when provided with a well-defined input to work with, such as the “six paths” of the Blue Ocean Strategy or the main steps of the Discovery Driven Planning approach.\n\nCustomizing the response in a timely manner based on such inputs yields better results.\n\n- While regeneration can help improve the rephrasing of answers, it may be less effective in generating significantly different responses.\n\n- After some time, ChatGPT may start repeating itself, requiring a change in the approach or prompt structure.\n\nFor example, explicitly asking for less obvious or intuitive items or exploring different perspectives can be beneficial.\n\n- It can be useful to provide ChatGPT with an “additive” input, expanding on the initial list or asking it to consider similar things or explore new directions.\n\n- Requesting concrete examples is important, as ChatGPT rarely generates them on its own.\n\nOften, the value of the response lies in the provision of specific, detailed, and tangible examples.",
    "### TONE/NARRATIVE\n\nChatGPT often includes unnecessary pleonastic padding, resulting in the use of two or three sentences when one would suffice.\n\n- It may be beneficial to ask ChatGPT for a more engaging or fitting rephrasing related to the topic, as its default responses can be somewhat flat, repetitive, and uninteresting.\n\n- The “regenerate” function can be employed to modify specific items, allowing for the rephrasing of answers by shortening, expanding, or making them less verbose through the use of examples.\n\nCreating narrative pieces by initially providing inputs or keywords helps establish the necessary connective structure, which can be further refined through multiple rounds of regeneration and requests for shorter or longer responses.\n\nAdditionally, fine-tuning can be employed to correct the wording in a more targeted manner (by instructing ChatGPT on the desired style), followed by the incorporation of personal touches.\n\nIt is important to let ChatGPT know what kind of narrative response we would like to obtain (number of words, paragraphs, narrative tone, ...)",
    "### CREATIVITY\n\nChatGPT demonstrated an impressive level of creativity, in some cases better than many domain experts.\n\n- When seeking disruptive thinking, encourage ChatGPT to explore extremes by requesting “wild ideas” that are radically different or unconventional.\n\n- If after some iterations ChatGPT becomes more predictable, it can be helpful to explicitly prompt it to be more creative and to look “outside the box”.\n\n- In first place, for ChatGPT the notion of “disruption” tends to be associated with technological disruptions (such as AR/VR, blockchain, etc.\n\n), therefore prompt it to also explore ideas that are not necessarily driven by technology.\n\n- Sometimes ChatGPT may propose similar patterns/ideas for different theories.\n\nWhen this happens, ask ChatGPT to better customize the idea to the specific context, adding concrete examples.\n\nChatGPT has demonstrated an impressive level of creativity, sometimes surpassing that of many management experts",
    "### CONTENTS / COGNITIVE BIAS\n\nIn the responses, the logic behind the sequence of the various bullets is often unclear.\n\nBy applying deliberate pressure and requesting clarification, it is possible to gain a better understanding and encourage ChatGPT to organize the information in a more logical and coherent manner.\n\n- ChatGPT may not be accurate in the distinction between AI and GenAI and between AI and other technologies, such as blockchain or VR/AR (where an AI component exists).\n\n- When ChatGPT is given a corrective input, it tends to incorporate it explicitly in subsequent answers, seemingly aiming to please and satisfy the user.\n\nTherefore, it is important to exercise judgment when providing corrective inputs.\n\n- ChatGPT sometimes may struggle to “connect the dots” between different theories and practices.\n\nWhen this happens, it’s useful to explicitly prompt it to look for analogies and connections.\n\n- ChatGPT has not displayed any significant errors or hallucinations that could be considered major mistakes in its responses.",
    "### HUMAN JUDGEMENT\n\nWhen evaluating responses and selecting generated items, it is crucial to possess strong domain expertise and maintain critical judgement.\n\nThis is necessary because ChatGPT tends to mix valuable insights (“gems”) with less valuable information.\n\n- Assessing the effectiveness of response phrasing also requires experience and familiarity with business jargon.\n\nThis expertise is essential to determine the appropriateness and relevance of the language used.\n\n- For individuals with robust domain expertise, ChatGPT can serve as an excellent source of inspiration.\n\nIt can act as an engaging sparring partner, stimulating motivational flow and generating intriguing insights.\n\n- Overall, ChatGPT is an amazing tool for generating lists of items and ideas that can be further screened, refined, and evaluated.",
    "## Guidelines for the Reader\n\nThe chapters in this eBook are grouped into two main categories: \n\n1. Business strategy and innovation management.\n\n2.\n\nOrganization and innovative culture.\n\nWhile the chapters are organized in a coherent sequence with interconnections between them, they can also be extracted as standalone pieces.\n\nDepending on your curiosity, domain of expertise, or specific business challenge, you can choose the chapters that resonate with you the most.\n\nThey can be read in sequence or independently, depending on your preference.\n\nWhen you engage with each chapter, it is important not to limit yourself solely to the labels assigned to each bullet point (be it a suggestion for enhancement or potential disruption).\n\nTake the time to read the descriptions, as they hold the true value of the content.\n\nAs emphasized in the previous chapter about key takeaways of this experiment, the labels may sometimes appear generic, but the descriptions and examples contain the essence of the material.\n\nFrom a content perspective, each chapter presents intriguing insights derived from ChatGPT.\n\nThese insights can help you apply frameworks more effectively and efficiently, or even challenge and rethink them in a more radical manner.\n\nThis book is an experimental project and does not aim to provide definitive answers.\n\nInstead, it introduces a novel way of interacting with artificial intelligence.\n\nThe current version serves as a starting point for further exploration and research experiments.\n\nWith these instructions in mind, we invite you to embark on this experimental journey.\n\nEmbrace the possibilities that emerge from this interaction with ChatGPT.\n\nMay it serve as a catalyst for your own exploration.\n\nLet the experiment continue...",
    "#### Theory & Practice We Selected\n\nInnovative business strategies have become essential for organizations seeking to differentiate themselves in the marketplace and create sustainable growth.\n\nThe leading reference theory and framework is Blue Ocean Strategy (book and article) by W. Chan Kim and Renée Mauborgne.\n\nIt suggests that companies can achieve remarkable success by creating new markets instead of competing in existing ones.\n\nThe theory emphasizes the pursuit of uncontested market spaces, known as “blue oceans”, where competition is irrelevant or non-existent.\n\nBy identifying customer needs and pain points and offering unique value propositions, companies can create new demand for their offerings.\n\nIn practice, Blue Ocean Strategy methodology follows some key steps: analysis of the current Industry, identification of unmet customer needs; design of a Value Curve; exploration of new market spaces; development of a Business Strategy.\n\nBlue Ocean Strategy is part of the broader Business Model Innovation concept, the process of creating, modifying, or redefining a company’s business model to achieve superior performance or gain a competitive edge.\n\nTo this regard, a relevant management tool is the Business Model Canvas, popularized by Alexander Osterwalder and Yves Pigneur: it allows organizations to define, visualize, and refine their business models.",
    "#### Our Hands-on Experience\n\nAs long-term partners and contributors of the Blue Ocean Strategy Network, we have applied the methodology on dozens of firms across various industries and sectors worldwide, with established large companies and with smaller ones.\n\nWe have witnessed the transformative power of Blue Ocean Strategy: simple and effective framework and visual tools, a clear process to follow, the ability to foster a culture of creativity and innovation within organizations, encouraging employees to think outside the box.\n\nDuring its application, we have often observed organizations struggling in terms of:\n\n- **Customer understanding:** organizations may struggle to fully grasp the needs, preferences, and behaviors of their target customers.\n\nLack of market research, inadequate customer feedback channels, or a failure to analyze and interpret customer data can contribute to this challenge.\n\n- **Internal biases:** organizations can face difficulties in overcoming preconceived notions about their industry, customers, or competition.\n\nThese biases can prevent the exploration of new opportunities, as decision-makers may be hesitant to deviate from established practices.\n\n- **Quantification of qualitative insights:** translating qualitative insights, such as customer feedback, into quantitative data can be a complex task for organizations.\n\nThe inability to quantify these insights makes it challenging to prioritize opportunities and make data-driven decisions.\n\n- **Complexity in implementation:** organizations often encounter challenges when moving from strategy formulation to implementation.\n\nThis can be due to various factors, including resistance to change, inadequate resource allocation, lack of clear action plans, or insufficient coordination.",
    "#### Enhancements - What Are the Most Valuable Enhancements That GenAI Can Bring to the Application of Innovative Business Strategies Theories?\n\n1.\n\n**AI-Enhanced Competitive Landscape Analysis:** GenAI can leverage data analytics to quantify qualitative insights on market dynamics and help identify gaps and differentiation opportunities.\n\nGenAI can provide evidence-based recommendations for divergent value propositions.\n\n2.\n\n**Idea Generation and Validation:** GenAI can facilitate idea generation sessions, leveraging historical data, market trends, and customer insights; it can also assist in conducting surveys and gathering feedback to validate and refine business ideas before moving to implementation.\n\nGenAI can help looking across industry boundaries, strategic clusters, and other “strategic paths” described in the Blue Ocean methodology to spur ideas for strategic reconfiguration.\n\n3.\n\n**Dynamic and Collaborative Business Modeling:** GenAI can enable real-time collaboration, version control, and dynamic updates.\n\nThis capability supports iterative testing, refinement, and adaptation of business models based on new insights and market dynamics.\n\n_Generative AI can facilitate idea generation sessions, using historical data, market trends, and customer info.\n\nIt can also help quickly conduct surveys and gather feedback to validate and refine new strategic ideas._",
    "**Additional enhancements include:**\n\n- **Explore and Unearth Value Barriers:** Regarding customer/user understanding, GenAI can facilitate the combination of quantitative data analysis with qualitative research methods and help building a comprehensive and nuanced understanding of customer needs, behaviors, and preferences.\n\n- **Concept Exploration and Visualization:** GenAI can help visualize different business concepts and models, or prototypes of potential business ideas, allowing to assess and refine them more effectively.\n\nThis is particularly useful when using tools like the Blue Ocean Value Curve or the Business Model Canvas, as GenAI can generate visualizations of various canvas components, such as customer segments, value attributes, value propositions, and revenue streams.\n\n- **Gamification and Simulations:** To enhance participant engagement and learning (for instance in a workshop session), gamified activities can encourage creative thinking, problem-solving, and the exploration of alternative business models.\n\nSimulations could simulate market dynamics, allowing participants to test and refine their strategic choices in a risk-free environment.\n\n_Exploration of Alternative Value Creation Models:_ To challenge the traditional notions of value creation and capture, GenAI can help consider alternative models such as the circular economy, access-based models, subscription-based models, or outcome-based models.\n\n**Scenario Planning:** GenAI can assist in simulating different scenarios based on quantified data, helping organizations evaluate the financial viability and feasibility of implementing Blue Ocean ideas before implementation.",
    "#### Disruptions - How Would GenAI Rethink or Radically Disrupt Innovative Business Strategies Theories from Within?\n\n1.\n\n**Incorporate Exponential Technologies:** Embrace the transformative power of exponential technologies such as AI, blockchain, Internet of Things, and Augmented Reality.\n\nExplore how these technologies can enable entirely new value propositions, business models, and customer experiences.\n\nIntegrate technology-driven innovation as a core component of the strategy and business model design process.\n\n2.\n\n**Leverage Openness and Co-creation:** Embrace the principles of open innovation and co-creation by engaging external stakeholders, including customers, suppliers, and even competitors, in the strategy and business model design process.\n\nFoster collaboration, crowdsourcing, and open platforms to generate innovative ideas and drive co-creation of value with diverse stakeholders.\n\n3.\n\n**Embrace Ecosystem Thinking:** Rather than focusing solely on individual businesses and industries, expand the scope to embrace ecosystem-level thinking.\n\nConsider how different players, technologies, and business models can collaborate and co-create value within interconnected ecosystems.\n\nThis approach involves identifying new ways to collaborate, leverage partnerships, and create synergies across multiple industries and stakeholders.",
    "**Additional areas of disruptions include:**\n\n- **Shift Towards Purpose-Driven Strategies:** Consider the broader societal and environmental impacts of business decisions.\n\nMove beyond purely profit-driven strategies and focus on creating shared value for multiple stakeholders.\n\nEmbrace sustainability, social responsibility, and ethical considerations as integral components of strategy formulation and business model design.",
    "**Human Factor**\n\n- **Strategic Decision-Making:** While GenAI can provide data-driven insights and recommendations, it is up to humans to interpret the information, make judgments, and set the strategic direction for the organization.\n\nThe human role shifts towards synthesizing AI-generated insights, evaluating trade-offs, and making strategic choices based on a holistic understanding of the business context.\n\n- **Contextual Knowledge and Expertise:** When formulating new strategies, humans bring deep industry knowledge, experience, and contextual understanding that GenAI may lack.\n\nThey possess tacit knowledge acquired through years of working in specific domains.\n\nThis expertise enables humans to apply critical thinking and judgment when interpreting GenAI’s outputs, ensuring that AI-generated insights align with the organization’s unique challenges, goals, and values.\n\n- **Human-AI Collaboration and Communication:** Collaboration between humans and GenAI becomes crucial.\n\nHuman professionals need to effectively communicate their strategic intent, objectives, and priorities to GenAI, enabling it to generate relevant insights.\n\nSimilarly, humans must interpret and communicate AI-generated insights to stakeholders, fostering understanding, buy-in, and alignment within the organization.\n\n- **Ethical Standards in Strategy Formulation:** Humans must ensure that the data used is unbiased, representative, and ethically sourced.\n\nThey are responsible for setting ethical guidelines, reviewing AI-generated recommendations from an ethical standpoint, and making strategic decisions that align with the organization’s values and societal expectations.\n\n- **Continuous Learning and Adaptation:** As GenAI evolves and brings new capabilities, humans need to stay updated with advancements to ensure that they can leverage the AI’s potential effectively, integrate new insights into decision-making, and continuously improve their strategies and approaches.",
    "### Theory & Practice We Selected\n\nIn today’s VUCA (Volatile, Uncertain, Complex, Ambiguous) world, testing hypotheses and learning in an iterative way is increasingly important in managing growth projects.\n\nThe reference framework to help managers plan in a VUCA world is Discovery Driven Planning from Professor Rita McGrath and Ian MacMillan.\n\nA similar theory that goes in the same direction of setting and validating key assumptions as cheaply and as fast as possible is the “Lean Start-Up” popularized by Eric Ries and Steve Blank.\n\nBoth frameworks offer a lower-risk way to move a new initiative forward by testing assumptions and learning as much as possible as early as possible.\n\nIt helps planners think critically, challenge assumptions, and pivot when needed.\n\nThe approach consists of a set of critical questions managers need to answer when developing a new initiative under uncertainties: what is our definition of success?\n\nWhat assumptions need to be true to achieve the desired success?\n\nHow can the assumptions be validated?\n\nHow can we adapt the plan as we test and learn?\n\nOver the last two decades, these dynamic planning methods have become increasingly popular as a disciplined approach to dealing with growth projects and initiatives in hyper-dynamic market environments.",
    "### Our Hands-On Experience\n\nOver the years, we have introduced the principles and tools in several large corporations, both in B2B and B2C environments and in different geographical markets.\n\nWe also published our research and experimentation findings to help companies adopt lean thinking more effectively, especially in R&D.\n\nWhile the approach brings several benefits, such as early risk identification, iterative learning, efficient resource allocation, and market-responsive decision-making, there are common pitfalls that planners can fall into:\n\n- Poorly defined assumptions: One common pitfall is the lack of clarity and specificity in defining assumptions.\n\nIf assumptions are vague or ambiguous, it becomes difficult to validate and measure.\n\n- Unrealistic assumptions: Planners may sometimes make overly optimistic assumptions about the market, customer behavior, or technology.\n\n- Inadequate validation: Planners may rely on their intuition or limited data, without conducting thorough validation experiments or collecting sufficient feedback from customers or stakeholders.\n\n- Insufficient iteration and learning: Planners may be resistant to change or may not allocate enough time and resources for iterative testing and refinement.\n\nThis can hinder the ability to adapt and improve the project based on new insights.\n\n- Failure to pivot: Some planners may resist changing direction or may not recognize the need to pivot in a timely manner.\n\nThis can lead to wasted resources and missed opportunities for corrections.\n\nAddressing these challenges is crucial for successful implementation.\n\nBy leveraging generative AI enhancements, planners can overcome these pitfalls and enhance the effectiveness of the methodology.",
    "## Enhancements - What are the most valuable enhancements that GenAI can bring to the application of Discovery Driven Planning?\n\n1.\n\n**Generate Assumptions**: AI algorithms can autonomously generate a diverse range of assumptions based on vast amounts of data and insights.\n\nPlanners can then collaborate with AI systems like ChatGPT to refine and iterate on these assumptions, combining human expertise and AI-generated insights.\n\nIn addition, GenAI’s can help uncover assumptions that might have been overlooked.\n\n2.\n\n**Design Experiments for Validation**: GenAI can provide recommendations for efficient experimental designs that maximize learning and minimize costs.\n\nFor example, by identifying the most critical assumptions to test first, and the right sequence (also based on sensitivity analysis and scenario simulation).\n\n3.\n\n**Simulate User Feedback for Validation**: Traditional validation methods such as interviews and market research can be time-consuming.\n\nGenAI can simulate feedback from potential users/customers based on the assumptions (planners can describe their target audience and context, and GenAI can generate responses as if it were the user or customer).\n\nIn addition, AI can help analyze large-scale data sets, customer feedback, online discussions, reviews, and other relevant sources.\n\nAdditional enhancements include:\n\n- **Connect with Experts**: Planners can describe the specific expertise they are seeking, and GenAI can suggest potential experts to engage with or connect planners with relevant communities or forums.\n\nPlanners can engage with these experts, seeking their perspectives for validating the assumptions.\n\n- **Challenge the Assumptions**: AI systems can act as critical “challenger”, questioning human-generated assumptions.\n\nGenAI can provide iterative feedback and challenge the assumptions developed by the planner, stimulating critical thinking or questions that help planners think deeper about the assumptions.\n\nIt can also simulate different perspectives by suggesting alternative viewpoints.\n\n- **Develop Metrics**: GenAI can help planners establish measurable milestones and targets to validate assumptions at specific stages, enabling more structured and actionable validation.\n\n- **Trigger Pivots**: By engaging in discussions and simulations, GenAI can highlight discrepancies between assumptions and evidence, suggest alternative approaches, or identify potential gaps or limitations.",
    "## Disruptions - How would GenAI rethink or radically disrupt Discovery Driven Planning from within?\n\n1.\n\n**AI as Project Planner**: Instead of human project leaders, an AI system takes the lead in driving the discovery process.\n\nIt generates and tests assumptions autonomously, making decisions based on data analysis and machine learning algorithms.\n\n2.\n\n**Quantum Computing for Assumption Generation**: Leveraging the power of quantum computing, assumptions are generated by exploring all possible scenarios simultaneously, allowing for a more comprehensive and nuanced understanding of potential outcomes.\n\n3.\n\n**Augmented Reality Assumption Testing**: Assumptions are tested in immersive augmented reality environments, where planners can visually experience and interact with simulated scenarios.\n\nThis experiential approach enhances the validation process and encourages creative problem-solving.\n\nAdditional areas of disruption include:\n\n- **Multi-Agent Assumption Validation**: GenAI can create virtual ecosystems where assumptions are tested through dynamic interactions between virtual entities.\n\nThis enables planners to understand the complex interdependencies that impact the success of assumptions.\n\n- **Collaborative Assumption Setting**: AI systems can facilitate collaborative brainstorming sessions by generating and sharing ideas in real-time.\n\nPlanners can leverage AI’s ability to generate a vast number of ideas, sparking collective creativity and fostering a collaborative ideation process.\n\n- **AI-Facilitated Assumption Gamification**: AI-powered gamification techniques can turn the assumption validation process into an engaging and interactive experience.\n\nPlanners can compete or collaborate in virtual games that simulate real-world scenarios, fostering creativity and enhancing the motivation to validate assumptions.",
    "## Human Factor\n\n- **Creativity (in Prompting)**: With AI systems providing insights and suggesting potential assumptions, humans must think creatively on which deep questions and creative prompting can leverage the full potential of ChatGPT.\n\n- **Critical Thinking / Ability to Challenge**: Humans must cultivate critical thinking skills, interpret results, assess the capabilities, accuracy, and reliability of AI models, understand their limitations, and make informed choices.\n\n- **Integration of AI-Generated Insights**: Human planners will need to synthesize AI-generated insights with their own expertise and industry knowledge to make informed decisions.\n\nThis requires skills in data interpretation, pattern recognition, and critical analysis to extract meaningful insights from AI-generated data and integrate them into the planning process.\n\n- **Iterative Adaptation and Pivoting**: Human planners will continue to be responsible for guiding the iterative adaptation and pivoting process.\n\nThey will need skills in monitoring and analyzing feedback, identifying opportunities for course correction, and making strategic decisions about when and how to pivot based on a combination of AI-generated insights, user feedback, and market dynamics.\n\n- **Ethical Considerations in AI Use**: Humans must understand the ethical implications of using GenAI and ensure responsible and unbiased decision-making.\n\nSkills in AI ethics, governance frameworks, and data privacy become essential.\n\nFor example, by judging safe and secure methods/data for testing the assumptions.",
    "## Theory & Practice We Selected\n\nThe main reference is the seminal book “Platform Revolution” by Geoffrey G. Parker, Marshall W. Van Alstyne, and Sangeet Paul Choudary; this theory has provided a comprehensive overview of the platform business model, its key components, strategies, and implications.\n\nIt has proven to be a valuable resource for understanding the transformative power of platforms in today’s digital economy.\n\nAnother key contributor is Michael Jacobides; his work has provided valuable insights into the role and dynamics of ecosystems in shaping competitive advantage and innovation.\n\nHis research emphasizes the need for firms to embrace ecosystem strategies, become effective orchestrators, and navigate the complexities of ecosystem governance to thrive in today’s interconnected business landscape.",
    "### Our Hands-On Experience\n\nWe applied the above theories and thinking in multiple contexts, drawing inspiration also for our publications and speeches/talks.\n\nOur original thinking was featured in leading magazines and various books.\n\nSee for instance How to Embrace the Power of Platforms, or Ecosystems: the How Factor.\n\nWe have observed in several projects how the concept and thinking captured in “Platform Revolution” and in Professor Michael G. Jacobides’s publications help business leaders in understanding and strategizing the potential of such innovative business models.\n\nAcademic research has clearly highlighted how network effects operate, how they harness the virtuous cycle of growth where more users attract more participants.\n\nHowever, in many cases, the application of platforms & business ecosystems theories has turned out to be challenging in several aspects:\n\n- **Unfamiliarity with Platform Logic**: Most managers are unfamiliar with platform concepts.\n\nUnderstanding the nuances of network effects, value sharing, and orchestration is crucial to leverage these theories successfully.\n\n- **Naïve Approach**: Some organizations approach network effects, value sharing, and orchestration in a simplistic or inexperienced manner.\n\nFailing to be scientific and rigorous can lead to ineffective implementation and missed opportunities for value creation.\n\n- **Organizational Inertia**: Overcoming factors such as organizational inertia, legacy systems, and resistance to change is essential for successful implementation in large organizations.",
    "## Enhancements - What are the most valuable enhancements that GenAI can bring to the application of Platforms & Business Ecosystems theories?\n\n1.\n\n**Data-Driven Insights on Market Frictions**: GenAI can analyze vast amounts of data to provide actionable insights on ecosystem dynamics, user behavior, and emerging trends.\n\nGenAI can help understand the frictions, the current mismatches between the various platform sides, the needs for curation that today do not allow to release full value.\n\n2.\n\n**Enhanced Curation and Recommendations**: GenAI can leverage user data to offer personalized recommendations and experiences within platform ecosystems.\n\nA stronger user interface (Q&A-based) can replace or complement search engines in most platforms.\n\nBy analyzing user behavior and preferences, GenAI can help businesses tailor their offerings, enhance customer satisfaction, and drive engagement.\n\n3.\n\n**Collaboration Facilitation**: GenAI can provide tools and recommendations to foster collaboration among ecosystem participants.\n\nBy analyzing data on capabilities, GenAI can help identify potential partners, assess compatibility and complementarities, and facilitate resource sharing.\n\nAdditional enhancements include:\n\n- **Ecosystem Mapping and Visualization**: GenAI can assist in visualizing and mapping ecosystems by analyzing data on stakeholders, relationships, and dependencies.\n\nThis can help businesses gain a holistic view of their ecosystem and understand the dynamics and interdependencies among participants.\n\n- **Quantitative Analysis and Simulation**: GenAI can assist in quantitative analysis and simulation of platform dynamics, in terms of network effects, side liquidity and matching between sides, complementarities, value sharing mechanisms, and incentives.\n\n- **Risk Assessment and Mitigation**: By analyzing data on security, privacy, regulatory compliance, and market dynamics, GenAI can help businesses proactively address potential risks and ensure the long-term sustainability of their ecosystems.",
    "## Disruptions - How would GenAI rethink or radically disrupt Platforms & Business Ecosystems theories from within?\n\n1.\n\n**Embrace Decentralization and Blockchain Technologies**: Traditional platform models often rely on centralized control and intermediaries.\n\nBy embracing decentralized technologies such as blockchain, it is possible to create self-governing ecosystems where participants have more autonomy and control over their data, transactions, and governance.\n\nThis shift can foster trust, reduce reliance on intermediaries, and enable more equitable value distribution.\n\n2.\n\n**AI and Machine Learning for Platform (Autonomous) Decision-Making**: By leveraging AI and machine learning algorithms, platforms can automate certain aspects of decision-making, such as matching supply and demand, optimizing pricing, or recommending personalized offerings.\n\nThis can enhance efficiency, scalability, and user experience within the ecosystem.\n\n3.\n\n**Foster User Empowerment and Data Sovereignty**: Shift the power dynamics in favor of users by enabling them to have greater control over their data and privacy.\n\nProvide users with transparent access to their data, allowing them to manage permissions and decide how their data is utilized within the ecosystem.\n\nEmpowering users can enhance trust and engagement within the ecosystem.\n\nAdditional areas of disruptions include:\n\n- **Embed “By-Design” Sustainability and Social Impact**: Encourage responsible and ethical practices, promote environmental sustainability, and support social causes within the ecosystem.\n\nConsider how the platform can contribute to the well-being of society and address broader societal challenges.\n\n- **Embrace Circular Economy Principles**: Encourage resource optimization, reduce waste, and promote sustainable practices across the value chain.\n\nConsider how the platform can facilitate the exchange, reuse, and repurposing of resources, products, and services within the ecosystem.\n\n- **Dynamically Upgrade Platforms with Latest Technologies**: Continuously scan and experiment with emerging technologies and trends that can disrupt the traditional platform and ecosystem models.\n\nThis may include exploring technologies like the Internet of Things (IoT), augmented reality (AR), virtual reality (VR), or edge computing.",
    "## Human Factor\n\n- **Sensemaking and Context Interpretation**: Humans play a crucial role in sensemaking by interpreting the insights and recommendations provided by GenAI within the specific context of their organization and ecosystem.\n\nA strong domain expertise (on platforms and ecosystems) is needed to interpret the insights and recommendations from GenAI.\n\n- **Adaptability and Learning Agility**: In the rapidly evolving landscape of platform and ecosystem strategies, humans need to be adaptable and quick learners.\n\nThey must stay updated on emerging trends, technologies, and market dynamics.\n\nThis enables them to understand the implications of GenAI’s insights and adapt strategies accordingly.\n\nThey need to stay informed also about advancements in AI technologies, understand GenAI’s capabilities and limitations, and develop the necessary skills to effectively collaborate with AI systems.\n\n- **Communication, Collaborative and Relationship-Building Skills**: Humans need to foster relationships, build trust, and engage with ecosystem participants to co-create value.\n\nThey must leverage their interpersonal and relationship-building skills to forge partnerships and foster a collaborative environment.\n\nThey need to effectively communicate the insights, recommendations, and strategic decisions to various stakeholders within the organization and the ecosystem.\n\nThey must engage and manage stakeholders’ expectations, address concerns, and foster alignment.\n\nThese skills will be even more critical as they complement the capabilities brought by AI.\n\n- **Ethical Decision-Making**: Humans need to assess the ethical implications of the recommendations provided by GenAI, consider potential biases, and make decisions aligned with ethical standards and social values.\n\nThink about how much privacy, sovereignty, sustainability, circular economy, etc.\n\nare related to the current and future ecosystems.\n\n- **Creativity and Innovation**: Humans are essential for driving creativity and innovation within platform and ecosystem strategies.\n\nThey can leverage the insights generated by GenAI to think creatively, identify new opportunities, and design novel approaches and directions for platform design and evolutionary paths.",
    "## Theory & Practice We Selected\n\nThe main reference is the theory of cooperative advantage, as proposed by Leon C. Prieto and Simone T.A.\n\nPhipps in their work “Cooperative Advantage: Rethinking the Company’s Purpose”.\n\nAccording to the authors, companies can create more value and achieve a sustainable competitive advantage by working collaboratively with stakeholders, rather than simply competing against rivals.\n\nThere are several well-established practices to working collaboratively with stakeholders: co-creation workshops; jamming sessions; stakeholder consultation meetings/summits; community engagement projects; participatory design/user-centered design.\n\nOver the last decade, an increasing number of companies have understood the benefits of multi-stakeholder co-creation and embraced it as a common practice.\n\nAmong the pioneers, it is worth mentioning for example:\n\n- **Procter & Gamble’s Connect + Develop program**: P&G collaborates with innovators outside the company to identify promising ideas that can be developed and brought to market.\n\n- **Philips’ Co-Creation Labs**: Philips brings together consumers, health professionals, and other stakeholders to collaboratively develop solutions to healthcare challenges.\n\n- **BASF Creator SpaceTM**: A global co-creation program initiated on BASF's 150th anniversary to harness the collective knowledge, creativity, and expertise of various stakeholders – including employees, customers, academics, and members of the public sector.",
    "### Our Hands-On Experience\n\nWe have applied the above multi-stakeholder collaboration and co-creation methodologies in various contexts.\n\nBack in 2013, we directly supported the design and implementation of BASF Creator SpaceTM, a pioneering multi-year initiative, which was then published in an HBS Case Study.\n\nSuch theories and related practices can be extremely beneficial in terms of cultural change, relationship building with key stakeholders, innovation marketing, and – in some cases – also result in successful product and service innovations.\n\nHowever, their application is complex and resource-intensive, especially in the preparation phase.\n\nBased on our direct experience, main pitfalls include:\n\n- **Lack of Alignment on the Problem Being Addressed**: Organizations may struggle to establish a clear understanding and agreement on the specific problem that the collaboration activity aims to solve.\n\n- **Selection and Engagement of Stakeholders**: Inadequate selection and engagement of stakeholders, characterized by under-representation, biases, or power dynamics, can lead to imbalanced decision-making processes and skewed outcomes.\n\n- **Insufficient Resources/Preparation**: Inadequate allocation of resources, both financial and human, can hinder the success of collaborative initiatives.\n\n- **Lack of Proper Follow-Up**: Failure to properly follow up on collaboration activities can result in a lack of accountability and progress monitoring.",
    "## Enhancements - What are the most valuable enhancements that GenAI can bring to the application of multi-stakeholder collaboration practices?\n\n1.\n\n**Improved Problem Framing**: GenAI can perform extensive data mining and analysis on existing internal and external data sources to identify relevant trends, customer insights, market dynamics, and emerging opportunities that can help improve the challenge statement definition.\n\n2.\n\n**Advanced Modeling and Simulation**: GenAI can be used to create complex models and simulations of different scenarios.\n\nThis can help stakeholders explore various possibilities, understand potential outcomes, and make more informed choices during the co-creation process.\n\n3.\n\n**Improved Insight & Opportunity Generation**: GenAI can analyze large amounts of data to detect patterns, trends, and correlations that may not be easily apparent to humans.\n\nThis can help stakeholders gain deeper insights, make better decisions, and co-create more effectively.\n\nAlso, GenAI can propose novel concepts, new product features, or innovative solutions based on the identified patterns and insights.\n\nAdditional enhancements include:\n\n- **Participant Selection and Matching**: GenAI can assist in the selection and matching of participants for the co-creation process.\n\nBy analyzing participant profiles, expertise, and past contributions, GenAI can recommend suitable stakeholders who bring diverse perspectives, relevant skills, and complementary knowledge to the table.\n\n- **Knowledge Sharing and Content Curation**: GenAI can curate and disseminate relevant knowledge, best practices, case studies, and research findings to participants before the co-creation process.\n\nThis ensures that participants have a shared baseline understanding of the subject matter, accelerates the onboarding process, and promotes a common knowledge foundation.\n\n- **Scheduling and Logistics Optimization**: GenAI can optimize the scheduling of the co-creation process, considering participants’ availability, time zones, and preferences.\n\nIt can also assist in coordinating logistics, such as venue selection, travel arrangements, and resource allocation, streamlining the planning phase and ensuring smooth execution.\n\n- **Preparation and Pre-Work Assignments**: GenAI can provide pre-work assignments or activities to participants to prepare them for the co-creation process.\n\nThis may include pre-reading materials, thought-provoking questions, or small individual or group tasks aimed at priming participants’ thinking and encouraging them to come prepared with initial ideas or insights.\n\nDuring the co-creation activity:\n\n- **Enhanced Communication and Understanding**: GenAI can use Natural Language Processing to understand, generate, and translate language (e.g., voice-to-text transcription, synthesis, semantic analysis, and connection mapping).\n\nThis can improve communication among diverse stakeholders, facilitate better understanding, and ensure that everyone’s voices are heard and understood.\n\n- **Personalization**: GenAI can help personalize the co-creation process for each stakeholder, considering their specific interests, perspectives, and needs.\n\nThis can enhance stakeholder engagement and satisfaction.\n\n- **Predictive Capabilities**: GenAI can predict potential obstacles or challenges that may arise during the co-creation process (e.g., IP topics), allowing stakeholders to proactively address these issues.\n\nAfter the co-creation activity:\n\n- **Consolidation and Analysis of Workshop Output**: GenAI can assist in consolidating the insights, ideas, and outcomes generated and provide a comprehensive summary of the discussions.\n\n- **Knowledge Management and Documentation**: GenAI can support the documentation and management of the co-creation process outcomes.\n\nIt can create structured repositories or knowledge bases that capture the generated ideas, concepts, prototypes, and artifacts.\n\n- **Idea Evaluation and Prioritization**: GenAI can employ algorithms and models to evaluate and prioritize the generated ideas based on predefined criteria or decision frameworks.\n\n- **Action Planning and Roadmap Development**: GenAI can help in developing action plans and roadmaps.\n\nIt can analyze dependencies, resource requirements, and potential risks to facilitate the creation of a realistic and comprehensive plan for implementing the co-created solutions.\n\n- **Monitoring and Progress Tracking**: GenAI can contribute to ongoing monitoring and progress tracking of the co-creation outcomes.\n\nBy analyzing relevant data points, metrics, or feedback, GenAI can provide insights into the implementation progress, identify potential bottlenecks or areas for improvement, and support continuous optimization of the co-created solutions.",
    "## Disruptions - How would GenAI rethink or radically disrupt multi-stakeholder collaboration practices from within?\n\n1.\n\n**Decentralized Collaboration**: Using blockchain technology could allow for more democratic and secure co-creation processes.\n\nEach participant would have a transparent record of every change and contribution made during the co-creation process, ensuring accountability, trust, and fair attribution of ideas.\n\n2.\n\n**Augmented Collaboration (VR/AR)**: Virtual and augmented reality can help create immersive co-creation environments that go beyond traditional meeting or workshop settings.\n\nWith VR/AR, stakeholders can interact in a fully immersive digital environment, manipulating ideas and designs in real time, regardless of their physical location.\n\n3.\n\n**Real-Time Sentiment Analysis and Adjustment**: Employ GenAI to continuously analyze the sentiment and engagement levels of participants during co-creation sessions and along the overall co-creation process.\n\nAlso, biosensors technology can monitor physiological responses (like heart rate, brainwaves, etc.)\n\nand thus provide real-time feedback during collaboration sessions.\n\nThis data could be used to adjust the process, ensuring optimal engagement and effectiveness.\n\nAdditional areas of disruptions include:\n\n- **Brain-Computer Interfaces (BCI)**: Though it may sound like science fiction, BCIs could potentially allow for direct communication between the brain and computer systems.\n\n```markdown\nHuman-Machine Integration and AGI\n\nCommunication between the human brain and digital systems, which could radically disrupt the way we collaborate and co-create.\n\nIn the future, this could allow for “thought sharing” and immediate translation of ideas into digital formats.\n\nArtificial General Intelligence (AGI) Co-Creator: While this is more speculative and future-facing, once AGI (AI with human-like understanding and capabilities) is realized, it could serve as another participant in the co-creation process, contributing its own insights and ideas, and even challenging human participants to think differently.",
    "## Advanced Digital Fluency & Data Literacy\n\nAs collaboration technologies evolve, so too will the need for digital fluency.\n\nUnderstanding VR/AR environments, blockchain technology, and brain-computer interfaces will become vital.\n\nThis includes not only how to use these technologies but also how to leverage them effectively for collaboration and creativity.\n\nWith more sophisticated technologies, an increased amount of data will be available - from sentiment analysis to neurofeedback.\n\nThe ability to interpret and utilize this data will become a critical skill.\n\nNeuro-Technological Understanding: If brain-computer interfaces become a reality, understanding how the human brain functions and how it can interact with technology will be crucial.\n\nEthical Considerations and Decision-Making: More advanced technology brings more complex ethical dilemmas.\n\nSkills in ethical decision-making, understanding of privacy implications, and managing the potential misuse of technology (like deepfakes in VR or security issues in blockchain) will become increasingly important.\n\nPsychological Safety and Empathy: As collaboration becomes more technologically advanced and potentially anonymous (in the case of blockchain), creating an environment of psychological safety and demonstrating empathy will be vital.\n\nThis ensures participants feel comfortable expressing their ideas and thoughts, despite the impersonal nature of some technologies.\n\nHuman-AI Interaction: As AGI may become a reality, humans will need to learn how to interact with AI not just as a tool, but as a co-creator.\n\nFor instance, if an AGI is participating as a co-creator in a workshop, it could propose design ideas based on its vast knowledge base, predict the market response to different designs, or identify potential sustainability issues based on the latest research.\n\nImportantly, it would be capable of adapting its responses based on the conversation flow, perhaps even challenging human ideas, or proposing novel solutions.\n\nDesign and Visualization: With VR/AR, the ability to design and visualize in three dimensions becomes crucial.\n\nFor example, if a multi-stakeholder group is co-designing a new product, they could create a 3D model of that product in the virtual environment.\n\nStakeholders could then “walk around” the product, viewing it from different angles, even interacting with it as if it were a physical object.\n\nChanges to the design can be made in real-time, with everyone witnessing and participating in the alterations.\n\nFor example, if a group of stakeholders is co-designing a new product, they could create a 3D model of that product in the virtual environment.\n\nStakeholders could then “walk around” the product, interacting with it as if it were a physical object.",
    "## Theory & Practice We Selected\n\nOpen innovation theory, coined by Henry Chesbrough, suggests that organizations can enhance their innovation capabilities by leveraging external ideas and resources.\n\nIt emphasizes the importance of collaboration.\n\nOriginating around 20 years ago, with a HBS book and seminal HBR article, open innovation has gained widespread adoption across industries.\n\nOver the years HBR has published more than 50 contributions (digital articles, podcasts, …) on Open Innovation, that demonstrates the solidity and continued timeliness of the theory until today.\n\nOpen innovation offers several benefits, including access to a broader pool of ideas, increased speed of innovation, reduced costs, and risk mitigation through external collaborations.\n\nIt enables organizations to tap into diverse perspectives, expertise, and resources, fostering a culture of innovation and growth.\n\nFrom an organizational perspective, it has also led to the emergence of various open innovation archetypes, such as corporate innovation outposts, corporate garage, venture builders, corporate accelerators, innovation labs, innovation catalysts, venture client.",
    "## Our Hands-On Experience\n\nOver the last ten years we have supported several companies in their open innovation journey, considering the tools and practices (like crowdsourcing) as well as the organizational and cultural implications for large corporations.\n\nIn particular, we looked into open innovation archetypes and how to help leaders design innovation units and outposts.\n\nMore recently we explored the broadening scope of Open Innovation towards Sustainability (as in the case of Enel’s Innovability) and beyond the traditional R&D domain.\n\nWe have observed the following common pitfalls when large companies embrace open innovation:\n\n- Strategic inconsistency, where innovations may not align with company goals or lack a long-term perspective.\n\n- Organizational siloes result in disconnected open innovation labs, leading to misaligned initiatives and lack of support for scaling.\n\n- Intellectual property and data privacy issues may arise due to sharing ideas externally.\n\n- Lack of alignment of central processes such as procurement, finance, legal, IT, and HR.\n\n- Risk aversion and fear of failure hinder innovative thinking.\n\n- Cultural resistance as employees perceive external sourcing as devaluing their contributions or threatening job security.",
    "## Enhancements - What are the Most Valuable Enhancements that GenAI Can Bring to the Application of Open Innovation Theories and Practices?\n\n1.\n\nPartners Discovery and Selection: GenAI can analyze potential partners based on various criteria, such as their innovation track record, compatibility with the company’s goals, and financial stability, among others, facilitating the process of partner discovery and selection.\n\n2.\n\nAI-driven Open Innovation Strategy: GenAI can assess and propose open innovation initiatives based on the company’s strategic goals, identifying and flagging those that may not be aligned.\n\nIt can help ensure strategic alignment, mitigating the risk of wasted resources or conflicts/duplications/silos.\n\nGenAI can identify and recommend which business units might be most open to.\n\n3.\n\nAI-Based Innovation Challenges: Organize innovation challenges or hackathons where AI algorithms are used to facilitate idea generation, evaluation, and collaboration.\n\nAI can provide real-time feedback, suggest improvements, and facilitate the selection of winning ideas.\n\nGenerative AI can analyze potential partners based on different criteria, thus facilitating the discovery and selection process.",
    "## Additional Enhancements Include:\n\n- Patent Screening, Evaluation, and Matching: GenAI can augment the process of identifying, screening, evaluating, and selecting patents or startups for collaboration.\n\nIt can scan databases using specific criteria, analyze and score potential matches, evaluate their depth by predicting their impact or success, and provide data-driven recommendations for selection.\n\n- Streamlining of Central Processes: GenAI can automate and streamline central processes, making them more adaptable and responsive to the needs of partners (startups, etc.\n\n), reducing the burden and improving efficiency.\n\n- AI-powered Needs Exploration: GenAI can rapidly analyze large volumes of data, uncovering hidden trends, patterns, and insights that can drive innovative solutions.\n\nGenAI can also help analyze social media data, customer reviews, and other online data to gain a deeper understanding of customer needs and preferences.\n\nThis could lead to the discovery of new innovative opportunities.\n\n- Enhanced Intellectual Property (IP) Management: GenAI can assist in intellectual property management by automatically analyzing patent databases, identifying potential infringements, and helping organizations navigate complex patent landscapes to protect their innovations.",
    "## Disruptions - How Would GenAI Rethink or Radically Disrupt Open Innovation Theories and Practices from Within?\n\n1.\n\nOpen Innovation AI Marketplace: Establish an AI-driven marketplace where organizations can share their innovation challenges and receive AI-curated solutions from a global network of innovators.\n\nThe marketplace would use GenAI to match the most relevant ideas with specific challenges, providing a diverse range of creative solutions.\n\n2.\n\nIntelligent Mapping of Innovation Ecosystem: AI could map the entire innovation ecosystem, including startups, research institutions, and potential partners.\n\nIt could analyze their expertise, capabilities, and compatibility, helping organizations navigate the complex landscape and identify optimal collaboration opportunities.\n\n3.\n\nAI-Enhanced Serendipity Engine: Create an AI algorithm that generates unexpected connections and serendipitous ideas by analyzing vast amounts of diverse data.\n\nThe algorithm would identify unusual patterns, combinations, and associations, leading to groundbreaking innovations by connecting seemingly unrelated concepts.\n\nThe algorithm can identify unusual patterns, combinations, and associations, leading to disrupting innovations by connecting seemingly unrelated concepts.",
    "## Additional Areas of Disruption Include:\n\n- Patent Mining for Innovation: Develop an AI system that autonomously scans and analyzes massive patent databases, identifying hidden patterns, cross-industry applications, and potential opportunities for collaboration or acquisition.\n\nThis system could uncover novel ways of leveraging existing intellectual property for innovative purposes.\n\n- Open Innovation with AI Entities: GenAI could create virtual entities or avatars (“synthetic innovators” with unique characteristics, skills, and perspectives) capable of participating in the open innovation process.\n\nThese AI entities could generate ideas, provide feedback, and even collaborate with human innovators, bringing a unique blend of AI-generated insights and human creativity.",
    "## Strategic Thinking and Decision-Making\n\nHumans with strong strategic thinking skills play a vital role in assessing AI-generated insights and making informed decisions regarding innovation strategies, partnership selection, and resource allocation over the portfolio of OI initiatives.\n\nSystems Thinking: Open innovation involves complex systems with interconnected elements, and individuals with systems thinking skills can understand the interdependencies, identify synergies, and navigate the collaborative and dynamic nature of the innovation ecosystem.\n\nInquisitiveness and Curiosity: Curiosity drives exploration and discovery, allowing individuals to uncover novel insights, challenge assumptions, and seek out innovative opportunities that may arise from open collaboration and diverse perspectives.\n\nDomain Expertise: Deep domain expertise allows humans to interpret and validate AI-generated insights, bringing industry-specific knowledge to evaluate the relevance and applicability of AI-generated suggestions in the open innovation process, for example providing valuable insights and guidance to startups, helping them refine their ideas and solutions.",
    "## Human Relationship with External and Internal Partners\n\nWhile several Open Innovation tasks can be taken by GenAI, some elements are human specific: building and nurturing relationships with external stakeholders, such as partners, startups, and research institutions, requires strong interpersonal skills (communication, negotiation, …) and the ability to foster trust and collaboration.\n\nOpen Innovation Ethics and Compliance: Upholding ethical standards, ensuring data privacy, and complying with intellectual property regulations are fundamental in open innovation.\n\nHumans play a key role in ensuring ethical practices and responsible innovation while protecting stakeholders’ interests.",
    "### Theory & Practice We Selected\n\nIn the last decade, extensive literature and academic research confirmed the critical role that leaders play in fostering and sustaining innovation within organizations.\n\nIn his award-winning Harvard Business Review article on the 5 paradoxes of innovative cultures, Professor Gary Pisano emphasizes the crucial role that leaders play in shaping and sustaining innovative cultures, while managing these paradoxes carefully:\n\n- Paradox #1 – Tolerance for Failure vs.\n\nAccountability: Encouraging and supporting experimentation while maintaining clear expectations for individual/team performance and holding individuals accountable.\n\n- Paradox #2 – Willingness to Experiment vs. Clear Strategy and Discipline: Promoting and supporting experimentation within the boundaries of the strategic objectives, ensuring that experiments are purposeful, aligned with the organization’s goals, and executed with discipline.\n\n- Paradox #3 – Psychological Safety (as theorized by Professor Amy Edmondson) vs.\n\nCandor: Creating an environment where individuals feel psychologically safe to share ideas and take risks, while also promoting open and honest communication, encouraging constructive feedback.\n\n- Paradox #4 – Collaboration vs.\n\nIndividual Accountability: Creating structures and processes that encourage collaboration and teamwork, promoting a culture of shared goals and collective problem-solving, while also establishing clear expectations for individual accountability, ownership, and delivering on commitments.\n\n- Paradox #5 – Flat Hierarchy vs. Strong Leadership: Fostering a culture of empowerment and distributed decision-making, encouraging individuals at all levels to contribute their ideas and perspectives, while also providing strong leadership that sets the vision, direction, and guidance.\n\nCompared to traditional approaches (mainly focused on rhetoric about abstract values), Pisano advocates a more concrete approach, where leaders need to embody and demonstrate a set of desired and observable behaviors themselves.\n\nAs a measurement tool, Pisano designed a quantitative survey to evaluate concrete behaviors related to collaboration, psychological safety, empowerment, attitude toward failure, and experimentation.\n\nThis data helps identify gaps, improvement areas, and weak spots within the organization.",
    "### Our Hands-On Experience\n\nUnder the supervision and guidance of Professor Pisano, we developed an innovative practice to help leaders lay the foundation for a successful cultural transformation.\n\nOur approach follows three main steps: (1) measure concrete behaviors rather than abstract values (tool: Pisano’s survey), (2) select the desired behaviors (tool: deck of 40 cards, with a mix of positive and negative behaviors, in line with Pisano’s 5 paradoxes), and (3) live the new, desired behaviors in practice (tool: Circles of Trust).\n\nThe output is the Leadership Social Contract—a set of actionable behaviors that leaders must commit to either stop doing, start doing or keep doing.\n\nWe applied our approach to thousands of executives in multiple contexts, mainly in Europe and US.\n\nOverall, it has helped in creating alignment and commitment to real change.\n\nHowever, in many cases the application has turned out to be challenging because of:\n\n- Failure to embed new behaviors: leaders may struggle to effectively live new behaviors in their daily tasks and decisions.\n\n- Measurement of behaviors: difficulty in measuring behaviors against a baseline to track progress quantitatively.\n\n- Difficult translation of behaviors into concrete actions: translating abstract values concepts into practical and actionable steps can pose challenges.\n\n- Unclear correlation between cultural change and business impact: the lack of measurable link between behavioral shift and tangible business results can make it challenging to justify the ROI of the initiative and sustain support.",
    "## Enhancements - What are the Most Valuable Enhancements that GenAI Can Bring to our Leadership Behaviors Approach?\n\n1.\n\nImproved Communication on Behaviors: GenAI can review and guide communications to ensure they align with desired behaviors and the organization’s new social contract.\n\nIt can also provide a consistent messaging across all communication channels to avoid mixed signals.\n\n2.\n\nAutomated Administration and Reporting: GenAI can automate various administrative tasks, such as conducting surveys, collating, and analyzing data, and generating reports.\n\nThis reduces the resource burden and allows for more efficient and effective behavior change initiatives.\n\n3.\n\nBehavioral Measurement & Monitoring: GenAI can use a combination of Natural Language Processing, sentiment analysis, and other machine learning techniques to objectively and quantitatively measure behaviors.\n\nThis could involve analyzing communication and actions within the organization, tracking progress over time, and providing real-time feedback to individuals.\n\nGenerative AI can use a combination of natural language processing, sentiment analysis, and other machine learning techniques to measure behaviors objectively.",
    "## Additional Enhancements Include:\n\n- Personalized Coaching and Guidance: GenAI could provide personalized suggestions for concrete actions individuals can take to align with desired behaviors.\n\nThis could be based on an individual’s role, context, and previous behavior.\n\nGenAI could also provide real-time coaching to leaders, helping them better embody and promote desired behaviors.\n\n- In-depth Understanding of Organizational Context: By analyzing a wide range of data, GenAI can provide valuable insights into the organizational context and how it may influence behavior change efforts.\n\nThis understanding can help leaders tailor their approaches to their specific organizational context, increasing the chances of success.\n\n- Interactive & Engaging Workshops: GenAI could facilitate more interactive and engaging workshops.\n\nFor example, it could create virtual scenarios or simulations where the participants can see the impact of their chosen behaviors in real-time.\n\nThis can provide immediate feedback and allow them to refine their choices.\n\nOr could provide collaborative tools and platforms that enable participants to contribute ideas, collaborate on documents or visualizations, and provide feedback in real-time.\n\n- Social Network Analysis: GenAI could analyze social network data within the organization to identify influential individuals or groups who can drive behavior change.\n\nIt could provide recommendations on how to leverage these influential nodes to cascade desired behaviors throughout the organization more effectively.\n\n- Continuous Behavioral Reinforcement: GenAI can provide ongoing reinforcement of new behaviors, sending reminders or prompts when needed.\n\nIt can also provide continuous feedback to individuals based on their progress, further promoting the adoption of new behaviors.",
    "## Disruptions - How Would GenAI Rethink or Radically Disrupt our Leadership Behaviors Approach from Within?\n\n1.\n\nReal-time Feedback and Coaching: GenAI could provide instant, personalized feedback and coaching to individuals and teams, leveraging its ability to process vast amounts of data and provide immediate insights.\n\nThis would enable individuals to adjust and course corrections in real-time, enhancing their behavior change efforts.\n\n2.\n\nVirtual Simulation and Experiential Learning: GenAI could create virtual simulations or immersive experiences where individuals can practice and refine the desired behaviors in a safe and controlled environment.\n\nThis would enable experiential learning, allowing individuals to build their skills and confidence in embodying the behaviors.\n\n3.\n\nAI-Driven Behavioral Analysis: GenAI could utilize advanced algorithms and machine learning techniques to analyze behaviors at a much deeper level.\n\nIt could identify patterns, correlations, and insights that humans may miss, enabling a more comprehensive understanding of behaviors and their impact on organizational culture.\n\nGenerative AI could identify patterns, correlations, and insights that humans may not grasp, enabling a more comprehensive understanding of behaviors and their impact on organizational culture.",
    "## Additional Areas of Disruption Include:\n\n- Predictive Behavior Modeling: GenAI could use advanced analytics to predict potential outcomes of behavior change initiatives.\n\nThis includes forecasting potential unintended consequences, as well as correlating changes in behavior with key business outcomes.\n\n- Ethical AI Guidance: GenAI could provide ethical guidelines and considerations to ensure that behavior change initiatives are aligned with ethical standards and avoid potential biases or unintended consequences.\n\nIt could continuously monitor the impact of behavior changes on diverse groups within the organization to promote fairness and inclusion.",
    "## Advanced Data Literacy\n\nHumans need to develop a strong understanding of data analysis, interpretation, and visualization to inform their behaviors.\n\nThey should be able to derive actionable insights from the data generated by GenAI, effectively leverage data to drive behavior changes, and communicate the implications of data to key stakeholders.\n\nChange Management & Enablement: Humans play a crucial leadership role in driving behavior change initiatives with the support of GenAI.\n\nThey need to provide vision, set strategic direction, inspire others, and navigate the organizational change process.\n\nLeadership skills become essential in creating an environment that embraces GenAI and fosters a culture of continuous improvement.",
    "## Empathy, Authenticity and Emotional Intelligence\n\nAs GenAI becomes an integral part of behavior change initiatives, humans need to maintain and reinforce strong interpersonal skills.\n\nEmpathy, emotional intelligence, and authenticity are vital in understanding the human experience, addressing concerns, providing support, and building trust between humans and AI, fostering genuine connections, and ensuring the authenticity of interactions in the behavior change process.",
    "## Ethical Awareness and Decision-Making\n\nHumans must be well-versed in ethical considerations related to the use of GenAI.\n\nThey should understand the potential biases, privacy concerns, and social implications of using AI in behavior change efforts.\n\nEthical decision-making becomes crucial when interpreting GenAI recommendations and ensuring fairness, transparency, and accountability.",
    "## Adaptability and Agility\n\nAs behavior change initiatives evolve in collaboration with GenAI, humans need to be adaptable and agile.\n\nThey should be open to new information, adjust strategies and approaches based on GenAI insights, and respond quickly to changes or emerging opportunities.\n\nEthical decision-making becomes crucial in interpreting generative AI recommendations and ensuring fairness, transparency, and accountability.",
    "## Theory & Practice We Selected\n\nIn today’s fast-changing business landscape, traditional hierarchical and bureaucratic models are giving way to innovative theories of organizational design.\n\nHenry Mintzberg was one of the first to observe a paradigm shift towards adhocracy accompanied by the emergence of the “networked firm” inspired by the internet economy.\n\nBuilding upon these foundations, Julian Birkinshaw and Jonas Ridderstråle introduced flexibility and informality within organizational structures in the book Fast Forward.\n\nFrederic Laloux’s influential book Reinventing Organizations presented a compelling vision of self-management and introduced the concept of teal organizations.\n\nMore recently the open-source movement, agile methodologies, and the sharing economy have inspired participative, responsive structures and a range of company-specific variations on self-organization.\n\nFor example, a prominent self-organization theory is Holacracy, which explicitly defines roles, enables distributed decision-making, and fosters adaptive governance.\n\nHolacracy encourages autonomy and accountability within teams, promoting a sense of ownership and unleashing collective intelligence.\n\nThe shift towards these modern theories of organizational design aims to strike a balance between reliability and adaptability.\n\nBy embracing flexibility, self-management, and participative structures, organizations create environments that foster collaboration, innovation, and resilience.",
    "## The Experience of Few Pioneers\n\nThe experience of few pioneers has offered valuable insights and lessons on potential benefits, as well as on barriers or pitfalls to avoid:\n\n- ING Bank - Tribes and Squads (extensively studied and researched by Professor Julian Birkinshaw): ING Bank transformation involved restructuring the organization into small, cross-functional teams called “squads” and implementing agile practices such as Scrum and Kanban, leveraging the organizational model of Spotify and other digital native firms.\n\nING Bank’s transformation has been widely recognized as a reference case of implementing adaptable organizations at scale within a large, established company.\n\n- Zappos Holacracy (analyzed by Professors Ethan Bernstein, Juan Pablo Vazquez Sampere and Georges Romme): An online shoe and clothing retailer, Zappos embraced self-management and organizational agility.\n\nIn 2013, they implemented a holacracy-based system called which involves distributing authority across self-organizing teams.\n\nThis approach encourages employees to take ownership of their work, make decisions autonomously, and adapt to changing circumstances.",
    "## Our Hands-On Experience\n\nOrganizational reinvention is an area we have extensively studied, consulted, and published in the last 15 years.\n\nAs domain experts, we are often engaged in webinars, conferences, and roundtable discussions.\n\nIn our experience with several companies, we have observed an increasing interest on more fluid and adaptable organizational models.\n\nAlthough tempting, copying the org.\n\nmodel of agile champions like Spotify or Zappos, is challenging and moreover, can bring significant unanticipated complexity.\n\nOur experience suggests that modernizing traditional organizational structures, especially for legacy companies, is challenging because:\n\n- Overlooking the specific organizational context: each organization has its unique characteristics.\n\nFailing to consider these factors when implementing an agile organizational structure can lead to challenges and suboptimal outcomes.\n\n- Lack of clarity in roles and responsibilities: old traditional practices such as RACI are no longer effective in fluid, flat organizational archetypes.\n\n- Resistance to continuous change: people are reluctant to give up established hierarchies and power dynamics, and even when they transition, they neglect continuous learning and adaptation.\n\n- Inadequate cross-functional collaboration: even if change happens, people keep working in isolation, inhibiting the flow of information.",
    "## Enhancements - What are the Most Valuable Enhancements that GenAI Can Bring to the Application of Fluid and Flexible Organizational Models?\n\n1.\n\nPromoting Cross-Functional Collaboration: GenAI can identify potential areas of resistance or collaboration gaps by analyzing communication patterns, dependencies, and information flow.\n\nIt can suggest strategies to promote cross-functional collaboration, such as implementing shared platforms, facilitating knowledge sharing, and fostering a culture of collaboration.\n\n2.\n\nModelling Change in Organizational Structure: GenAI can analyze organizational data (such as size, industry, and existing culture), industry benchmarks, and best practices to provide insights and recommendations for organizational transformation.\n\nIt can offer data-driven guidance on designing adaptable structures, role clarity, and decision-making processes.\n\n[...]\n\n```",
    "## Supporting Self-Organizing Teams\n\nGenAI can provide resources and training materials to support self-organizing teams.\n\nIt can offer guidance on decision-making frameworks, conflict resolution techniques, and best practices for effective self-organization.\n\nGenAI can also facilitate communication channels for teams to share knowledge and align their efforts.\n\nGenerative AI can analyze organizational data (such as size, industry, and existing culture), industry benchmarks, and best practices to provide recommendations on changes to the organizational structure.",
    "### Additional Enhancements Include\n\n- **Clear the Organizational Fog:** GenAI can assist in defining clear roles and responsibilities by analyzing job descriptions, skill sets, and team dynamics.\n\nIt can provide recommendations on role definitions, accountability frameworks, and optimal composition of teams within a fluid and flexible organizational structure.\n\n- **Performance Tracking and Organizational Improvement:** GenAI can monitor team performance, provide real-time feedback, and help identify areas for improvement within the fluid and flexible organizational structure.\n\nIt can offer insights, recommend iterative adjustments to the organizational structure, and facilitate ongoing learning and adaptation.",
    "## Support the Shift in Management Styles\n\nGenAI can assist in promoting servant leadership and a more empowering management style.\n\nIt can provide training materials and resources to help leaders transition from command and control to a supportive and facilitative role.\n\nIn addition, GenAI can provide insights on establishing guidelines, frameworks, and feedback mechanisms to ensure that individuals/teams have the autonomy to make decisions.",
    "### Addressing Resistance to Change\n\nGenAI can help identify sources of resistance to change within the organization by analyzing employee sentiment, conducting surveys, or collecting feedback.\n\nIt can provide targeted interventions, such as personalized communication, training resources, and change management strategies, to address resistance and foster a positive mindset towards the agile organizational structure.",
    "## Fluid Organizational Networks\n\nInstead of rigid team structures, organizations could embrace a fluid network model.\n\nEmployees would form dynamic networks based on project needs and personal interests, allowing for more agility and flexibility.\n\nGenAI can assist in the self-organization of fluid networks by analyzing project needs, individual availability, and team dynamics.\n\nIt can provide recommendations on optimal team compositions, helping teams form and disband as projects evolve.",
    "## Decentralized Decision-Making Ecosystem\n\nRather than relying on a few designated leaders or managers, organizations could establish a decentralized decision-making ecosystem.\n\nAt the individual level, GenAI can provide decision support tools that offer data-driven insights and recommendations to individuals at all levels.\n\nOverall, GenAI can help ensure alignment and coordination within a decentralized decision-making ecosystem.\n\nIt can provide mechanisms for sharing information, tracking decisions, and facilitating collaboration, enabling distributed leadership while maintaining organizational coherence.",
    "## Purpose-Driven Autonomous Cells\n\nOrganizations could transition towards purpose-driven autonomous cells that operate as independent entities.\n\nGenAI can assist in resource allocation within cells.\n\nIt can analyze project requirements, individual capabilities, and resource availability to optimize resource allocation and decision-making within each cell.\n\nAlso, GenAI can facilitate knowledge sharing and collaboration across cells.\n\nGenerative AI can analyze project needs, individual capabilities, and resource availability to optimize resource allocation and decision-making.",
    "### Additional Areas of Disruption Include\n\n- **Dynamic Performance Evaluation and Rewards:** Moving away from traditional annual performance reviews, organizations could implement dynamic and real-time performance evaluation systems.\n\nThese systems would provide continuous feedback to individuals and teams based on their contributions, achievements, and alignment with organizational values and goals.\n\nGenAI can assist in identifying and recognizing individual and team contributions based on predefined criteria.\n\nIt can provide recommendations for personalized rewards and recognition, promoting transparency and motivation.\n\n- **Holistic Skill Sharing Platforms:** Organizations could develop holistic skill-sharing platforms that go beyond traditional job roles and functions.\n\nGenAI can analyze employee profiles, skills, and project requirements to identify potential matches for skill sharing.\n\nIt can recommend individuals with complementary skills and interests to collaborate with like-minded colleagues or suggest relevant projects based on their skills and interests.",
    "## Flexibility, Adaptability, & Continuous Learning\n\nIn fluid and flexible organizational models, individuals may need to take on more adaptive roles that transcend traditional job descriptions.\n\nThey may need to be flexible and adaptable, assuming multiple responsibilities based on the needs of the organization or project at hand.\n\nThis requires a willingness to learn and embrace new challenges, as well as the ability to quickly acquire new skills and knowledge.",
    "## Collaborative Organizational Design\n\nHumans actively engage in the collaborative design and customization of fluid and flexible archetypes in conjunction with AI systems.\n\nThey work alongside GenAI to co-create organizational structures that promote agility, collaboration, and effective decision-making.\n\nHumans leverage their domain expertise, contextual knowledge, and understanding of team dynamics.\n\nHumans working together with generative AI to co-create organizational structures that promote agility, collaboration, and effective decision-making.",
    "## Theory & Practice We Selected\n\nAgile originated as an approach for software development 20 years ago.\n\nGradually, Agile principles and practices have expanded “at scale” well beyond IT teams and projects, permeating the entire organization, beyond just individual teams or projects.\n\nThis requires extending the Agile mindset to all levels and functions of the business, which can lead to increased flexibility, faster decision-making, and improved responsiveness to customer needs or market changes.\n\nThe principles underpinning an agile way of working are: decentralized decision-making (teams closest to the work make decisions, enhancing speed and efficiency); cross-functionality (teams possess all required skills to deliver a product or service, encouraging self-organization); alignment and autonomy (teams are guided by strategic goals but have the freedom to decide their path to those objectives); iterative and incremental development (regularly reassessing strategies and plans based on feedback).\n\nFor the successful adoption of an Agile way of working, it’s important to ensure a realignment of culture, leadership styles, as well as processes and systems of an organization.",
    "## Our Hands-On Experience\n\nOver the last decade, we have contributed to the debate about Enterprise Agility with multiple publications: Agility Inc (Thinkers50), Agile Planning (HBR), Agile Feedback (HBR), Agile Processes (EBR) and Agile Culture & Behaviors (Thinkers50).\n\nBased on our research and direct experience with companies undergoing agile transformations, we have identified common pitfalls and how to unmask “fake agile”.\n\nIn our experience, Agile transformations often stumble due to common pitfalls:\n\n- **“Agile Theater”:** While Agile jargon and tools are adopted, the enterprise still operates in the traditional way with no signs of real systemic change (e.g., rigid goal setting and resource allocation, hierarchical authorization mechanisms, unaltered traditional decision-making processes, lack of continuous feedback mechanisms).\n\n- **Inadequate Leadership Behaviors:** Lack of effective shift in leaders’ actions, failing to concretely support and empower agile teams.\n\n- **Burden of Legacy Systems:** Existing legacy systems and processes can pose challenges during Agile transformations, as they may impede flexibility and responsiveness.\n\n- **Overemphasis on Tools:** Placing excessive focus on Agile tools rather than the Agile mindset, collaboration, and adaptive thinking can hinder the success of Agile transformations.\n\n- **Insufficient Training:** Inadequate training and education on Agile principles and practices can limit the understanding and application of Agile concepts within the organization.\n\nRecognizing and addressing these pitfalls is crucial for a true Agile transformation.",
    "### Additional Enhancements Include\n\n- **Dynamic Planning and Resource Allocation:** To overcome rigid goal setting and resource allocation, GenAI can assess resource flexibility, budget forecasting, team capacity, skill requirements, and market opportunities to recommend efficient and effective resource allocation.\n\n- **Promote Transformative Leadership Behaviors:** AI can employ sentiment analysis to understand employees’ perceptions of leadership styles, thereby delivering essential feedback to guide leaders’ development.",
    "### Artificial Scrum Master\n\nGenAI could potentially take over the role of a Scrum Master.\n\nIt could automatically assign tasks, monitor progress, identify potential risks, and even suggest improvements based on data from previous iterations.\n\nAI could facilitate daily stand-ups, sprint planning, and retrospectives by processing data in real-time and providing valuable insights.",
    "## New Roles in AI-Agile\n\nTraditional Agile roles (like Scrum Master, Product Owner) may need to be redefined in the context of GenAI.\n\nFor instance, teams may need roles focusing on AI ethics, data management, and AI model training and evaluation.\n\nAI, based on the initial requirements, could use this information to suggest an optimal project structure, team size, and sprint duration, dramatically improving project planning and execution.",
    "### Additional Areas of Disruption Include\n\n- **Transforming Traditional Agile Ceremonies:** Daily stand-ups may become unnecessary as generative AI provides real-time data and progress tracking.\n\nSprint planning can be influenced by AI, which analyzes historical data and dependencies to suggest optimal plans.\n\nRetrospectives can be streamlined with AI-generated insights and recommendations for process improvements.\n\n- **Immersive Agile Training:** Generative GenAI could simulate immersive training experiences to develop an Agile mindset in team members.\n\nAI-powered virtual environments would challenge individuals with realistic Agile scenarios, providing instant feedback, coaching, and adaptive learning paths to foster Agile thinking and behavior.\n\n- **GenAI as a Team Member:** AI could become an active contributor as a team member, working on tasks, writing code, designing solutions, and contributing to discussions.\n\nThis could completely change the dynamics of Agile teams and open the door to 24/7 development cycles.\n\nWith AI becoming a prominent part of the team, effective collaboration and communication with AI tools, virtual assistants, and human colleagues are essential.",
    "## Theory & Practice We Selected\n\nIn today’s dynamic context, characterized by rapid changes and evolving market conditions, there is a growing need for less rigidity and a greater emphasis on faster, autonomous decision-making processes.\n\nEmpowerment becomes crucial as it allows individuals at all levels of an organization to have a voice and contribute their expertise.\n\nRethinking decision-making practices and criteria, along with control models, becomes necessary to reduce excessive hierarchical bureaucracy.\n\nThis shift towards employee empowerment helps reduce the bureaucratic burden on individuals and fosters a culture of innovation.\n\nOver the last decade, noteworthy books such as Gary Hamel’s and Michele Zanini’s “Humanocracy,” Reed Hastings and Erin Meyer’s “No Rules Rules,” Donald Sull’s and Kathleen M. Eisenhardt’s “Simple Rules for a Complex World” have provided valuable insights and methodologies to reshape decision-making processes in large bureaucratic corporations, emphasizing the importance of empowerment.\n\nVarious companies have pioneered innovative decision-making practices and empowerment, including Haier, Nucor, Netflix, Michelin, Handelsbanken, Southwest Airlines, and Vinci.",
    "## Our Hands-On Experience\n\nOver recent years, we have further contributed to enhancing these theories and practices by developing a framework for innovative control models for more decentralized and empowered decision-making and enhancing the concept of debureaucratization.\n\nSeven innovative control models are presented to help achieve these goals: guiding principles, boundary rules, context setting, sounding board, peer review, social pressure, and post-detection.\n\nWe advocated “Trust-by-Design” as a way to embed trust in processes and decision-making criteria (see our article on IbyIMD).\n\nIntroducing empowerment theories and practices in large companies we have observed common pitfalls that organizations need to deal with:\n\n- **Resistance from Middle Management:** Managers may resist losing their traditional power of control.\n\nUnlearning the old paradigm and embrace the new one can be challenging.\n\n- **Organizational Inertia:** Existing structures and processes can exhibit inertia, making it difficult to effectively empower people.\n\n- **Lack of Trust:** Building trust within the hierarchy and the layers of an organization is crucial for empowering employees and may be a challenge to overcome.\n\n- **Employee Skill Gaps:** Lack of capability among employees that can hinder their ability to take on additional responsibilities and make independent decisions with autonomous judgment.\n\n- **Legal and Compliance Constraints:** Formal requirements may limit the actual degree of autonomy that should be granted to employees in empowerment initiatives.\n\n- **Technology Gaps:** Monolithic technology infrastructure can impede the effective implementation of empowerment practices due to rigidity and lack of flexibility and adaptability of systems.\n\nThe good news is that technology advancement, like GenAI, is changing the game.\n\nIt can make bureaucracy harmless without necessarily eliminating its control system.\n\nIt can simply replace low-value tasks and decisions that were traditionally made by frustrated and demotivated humans; on the other side, GenAI tools like ChatGPT can augment human judgment to make faster, better, and more autonomous decisions.",
    "### What Are the Most Valuable Enhancements GenAI Can Bring?\n\n1.\n\n**Context Understanding for Decisions:** GenAI can provide a wide range of information to employees, helping them understand the larger context in which they are operating.\n\n2.\n\n**AI-Facilitated Decision Making:** GenAI can provide support by analyzing large datasets to offer predictive insights, forecasts, and recommendations (pondering pros and cons of decisions).\n\nThis helps managers in making decisions faster and more accurately, reducing the time spent on deliberation.\n\n3.\n\n**Modeling and Simulating Decision Outcome:** GenAI can model potential outcomes of decision-making scenarios, allowing individuals and teams to experiment with different approaches and see their potential impacts before actual implementation.\n\nThis encourages a shift from a risk-averse mindset to one that embraces innovation and learns from failures.\n\nGenerative AI can provide support by analyzing large datasets to offer forecasts, insights, and recommendations by weighing the pros and cons of decisions.",
    "### Additional Enhancements Include\n\n- **Streamlining Complex Rules:** GenAI could help streamline complex rules, turning them into simple, actionable, and understandable guidelines.\n\nIt could suggest the most essential rules of a process or a policy.\n\nBy defining common rules and leveraging AI-generated insights, organizations can reduce variations in adoption and interpretation, promoting consistent practices.\n\n- **Human-Machine Decision Boundaries:** AI can be designed with built-in decision-making boundaries, which can be quantitative or rule-based.\n\nThe AI could make decisions autonomously within these boundaries, while escalating decisions outside the boundaries to human oversight.\n\n- **Personalized Recommendations:** By analyzing individual preferences, past decisions, and performance data, GenAI can provide personalized recommendations tailored to each employee’s specific profile.\n\n- **Trust-Building:** GenAI can help provide transparency.\n\nAI tools can track and analyze decision-making processes, ensuring that all decisions are fair, unbiased, and data-driven.\n\nAI can help ensure that decisions are transparent and traceable, providing an audit trail for accountability.",
    "## Disruptions in Decision Making\n\n1.\n\n**AI-Powered Voting:** Introduce AI-powered platforms that allow employees to directly participate in decision-making processes through voting and consensus mechanisms.\n\nGenAI algorithms can aggregate and analyze the collective input, enabling a more inclusive and democratic approach to decision-making.\n\n2.\n\n**AI-Augmented Collective Intelligence:** GenAI can facilitate collective decision-making by aggregating and synthesizing diverse perspectives from multiple individuals.\n\nThrough natural language processing and sentiment analysis, AI can extract insights from discussions, forums, and social media, enabling a broader range of input in decision-making processes.\n\n3.\n\n**Decentralized Decision Networks:** GenAI facilitates the creation of decentralized decision networks, where decision-making authority is distributed among various nodes in the network.\n\nEach node, represented by AI or human decision-makers, has the autonomy to make decisions within their domain of expertise, enabling faster and more agile decision-making.\n\nAs humans work in collaboration with AI systems, developing metacognitive skills becomes increasingly important.\n\nThis includes awareness of one’s cognitive biases, assumptions, and limitations, as well as actively monitoring and adapting decision-making strategies.",
    "### Additional Areas of Disruption Include\n\n- **Emotion and Sentiment Analysis:** GenAI can detect and analyze emotions and sentiments expressed in text, voice, or facial expressions.\n\nBy understanding the emotional context, AI can help decision-makers gauge the impact of their choices on stakeholders’ well-being and make more empathetic decisions.\n\n- **Responsible Decision Navigator:** GenAI acts as a navigator, guiding decision-makers through the complex landscape of sustainability considerations and trade-offs.\n\nIt helps identify potential risks, evaluate the environmental and social implications, and suggest alternatives that align with sustainable practices.\n\n- **Inclusive Decisions:** AI facilitates the integration of stakeholder voices by aggregating and analyzing data from multiple sources.\n\nThis inclusive approach helps organizations make decisions that balance the interests and needs of different stakeholders, fostering a more inclusive and socially responsible outcome.\n\n- **Intelligent Decision Routing:** GenAI can intelligently route decision requests to the most suitable individuals or teams based on their expertise, availability, and workload.\n\nBy optimizing decision routing, AI ensures that decisions are handled by the most qualified and capable individuals, for better outcomes.\n\n- **Multi-Agent Decision Optimization:** GenAI can employ multi-agent systems where different AI agents with specialized expertise collaborate to optimize decisions.\n\nEach agent contributes unique perspectives, allowing for holistic decision-making that considers a wide range of factors.",
    "### Domain Expertise\n\nEmployees bring their domain expertise and deep knowledge of specific areas to the decision-making process.\n\nThey leverage GenAI tools to augment their expertise, analyze data, and gain insights, combining them with their domain knowledge to make decisions.\n\nThey are also aware that GenAI’s answers can have limitations and should be carefully reviewed.",
    "### Csikszentmihalyi’s Flow Theory\n\nThis theory focuses on the experience of being in a state of flow, characterized by complete immersion, deep focus, and a sense of enjoyment in an activity.\n\nCsikszentmihalyi identified several factors contributing to the flow state, such as the match between the challenge level of the activity and an individual’s skill level, clear goals, immediate feedback, and the merging of action and awareness.\n\nFlow theory emphasizes the intrinsic rewards of engagement and the importance of finding activities that provide an optimal balance between challenge and skill.\n\nCsikszentmihalyi’s theories served as the foundation and inspiration for Daniel Pink, who further elaborated on and popularized these ideas in his book Drive.",
    "### Daniel Pink’s Concepts\n\nPink introduced the concepts of autonomy, mastery, and purpose as the key drivers of intrinsic motivation, highlighting their significance in fostering engagement, creativity, and satisfaction in individuals.\n\nAutonomy refers to having control over one’s work, mastery to the desire to get better at what one does, and purpose to the belief that the work matters.\n\nResearch in the field of organizational psychology and management has demonstrated that intrinsic motivation plays a crucial role in driving employee performance, satisfaction, and overall organizational outcomes (Lindsay McGregor and Neel Doshi in their article “How Company Culture Shapes Employee Motivation”).\n\nWhen empl...",
    "# Intrinsic Motivation and the Role of GenAI\n\nEmployees are intrinsically motivated, they tend to exhibit higher levels of engagement, creativity, and commitment to their work.\n\nCases often cited in the management literature include:\n\n- **Best Buy** | Hubert Joly’s human-centered approach (described in his book *The heart of business*), leveraging the full potential of Best Buy employees by ensuring they are well-motivated, adequately trained, and fully engaged in their roles.\n\n- **Michelin** | Responsabilisation (French for “empowerment”) Program which aimed at giving employees more responsibility and autonomy in their roles.\n\nBy doing this, Michelin fostered a sense of ownership among employees which led to increased motivation and productivity.",
    "## Our Hands-on Experience\n\nIn applying these principles and practices we often observed that they can be very powerful, but their implementation and scale up can pose difficult challenges such as:\n\n- **Lack of standardization**: Without a standardized method to define and measure motivation, the process can become highly subjective.\n\nDifferent individuals may have different interpretations of motivation, which can lead to inconsistencies in measurement and interpretation of results.\n\n- **Difficulty in deriving actionable insights**: Surveys can provide valuable data on employee motivation but converting them into actionable strategies is challenging because of the complex nature of motivation.\n\n- **Perception that motivation is qualitative without a business case**: Numerous studies have shown a strong link between employee motivation and business outcomes such as productivity, retention, and customer satisfaction, but this connection is not yet obvious.\n\nTo help firms overcome these challenges we supported the development of a practical approach called “kindness triangle”.\n\nAt the base of the triangle are motivation and wellbeing indicators.\n\nAt its top are business results.\n\nEvery vertex of the triangle has an associated set of indexes, with analytical correlations linking the three vertices.\n\nThe logic is that a compassionate leadership results in higher rates of trust and engagement in people, which in turn translates into improved company performance.\n\nTo convincingly show the link between kindness, motivation, and business results, anecdotal evidence is not enough.\n\nWe need analytical measurement, such as the “Kindness Performance Indicators”: employees are surveyed on their motivation with questions around purpose, autonomy, and relationship.\n\nResults by unit (or sub-unit) are compared with the unit leader’s individual score on the Trust Behavioral Index.\n\nIn the companies and units where we ran the correlation, we found consistent links between kind leaders and motivated employees.",
    "### What are the most valuable enhancements that GenAI can bring to the application of intrinsic motivation practices such as Kindness Performance Indicators (KPIs)?\n\n1.\n\n**Gathering Actionable Insights on Motivation**: GenAI can process survey data and other relevant information to generate actionable insights to harness intrinsic motivation.\n\nBy using advanced analytics and machine learning algorithms, GenAI can identify patterns, trends, and relationships that might be missed by human analysis.\n\nIt can also suggest actions or motivation strategies.\n\n2.\n\n**Standardization of Motivation Analysis**: GenAI can aid in creating an ad-hoc standardized model of measuring and understanding motivation across the organization.\n\nIt can process a large amount of data to identify common factors and patterns, which can serve as a standard.\n\nThe model can be used for individual assessments and comparisons across units or the whole organization.\n\n3.\n\n**Continuous Measurement and Real-time Tracking**: GenAI can automate the process of regular measurement and tracking of KPIs.\n\nIn traditional settings, these would typically be assessed through employee surveys and observations, usually conducted periodically.\n\nWith the aid of GenAI, this process can be automated, providing real-time insights and analyses.\n\nThe AI system can be trained to analyze various sources of data to measure indicators of kindness continuously.\n\nAdditional enhancements include:\n\n- **Demonstrating Business Case for Motivation**: By analyzing historical and real-time data, GenAI can draw clear connections between motivation levels and key business outcomes.\n\nIt can provide evidence-based arguments for the impact of motivation on performance, helping to establish the business case for investment in motivation strategies.\n\n- **Enhanced Qualitative Analysis**: While GenAI can analyze quantitative data, it can also process qualitative information, offering a deeper, more nuanced understanding of the qualitative aspects of motivation and kindness, often reflected in unstructured data such as open-ended survey responses, employee feedback, meeting transcripts, emails, and other text data.\n\nIt can identify recurring themes, detect sentiment (positive, negative, neutral), and even comprehend subtle emotions (like joy, frustration, etc.).\n\n- **Engaging Employees**: With capabilities like chatbots, GenAI can facilitate two-way communication with employees, inviting their input and feedback in a more interactive manner.\n\nThis can help increase employee engagement in the process.",
    "### How would GenAI rethink or radically disrupt application of intrinsic motivation practices such as Kindness Performance Indicators (KPIs) from within?\n\n1.\n\n**“Kind” AI**: GenAI systems can be designed to embody kindness in their interactions, serving as a digital exemplar of kindness in the organization.\n\nThrough providing consistently empathetic and supportive responses, these AI systems foster a culture of kindness and provide a touchpoint for guidance.\n\n2.\n\n**Automated Sentiment Analysis and Emotion Recognition**: GenAI, especially advanced Natural Language Processing (NLP) and machine learning algorithms, could process large amounts of text data (like emails, chat transcripts, social media posts) or even voice and video data to understand employees’ emotions, satisfaction, and overall sentiments about their work and the company culture.\n\nThis could provide real-time insights into motivation and kindness at work beyond traditional survey methods.\n\n3.\n\n**Personalized Feedback and Improvement Plans**: Using AI, personalized feedback can be provided to each employee, including recommendations for enhancing motivation.\n\nThese suggestions can be derived from analyzing vast amounts of data on successful interactions and patterns of motivation and kindness within the organization or across similar organizations.\n\nOther areas of disruption include:\n\n- **Kindness Forecasting & Proactive Intervention**: Through machine learning and data mining, GenAI could forecast potential pitfalls or opportunities related to motivation and kindness in the organization.\n\nIt could predict how changes in policies or procedures might impact motivation, trust and kindness, or it could foresee which departments or individuals might need extra support.\n\n- **Immersive Empathic Training**: Advanced AI can utilize AR/VR to simulate real-life scenarios and help in training employees and leaders to exhibit more kindness in their interactions.\n\n- **Decentralization of Motivation and Kindness Initiatives**: Advanced AI technologies such as blockchain could be used to create a decentralized, transparent record of kindness-related initiatives, actions, and their impacts in the organization.\n\nThis would democratize the process, giving every employee a chance to contribute to and access kindness-related data, and to certify the quality of data.",
    "## Human Factor\n\n- **Data Interpretation**: As GenAI automates the process of data collection and initial analysis, humans can focus more on interpreting the results, understanding their implications, and developing strategic responses to foster employee motivation\n- **Interactive & Kind Engagement**: Instead of disseminating information about kindness in a one-way manner, humans will play a vital role in engaging with employees interactively and having meaningful conversations, aided by GenAI tools such as chatbots.\n\nThey’ll focus on fostering relationships, building trust, and promoting a culture of kindness.\n\n- **Anticipatory Kindness & Motivation Management**: Leveraging AI insights to proactively detect, address, and capitalize on shifts in motivation and kindness within the organization.\n\n- **Immersive Learning Skills**: Mastery in designing and facilitating immersive, experiential learning experiences (possibly using AR/VR technologies) to enhance the understanding and practice of kindness.\n\n- **Authentic & Compassionate Leadership**: Emphasizing the human ability to show empathy, exhibit personal authenticity, and serve as a role model of kindness.\n\nInterpersonal connection, understanding, and emotional intelligence are key to complement AI’s analytical capabilities.\n\nInterpersonal connection, understanding, and emotional intelligence are crucial for integrating the analytical capabilities of AI.",
    "## BIOS\n\n- **Enrico Sassoon** is the Chief Editor of Harvard Business Review Italia.\n\n- **Paolo Cervini** is HBR.org author and member of the Thinkers50 Radar class of 2023 (the global ranking of 30 up-and-coming thinkers).\n\nHe co-leads the Capgemini Invent’s Management Lab.\n\nPreviously, content coordinator of Harvard Business Review Italy.\n\n- **Elisa Farri** is HBR.org author and member of the Thinkers50 Radar class of 2023 (the global ranking of 30 up-and-coming thinkers).\n\nShe co-leads the Capgemini Invent’s Management Lab.\n\nPreviously, researcher at the Harvard Business School’s Europe Research Center.\n\n- **Gabriele Rosani** is HBR.org author and Director of Content and Research at Capgemini Invent’s Management Lab.\n\nPreviously, founding member of the European Center for Strategic Innovation.",
    "# Northwestern Guidance on the Use of Generative AI\n\nGenerative AI offers the potential for new capabilities in research, education, and productivity.\n\nExpectedly, use of tools and services, including OpenAI’s ChatGPT, Microsoft’s Bing Chat, and Google’s Bard, is growing within higher education and across Northwestern University.\n\nUnderstanding what to look for when adopting these tools is key to ensuring the intended use is met while protecting University data.",
    "## Background\n\nGenerative AI is a general term for artificial intelligence that creates new content based on patterns from the data sets used to train it.\n\nThese generative models learn patterns, structures, and features from the input data and can create content with similar characteristics.\n\nGenerative AI tools commonly known today include ChatGPT, Bard, Bing Chat, Starry AI MidJourney, etc.\n\nGenerative AI is also now embedded in many commercial products that can be integrated with other tools, such as OtterAI, Microsoft Co-Pilot, Zoom IQ, and Fireflies.",
    "## Privacy\n\nIn most cases, the data you share as part of your queries in generative AI tools will be accessible by others using the same tools.\n\nThis is because generative AI learns by collecting, analyzing, and storing user-provided information.\n\nTherefore, University faculty, staff, students, and affiliates should not enter institutional data into any generative AI tools that have not been validated by the University for appropriate use and have explicit permission of the data provider.",
    "## Data Approved for Use with Generative AI\n\nTo determine whether your data requires special attention, consult Northwestern’s Data Classification Policy ( If your data is Level 1 (non-confidential and public data), uploading it to generative AI tools is permissible.\n\nTo process data above Level 1, any generative AI tool must have been approved through Northwestern IT’s procurement and security review processes.\n\nThe following table outlines Northwestern’s current services posture based on data classification:\n\n- **Copilot for Bing** is approved for Level 2 and generally Level 3 data.\n\nSome Level 3 data types require additional non-technical controls that may not be available in Copilot for Bing.\n\nPlease check your contractual or legal obligations to determine if Copilot for Bing can be used with your Level 3 data.\n\nIf you have additional questions about whether your data type is appropriate for Copilot for Bing, please contact the Information Security Office ().\n\nThis page will be updated as other products are evaluated for security and privacy with Northwestern data.",
    "## Academic Use at Northwestern\n\nUse of generative AI for teaching and learning purposes is governed by the Provost’s Committee on Generative AI, in tandem with Northwestern IT.\n\nGuidance on generative AI tools and the impact on teaching and learning can be found on the Office of the Provost website.",
    "## Further Considerations for Adoption\n\nGenerative AI models are designed to output the most common results possible based on their training data and will invariably tend to suppress less common or marginalized information.\n\nFurther, these tools can “hallucinate” or make up random data that is not true as they do not have the ability to determine what is true or false.\n\n- **Revalidation of output is required**: Given that generative AI models continue to adjust over time, the output of their responses could change too.\n\nIt is also possible for generative AI models to “hallucinate,” or otherwise use false information in their responses.\n\nWhen deployed where output requires accuracy, these tools should be periodically re-validated to ensure appropriate responses.\n\n- **Original work and intellectual property**: Ensure necessary approvals for use of original works and intellectual property are obtained prior to input into generative AI tools.\n\nExamples here could be original works developed by students, research results, or obtained copyrighted material.\n\n- **Informed consent**: Users have the right to know when they are using generative AI tools.\n\nThis knowledge can encourage informed decisions about whether to engage, share, or rely on the generated content.\n\nBack to Top  \nRequest Help  \nClose\n\nNorthwestern University",
    "## Purpose and Scope\n\nThis policy sets out the guidelines for the use of Generative Artificial Intelligence (GenAI) systems by the University of Westminster and should be read in conjunction with the University’s position statement on the use of GenAI.\n\nIt applies to all students, colleagues, third-party suppliers, and partners engaged in using or developing such systems within or on behalf of our institution.\n\nThe purpose of these guidelines is to support the safe and ethical use of GenAI at the University and help prepare students and colleagues for the widespread use of GenAI across industry, commerce, and the professions.",
    "## Definitions\n\nFor the purposes of this policy GenAI is defined as an AI system that generates new outputs in a range of possible formats such as text, images, or sounds (including for example music, singing, voice narration).\n\nSome specific current examples of such GenAI systems include ChatGPT, Bing AI, Bard, Jasper, and Synthesia.",
    "## Ethical Considerations\n\nAll users of GenAI must consider the ethical and legal implications of their work.\n\nThis includes being mindful that GenAI systems can generate harmful, misleading, biased, or discriminatory content.\n\nGenAI must not be used to promote discrimination, bias, or harm.\n\nThe following AI ethics principles will guide the use of GenAI at the University:",
    "### Transparency\n\nThe use of GenAI should be as transparent as possible in all use case scenarios, so that users understand how the technology and its algorithms are being deployed and if and how any of their University data is being used.\n\nUsers should be made explicitly aware when GenAI is being used, for what reason and for what purpose.",
    "## Data Privacy and Information Security\n\nThe University undertakes to protect the privacy of users when using and training GenAI.\n\nThis includes not collecting storing, accessing, or sharing personal data without consent and using or ensuring appropriate security measures to protect data from unauthorized access.\n\nAll data used in the development and operation of GenAI systems must respect the privacy and rights of individuals involved and be securely transmitted, stored, and managed in accordance with the University of Westminster’s Data Protection Policy and relevant data protection laws.\n\nIn the case of developing GenAI systems by researchers for research, and creating datasets to train such a system, this should also be done in accordance with the University's Research Data Management Policy.\n\nThe University will take all reasonable steps to protect the security of its computer systems and networks when GenAI is being used through corporate systems.\n\nThis includes using appropriate security measures to protect against malicious attacks and ensuring that users are aware of the risks associated with using the technology.\n\nSimilarly, the University will work with third-party suppliers to ensure that services used by the University are equally secure.",
    "## Usage Guidelines\n\nGenAI must be used responsibly and not used to deliberately deceive or falsify.\n\nThis includes checking as far as possible that outputs are accurate and non-biased.\n\nInappropriate uses, such as creating deepfakes or generating misleading content, are strictly prohibited.\n\nAll users must always clearly disclose the use of GenAI when sharing or publishing generated content.\n\nStudents who use GenAI in their coursework must ensure that the use is permissible.\n\nColleagues who use GenAI in their research and/or discipline of work must ensure that the use is permissible and that they are open about the use of GenAI.\n\nThe University owns all intellectual property that is created using GenAI through corporate systems (such as Microsoft Office or the Blackboard VLE) unless otherwise agreed to in writing.",
    "## Risk Tolerance and Ownership\n\nThe University of Westminster recognizes the need to define its risk tolerance for the use of GenAI.\n\nA balanced approach will be adopted, considering both the potential benefits and the associated risks.\n\nGenAI systems that negatively affect safety or fundamental rights or raise significant ethical issues in research and knowledge exchange activity, will be considered high risk.\n\nSystems using GenAI considered as high risk should not go into production without remediation.\n\nResidual risks should be escalated to the University Senior Information Risk Officer, namely the University Secretary and Chief Operating Officer, for formal acceptance and auditing purposes in accordance with the University risk management methodology.\n\nThe responsibility for conducting actions to mitigate risks should lie with the individuals or teams directly involved in the use of the GenAI system concerned, while the responsibility for assessing novel use cases or the output of approved use cases should be designated to the appropriate oversight body (Ethics Committee for University Research and Knowledge Exchange or the relevant College Teaching Committee).\n\nIn the case of the use of GenAI by Professional Services, ethical oversight will be provided by the Professional Services Director’s Group.",
    "## Use Cases and Restrictions\n\nTo mitigate risks associated with GenAI use, the University of Westminster will classify use cases according to whether they are appropriate to be considered through a Self-Assessment and notification route or whether they require more formal committee Scrutiny and Approval.\n\nSome indicative use cases covering learning and teaching and research are shown as examples in this list of GenAI Use Cases.\n\nIt can be anticipated that most proposed uses conducted in internal facing, secured corporate tools and systems are likely to be categorized within the Self-Assessment and Notification category.\n\nIt is also intended that going forward the GenAI Use Case table will be regularly updated as new low-risk use cases emerge.\n\nHowever, it can also be anticipated that some use cases, even if they use internal facing systems, may necessitate scrutiny by the relevant ethics body.",
    "## Authority for Decision Making\n\nThe authority to make decisions regarding the uses of GenAI that require formal scrutiny, rests with the relevant Ethics Group.\n\nThese groups, comprising representatives from relevant departments and key stakeholders, will be responsible for evaluating proposed use cases, approving new applications, assessing risks, monitoring compliance, and addressing ethical concerns related to GenAI.",
    "### Disclosure to Students, Employees, and Third Parties\n\nThe University of Westminster recognizes the importance of transparency and open communication.\n\nTherefore, information about the use of GenAI should be disclosed to students, employees, and relevant third parties.\n\nSuch disclosure should include the nature of the GenAI system being used, the purpose for which it is being used, and any potential impact on individuals' data or privacy.",
    "### Disclosure Requirements\n\nStudents and employees who use GenAI to generate content through systems or software that they would not be expected to be using within their subject context and the guidance of their tutors must disclose this fact when submitting or sharing the generated content.\n\nThey should clearly indicate that GenAI was involved in the creation process.",
    "### Employee Monitoring\n\nThe University reserves the right to monitor the use of GenAI by employees to ensure compliance with this policy.\n\nEmployees and students should be aware that the use of GenAI, and particularly external public facing systems, without due consideration of the legal and ethical implications of their work, may result in disciplinary action.\n\nAny suspected misuse of GenAI should be reported in the first instance by email to .",
    "## Training and Awareness\n\nThe University of Westminster is committed to promoting the use of GenAI in ethical ways through training and awareness programs.\n\nAll users of GenAI are required to complete a training program on the ethical use of AI.\n\nThis training program will cover topics such as the responsible use of GenAI, ethical considerations, privacy, data protection, and compliance with this policy.",
    "## Monitoring and Auditing\n\nThe University of Westminster will periodically monitor the use of GenAI to ensure compliance with this policy.\n\nAudits will also be carried out by relevant departments (e.g., Learning Innovation and Digital Engagement for the virtual learning environment and Information Systems and Services for Microsoft Office 365) to assess adherence to ethical guidelines and identify areas for improvement.",
    "## Review and Updates\n\nThis policy will be reviewed at least bi-annually, or more frequently, if necessary, to ensure it remains relevant and up to date with technological advancements and legislative changes.\n\nRecommendations for updates or improvements to the policy may be submitted to either the Ethics Committee for University Research and Knowledge Exchange (for research or knowledge exchange linked suggestions) or the relevant College Teaching Committee (for teaching related matters) or the Professional Services Director’s Group (for corporate services matters).",
    "## Related Policies\n\nThis policy forms part of the information security management system (ISMS) at the University of Westminster.\n\nThe Policy for the safe and ethical use of generative GenAI should be read in conjunction with all other University information management policies, which are reviewed and updated as necessary to maintain an effective Information Security Management System to meet the University’s business needs and legal obligations.\n\nCode of Practice Governing the Ethical Conduct of Research.",
    "### Table of Contents\n\n- Tips for Setting Your Course Approach to AI Use\n- Template Syllabus Statements for Generative AI Use\n- Additional Resources\n\nThis resource is part of our suite on Teaching and AI.\n\nSee also Teaching in the AI Era and Starting Small with AI in the Classroom.",
    "## Tips for Setting Your Course Approach to AI Use\n\nStanford instructors may set course policies regarding generative AI use as they choose, according to guidance from the Board on Conduct Affairs.\n\nThe guidance recommends that such policies are stated in the course syllabus and communicated clearly to students.\n\nConsider your learning goals to help guide how you encourage and/or limit students’ AI use, such as on which types of coursework or which portions of assignments AI use is permitted and in what ways.\n\nExplain limitations on AI use to students, perhaps by connecting to the learning goals of the course, how assignment expectations mirror exam or quiz expectations, industry standards (e.g., publication standards), or common industry cases (e.g., needing to give thoughtful/persuasive/clear input on the spot during board meetings).\n\nState your policy in your course syllabus and the Course Policies and Norms form (if desired), and then follow up in class with your students.\n\nContinually remind students of AI use policies, such as when major assignments are given, not just at the start of the quarter.\n\nIf you allow or encourage AI tool use, assign coursework or offer alternatives that all students may complete effectively if they are unable or choose not to use AI tools.\n\nStanford does not yet support any generative AI tools for use.\n\nLearn more\n\nLooking for information about AI detectors?\n\nIf you choose to use them, we recommend a high degree of caution.\n\nSee more under our FAQs for Reducing Unpermitted Use of AI Tools.\n\nFor more guidance on developing your course policy on AI, see Stanford Teaching Commons’ resource on Creating Your Course Policy on AI.",
    "### AI Tools Permitted Freely\n\nIn this course, you may use generative AI tools for all coursework according to the following data privacy guidelines.\n\nBe sure to review and follow the guidelines provided in Stanford IT’s resource on Responsible AI at Stanford.\n\nUIT has advised the Stanford community to avoid inputting information that should not be made public when using a generative AI tool.\n\nThis includes personal or confidential information of your own or that others share with you, as well as proprietary or copyrighted materials ([Include relevant material types for your course: e.g., case studies, data sets, assignment prompts]) that may be included in your coursework.\n\nInformation you enter into a generative AI tool may be shared with third parties, and the tool may use your prompts or questions to inform content generated for other users.",
    "### AI Usage Permitted with Limitations\n\nIn this course, you may use generative AI tools according to the following guidelines.\n\nAny use of AI-generated material in student work outside of these guidelines will be considered a violation of the Honor Code and subject to consequences.\n\nGuidelines for use:\n- Be sure to review and follow the guidelines provided in Stanford IT’s resource on Responsible AI at Stanford.\n\nUIT has advised the Stanford community to avoid inputting information that should not be made public when using a generative AI tool.\n\nThis includes personal or confidential information of your own or that others share with you, as well as proprietary or copyrighted materials ([Include relevant material types for your course: e.g., case studies, data sets, assignment prompts]) that may be included in your coursework.\n\nInformation you enter into a generative AI tool may be shared with third parties, and the tool may use your prompts or questions to inform content generated for other users.\n\n- [Optional: Select one or more additional guidelines for use from the list below, according to what applies for your course.\n\nHighlighted text should be edited to fit your course policies.]\n\n- Use generative AI tools only according to the specific guidelines given for the assignment.\n\nReview the instructions for each assignment for more details.\n\nIt is also acceptable to not use generative AI tools for an assignment that allows you to do so.\n\n- You may use generative AI tools for [specify allowable assignment types, e.g., homework assignments, problem sets, projects/papers, and exam prep], but not on [specify not allowable assignment types, e.g., for in-class work or on exams or quizzes].\n\n[May parallel the categories given in and/or refer to the Course Policies and Norms form, if using.]\n\n[Instead of specifying assignment types here, include: Refer to the Course Policies and Norms form for the assignment types on which AI tools are allowed.]\n\nIt is your responsibility to avoid becoming dependent on AI tools in your learning such that you are unable to master the course goals, which [always/sometimes] require you to complete tasks and demonstrate skills without the use of AI tool assistance.\n\n[May explain additional allowances for using AI tools, e.g., you may use AI tools during the brainstorming or editing phases but not the drafting process of a writing assignment / you may use AI tools to help you understand and digest challenging readings but not in assignments, which test and enhance your understanding by asking you to apply those concepts to new contexts.]\n\n- All ideas, reflections, and analyses you present in your coursework must be your own.\n\nAI tools are only permitted in this course to help you brainstorm starting points, develop prototypes and placeholders, and revise your work.\n\n[May include additional guidelines, e.g., Use AI tools only in the ways you might get help from a classmate, TA, or tutor — to brainstorm suggestions or practice skills, but not to generate ideas or arguments or to create final products.\n\nIf you use AI tools to enhance your learning, make sure you are learning in a way that allows you to complete assignments or assessments without AI tools, if required.]\n\n- The use of AI tools during class time is not permitted, [May include rationale, e.g., so that we can continue to get the most out of our in-person interactions].\n\n- Cite all AI-generated material and/or explain how you have drawn on AI-generated material in your work.\n\nPlease cite AI-generated materials using [MLA style / APA style / Chicago style] formatting [or explain your preferred method of citing content, e.g., include a short paragraph with each assignment explaining how you used generative AI tools / attach a copy of the chatbot conversation or other prompting that helped you produce assignment material].\n\n- Be prepared to fact-check and critically evaluate all AI-generated information.\n\n[May include additional explanation, e.g., Most AI chatbots aren’t designed to write sentences that are true — they are designed to write sentences that are plausible.\n\nMany AI tools get their training sets and information from the internet and can’t make judgements about the information they draw on.]\n\nGenerative AI tools can provide false information (called ‘hallucinations’), perpetuate biases and/or stereotypes, or draw on copyrighted information without proper attribution, and such problematic information is often presented very convincingly.\n\nThe materials these tools generate do not necessarily meet the standards of this course.",
    "### No AI Usage Permitted\n\nIn this course, students may not use generative AI tools to complete any part of their assignments or assessments.\n\nThis includes using AI tools that generate written, visual, or audio content, even if you revise or recreate the content before submitting.\n\n[May explain why AI tools are not allowed, e.g., This course is designed to challenge your creative, analytic, and critical thinking skills, and using generative AI tools can undermine your learning of these skills.\n\nThis is true even in cases where generative AI tools may be used to replicate portions of assignments or assessments we are asking you to complete.]\n\nTherefore, generative AI tools are considered “unpermitted aid” according to the Stanford Honor Code for all student work in this course, and their use will be considered plagiarism.",
    "### Important Note:\n\nTechnology is changing at a rapid pace.\n\nWhile we make every attempt to ensure our content is updated to reflect changes to the interface and functionality, we can only guarantee the accuracy of the content on this resource page when it was written or recorded.\n\nPlease be sure to check the software developer's website for the latest updates and release notes for the most up-to-date information.\n\nIf you have questions or concerns, or need additional support, please contact us.\n\nUnless otherwise noted, you may copy and redistribute this resource in its unmodified form under the terms of the Creative Commons BY-NC-ND 4.0 International license (attribution, non-commercial, no-derivatives).",
    "## Teaching and Learning Hub\n\nStanford Graduate School of Business  \n655 Knight Way  \nStanford, CA 94305  \n\nCompanies, Organizations & Recruiters Stanford Community  \nNewsroom Jobs Library myGSB Dashboard  \nContact GSB Visit Campus Stay in Touch  \n\n© Stanford Graduate School of Business  \n\nNon-discrimination Policy Privacy Policy Accessibility Terms of Use Stanford University",
    "## Quick Link to Course Activity Ideas\n\nGenerative artificial intelligence (GenAI) systems such as ChatGPT, Bard, Stable Diffusion, and DALL•E2 are digital tools that generate content based on prompts provided by users.\n\nGiven a user prompt, a GenAI tool uses algorithms to learn patterns from existing data sets (such as internet databases) and then produces new content - often in a matter of seconds.\n\nDesigners have created GenAI systems that can generate natural language text, computer code, images, video, audio, and 3D models.\n\nSeveral hundred GenAI systems are now available, including tools that assist with scholarly research (e.g., ResearchRabbit, Semantic Scholar, or Consensus).\n\nThe wide availability of GenAI systems and the ease and speed with which they can generate content raises important considerations for teaching and learning in higher education.\n\nThis resource provides instructors with suggestions and options for how to address AI use in their courses, plus links to additional resources.\n\nBecause the GenAI field is fast evolving, this resource will be continually updated.\n\nWe welcome suggestions for additional resources and information.",
    "### Learning-Centered\n\nLike any tool students might use to engage in the work of a course—from library books to research databases to internet search engines—GenAI systems present opportunities for students to learn important skills, including creativity, critical thinking, ethical decision-making, and discerning use of resources, among others.\n\nWe encourage instructors to talk explicitly with students about the pluses and minuses of GenAI systems as they help or hinder learning in a course.\n\nIn this way, regardless of one's view of GenAI or concern with its implications, the emphasis is on learning and what might assist or inhibit the learning process.\n\nFor example, if a course learning objective is to help students develop their own voices and perspectives through reflective writing, then use of GenAI systems to produce text that ostensibly represents \"personal voice\" or \"perspective\" might undermine that objective.\n\nAlternatively, if a learning objective is to help students structure and refine the presentation of arguments in writing, a GenAI system might play a supportive role in the process of students' learning how to organize an argument in written form (which is not the same as a student trying to use a GenAI system to formulate the substantive claim—with accompanying reasoning and evidence—that is being structured in written form).",
    "### Transparent\n\nFaculty and GE instructors have flexibility in how they approach newly available GenAI tools, and this means students will shift among a variety of approaches in their different courses.\n\nIt is therefore essential for instructors to prioritize transparency so that students know clearly the GenAI policy in each individual class and the expectations for specific course activities and assignments, especially if the parameters for GenAI use vary among the latter.\n\nIn addition to including a GenAI course policy on the syllabus and explicit details about GenAI use in assignment instructions, we encourage faculty and GE instructors to talk explicitly with students about the rationale for their policy and expectations, including the relationship to students' learning, as suggested above.",
    "## Course Policies\n\nWe strongly encourage instructors to have an explicit policy about GenAI in their course syllabus, including any relevant distinctions between GenAI use (as process) and GenAI content (as product).\n\nWe also encourage instructors to reinforce their expectations in assignment instructions and in conversation with students.\n\nBased on helpful considerations articulated by Josef Brandauer and Melissa Forbes at Gettysburg College, the sample course policies below indicate a range of options that instructors can adapt depending on their specific course context and student learning goals.\n\nWe also encourage exploration of the Syllabus Resources from the Sentient Syllabus Project, which offers a variety of considerations and example policies that can be used or adapted.\n\nInstructors can also find dozens of sample course policies at this crowdsourced document, including an option to search by discipline and course topic.",
    "## Course Assignments and Class Activities\n\nInstructors are experimenting with a variety of ways to include use of GenAI systems in their courses, including both classroom activities and assignments.\n\nBelow are a few ideas to consider, including examples from UO instructors, and we also list a few ideas for those wanting to mitigate use of GenAI.\n\nYou can find additional ideas for assignments and activities in this resource on ChatGPT Assignments to Use in Your Classroom Today, as well as this resource on Teaching with Text Generation Technologies.\n\nAlso consider this helpful decision tree for reviewing your course assignments, at the University of Michigan Course and Assignment (Re-)Design page.",
    "## Data Privacy and Security Considerations\n\nUO Information Services reminds all faculty, staff, and students that:\n\nThe University of Oregon does not have a contract with popular AI systems such as ChatGPT, Bard, and DALL•E2, which would include the security compliance assessment of such systems to gauge whether they satisfy requirements associated with the university academic, operational, business, and research needs.\n\nIn addition, the privacy policies associated with the use of these systems require the users to acknowledge that the data input into the AI system will remain a part of the environment.\n\nAs such, we recommend that users exercise caution when interacting with these systems, to avoid unintended release of intellectual property, copyrighted materials, or trade secrets.\n\nWe therefore strongly recommend that instructors who ask or encourage students to use any AI system remind students that they should avoid providing any personal or other sensitive data to AI prompts.\n\nWe also advise that instructors consider making AI use voluntary or, if AI use is part of a required course assignment or activity, include an opt-out alternative for students who do not want to create an account with an AI system or interact with them.\n\nSuch a recommendation is in alignment with UO's policy on external vendor digital tools, for instance, use of social media such as blogs as part of course assignments, which must include an option for students to keep their information and identities private.",
    "### How to Prompt GenAI\n\n- AI Prompts for Teaching is a compendium of prompts that instructors can use to discern how GenAI would respond to their course assignments, materials, etc.\n\nIt also includes prompts for how students can get GenAI feedback on various aspects of their work.\n\n- What is a Prompt Engineer?\n\nprovides a basic conceptual introduction to prompts and prompt engineering, by Ramón Alvarado, UO Assistant Professor of Philosophy.",
    "### What GenAI Is and How it Works\n\n- Alchemy: AI in Higher Education (series of free webinar recordings)\n- Modern Language Association: What AI Means for Teaching (webinar recording requires free sign-up)\n- UO Presidential Initiative in Data Science: The Ethics of AI Explained, featuring Ramón Alvarado, UO Assistant Professor of Philosophy\n- Writing Across the Curriculum Clearinghouse: AI Text Generators: Sources to Stimulate Discussion among Teachers\n- Writing Across the Curriculum Clearinghouse: AI Text Generators and Teaching Writing: Starting Points for Inquiry",
    "## Helpful Articles\n\n- Artificial Intelligence and the Future of Teaching and Learning: Insights and Strategies by the U.S. Department of Education, Office of Educational Technology\n- A Blueprint for an AI Bill of Rights for Education by Kathryn Conrad\n- Should you Trust an AI Detector?\n\nby Kristi Hines\n- The End of the Take-Home Essay?\n\nby Corey Robin\n- New Modes of Learning Enabled by AI Chatbots by Ethan Mollick and Lilach Mollick\n- Using AI to Implement Effective Teaching Strategies in Classrooms by Ethan Mollick and Lilach Mollick\n- ChatGPT Advice Academics Can Use Now by Susan D'Agostino\n- Adapting College Writing for the Age of Large Language Models such as ChatGPT: Some Next Steps for Educators by Anna Mills and Lauren M. E. Goodlad\n- Embrace the Bot: Designing Writing Assignments in the Face of AI by Eric Prochaska\n- I'm a Student.\n\nYou Have No Idea How Much We're Using ChatGPT by Owen Kichizo Terry\n- Students' Voices on Generative AI: Perceptions, Benefits, and Challenges in Higher Education by Cecilia Ka Yuk Chan\n- Everybody is cheating: Why this teacher has adopted an open ChatGPT policy by Patrick Wood and Mary Louise Kelly\n- Teaching: What You Need to Know About ChatGPT by Beth McMurtrie\n- Decoding the Hype About AI: A conversation with Arvind Narayanan by Julia Angwin",
    "## Instructor Guidelines for Student Use of Generative Artificial Intelligence for Academic Work\n\nThe University of Southern California's Academic Senate Committee on Information Services recommends that all USC schools, academic departments, and instructors adopt the following guidelines regarding student use of generative artificial intelligence for academic work.\n\nInstructors should encourage USC students to explore generative artificial intelligence (AI), using these new tools to create, analyze, and evaluate new concepts and ideas that inspire them to generate their own academic work.\n\nIn advance of this exploration, instructors should help students recognize that some contemporary AI-generated content may be specifically designed to appear plausible and persuasive but is sometimes factually inaccurate.\n\nMany of the issues that have surfaced with the introduction of ChatGPT – questions about academic integrity, authorship and citations, student engagement, misinformation and disinformation – are issues higher education and society have encountered in the past in response to the need for digital literacy.\n\nWe suggest that generative AI is simply the newest addition to USC’s digital literacy tools.\n\nUltimately, this committee leaves instructors to set their own course policies regarding student use of generative AI.\n\nWhatever any given individual instructor decides should be clearly communicated to students in course materials.\n\nHowever, the committee recommends that instructors remind students that the acquisition of academic work in whole or in part from any source (from textbooks and journal articles to web resources to generative AI) and the subsequent presentation of those materials as the student's own work (whether that material is paraphrased or copied in verbatim or near-verbatim form) constitutes an academic integrity violation unless otherwise allowed by the instructor.\n\nIndividual assignments and exams may have additional, specific requirements related to original work which should be clearly defined by the instructor.\n\nBecause generative AI is a constantly evolving space, the committee encourages USC’s instructors to begin to learn more about generative AI so they can better adjust their pedagogy and evolve as educators.\n\n_Last updated: 13 February 2023_",
    "### Limitations of Guidelines\n\nOur focus in this document is narrow: to highlight the steps that USC’s instructors need to take today to identify and to state their expectations with respect to student use of generative AI for academic work.\n\nIn the weeks and months to come, we expect that USC’s academic community will be asked to participate in broader conversations about generative AI and its place in academe and society.",
    "### What is ChatGPT and what can it do?\n\nIt is hard to overstate how much attention ChatGPT, a free online artificial (AI) chatbot that generates text in response to prompts, has received since its launch on November 30.\n\nOne reason it has become so popular so quickly, surpassing 100 million monthly active users to become the fastest-growing consumer internet application in history, is the ease and speed with which it can generate text on demand.\n\nAsk ChatGPT to \"write a five-paragraph essay on the impact John Stuart Mill's recantation of the wages fund doctrine had on classical economics\" or \"explain what it means when people say, 'knowledge is knowing that Frankenstein is not the monster but wisdom is knowing that Frankenstein is the monster'\" or \"write a sonnet about Clay Helton\" and it creates plausible, human-like text responses in a matter of seconds.\n\nScholars, researchers, and educators have demonstrated that, given the right prompts, ChatGPT can:\n\n- Pass the final exam of an MBA-level Operations Management class at Wharton.\n\n- (Barely) pass questions on a law school exam at the University of Minnesota.\n\n- Pass all three exams that comprise the United States Medical Licensing Examination.\n\n- Pass Google's coding exams and interviews at the level of an entry-level software engineer with no prior industry experience.",
    "### Are students already using ChatGPT for academic work?\n\nYes.\n\nStudents are starting to use ChatGPT for their academic work.\n\nIn a late-2022 nationwide survey of 1,000 students currently enrolled in US colleges and universities, nearly one-third of the students said they have already used ChatGPT to complete a written college assignment and nearly two-thirds of that group say that they have used it for 50% or more of their assignments.\n\nWe expect both percentages to be even higher today.",
    "### Are there other generative AI text tools like ChatGPT?\n\nYes.\n\nChatGPT is just the first in a wave of generative AI tools that will soon become ubiquitous.\n\nGenerative AI is algorithms and tools that can be used to create new content, including audio, code, images, text, simulations, and videos.\n\nRecent new breakthroughs in the field have the potential to drastically change the way we approach content creation.\n\n(McKinsey & Company) Most of these new tools rely on ‘large language models,’ AI systems that use advanced statistical techniques to analyze and understand natural language data, such as text or speech, and generate human-like responses.\n\nMicrosoft, already a major investor in OpenAI (makers of ChatGPT), launched a new AI-powered Bing search engine and Edge browser in early February 2023 and intends to incorporate AI-content generation into Microsoft Office programs like Outlook, Word, and PowerPoint.\n\nLater in 2023, Microsoft will release technology that will allow companies, schools, and governments to create their own custom ChatGPT-powered AI text generators.\n\nGoogle is expected to launch its own ChatGPT competitor named Bard in early 2023.\n\nIn the summer of 2022, Meta (formerly Facebook) released its Open Pretrained Transformer (OPT) large language model to developers and researchers.\n\nAnd that is just the beginning (see  for a list of hundreds of new AI tools that be used to generate text, images, audio, and more).\n\nConsidering this, we suggest that rather than focusing solely on ChatGPT, instructors should instead focus on what role they would like all generative AI tools to play in their classes and in their students' work going forward.",
    "#### Embrace and Enhance\n\nThe good news is that many of the proven teaching and assessment techniques that worked in a pre-generative AI world still work in a world where any student with a cell phone and a generative AI account can create walls of academic(-sounding) text.\n\nUSC's Center for Excellence in Teaching (CET) recently published a guide titled \"Using AI text, image, and music-generating tools in your courses\" that includes helpful ideas for incorporating AI-generators and AI-generated content in your course, including evaluating and critiquing AI-generated content (including interrogating the content for biases) and asking students to create rebuttals.\n\nLike our recommendation that students should use generative AI to create, analyze, and evaluate new concepts and ideas that inspire them to generate their own academic work, CET recommends having your students use AI generators to brainstorm ideas, formulate and iterate question prompts, and refine responses, adding that you should:\n\n- Frame using AI tools as something to build upon.\n\nRemind students of the best way to use these tools in their discipline, such as for idea generation, essentializing, brainstorming, or gathering information about the typical understanding of a topic.\n\nAll uses of AI tools should be supplemented with appropriate evidentiary support and reflection.\n\nSome students may lack the foundational knowledge to understand why or how AI-generated content is inaccurate.\n\nThis presents an excellent teaching opportunity for you to demonstrate for your students generative AI's strengths and weaknesses.\n\nCET also offers several suggestions on designing assignments and assessments in the age of AI generators, including:\n\n- Asking more nuanced questions, beyond simple definitions and common comparisons, related to the course text, articles, media, or activities that may be unknown to or beyond the capabilities of current AI generators,\n- Having students complete assignments and assessments during class time,\n- Requiring students to submit drafts of their papers or projects before they submit their finished work, and\n- Augmenting written papers with additional oral presentations, concept maps, group work, or case studies so that students can further demonstrate their understanding of the course objectives.\n\nIf you are going to allow your students to use generative AI as a source in their academic work, you may want to consider requiring your students to clearly disclose the role generative AI played in formulating their work.\n\nOpenAI also has recommended language that you may want to adapt and adopt: \"The author generated this text in part with GPT-3, OpenAI’s large-scale language-generation model.\n\nUpon generating draft language, the author reviewed, edited, and revised the language to their own liking and takes ultimate responsibility for the content of this publication.\"",
    "#### Discourage and Detect\n\nSome educators have asked how they can block student use of ChatGPT (see New York City Department of Education and others).\n\nWe suggest that that would be akin to standing on the shore hoping to block a rising tide.\n\nGenerative AI is here and is not going away.\n\nThat said, if you wish to discourage student use of generative AI, let your students know this expectation both in your syllabus and in class.\n\nThe guidelines listed at the beginning of this document should serve as a good starting point.\n\nYou may also consider adapting and adopting something like Science Journals' artificial intelligence (AI) policy: \"Text generated from AI, machine learning, or similar algorithmic tools cannot be used in papers published in Science journals, nor can the accompanying figures, images, or graphics be the products of such tools, without explicit permission from the editors.\n\nIn addition, an AI program cannot be an author of a Science journal paper.\n\nA violation of this policy constitutes scientific misconduct.\"\n\nAnother approach is Nature Journals' large language model guidelines: \"Large Language Models (LLMs), such as ChatGPT, do not currently satisfy our authorship criteria.\n\nNotably an attribution of authorship carries with it accountability for the work, which cannot be effectively applied to LLMs.\n\nUse of an LLM should be properly documented in the Methods section (and if a Methods section is not available, in a suitable alternative part) of the manuscript.\"\n\nIn class, you can discourage student use of generative AI by encouraging students to use other tools and techniques instead.\n\nIn fact, many of the teaching and assessment techniques recommended by the CET – ask more nuanced questions, have students complete assignments and assessments during class, require students to submit drafts, augment written papers with other activities that demonstrate students’ content knowledge – work equally well if you want to embrace and enhance or discourage and detect student use of AI generators.\n\nHowever, we do not consider requiring handwritten assignments to be an effective technique to discourage or detect.\n\nStudents who have academic accommodations may need to use assistive technology in your class.\n\nProhibiting student use of technology or requiring that all students handwrite their work may create a situation that singles out students with accommodations if they can use technology while others cannot.\n\nAs for detecting if students' typed academic work contains AI-generated text, the best way is to honestly grade that work and look for errors.\n\nSome, if not most, contemporary AI-generated text is specifically designed to appear plausible and persuasive but is not necessarily accurate.\n\nOpenAI cautions that \"ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers.\"\n\nBecause of this, current generation AI text generators have the propensity to make easily discernable, fundamental mistakes that a subject matter expert or even an 'experienced novice' would never make (see \"CNET's Article-Writing AI Is Already Publishing Very Dumb Errors\", \"Why Meta's latest large language model survived only three days online\", \"ChatGPT Needs Some Help With Math Assignments,\" and \"Alphabet shares dive after Google AI chatbot Bard flubs answer in ad\" for some recent examples).\n\nEven if students rewrite AI-generated text to avoid detection, the structural and factual errors in current-generation AI-generated text should remain.\n\nBut those detectable weaknesses may not last forever, especially with future generations of AI text generators.\n\nAnother option is to use a 'similarity detector' (often mistakenly called a 'plagiarism detector') like Turnitin.\n\nTurnitin, which is available in every USC Blackboard course, scans submitted text and highlights any phrases or paragraphs that are identical or closely like other sources known to Turnitin.\n\nSince some AI text generators have been known to copy from other sources without attribution – see \"CNET's AI Journalist Appears to Have Committed Extensive Plagiarism\" – tools like Turnitin may detect AI-generated text.\n\nOr not.\n\nTurnitin has announced that it will roll out additional, built-in AI writing and ChatGPT detection features soon.\n\nOpenAI, makers of ChatGPT, recently released their own AI Text Classifier tool at  that \"predicts how likely it is that a piece of text was generated by AI from a variety of sources.\"\n\nThe tool currently \"requires a minimum of 1,000 characters, which is approximately 150 - 250 words.\"\n\nPrinceton student Edward Tian also developed a popular tool named GPTZeroX at  that \"highlights portions of text that are most likely to be AI generated\" and allows you batch upload files \"in PDF, Word, and .txt format.\"\n\nOne word of caution: Students may be able to circumvent these tools.",
    "One word of caution: Students may be able to circumvent these tools.\n\nAs Melissa Heikkilä noted in an article in the MIT Technology Review in December, \"[b]ecause large language models work by predicting the next word in a sentence, they are more likely to use common words like 'the,' 'it,' or 'is' instead of wonky, rare words.\"\n\nWith this in mind, Michael Webb at the National Center for AI in Tertiary Education found a simple technique to fool many ChatGPT detectors, including GPTZeroX:\n\n- Have the AI text generator create some text.\n\n- Then ask the AI generator to \"Use the word 'the' less\".\n\nWe tried this technique and were able to 'fool' both OpenAI’s AI Text Classifier and Tian's GPTZeroX into claiming that text copied straight from ChatGPT was most likely written by a human and not AI.",
    "## Conclusion\n\nThe age of AI-generated content is upon us.\n\nIn a little over two months, OpenAI’s ChatGPT has created a 100 million user market for AI-generated text content that did not exist at scale last fall.\n\nAnd ChatGPT is just the first in a wave of generative AI tools that will soon become ubiquitous.\n\nBecause of that, rather than focusing solely on ChatGPT, we strongly recommend that instructors instead focus on what role they would like all generative AI tools to play in their classes and in their students' work going forward – either by embracing and enhancing or discouraging and detecting students’ use of this technology.",
    "## Additional Resources\n\n- USC Center for Excellence in Teaching\n- AI Generators in the News\n- EDUCAUSE QuickPoll Results\n- MIT Technology Review – Artificial Intelligence\n- The Register – Artificial Intelligence\n- National Centre for AI in Tertiary Education\n- Santa Fe Community College Library's Repository of Information about the Impact of ChatGPT on/in Higher Education",
    "# Guidance for Faculty on AI-Based Writing Tools\n\nArtificial intelligence tools that can conduct research, synthesize course materials and other content, and produce coherent writing have become a focus of discussion in higher education.\n\nChatGPT, Bard, Copy.ai, and similar tools are powerful algorithmic language models that use artificial intelligence to generate writing.\n\nThey have been trained on a vast amount of data, allowing them to generate convincing responses to a wide range of prompts.\n\nDue to ChatGPT’s capabilities and accessibility, many students are starting to use it and other AI-based tools to complete or contribute to assignments in various fields of study, from composition to computer programming.\n\nAs these tools are already being integrated into word processors, email apps, and browser plug-ins, we can expect them to become even more ubiquitous and integrated into daily life in the years to come.",
    "## Guidance and Recommendations\n\nHere we provide some guidance for instructors related to these emerging technologies:\n\n- **Communication:** Clearly communicate and establish a policy for the use of AI-generated text in your course through the syllabus and assignment prompts.\n\nOutline the steps to be taken in case of suspected academic misconduct involving AI tools.\n\nMake it clear when you allow or encourage the use of AI as part of particular assignments.\n\n- **Explanation:** Clarify with students that struggle, challenge, and uncertainty are an integral part of learning and that the use of AI tools as a shortcut or workaround to such learning may undermine their goals of becoming informed, critical thinkers.\n\nEmphasize the value and relevance of the skills and knowledge being taught in the course, and how they are integral to a liberal arts education.\n\n- **Writing as Process:** Emphasize to students the value of writing as a process that yields learning outcomes beyond the end product itself: learning to develop and draft ideas, editing and clarifying text, reviewing and discussing feedback, revising and refining the final draft.\n\nNote the enduring value of these skills even within an environment where AI use is prevalent.\n\n- **Intentional Assignment Design:** Design assignments that require students to connect in novel ways to course content, class discussions, and personal experience.\n\nConsider in-class assignments, oral presentations, and question-and-answer sessions that accompany/support written assignments (however, keep in mind how different assessment methods can potentially create barriers for students with disabilities).\n\nThis article provides additional suggestions for intentional, process-focused assignment design.\n\n- **Assessing Process:** Consider ways to prioritize assessment of the learning process over (only) the end product by implementing low or no-stakes formative assignments to encourage students to build their skills over the semester, and by using in-class writing to establish benchmarks.\n\n- **Incorporate AI:** Consider productive ways to incorporate AI-based tools into your teaching to prepare students for the use of technology in their future personal and professional lives.\n\nAI writing tools are increasingly impressive in their capabilities, but are still limited — familiarization with them can help encourage critical thinking about digital literacy, sources of evidence, writing style, tone, and what constitutes effective written communication.",
    "## Academic Honesty\n\nOne of the main concerns many faculty have with the use of ChatGPT and other AI-based tools in academic settings is the potential for academic misconduct.\n\nAs these tools are constantly evolving and students may use their text in different ways, it is challenging to accurately identify AI-generated student submissions.\n\nBecause AI writing detectors are unreliable and easily subverted by light editing of the text, we recommend focusing on the strategies outlined above to build assignments and assessment approaches that emphasize academic honesty and critical thinking.\n\nPlease keep in mind that we are preparing our students to live, work, and thrive in a world that will be saturated with AI tools.\n\nThis does not mean students should be using AI tools to unethically complete assignments (see \"Violations of Academic Integrity\" below) but rather that students would be best served by robust engagement with AI rather than only avoidance or prohibition.",
    "## Resources\n\n- Tim Laquintano, Carly Schnitzler, and Annette Vee.\n\nAn Introduction to Teaching with Text Generation Technologies.\n\nThe WAC Clearinghouse, 2023.\n\n- How do we prevent learning loss due to AI text generators?\n\n(A collaborative google doc led by Anna Mills; includes practical suggestions for assignments that challenge AI tools)\n- AI Text Generators: Sources to Stimulate Discussion among Teachers (a continuously updated, collaborative bibliography on AI language generators and teaching — if you’re interested in reading more about AI writing tools, this is an excellent source)\n- McKnight, Lucinda.\n\n\"Robot writers in education: Cheating… or world-beating?\"\n\nScan: The Journal for Educators 40, no.\n\n6 (2021): 4-6.\n\n- “The Impact of AI on Writing and Writing Instruction,” Digital Rhetoric Collaborative blog post from H. McKee and J. Porter, April 25, 2018.\n\n- C.M.\n\nAnson and I.S.\n\nStraume, “Amazement and trepidation: implications of AI-based natural language production for the teaching of writing,” Journal of Academic Writing, 12, no.\n\n1 (Winter 2022): 1-9.\n\n- El-Sayed Abd-Elaal, Sithara HPW Gamage, and Julie E. Mills.\n\n\"Artificial intelligence is a tool for cheating academic integrity.\"\n\nIn 30th Annual Conference for the Australasian Association for Engineering Education (AAEE 2019): Educators Becoming Agents of Change: Innovate, Integrate, Motivate, pp.\n\n397-403.\n\nBrisbane, Queensland: Engineers Australia, 2019.\n\n- Beth McMurtrie, “AI and the future of undergraduate writing,” The Chronicle of Higher Education, Dec. 13, 2022.\n\n- “Adapting College Writing for the Age of Large Language Models such as ChatGPT: Some Next Steps for Educators” in Critical AI Jan 17, 2023.",
    "## Violations of Academic Integrity\n\nAt Puget Sound, violations of academic integrity are taken very seriously as they threaten the atmosphere of trust, fairness, and respect essential to learning and the dissemination of knowledge.\n\nClaiming the work of others as your own, whether created by another human or an artificial intelligence, is regarded as plagiarism, and as such is a violation of academic integrity.\n\nIn situations involving suspected violations of academic integrity, procedures and sanctions established for the Hearing Board will be followed.\n\nStudents are expected to be aware of and to abide by the university’s Academic Integrity Policy.\n\nAdditionally, faculty members are urged to review university and course policies regarding academic integrity in their classes to ensure students clearly understand them.\n\nThis document was developed by the Office of the Academic Deans in robust collaboration with the Center for Writing and Learning, the Faculty Development Center, Collins Memorial Library, Educational Technology, and the Academic Standards Committee.\n\nIt will be updated if and when any relevant policies change, as well as to add new resources and guidance.\n\nImage produced with OpenAI’s ChatGPT 4.\n\n---",
    "# Principles of Generative AI A Technical Introduction\n\nGenerative artificial intelligence (GenAI) tools are an emerging class of new-age artificial intelligence algorithms capable of producing novel content—in varied formats such as text, audio, video, pictures, and code—based on user prompts.\n\nRecent advances in machine learning (ML), massive datasets, and substantial increases in computing power have propelled such tools to human-level performance on academic and professional benchmarks, comparable to the ninetieth percentile on the SAT and the bar exam.\n\nThis rapid progress has led many to believe that the metamorphosis of these technologies from research-grade demos to accessible and easy-to-use production-grade goods and services carries the potential to supercharge business processes and operations while enabling entirely new deliverables heretofore rendered infeasible by economic or technological factors.\n\nIt took OpenAI’s ChatGPT, a conversational web app based on a generative (multimodal) language model, about five days to reach one million users (compared to 2.5 months for Instagram).\n\nOn the business side, the Economist reports that the number of jobs mentioning AI-related skills quadrupled from 2022 to 2023.\n\nThis enthusiasm has not gone unmet by investors.\n\nGenerative AI startups reportedly raised 600% more capital in 2022 than in 2020.",
    "## Purpose and Scope\n\nWhat are these new-era AI technologies?\n\nHow do they function?\n\nWhat principles do they operate on?\n\nWhat makes them different than already-hyped-up conventional machine learning (ML) models?\n\nFor what tasks is this class of technology most impactful?\n\nWhat future advances might one look forward to?\n\nThese are the questions this report attempts to shed some light on.\n\nThe report will also tease out how this understanding foundationally informs the best uses (and misuses) of GenAI in applied contexts.\n\nA word of disclaimer: this gradient of topics also means that, while the initial sections deal with factual, if somewhat simplified, nuts-and-bolt workings of such models, the later sections delve into hopefully reasonable, but in a manner that only time may attest to, extrapolations and speculations, as necessitated by the developing nature of this technology and its current phase in the technology adoption cycle.\n\nWhile generative AI models come in many different shapes, utilizing varied statistical and computational techniques to target various modalities, ranging from code and text to audio and video, this report focuses almost exclusively on large language models (LLMs) capable of generating novel text from textual prompts.\n\nThis choice is partly due to the substantial lead LLMs have in driving the overall usage of generative AI models and partly due to the centrality of language in formulating and addressing commonplace information-processing tasks.\n\nThat said, image- and code-based GenAI models have already witnessed successful commercial product deployment, for example, by Adobe for creating visual content and by Github as a programming assistance tool.",
    "## A Quick First Introduction to Language Models\n\nAt its core, a language model implements a simple functionality—to predict the next word (or token) given a context window specifying preceding words.\n\nMore precisely, given a context window, a language model outputs a probability distribution over all possible words in its vocabulary, indicating the probability with which each possible word follows the given list of words.\n\nUpon sampling a guess of the next word from the said distribution, the language model incrementally repeats this ostensibly primitive step to produce a more extensive body of text.",
    "### We make two observations here:\n\n1.\n\n**Completions are random.\n\n** The predicted completion, given a context window, is not deterministic.\n\nSampling the next word in each step from the output distribution introduces enough randomness to permit that the predicted completions could be meaningfully different on every fresh run.\n\nThis stochasticity is why ChatGPT, for instance, can offer varied answers for the same prompt across successive runs.\n\nReplacing the sampling step with choosing (greedily) the most likely immediate word is known to degrade the quality of the produced text.\n\nThe randomness in responses is also desirable from a user perspective in getting varied responses.\n\nFrom the deployer’s perspective, this optionally allows the model to gather user feedback regarding the quality of seemingly plausible responses.\n\nThis choice partly also contributes to hallucination in language models.\n\n2.\n\n**Initial prompt matters.\n\n** Language models are conditional probabilistic models.\n\nThey produce a completion conditioned on the initial set of words.\n\nIn this way, the initial context window, termed prompt, matters crucially to the produced completion.\n\nOne hallmark of modern language models is that they keep track of the initial prompt even when generating large bodies of text, unlike the earlier generation of models, thus producing more coherent responses.\n\nArtful and cleverly crafted prompts can significantly improve the quality and utility of the synthesized text.\n\nPrompt engineering, for example, practices that encourage the language model to solve a problem by decomposing it into intermediate subproblems, has been known to improve the performance on logical reasoning tasks.",
    "## Contextualizing LLMs in terms of Recent AI Advances\n\nAlthough we describe the text generation procedure above, many questions still need to be addressed: How do language models function internally?\n\nHow are the output probabilities for the next word determined?\n\nWhat goes into creating (and indeed using) a language model?\n\nHow are language models different from more traditional predictive models if all they do is predict the next token?\n\nWe address these questions indirectly in the present section by taking a tour of the essential significant developments in machine learning and artificial intelligence that have occurred in the last decade and have fueled the creation of modern large language models.",
    "### Classical Machine Learning as Prediction Machines\n\nWe start with the most well-understood subset of machine learning techniques: supervised learning.\n\nThe central objective in supervised learning is to produce a prediction rule that predicts well on unseen data, given enough labeled examples.\n\nFor example, consider predicting house prices from the square footage in a given zip code.\n\nInstead of creating a hand-crafted prediction rule, the machine learning methodology advocates for choosing a prediction rule from an expressive but non-exhaustive class of rules, such as linear predictors, that provides the best fit on an existing collection of size-price examples.\n\nThe statistically well-substantiated leap of faith here is that we expect (or at least hope) that a parsimonious prediction rule that predicts well on collected data, for which we know the correct answers, continues to maintain its predictive edge on unseen data, where answers or prices are unknown.\n\nSuch a predictive methodology benefits from an abundance of labeled examples, hoping that a prediction rule learned from more examples is more robust in that its superior predictive performance on seen data is less ascribable to chance alone.\n\nAnother example of a supervised learning task is to separate spam from non-spam mail, given the text in email messages.\n\nAgain, having more examples of spam and non-spam emails is helpful to a supervised learning algorithm.",
    "**Characteristics common to both language models and supervised learning:**\n\n- **Predicting Well is the Yardstick.\n\n** A prediction rule is good as long as it makes reasonable predictions on average.\n\nCompared to more ambitious sub-disciplines in statistics, any statements about causality, p-values, and recovering latent structure are absent.\n\nWe are similarly impervious to such considerations in language models.\n\nSuch simplicity of goals enables very flexible prediction rules in machine learning.\n\nAlthough seeming modest in its aim, the art of machine learning has long been to cast as many disparate problems as questions about prediction as possible.\n\n- **Model Agnosticism.\n\n** Supervised learning algorithms realize the adage that all models are wrong, but some are useful.\n\nFor example, when building the price predictor above, a data scientist does not believe that the genuine relationship between prices and area is linear or well-specified.\n\nSimilarly, when using neural networks to predict the next word in language models, we don’t believe that this is how Shakespeare must have employed a neural network to compose his texts.",
    "**Yet, there are crucial differences:**\n\n- **Fidelity of Seen Data vs.\n\nUnseen Data.\n\n** Classical supervised learning operates on the assumption that seen data must be representative of unseen data in a particular sense, namely that any fixed example is equally likely to be in the seen or unseen bucket.\n\nIn the absence of temporal effects, this is reasonable for house prices.\n\nMore generally, supervised learning requires a well-curated dataset that is closely aligned with the prediction task at hand.\n\nBut, as we will see, language models are trained on vast corpora of somewhat ruthlessly collected texts from the internet.\n\nYet, completing a random partial sentence from the internet is presumably not what businesses using language models care about.",
    "## Deep Learning as Automated Representation Learning\n\nAlthough useful for panel or tabular data, pre-deep-learning-era supervised algorithms struggled to predict well when presented with visual or auditory inputs.\n\nAlthough the promise of machine learning is predicated on the automation of learning, in practice, supervised learning algorithms require carefully crafted representations of input data in which operations like additions and multiplications, for example, for linear regression, were semantically relevant.\n\nDecades of painstaking research in signal processing and computer vision had resulted in domain-specific hand-crafted representations, each useful for a specific modality (images, audio, or video).\n\nThe predictive performance of ML algorithms was limited by how good such representations were.\n\nThe revolution in deep learning was to automate the process of representation learning itself.\n\nDeep learning uses neural networks with multiple layers, each layer incrementally converting the data into a more manageable form, all to make better predictions.\n\nThis form of automated hierarchical representation learning heralded a decade of tremendous progress in image and speech recognition and machine translation, starting with the breakthrough work of Krizhevsky, Sutskever, and Hinton in 2012 on the Imagenet challenge.\n\nTaking advantage of GPUs (a form of shared-memory parallel computing) and the availability of a large public dataset, this seminal work slashed the error rate for image recognition by a substantial multiple.\n\nParallel gains were later realized using similar deep neural network architectures in speech recognition and other machine learning domains.\n\nIn this sense, the advances deep learning enabled were (relatively) domain agnostic.\n\nAlthough deep neural networks are data-hungry in that they require a substantially large dataset to start predicting well, they also successfully realize a long-promised advantage of neural networks.\n\nThis factor is crucial to the practice of modern-day machine learning.\n\nIn the process of hierarchically learning representations, deep nets learn task- (or label--) agnostic features of the dataset in the lower layers, while higher layers closer to the output account for task-specific representations.\n\nThis permits us to (a) train a deep net to separate images of cats and dogs on a large dataset and (b) subsequently build a shallow (even linear) performant neural net that uses the lower layers of the former to craft useful representations to classify images of zebra and giraffes.\n\nStep A is often called pre-training, and step B is referred to as supervised fine-tuning.\n\nThis manner of amortizing the learning across tasks that are not individually data-rich is central to language models.",
    "## Word Embeddings and Contrastive Learning\n\nWhile the progress of deep learning in speech and audio was made possible by the availability of large crowd-labeled datasets (with 10s of millions of annotated images), such large high-quality datasets were absent in the textual domain, despite a plethora of unlabelled data in the form of books, Wikipedia articles, and articles on the internet.\n\nCould a machine learning algorithm make use of the cheap, unlabelled data instead?\n\nIn computational linguistics, the distributional hypothesis codifies an appealing and intuitive idea that similar words occur in similar contexts.\n\nIn 2013, inspired by this observation, Mikolov et al trained a neural network, termed Word2Vec, to predict randomly selected words in a text corpus given neighboring words for each.\n\nNote that this step doesn’t require any need human annotators.\n\nThey observed that the 300-dimensional vector representations the neural net learned for words had excellent linear algebraic properties that transparently reflected the underlying semantics.\n\nFor example, one obtained Queen when queried for the word with the vector closest to King - Man + Woman.\n\nThus, each vector dimension captured some abstract semantic degree of freedom.\n\nThese representations were also valuable for natural classification tasks with limited data, such as sentiment classification, given a small number of examples.\n\nThe approach of creating auxiliary labeling tasks for free from unlabelled data to learn semantically relevant representation is called contrastive learning and has proved helpful in other domains, too.\n\nFor example, given a set of unlabelled images, a classifier trained to recognize random crops from the same image as a positive match and those from distinct images as a negative match (pre-training step) learns representations useful for supervised fine-tuning on genuine classification tasks downstream.",
    "## Transformers mollify the Optimization Landscape\n\nWhile word embeddings serve as proof that textual semantic regularities can be assessed without labeled data, substantive language processing tasks need an algorithmic implementation of the concept of memory to capture relationships between words that are positionally far apart.\n\nFor example, a common motif in stories is that the next act derives from some event that occurred a while ago.\n\nThe first generation of neural networks that captured the notion of memory were Recurrent Neural Networks (RNNs), by sequentially processing a piece of text one word at a time while updating an internal state to maintain continuity, a proxy for memory.\n\nUnfortunately, optimizing such recurrent neural nets to find one that best fits a given dataset proved extraordinarily error-prone and challenging.\n\nIn 2017, Vaswani et al introduced a different neural network architecture, termed transformer, that could efficiently capture long-range relations between tokens compactly (non-sequentially) by processing the entire surrounding context window at once while remaining amenable to gradient-based optimization.\n\nThe introduction of transformers spurred a line of research on language models, culminating in training models with an increasingly higher number of parameters trained on ever larger datasets.\n\nFor example, GPT2 (Generative Pre-trained Transformer 2), released in 2019, is a 1.5 billion parameter model trained on 40 GB of data, while GPT3, released in 2020, is a 175 billion parameter model trained on 570 GB of text data.\n\nWhile larger models resulted in better performance, the open-market cost for training these enormous models was estimated to be tens of millions of dollars.",
    "## General-Purpose Language Models: Supervised Fine-tuning & GPT3\n\nThe general paradigm brought about by contrastive learning was first to learn a large model on auxiliary tasks created using an unlabelled dataset (the pre-training step) and subsequently to use these learned representations in a downstream supervised learning task given a few task-specific labeled examples (the supervised fine-tuning step).\n\nWhile broadly useful and practical, supervised fine-tuning requires replicas of the baseline pre-trained model for each downstream task; further, the large size of language models makes running even a few steps of gradient-based iterative optimization for supervised learning prohibitive except on computationally expensive hardware setups.\n\nThe paper describing the architecture of the GPT3 model presents a far cheaper and more convenient way of repurposing pre-trained language models for specific downstream tasks, namely, by specifying a few labeled examples in the prompt before asking for a label or response for unseen data.\n\nThis mode of inference, in-context learning, does not require computationally expensive adjustments to the weights or parameters of an LLM and instead treats the entire downstream supervised task as a prompt for the language model to complete.\n\nThis makes LLMs very attractive for end-users, who no longer have to create copies of the large model to customize, nor do they have to run a sophisticated optimization procedure to adjust parameters; each downstream task, in effect, becomes a conversation.\n\nWhile fine-tuning may still result in additional performance gains over in-context learning for some tasks in exchange for a massive increase in computational load, a crucial advance of GPT3 is that this substantially lowers this gap, democratizing the use (although not the training) of LLMs.",
    "## Towards Conversational AI: Learning from Human Feedback\n\nWhile GPT3-like models happen to be good at conversation-centered tasks, they are not explicitly trained or incentivized to follow instructions.\n\nOpenAI’s InstructGPT model, post pre-training aligns the model to follow the users’ instructions by fine-tuning the model to mimic labeled demonstrations of the desired behavior (via supervised learning) and highly-ranked responses to prompts as collected using human feedback (via reinforcement learning).",
    "## The Future: Foundation Models\n\nGiven the success of language models, there has been increased interest in the possibility of recreating the magic of LLMs in other domains.\n\nSuch models, generically termed foundation models, attempt to amortize the cost of limited-data downstream tasks by pre-training on large corpora of broadly related tasks or unlabelled datasets.\n\nFor example, one might be able to repurpose the LLM paradigm to train a generalist robot or decision-making agent that learns from supply chain operations across all industries.",
    "## Conclusion\n\nThis report contextualizes large-language models within the more extensive machine learning and artificial intelligence landscape by training the origins of the principal ideas that fuel today’s large language models.\n\nBy bringing out their essential characteristics and differences against traditional modes of machine learning, we hope that a user of such models can be better informed of the underlying tradeoffs such models induce, e.g., the performance-resource tradeoffs between fine-tuning and in-context learning.",
    "# Artificial Intelligence and the Future of Teaching and Learning\n\nInsights and Recommendations  \nMay 2023  \n\nMiguel A. Cardona, Ed.D.\n\nSecretary, U.S. Department of Education  \nRoberto J. Rodríguez  \nAssistant Secretary, Office of Planning, Evaluation, and Policy Development  \nKristina Ishmael  \nDeputy Director, Office of Educational Technology May 2023  \n\nExamples Are Not Endorsements  \nThis document contains examples and resource materials that are provided for the user’s convenience.\n\nThe inclusion of any material is not intended to reflect its importance nor is it intended to endorse any views expressed or products or services offered.\n\nThese materials may contain the views and recommendations of various subject matter experts as well as hypertext links, contact addresses, and websites to information created and maintained by other public and private organizations.\n\nThe opinions expressed in any of these materials do not necessarily reflect the positions or policies of the U.S. Department of Education.\n\nThe U.S. Department of Education does not control or guarantee the accuracy, relevance, timeliness, or completeness of any information from other sources that are included in these materials.\n\nOther than statutory and regulatory requirements included in the document, the contents of this guidance do not have the force and effect of law and are not meant to bind the public.\n\nContracts and Procurement  \nThis document is not intended to provide legal advice or approval of any potential federal contractor’s business decision or strategy in relation to any current or future federal procurement and/or contract.\n\nFurther, this document is not an invitation for bid, request for proposal, or other solicitation.\n\nLicensing and Availability  \nThis report is in the public domain and available on the U.S. Department of Education’s (Department’s) website at   \n\nRequests for alternate format documents such as Braille or large print should be submitted to the Alternate Format Center by calling 1-202-260-0852 or by contacting the 504 coordinator via email at .\n\nNotice to Limited English Proficient Persons  \nIf you have difficulty understanding English, you may request language assistance services for Department information that is available to the public.\n\nThese language assistance services are available free of charge.\n\nIf you need more information about interpretation or translation services, please call 1-800-USA-LEARN (1-800-872-5327) (TTY: 1-800-437-0833); email us at ; or write to U.S. Department of Education, Information Resource Center, LBJ Education Building, 400 Maryland Ave. SW, Washington, DC 20202.\n\nHow to Cite  \nWhile permission to reprint this publication is not necessary, the suggested citation is as follows:  \nU.S. Department of Education, Office of Educational Technology, Artificial Intelligence and Future of Teaching and Learning: Insights and Recommendations, Washington, DC, 2023.\n\nThis report is available at",
    "# Introduction\n\nThe U.S. Department of Education (Department) is committed to supporting the use of technology to improve teaching and learning and to support innovation throughout educational systems.\n\nThis report addresses the clear need for sharing knowledge and developing policies for “Artificial Intelligence,” a rapidly advancing class of foundational capabilities which are increasingly embedded in all types of educational technology systems and are also available to the public.\n\nWe will consider “educational technology” (edtech) to include both (a) technologies specifically designed for educational use, as well as (b) general technologies that are widely used in educational settings.\n\nRecommendations in this report seek to engage teachers, educational leaders, policymakers, researchers, and educational technology innovators and providers as they work together on pressing policy issues that arise as Artificial Intelligence (AI) is used in education.\n\nAI can be defined as “automation based on associations.” When computers automate reasoning based on associations in data (or associations deduced from expert knowledge), two shifts fundamental to AI occur and shift computing beyond conventional edtech: (1) from capturing data to detecting patterns in data and (2) from providing access to instructional resources to automating decisions about instruction and other educational processes.\n\nDetecting patterns and automating decisions are leaps in the level of responsibilities that can be delegated to a computer system.\n\nThe process of developing an AI system may lead to bias in how patterns are detected and unfairness in how decisions are automated.\n\nThus, educational systems must govern their use of AI systems.\n\nThis report describes opportunities for using AI to improve education, recognizes challenges that will arise, and develops recommendations to guide further policy development.",
    "# Rising Interest in AI in Education\n\nToday, many priorities for improvements to teaching and learning are unmet.\n\nEducators seek technology-enhanced approaches addressing these priorities that would be safe, effective, and scalable.\n\nNaturally, educators wonder if the rapid advances in technology in everyday lives could help.\n\nLike all of us, educators use AI-powered services in their everyday lives, such as voice assistants in their homes; tools that can correct grammar, complete sentences, and write essays; and automated trip planning on their phones.\n\nMany educators are actively exploring AI tools as they are newly released to the public.\n\nEducators see opportunities to use AI-powered capabilities like speech recognition to increase the support available to students with disabilities, multilingual learners, and others who could benefit from greater adaptivity and personalization in digital tools for learning.\n\nThey are exploring how AI can enable writing or improving lessons, as well as their process for finding, choosing, and adapting material for use in their lessons.\n\nEducators are also aware of new risks.\n\nUseful, powerful functionality can also be accompanied with new data privacy and security risks.\n\nEducators recognize that AI can automatically produce output that is inappropriate or wrong.\n\nThey are wary that the associations or automations created by AI may amplify unwanted biases.\n\nThey have noted new ways in which students may represent others’ work as their own.\n\nThey are well-aware of “teachable moments” and pedagogical strategies that a human teacher can address but are undetected or misunderstood by AI models.\n\nThey worry whether recommendations suggested by an algorithm would be fair.\n\nEducators’ concerns are manifold.\n\nEveryone in education has a responsibility to harness the good to serve educational priorities while also protecting against the dangers that may arise as a result of AI being integrated in edtech.\n\nTo develop guidance for edtech, the Department works closely with educational constituents.\n\nThese constituents include educational leaders—teachers, faculty, support staff, and other educators—researchers; policymakers; advocates and funders; technology developers; community members and organizations; and, above all, learners and their families/caregivers.\n\nRecently, through its activities with constituents, the Department noticed a sharp rise in interest and concern about AI.\n\nFor example, a 2021 field scan found that developers of all kinds of technology systems—for student information, classroom instruction, school logistics, parent-teacher communication, and more—expect to add AI capabilities to their systems.\n\nThrough a series of four listening sessions conducted in June and August 2022 and attended by more than 700 attendees, it became clear that constituents believe that action is required now in order to get ahead of the expected increase of AI in education technology—and they want to roll up their sleeves and start working together.\n\nIn late 2022 and early 2023, the public became aware of new generative AI chatbots and began to explore how AI could be used to write essays, create lesson plans, produce images, create personalized assignments for students, and more.\n\nFrom public expression in social media, at conferences, and in news media, the Department learned more about risks and benefits of AI-enabled chatbots.\n\nAnd yet this report will not focus on a specific AI tool, service, or announcement, because AI-enabled systems evolve rapidly.\n\nFinally, the Department engaged the educational policy expertise available internally and in its relationships with AI policy experts to shape the findings and recommendations in this report.",
    "# Three Reasons to Address AI in Education Now\n\n“I strongly believe in the need for stakeholders to understand the cyclical effects of AI and education.\n\nBy understanding how different activities accrue, we have the ability to support virtuous cycles.\n\nOtherwise, we will likely allow vicious cycles to perpetuate.”  \n—Lydia Liu\n\nDuring the listening sessions, constituents articulated three reasons to address AI now:  \nFirst, AI may enable achieving educational priorities in better ways, at scale, and with lower costs.\n\nAddressing varied unfinished learning of students due to the pandemic is a policy priority, and AI may improve the adaptivity of learning resources to students’ strengths and needs.\n\nImproving teaching jobs is a priority, and via automated assistants or other tools, AI may provide teachers greater support.\n\nAI may also enable teachers to extend the support they offer to individual students when they run out of time.\n\nDeveloping resources that are responsive to the knowledge and experiences students bring to their learning—their community and cultural assets—is a priority, and AI may enable greater customizability of curricular resources to meet local needs.\n\nAs seen in voice assistants, mapping tools, shopping recommendations, essay-writing capabilities, and other familiar applications, AI may enhance educational services.\n\nSecond, urgency and importance arise through awareness of system-level risks and anxiety about potential future risks.\n\nFor example, students may become subject to greater surveillance.\n\nSome teachers worry that they may be replaced—to the contrary, the Department firmly rejects the idea that AI could replace teachers.\n\nExamples of discrimination from algorithmic bias are on the public’s mind, such as a voice recognition system that doesn’t work as well with regional dialects, or an exam monitoring system that may unfairly identify some groups of students for disciplinary action.\n\nSome uses of AI may be infrastructural and invisible, which creates concerns about transparency and trust.\n\nAI often arrives in new applications with the aura of magic, but educators and procurement policies require that edtech show efficacy.\n\nAI may provide information that appears authentic, but actually is inaccurate or lacking a basis in reality.\n\nOf the highest importance, AI brings new risks in addition to the well-known data privacy and data security risks, such as the risk of scaling pattern detectors and automations that result in “algorithmic discrimination” (e.g., systematic unfairness in the learning opportunities or resources recommended to some populations of students).\n\nThird, urgency arises because of the scale of possible unintended or unexpected consequences.\n\nWhen AI enables instructional decisions to be automated at scale, educators may discover unwanted consequences.\n\nIn a simple example, if AI adapts by speeding curricular pace for some students and by slowing the pace for other students (based on incomplete data, poor theories, or biased assumptions about learning), achievement gaps could widen.\n\nIn some cases, the quality of available data may produce unexpected results.\n\nFor example, an AI-enabled teacher hiring system might be assumed to be more objective than human-based résumé scoring.\n\nYet, if the AI system relies on poor quality historical data, it might de-prioritize candidates who could bring both diversity and talent to a school’s teaching workforce.\n\nIn summary, it is imperative to address AI in education now to realize key opportunities, prevent and mitigate emergent risks, and tackle unintended consequences.",
    "# Toward Policies for AI in Education\n\nThe 2023 AI Index Report from the Stanford Institute for Human-Centered AI has documented notable acceleration of investment in AI as well as an increase of research on ethics, including issues of fairness and transparency.\n\nOf course, research on topics like ethics is increasing because problems are observed.\n\nEthical problems will occur in education, too.\n\nThe report found a striking interest in 25 countries in the number of legislative proposals that specifically include AI.\n\nIn the United States, multiple executive orders are focused on ensuring AI is trustworthy and equitable, and the White House Office of Science and Technology Policy has introduced a Blueprint for an AI Bill of Rights (Blueprint) that provides principles and practices that help achieve this goal.\n\nThese initiatives, along with other AI-related policy activities occurring in both the executive and legislative branches, will guide the use of AI throughout all sectors of society.\n\nIn Europe, the European Commission recently released Ethical guidelines on the use of artificial intelligence (AI) and data in teaching and learning for educators.\n\nAI is moving fast and heralding societal changes that require a national policy response.\n\nIn addition to broad policies for all sectors of society, education-specific policies are needed to address new opportunities and challenges within existing frameworks that take into consideration federal student privacy laws (such as the Family Educational Rights and Privacy Act, or FERPA), as well as similar state-related laws.\n\nAI also makes recommendations and takes actions automatically in support of student learning, and thus educators will need to consider how such recommendations and actions can comply with laws such as the Individuals with Disabilities Education Act (IDEA).\n\nWe discuss specific policies in the concluding section.\n\nAI is advancing exponentially (see Figure 1), with powerful new AI features for generating images and text becoming available to the public, and leading to changes in how people create text and images.\n\nThe advances in AI are not only happening in research labs but also are making news in mainstream media and in educational-specific publications.\n\nResearchers have articulated a range of concepts and frameworks for ethical AI, as well as for related concepts such as equitable, responsible, and human-centered AI.\n\nListening session participants called for building on these concepts and frameworks but also recognized the need to do more; participants noted a pressing need for guardrails and guidelines that make educational use of AI advances safe, especially given this accelerating pace of incorporation of AI into mainstream technologies.\n\nAs policy development takes time, policymakers and educational constituents together need to start now to specify the requirements, disclosures, regulations, and other structures that can shape a positive and safe future for all constituents—especially students and teachers.\n\nPolicies are urgently needed to implement the following:  \n- Leverage automation to advance learning outcomes while protecting human decision making and judgment;\n- Interrogate the underlying data quality in AI models to ensure fair and unbiased pattern recognition and decision making in educational applications, based on accurate information appropriate to the pedagogical situation;\n- Enable examination of how particular AI technologies, as part of larger edtech or educational systems, may increase or undermine equity for students; and\n- Take steps to safeguard and advance equity, including providing for human checks and balances and limiting any AI systems and tools that undermine equity.",
    "# Building Ethical, Equitable Policies Together\n\nIn this report, we aim to build on the listening sessions the Department hosted to engage and inform all constituents involved in making educational decisions so they can prepare for and make better decisions about the role of AI in teaching and learning.\n\nAI is a complex and broad topic, and we are not able to cover everything nor resolve issues that still require more constituent engagement.\n\nThis report is intended to be a starting point.\n\nThe opportunities and issues of AI in education are equally important in K-12, higher education, and workforce learning.\n\nDue to scope limitations, the examples in this report will focus on K-12 education.\n\nThe implications are similar at all levels of education, and the Department intends further activities in 2023 to engage constituents beyond K-12 schools.",
    "# Guiding Questions\n\nUnderstanding that AI increases automation and allows machines to do some tasks that only people did in the past leads us to a pair of bold, overarching questions:  \n- What is our collective vision of a desirable and achievable educational system that leverages automation to advance learning while protecting and centering human agency?\n\n- How and on what timeline will we be ready with necessary guidelines and guardrails, as well as convincing evidence of positive impacts, so that constituents can ethically and equitably implement this vision widely?\n\nIn the Learning, Teaching, and Assessment sections of this report, we elaborate on elements of an educational vision grounded in what today’s learners, teachers, and educational systems need, and we describe key insights and next steps required.\n\nBelow, we articulate four key foundations for framing these themes.\n\nThese foundations arise from what we know about the effective use of educational technology to improve opportunity, equity, and outcomes for students and also relate to the new Blueprint.",
    "## Foundation 1: Center People (Parents, Educators, and Students)\n\nEducation-focused AI policies at the federal, state, and district levels will be needed to guide and empower local and individual decisions about which technologies to adopt and use in schools and classrooms.\n\nConsider what is happening in everyday lives.\n\nMany of us use AI-enabled products because they are often better and more convenient.\n\nFor example, few people want to use paper maps anymore; people find that technology helps us plan the best route to a destination more efficiently and conveniently.\n\nAnd yet, people often do not realize how much privacy they are giving up when they accept AI-enabled systems into their lives.\n\nAI will bring privacy and other risks that are hard to address only via individual decision making; additional protections will be needed.\n\nThere should be clear limits on the ability to collect, use, transfer, and maintain our personal data, including limits on targeted advertising.\n\nThese limits should put the burden on platforms to minimize how much information they collect, rather than burdening Americans with reading fine print.\n\nAs protections are developed, we recommend that policies center people, not machines.\n\nTo this end, a first recommendation in this document (in the next section) is an emphasis on AI with humans in the loop.\n\nTeachers, learners, and others need to retain their agency to decide what patterns mean and to choose courses of action.\n\nThe idea of humans in the loop builds on the concept of “Human Alternatives, Consideration, and Fallback” in the Blueprint and ethical concepts used more broadly in evaluating AI, such as preserving human dignity.\n\nA top policy priority must be establishing human in the loop as a requirement in educational applications, despite contrary pressures to use AI as an alternative to human decision making.\n\nPolicies should not hinder innovation and improvement, nor should they be burdensome to implement.\n\nSociety needs an education-focused AI policy that protects civil rights and promotes democratic values in the building, deployment, and governance of automated systems to be used across the many decentralized levels of the American educational system.",
    "## Foundation 2: Advance Equity\n\n“AI brings educational technology to an inflection point.\n\nWe can either increase disparities or shrink them, depending on what we do now.”  \n—Dr.\n\nRussell Shilling\n\nA recent Executive Order issued by President Biden sought to strengthen the connection among racial equity, education and AI, stating that “members of underserved communities—many of whom have endured generations of discrimination and disinvestment—still confront significant barriers to realizing the full promise of our great Nation, and the Federal Government has a responsibility to remove these barriers” and that the Federal Government shall both “pursue educational equity so that our Nation’s schools put every student on a path to success” and also “root out bias in the design and use of new technologies, such as artificial intelligence.” A specific vision of equity, such as described in the Department’s recent report, Advancing Digital Equity for All is essential to policy discussion about AI in education.\n\nThis report defines digital equity as “the condition in which individuals and communities have the information technology capacity that is needed for full participation in the society and economy of the United States.”\nIssues related to racial equity and unfair bias were at the heart of every listening session we held.\n\nIn particular, we heard a conversation that was increasingly attuned to issues of data quality and the consequences of using poor or inappropriate data in AI systems for education.\n\nDatasets are used to develop AI, and when they are non-representative or contain undesired associations or patterns, resulting AI models may act unfairly in how they detect patterns or automate decisions.\n\nSystematic, unwanted unfairness in how a computer detects patterns or automates decisions is called “algorithmic bias.” Algorithmic bias could diminish equity at scale with unintended discrimination.\n\nAs this document discussed in the Formative Assessment section, this is not a new conversation.\n\nFor decades, constituents have rightly probed whether assessments are unbiased and fair.\n\nJust as with assessments, whether an AI model exhibits algorithmic bias or is judged to be fair and trustworthy is critical as local school leaders make adoption decisions about using AI to achieve their equity goals.\n\nWe highlight the concept of “algorithmic discrimination” in the Blueprint.\n\nBias is intrinsic to how AI algorithms are developed using historical data, and it can be difficult to anticipate all impacts of biased data and algorithms during system design.\n\nThe Department holds that biases in AI algorithms must be addressed when they introduce or sustain unjust discriminatory practices in education.\n\nFor example, in postsecondary education, algorithms that make enrollment decisions, identify students for early intervention, or flag possible student cheating on exams must be interrogated for evidence of unfair discriminatory bias—and not only when systems are designed, but also later, as systems become widely used.",
    "## Foundation 3: Ensure Safety, Ethics, and Effectiveness\n\nA central safety argument in the Department’s policies is the need for data privacy and security in the systems used by teachers, students, and others in educational institutions.\n\nThe development and deployment of AI requires access to detailed data.\n\nThis data goes beyond conventional student records (roster and gradebook information) to detailed information about what students do as they learn with technology and what teachers do as they use technology to teach.\n\nAI’s dependence on data requires renewed and strengthened attention to data privacy, security, and governance (as also indicated in the Blueprint).\n\nAs AI models are not generally developed in consideration of educational usage or student privacy, the educational application of these models may not be aligned with the educational institution’s efforts to comply with federal student privacy laws, such as FERPA, or state privacy laws.\n\nFurther, educational leaders are committed to basing their decisions about the adoption of educational technology on evidence of effectiveness—a central foundation of the Department’s policy.\n\nFor example, the requirement to base decisions on evidence also arises in the Elementary and Secondary Education Act (ESEA), as amended, which introduced four tiers of evidence (see Figure 2).\n\nOur nation’s research agencies, including the Institute of Education Sciences, are essential to producing the needed evidence.\n\nThe Blueprint calls for evidence of effectiveness, but the education sector is ahead of that game: we need to insist that AI-enhanced edtech rises to meet ESEA standards as well.",
    "## Foundation 4: Promote Transparency\n\nThe central role of complex AI models in a technology’s detection of patterns and implementation of automation is an important way in which AI-enabled applications, products, and services will be different from conventional edtech.\n\nThe Blueprint introduces the need for transparency about AI models in terms of disclosure (“notice”) and explanation.\n\nIn education, decision makers will need more than notice—they will need to understand how AI models work in a range of general",
    "# Educational Use Cases\n\nAI models in edtech will be approximations of reality and, thus, constituents can always ask these questions: How precise are the AI models?\n\nDo they accurately capture what is most important?\n\nHow well do the recommendations made by an AI model fit educational goals?\n\nWhat are the broader implications of using AI models at scale in educational processes?\n\nBuilding on what was heard from constituents, the sections of this report develop the theme of evaluating the quality of AI systems and tools using multiple dimensions as follows:",
    "## Recommendations\n\nUse of AI systems and tools must be safe and effective for students.\n\nThey must include algorithmic discrimination protections, protect data privacy, provide notice and explanation, and provide a recourse to humans when problems arise.\n\nThe people most affected by the use of AI in education must be part of the development of the AI model, system, or tool, even if this slows the pace of adoption.\n\nWe return to the idea that these considerations fit together in a comprehensive perspective on the quality of AI models in the Recommendations section.",
    "# Overview of Document\n\nWe begin in the next section by elaborating a definition of AI, followed by addressing learning, teaching, assessment, and research and development.\n\nOrganizing key insights by these topics keeps us focused on exploring implications for improving educational opportunity and outcomes for students throughout the report.\n\nWithin these topics, three important themes are explored:\n\n- **Opportunities and Risks.\n\n** Policies should focus on the most valuable educational advances while mitigating risks.\n\n- **Trust and Trustworthiness.\n\n** Trust and safeguarding are particularly important in education because we have an obligation to keep students out of harm’s way and safeguard their learning experiences.\n\n- **Quality of AI Models.\n\n** The process of developing and then applying a model is at the heart of any AI system.\n\nPolicies need to support evaluation of the qualities of AI models and their alignment to goals for teaching and learning during the processes of educational adoption and use.\n\n\"AI in education can only grow at the speed of trust.\"\n\n—Dr.\n\nDale Allen",
    "# What is AI?\n\nOur preliminary definition of AI as automation based on associations requires elaboration.\n\nBelow we address three additional perspectives on what constitutes AI.\n\nEducators will find these different perspectives arise in the marketing of AI functionality and are important to understand when evaluating edtech systems that incorporate AI.\n\nOne useful glossary of AI for Education terms is the CIRCLS Glossary of Artificial Intelligence Terms for Educators.\n\nAI is not one thing but an umbrella term for a growing set of modeling capabilities.",
    "## Perspective: Human-Like Reasoning\n\n\"The theory and development of computer systems able to perform tasks normally requiring human intelligence such as, visual perception, speech recognition, learning, decision-making, and natural language processing.\"\n\nBroad cultural awareness of AI may be traced to the landmark 1968 film \"2001: A Space Odyssey\"—in which the \"Heuristically-programmed ALgorithmic\" computer, or \"HAL,\" converses with astronaut Frank.\n\nHAL helps Frank pilot the journey through space, a job that Frank could not do on his own.\n\nHowever, Frank eventually goes outside the spacecraft, HAL takes over control, and this does not end well for Frank.\n\nHAL exhibits human-like behaviors, such as reasoning, talking, and acting.\n\nLike all applications of AI, HAL can help humans but also introduces unanticipated risks—especially since AI reasons in different ways and with different limitations than people do.",
    "## Perspective: An Algorithm that Pursues a Goal\n\n\"Any computational method that is made to act independently towards a goal based on inferences from theory or patterns in data.\"\n\nThis second definition emphasizes that AI systems and tools identify patterns and choose actions to achieve a given goal.\n\nThese pattern recognition capabilities and automated recommendations will impact the educational process, including student learning and teacher instructional decision making.",
    "## Perspective: Intelligence Augmentation\n\n\"Augmented intelligence is a design pattern for a human-centered partnership model of people and artificial intelligence (AI) working together to enhance cognitive performance, including learning, decision making, and new experiences.\"\n\nFoundation #1 (above) keeps humans in the loop and positions AI systems and tools to support human reasoning.\n\n\"Intelligence Augmentation\" (IA) centers \"intelligence\" and \"decision making\" in humans but recognizes that people sometimes are overburdened and benefit from assistive tools.",
    "# Definition of \"Model\"\n\nThe above perspectives open a door to making sense of AI.\n\nYet, to assess AI meaningfully, constituents must consider specific models and how they are developed.\n\nIn everyday usage, the term \"model\" has multiple definitions.\n\nWe clarify our intended meaning, which is a meaning similar to \"mathematical model,\" below.\n\nAI models are like financial models: an approximation of reality that is useful for identifying patterns, making predictions, or analyzing alternative decisions.\n\nInsight: AI Systems Enable New Forms of Interaction\n\nAI models allow computational processes to make recommendations or plans and also enable them to support forms of interaction that are more natural, such as speaking to an assistant.\n\nAI-enabled educational systems will be desirable in part due to their ability to support more natural interactions during teaching and learning.",
    "# Key Recommendation: Human in the Loop AI\n\nMany have experienced a moment where technology surprised them with an uncanny ability to recommend what feels like a precisely personalized product, song, or even phrase to complete a sentence in a word processor such as the one being used to draft this document.\n\n\"We can use AI to study the diversity, the multiplicity of effective learning approaches and think about the various models to help us get a broader understanding of what effective, meaningful engagement might look like across a variety of different contexts.\"\n\n—Dr.\n\nMarcelo Aaron Bonilla Worsley\n\nThese limitations lead to our first recommendation: that we pursue a vision of AI where humans are in the loop.\n\nThat means that people are part of the process of noticing patterns in an educational system and assigning meaning to those patterns.\n\nIt also means that teachers remain at the helm of major instructional decisions.",
    "# Learning\n\nThe Department’s long-standing edtech vision sees students as active learners; students participate in discussions that advance their understanding, use visualizations and simulations to explain concepts as they relate to the real world, and leverage helpful scaffolding and timely feedback as they learn.\n\nConstituents want technology to align to and build on these and other research-based understandings of how people learn.",
    "## Insight: AI Enables Adaptivity in Learning\n\nAdaptivity has been recognized as a key way in which technology can improve learning.\n\nAI can be a toolset for improving the adaptivity of edtech.\n\nAI may improve a technology’s ability to meet students where they are, build on their strengths, and grow their knowledge and skills.\n\nIntelligent Tutoring Systems: An Example of AI Models\n\nOne long-standing type of AI-enabled technology is an Intelligent Tutoring System (ITS).\n\nIn an early success, scientists were able to build accurate models of how human experts solve mathematical problems.\n\nThe resulting model was incorporated into a system that would observe student problem solving as they worked on mathematical problems on a computer.\n\nResearchers who studied human tutors found that feedback on specific steps (and not just right or wrong solutions) is a likely key to why tutoring is so effective.",
    "# References\n\n1.\n\nMousavinasab, E., Zarifsanaiey, N., Niakan Kalhori, S.R., Rakhshan, M., Keikha, L., & Ghazi Saeedi, M. (2021).\n\nIntelligent tutoring systems: A systematic review of characteristics, applications, and evaluation methods.\n\nInteractive Learning Environments, 29(1), 142–163.\n\n2.\n\nVan Lehn, K. (2011).\n\nThe relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems.\n\nEducational Psychologist, 46(4), 197-221.\n\n3.\n\nRitter, S., Anderson, J.R., Koedinger, K.R., & Corbett, A.\n\n(2007).\n\nCognitive Tutor: Applied research in mathematics education.\n\nPsychonomic Bulletin & Review, 14, 249–255.\n\n4.\n\nWinne, P.H.\n\n(2021).\n\nOpen learner models working in symbiosis with self-regulating learners: A research agenda.\n\nInternational Journal of Artificial Intelligence in Education, 31(3), 446-459.",
    "## Important Directions for Expanding AI-Based Adaptivity\n\nAlthough R&D along the lines of an ITS should not limit the view of what’s possible, such an example is useful because so much research and evaluation has been done on the ITS approach.\n\nResearchers have looked across all the available high-quality studies in a meta-analysis and concluded that ITS approaches are effective.\n\nRight now, many school systems are looking at high-intensity human tutoring to help students with unfinished learning.\n\nHuman tutoring is very expensive, and it is hard to find enough high-quality human tutors.\n\nWith regard to large-scale needs, if it is possible for an ITS to supplement what human tutors do, it might be possible to extend beyond the amount of tutoring that people can provide to students.",
    "### Personalization in Education\n\nAdaptivity is sometimes referred to as “personalization.” Although this is a convenient term, many observers have noted how imprecise it is.\n\nFor some educators, personalization means giving learners “voice and choice,” and for others it means that a learning management system recommends an individual “playlist” of activities to each student.\n\nHidden in that imprecision is the reality that many edtech products that personalize do so in limited ways.\n\nAdjusting the difficulty and the order of lesson materials are among the two most common ways that edtech products adapt.\n\nAnd yet, any teacher knows there is more to supporting learning than adjusting the difficulty and sequence of materials.\n\nFor example, a good teacher can find ways to engage a student by connecting to their own past experiences and can shape explanations until they really connect in an “aha!” moment for that student.\n\nWhen we say, “meet the learner where they are,” human teachers bring a much more complete picture of each learner than most available edtech.\n\nThe teacher is also not likely to “over personalize” (by performing like an algorithm that only presents material for which the learner has expressed interest), thereby limiting the student’s exposure to new topics.",
    "#### From deficit-based to asset-oriented\n\nListening session attendees noted that the rhetoric around adaptivity has often been deficit-based; technology tries to pinpoint what a student is lacking and then provides instruction to fill that specific gap.\n\nTeachers also orient to students' strengths; they find competencies or “assets” a student has and use those to build up the students’ knowledge.\n\nAI models cannot be fully equitable while failing to recognize or build upon each student’s sources of competency.\n\nAI models that are more asset-oriented would be an advance.",
    "#### From individual cognition to including social and other aspects of learning\n\nThe existing adaptivity rhetoric has also tended to focus on individualized learning and mostly on cognitive elements of learning, with motivational and other elements only brought in to support the cognitive learning goals.\n\nAttendees observe that their vision for learning is broader than cognition.\n\nSocial learning is important, for example, especially for students to learn to reason, explain, and justify.\n\nFor students who are learning English, customized and adaptive support for improving language skills while learning curricular content is clearly important.\n\nDeveloping self-regulation skills is also important.\n\nA modern vision of learning is not individualistic; it recognizes that students learn in groups and communities too.",
    "#### From neurotypical to neurodiverse learners\n\nAI models could help in including neurodiverse learners (students who access, process, and interact with the world in less common ways than “neurotypical” students) who could benefit from different learning paths and from forms of display and input that fit their strengths.\n\nConstituents want AI models that can support learning for neurodiverse learners and learners with disabilities.\n\nThus, they want AI models that can work with multiple paths to learning and multiple modalities of interaction.\n\nSuch models should be tested for efficacy, to guard against the possibility that some students could be assigned a “personalized” but inadequate learning resource.\n\nIn addition, some systems for neurodiverse students are presently underutilized, so designs that support intended use will also be important.",
    "#### From fixed tasks to active, open, and creative tasks\n\nAs mentioned above, AI models are historically better at closed tasks like solving a math problem or logical tasks like playing a game.\n\nIn terms of life-wide and lifelong opportunities, we value learning how to succeed at open-ended and creative tasks that require extended engagement from the learner, and these are often not purely mathematical or logical.\n\nWe want students to learn to invent and create innovative approaches.\n\nWe want AI models that enable progress on open, creative tasks.",
    "#### From correct answers to additional goals\n\nAt the heart of many adaptivity approaches now on the market, the model inside the technology counts students' wrong answers and decides whether to speed up, slow down, or offer a different type of learning support.\n\nYet, right and wrong answers are not the only learning goals.\n\nWe want students to learn how to self-regulate when they experience difficulties in learning, for example, such as being able to persist in working on a difficult problem or knowing how and when to ask for help.\n\nWe want learners to become skilled in teamwork and in leading teams.\n\nAs students grow, we want them to develop more agency and to be able to act on their own to advance toward their own learning goals.\n\nListing every dimension of expansion that we heard in our listening sessions is beyond the scope of this report.\n\nSome additional dimensions are presented in the following sections on Teaching, Assessment, and Research.\n\nFor example, in Research, we discuss all the ways in which AI systems have trouble with context—context that humans readily grasp and consider.\n\nOverall, constituents in the listening sessions realized we need an ambitious outlook on learning to respond to the future today’s learners face.\n\nConstituents were concerned about ways in which AI might narrow learning.\n\nFor example, if the incorporation of AI into education slowed attention to students’ skills on creative, open-ended tasks and their ability to lead and collaborate in teams, then school districts may be less able to realize their students’ progress in relation to a Portrait of a Graduate who excels in communication and other skills valued in communities and careers.",
    "# A Duality: Learning With and About AI\n\nAs AI is brought into schools, two broad perspectives about AI in education arise: (1) AI in support of student learning; and (2) support for learning about AI and related technologies.\n\nSo far, we’ve discussed AI systems and tools to support student learning and mastery of subjects like mathematics and writing.\n\nYet, it is also important that students learn about AI, critically examine its presence in education and society, and determine its role and value in their own lives and careers.\n\nWe discuss risks across each section in this report.\n\nHere, it is important for students to become more aware of and savvy to the risks of AI—including risks of bias and surveillance—as they appear in all elements of their lives.\n\nIn the recent past, schools have supported students’ understanding of cybersecurity, for example.\n\nAI will bring new risks, and students need to learn about them.\n\nWe are encouraged by efforts we’ve seen underway that would give students opportunities to learn about how AI works while also giving them opportunities to discuss relevant topics like privacy and security.\n\nOther learning goals are noted in the K-12 Computer Science Framework.\n\nWe’ve seen that students can begin learning about AI in elementary, middle, and high school.\n\nThey can use AI to design simulations and products that they find exciting.\n\nAnd we’ve seen that students want to talk about the ethics of products they experience in their everyday lives and have much to say about the kinds of products they’d like to see or not see in school.\n\n(And later, in the Research section, we note the desire for co-design processes that involve students in creating the next generation of AI-enabled edtech).\n\nOverall, it’s important to balance attention to using AI to support learning and giving students opportunities to learn about AI.",
    "# A Challenge: Systems Thinking About AI in Education\n\nAs AI expands into the educational system, our listening session attendees reminded us that it will be entering parts or locations of the system that are presently dysfunctional.\n\nAI is certainly not a fix for broken systems, and instead, must be used with even more care when the systems’ context is unstable or uncertain.\n\n“First and foremost, they are getting deployed in educational contexts that are already fragmented and broken and unequal.\n\nTechnology doesn't discriminate—we do.\n\nSo, as we think about the application of these new systems, we have to really think about the contextual application of AI.”\n—Dr.\n\nNicole Turner\n\nAs discussed previously, because AI systems and tools do not fully align with goals for learning, we have to design educational settings to situate AI in the right place, where educators and other adults can make effective use of these tools for teaching and learning.\n\nWithin the ITS example, we saw that AI could make learning by practicing math problems more effective, and a whole curricular approach might include roles for teachers that emphasize mathematical practices like argumentation and modeling.\n\nFurther, small-group work is likely to remain important: Students might work in small groups to use mathematics to predict or justify as they work on responding to a realistic challenge.\n\nAt the present, one “right place” for people, and not AI, is understanding how learning can be culturally responsive and culturally sustaining, as AI is not even close to being ready to connect learning to the unique strengths in a student’s community and family.",
    "# Open Questions About AI for Learning\n\nWith advances occurring in the foundations for AI, opportunities to use AI in support of learning are rapidly expanding.\n\nAs we explore these opportunities, the open questions below deserve ongoing attention:\n\n- To what extent is AI enabling adaptation to students’ strengths and not just deficits?\n\nIs AI enabling improved support for learners with disabilities and English language learners?\n\n- How are youth voices involved in choosing and using AI for learning?\n\n- Is AI leading to narrower student activities (e.g., procedural math problems), or the fuller range of activities highlighted in the National Educational Technology Plan (NETP), which emphasizes features such as personalized learning, project-based learning, learning from visualizations, simulations, and virtual reality, as well as learning across school, community, and familial settings?\n\n- Is AI supporting the whole learner, including social dimensions of learning such as enabling students to be active participants in small group and collaborative learning?\n\nFor example, does AI contribute to aspects of student collaboration we value like shared attention, mutual engagement, peer help, self-regulation, and building on each other’s contributions?\n\n- When AI is used, are students’ privacy and data protected?\n\nAre students and their guardians informed about what happens with their data?\n\n- How strong are the processes or systems for monitoring student use of AI for barriers, bias, or other undesirable consequences of AI use by learners?\n\nHow are emergent issues addressed?\n\n- Is high-quality research or evaluations about the impacts of using the AI system for student learning available?\n\nDo we know not only whether the system works but for whom and under what conditions?",
    "# Key Recommendation: Seek AI Models Aligned to a Vision for Learning\n\nWe’ve called attention to how advances in AI are important to adaptivity but also to ways in which adaptivity is limited by the model’s inherent quality.\n\nWe noted that a prior wave of edtech used the term “personalized” in differing ways, and it was often important to clarify what personalization meant for a particular product or service.\n\nThus, our key recommendation is to tease out the strengths and limitations of AI models inside forthcoming edtech products and to focus on AI models that align closely to desired visions of learning.\n\nAI is now advancing rapidly, and we should differentiate between products that have simple AI-like features inside and products that have more sophisticated AI models.\n\nLooking at what’s happening in research and development, we can see significant effort and push toward overcoming these limitations.\n\nWe noted that decision makers need to be careful about selecting AI models that might narrow their vision for learning, as general artificial intelligence does not exist.\n\nAnd because AI models will always be narrower than real world experience, we need to proceed with systems thinking in which humans are in the loop, with the strengths and weaknesses of the specific educational system considered.\n\nWe hold that the full system for learning is broader than its AI component.",
    "# Teaching\n\nTeachers have long envisioned many things that technology could make possible for teachers, their classrooms, and their students but not the changes wrought by the recent pandemic.\n\nToday, nearly all teachers have experienced uses of technologies for instruction that no one anticipated.\n\nSome of those experiences were positive, and others were not.\n\nAll of the experiences provide an important context as we think further about teaching and technology.\n\nThere is a critical need to focus on addressing the challenges teachers experience.\n\nIt must become easier for teachers to do the amazing work they always do.\n\nWe must also remember why people choose the teaching profession and ensure they can do the work that matters.\n\nThis section discusses examples of AI supporting teachers and teaching including these concepts: AI assistants to reduce routine teaching burdens; AI that provides teachers with recommendations for their students’ needs and extends their work with students; and AI that helps teachers to reflect, plan, and improve their practice.\n\n“One opportunity I see with AI is being able to reduce the amount of attention I have to give to administrative things and increase the amount of attention I can give to my students with their learning needs in the classroom.\n\nSo that's the first one that I'd say that I'm super excited about the possibility of AI to support me as a teacher.\"\n\n—Vidula Plante",
    "## Always Center Educators in Instructional Loops\n\nTo succeed with AI as an enhancement to learning and teaching, we need to always center educators (ACE).\n\nPractically speaking, practicing “ACE in AI” means keeping a humanistic view of teaching front and center.\n\nACE leads the Department to confidently respond “no” when asked “will AI replace teachers?” ACE is not just about making teachers’ jobs easier but also making it possible to do what most teachers want to do.\n\nThat includes, for example, understanding their students more deeply and having more time to respond in creative ways to teachable moments.\n\nTo bring more precision to how and where we should center educators, we return to our advocacy for human in the loop AI and ask, what are the loops in which teachers should be centered?\n\nFigure 5 suggests three key loops (inspired by research on adaptivity loops):",
    "### Three Key Loops for Educators\n\n1.\n\nThe loop in which teachers make moment-to-moment decisions as they do the immediate work of teaching.\n\n2.\n\nThe loop in which teachers prepare for, plan, and reflect on teaching, which includes professional development.\n\n3.\n\nThe loop in which teachers participate in decisions about the design of AI-enabled technologies, participate in selecting the technologies, and shape the evaluation of technologies—thus setting a context for not only their own classroom but those of fellow teachers as well.\n\nPlease note that in the next section, on Formative Assessment, we also discuss teachers’ important role in feedback loops that support students and enable school improvement.\n\nThat section also includes a discussion of the concepts of “bias” and “fairness,” which are important to teachers.",
    "# EP, AI, and Lesson Plans\n\nAI could help with finding components to add to lesson plans to fully address standards and expectations and to meet each student’s unique requirements.\n\nEven beyond finding components, AI might help adapt standardized resources to better fit specific needs—for example, providing a voice assistant that allows a student with a visual difficulty to hear material and respond to it or permitting a group of students to present their project using American Sign Language (ASL) which could be audibly voiced for other students using an AI ASL-to-Spoken-English translation capability.\n\nIndeed, coordinating IEPs is time-consuming work that might benefit from supportive automation and customized interactivity that can be provided by AI.\n\nReflection is important too.\n\nIn the bustle of a classroom, it is sometimes difficult to fully understand what a student is expressing or what situations lead to certain positive or negative behaviors.\n\nAgain, context is paramount.\n\nIn the moment, teachers may not be aware of external events that could shape their understanding of how students are showing up in their classrooms.\n\nTools that notice patterns and suggest ways to share information might help students and teachers communicate more fully about strengths and needs.",
    "# Designing, Selecting, and Evaluating AI Tools\n\nThe broadest loop teachers should be part of is the loop that determines what classroom tools do and which tools are available.\n\nToday, teachers already play a role in designing and selecting technologies.\n\nTeachers can weigh in on usability and feasibility.\n\nTeachers examine evidence of efficacy and share their findings with other school leaders.\n\nTeachers already share insights on what is needed to implement technology well.\n\nWhile these concerns will continue, AI will raise new concerns too.\n\nFor example, the following Formative Assessment section raises concerns about bias and fairness that can lead to algorithmic discrimination.\n\nThose concerns go beyond data privacy and security; they raise attention to how technologies may unfairly direct or limit some students' opportunities to learn.\n\nA key takeaway here is that teachers will need time and support so they can stay abreast of both the well-known and the newer issues that are arising and so they can fully participate in design, selection, and evaluation processes that mitigate risks.",
    "# Challenge: Balancing Human and Computer Decision-Making\n\nOne major new challenge with AI-enabled tools for teachers is that AI can enable autonomous activity by a computer, and thus when a teacher delegates work to an AI-enabled tool, it may carry on with that work somewhat independently.\n\nProfessor Inge Molenaar has wondered about the challenges of control in a hybrid teaching scenario: When should a teacher be in control?\n\nWhat can be delegated to a computational system?\n\nHow can a teacher monitor the AI system and override its decisions or take back control as necessary?\n\nFigure 8 expresses the tension around control.\n\nTo the left, the teacher is fully in control, and there is no use of AI in the classroom.\n\nTo the right, the technology is fully in control with no teacher involved—a scenario which is rarely desirable.\n\nThe middle ground is not one dimensional and involves many choices.\n\nMolenaar analyzed products and suggests some possibilities:\n- The technology only offers information and recommendations to the teacher.\n\n- The teacher delegates specific types of tasks to the technology, for example, giving feedback on a particular math assignment or sending out reminders to students before an assignment is due.\n\n- The teacher delegates more broadly to the technology, with clear protocols for alerts, for monitoring, and for when the teacher takes back control.\n\nThese and other choices need to be debated openly.\n\nFor example, we may want to define instructional decisions that have different kinds of consequences for a student and be very careful about delegating control over highly consequential decisions (for example, placement in a next course of study or disciplinary referrals).\n\nFor human in the loop to become more fully realized, AI technologies must allow teacher monitoring, have protocols to signal a teacher when their judgment is needed, and allow for classroom, school, or district overrides when they disagree with an instructional choice for their students.\n\nWe cannot forget that if a technology allows a teacher choice—which it should—it will take significant time for a teacher to think through and set up all the options, requiring greater time initially.",
    "# Challenge: Making Teaching Jobs Easier While Avoiding Surveillance\n\nWe also recognize that the very technologies that make jobs easier might also introduce new possibilities for surveillance.\n\nIn a familiar example, when we enable a voice assistant in the kitchen, it might help us with simple household tasks like setting a cooking timer.\n\nAnd yet the same voice assistant might hear things that we intended to be private.\n\nThis kind of dilemma will occur in classrooms and for teachers.\n\nWhen they enable an AI-assistant to capture data about what they say, what teaching resources they search for, or other behaviors, the data could be used to personalize resources and recommendations for the teacher.\n\nYet the same data might also be used to monitor the teacher, and that monitoring might have consequences for the teacher.\n\nAchieving trustworthy AI that makes teachers’ jobs better will be nearly impossible if teachers experience increased surveillance.\n\nA related tension is that asking teachers to be “in the loop” could create more work for teachers if not done well, and thus, being in the loop might be in tension with making teaching jobs easier.\n\nAlso related is the tension between not trusting AI enough (to obtain assistance) or trusting it too much (and incurring surveillance or loss of privacy).\n\nFor example, researchers have documented that people will follow instructions from a robot during a simulated fire emergency even when (a) they are told the robot is broken and (b) the advice is obviously wrong.\n\nWe anticipate teachers will need training and support to understand how and when they will need to exercise human judgement.",
    "# Challenge: Responding to Students’ Strengths While Protecting Their Privacy\n\nEducators seek to tackle inequities in learning, no matter how they manifest locally (e.g.\n\nin access to educational opportunities, resources, or supports).\n\nIn culturally responsive and culturally sustaining approaches, educators design materials to build on the “assets”—individual, community, and cultural strengths that students bring to learning.\n\nAlong with considering assets, of course, educators must meet students where they are, including both strengths and needs.\n\nAI could assist in this process by helping teachers with customizing curricular resources, for example.\n\nBut to do so, the data inputted in an AI-enabled system would have to provide more information about the students.\n\nThis information could be, but need not be, demographic details.\n\nIt could also be information about students’ preferences, outside interests, relationships, or experiences.\n\nWhat happens to this data, how it is deleted, and who sees it is of huge concern to educators.\n\nAs educators contemplate using AI-enabled technologies to assist in tackling educational inequities, they must consider whether the information about students shared with or stored in an AI-enabled system is subject to federal or state privacy laws, such as FERPA.\n\nFurther, educators must consider whether interactions between students and AI systems create records that must be protected by law, such as when a chatbot or automated tutor generates conversational or written guidance to a student.\n\nDecisions made by AI technologies, along with explanations of those decisions that are generated by algorithms may also be records that must be protected by law.\n\nTherein, a third tension emerges, between more fully representing students and protecting their privacy.\n\nFurther, representation would be just a start toward a solution.\n\nAs discussed earlier in this report, AI can introduce algorithmic discrimination through bias in the data, code, or models within AI-enhanced edtech.\n\nEngineers develop the pattern detection in AI models using existing data, and the data they use may not be representative or may contain associations that run counter to policy goals.\n\nFurther, engineers shape the automations that AI implements when it recognizes patterns, and the automations may not meet the needs of each student group with a diverse population.\n\nThe developers of AI are typically less diverse than the populations they serve, and as a consequence, they may not anticipate the ways in which pattern detection and automation may harm a community, group, or individual.\n\nAI could help teachers to customize and personalize materials for their students, leveraging the teacher’s understanding of student needs and strengths.\n\nIt is time consuming to customize curricular resources, and teachers are already exploring how AI chatbots can help them design additional resources for their students.\n\nAn elementary school teacher could gain powerful supports for changing the visuals in a storybook to engage their students or for adapting language that poorly fits local manners of speaking or even for modifying plots to incorporate other dimensions of a teacher’s lesson.\n\nIn the Learning section, we noted that AI could help identify learner strengths.\n\nFor example, a mathematics teacher may not be aware of ways in which a student is making great sense of graphs and tables about motions when they are in another teacher’s physics classroom and might not realize that using similar graphs about motion could help with their linear function lesson.\n\nAI might help teachers when they seek to reflect student strengths by creating or adapting instructional resources.\n\nYet, the broad equity challenges of avoiding algorithmic discrimination while increasing community and cultural responsiveness must be approached within the four foundations we earlier outlined: human in the loop, equity, safety and effectiveness, and evaluation of AI models.\n\nWe cannot expect AI models to respect cultural responsiveness.\n\nThe Department is particularly concerned that equity is something that engaged educators and other responsive adults are in the best position to address and something that is never solely addressable as a computational problem.",
    "# Questions Worth Asking About AI for Teaching\n\nAs leaders in both pre-service and post-service teacher education contemplate how AI can improve teaching (along with policymakers, developers, and researchers), we urge all in the ecosystem to spend more time asking these questions:\n- Is AI improving the quality of an educator’s day-to-day work?\n\nAre teachers experiencing less burden and more ability to focus and effectively teach their students?\n\n- As AI reduces one type of teaching burden, are we preventing new responsibilities or additional workloads being shifted and assigned to teachers in a manner that negates the potential benefits of AI?\n\n- Is classroom AI use providing teachers with more detailed insights into their students and their strengths while protecting their privacy?\n\n- Do teachers have oversight of AI systems used with their learners?\n\nAre they exercising control in the use of AI-enabled tools and systems appropriately or inappropriately yielding decision-making to these systems and tools?\n\n- When AI systems are being used to support teachers or to enhance instruction, are the protections against surveillance adequate?\n\n- To what extent are teachers able to exercise voice and decision-making to improve equity, reduce bias, and increase cultural responsiveness in the use of AI-enabled tools and systems?",
    "# Key Recommendation: Inspectable, Explainable, Overridable AI\n\nIn the Introduction, we discuss the notion that when AI is incorporated into a system, the core of the AI is a model.\n\nIn the Learning section, we discuss that we need to be careful that models align to the learning we envision (e.g., that they aren’t too narrow).\n\nNow, based on the needs of teachers (as well as students and their families/caregivers), we add another layer to our criteria for good AI models: the need for explainability.\n\nSome AI models can recognize patterns in the world and do the right action, but they cannot explain why (e.g., how they arrived at the connection between the pattern and the action).\n\nThis lack of explainability will not suffice for teaching; teachers will need to know how an AI model analyzed the work of one of their students and why the AI model recommended a particular tutorial, resource, or next step to the student.\n\nThus, explainability of an AI system’s decision is key to a teacher’s ability to judge that automated decision.\n\nSuch explainability helps teachers to develop appropriate levels of trust and distrust in AI, particularly to know where the AI model tends to make poor decisions.\n\nExplainability is also key to a teacher’s ability to monitor when an AI system may be unfairly acting on the wrong information (and thus may be biased).\n\nWe discuss bias and fairness more in the Assessment section next.\n\nSurrounding the idea of explainability is the need for teachers to be able to inspect what an AI model is doing.\n\nFor example, what kinds of instructional recommendations are being made and to which students?\n\nWhich students are being assigned remedial work in a never-ended loop?\n\nWhich are making progress?\n\nDashboards in current products present some of this information, but with AI, teachers may want to further explore which decisions are being made and for whom and know of the student-specific factors that an AI model had available (and possibly which factors were influential) when reaching a particular decision.\n\nFor example, some of today’s adaptive classroom products use limited recommendation models that only consider student success on the last three mathematics problems and do not consider other variables that a teacher would know to consider, such as whether a student has an IEP Plan or other needs.\n\nOur call for attending to equity considerations as we evaluate AI models requires information about how discriminatory bias may arise in particular AI systems and what developers have done to address it.\n\nThis can only be achieved with transparency for how the tools use datasets to achieve outcomes and what data they have available or that a teacher could include in her judgement but are not available to the system (IEP status is offered as an example above).\n\nTeachers will also need the ability to view and make their own judgement about automated decisions, such as decisions about which set of mathematics problems a student should work on next.\n\nThey need to be able to intervene and override decisions when they disagree with the logic behind an instructional recommendation.\n\nTeachers need protection against adverse ramifications when they assert human judgement over an AI system’s decision.",
    "# Formative Assessment\n\nFormative assessment is traditionally a key use of edtech because feedback loops are vital to improving teaching and learning.\n\nAs we have emphasized throughout this report, a top priority with AI is to keep humans in the loop and in control, which includes focusing on the people engaged with formative assessments: students, teachers, school leaders, families/caregivers, and others who support learners.\n\nIn the definition below, please note the overlap between definitions of AI and formative assessment; both have to do with detecting patterns and choosing a future course of action (that adapts to learner strengths and needs).\n\nAssessment refers to all those activities undertaken by teachers, and by the students in assessing themselves, which provide information to be used as feedback to modify the teaching and learning activities in which they are engaged.\n\nSuch assessment becomes “formative assessment” when the evidence is actually used to adapt the teaching to meet the needs.",
    "# Building on Best Practices\n\nA number of dimensions hold potential for shaping the future of formative assessments, and many have ready extensions to the field of AI-enabled systems and tools.\n\nFor example, the 2017 NETP discussed how technology can lead to improved formative assessments along seven dimensions, listed below:\n- Enabling Enhanced Question Types: to give students more ways to show what they know and can do.\n\n- Measurement of Complex Competencies: to better elicit growth in important skills that go beyond typical subject matter standards, for example, in measuring practices, social skills like teamwork, self-regulation, and work-relevant skills (e.g., making presentations or leading teams).\n\n- Providing Real-Time Feedback: to maintain and increase student engagement and to support effective learning, providing timely and helpful responses and suggestions to each learner.\n\n- Increasing Accessibility: to include neurodiverse learners and to engage learners’ best communication capabilities as they share what they know and can do.\n\n- Adapting to Learner Ability and Knowledge: to make assessments more precise and efficient.\n\n- Embedded Assessment in the Learning Process: to emphasize an assessment’s role in improving teaching and learning (this report does not focus on assessment for accountability purposes).\n\n- Assess for Ongoing Learning: to reveal progress over time and not just predetermined milestones.\n\nAI models and AI-enabled systems may have potential to strengthen formative assessments.\n\nIn one example, a question type that invites students to draw a graph or create a model can be analyzed with AI algorithms, and similar student models might be grouped for the teacher to interpret.\n\nEnhanced formative assessment may enable teachers to better respond to students’ understanding of a concept like “rate of change” in a complex, real-world situation.\n\nAI can also give learners feedback on complex skills, such as learning American Sign Language or speaking a foreign language and in other practice situations where no person is available to provide immediate feedback.\n\nGenerally, an AI assistant may be able to reduce the load for teachers related to grading simpler aspects of student responses, allowing the teacher to focus their specialized judgment on important qualities of a whole essay or a complex project.\n\nWe also may be able to better provide feedback with accessibility.\n\nFor example, an AI-enabled learning technology may be able to interact verbally with a student about their response to an essay prompt, asking questions that guide the student to clarify their argument without requiring the student to read a screen or type at a keyboard.\n\nIn the examples shared earlier in the Learning section, we also see that AI can be embedded in the learning process, providing feedback to students as they work to solve a problem, rather than only later after the student has reached a wrong answer.\n\nWhen formative assessment is more embedded, it can better support learning, and timely feedback is critical.\n\nAlthough there are many points of connection like these between AI and formative assessments, our listening sessions also revealed attendees’ desire to tackle some existing shortcomings in the field of formative assessment; namely, the time-consuming and sometimes onerous nature of taking tests, quizzes, or other assessments and the lack of perceived value in the feedback loop by teachers and students.",
    "# Implications for Teaching and Learning\n\nReal-time instructional feedback can be beneficial when it helps learners and teachers to improve.\n\nBut common experience too often leaves students and teachers with unpleasant feelings toward assessment and thus poses a provocative conflict between the potential benefits of data collected through formative assessments and the practical implications of administering additional assessments in classrooms and schools.\n\nSome AI-enabled systems and tools seek to address this potential conflict.\n\nFor example, one AI-enabled reading tutor listens to students as they read aloud and provides on-the-spot feedback to improve their reading.\n\nStudents reportedly enjoyed reading aloud, and the approach was effective.\n\nResearchers have also embedded formative assessments in games so that students can show how well they understand Newtonian physics as they play increasingly difficult levels of a game.\n\nIf a student can more easily ask for and receive help when they feel frustrated or confused, reducing those feelings can feel encouraging.\n\nStudent feelings of safety, confidence, and trust in the feedback generated by these AI-enabled systems and tools are essential to showcase their learning.\n\nThat focus on learning growth and gains is optimal (absent negative consequences or a high-stakes environment).\n\nAI-enhanced formative assessments may have the potential to save teachers’ time (e.g., time spent on grading), allowing the instructor to spend more time engaged in helping students.\n\nAI-enhanced assessments may also benefit teachers if they provide detailed insights about student strengths or needs that may not be visible and if they support instructional adaptation or improvement by suggesting a small set of evidence-based recommendations for helping students master content.\n\nSuch assessments may also be helpful outside of the classroom if it can provide feedback when the teacher is not available, for example, in completing homework or practicing a concept during study hall.\n\nAs we discussed in the Teaching section, an essential aspect of deploying AI-based formative assessment must be centering teachers in system design.",
    "# Insight: AI Can Enhance Feedback Loops\n\nThe term “formative assessment” does not singularly mean a test or a measurement.\n\nAssessment becomes formative when it results in useful reflections and changes to the course of teaching, learning, or both.\n\nThe term “feedback loops” emphasizes that measurement is only part of the process.\n\nFeedback loops that lead to instructional improvement—including adaptations in teaching and learning—yield the strongest outcomes for students.\n\nWe also use “feedback loops” as a plural term because there are many types and levels of loops that are important.\n\nStudents can benefit from feedback when they work individually, as a member of a small group, or in a classroom discussion.\n\nFeedback loops are valuable “in the moment”—for example, as a student practices a skill.\n\nFurther, feedback loops are valuable when they cover larger spans of effort and reflections, such as at the end of presenting a project or term paper.\n\nIn addition, feedback loops can assist teachers, for example, helping them notice their own patterns of responding to students’ ideas.\n\nMoreover, feedback loops are critical to the continuous improvement of products and the implementation of programs.\n\nDue to the importance of feedback loops, formative assessment could be a leading area for schools’ explorations of powerful uses of AI in teaching and learning.\n\nEducators can build upon alignments between their long-standing visions for formative assessment and the emerging capabilities that AI holds.\n\nFurther, the professional assessment community brings a toolkit for asking and answering questions about topics like bias and fairness.\n\nThe psychometric toolkit of methods is a strong start toward the questions that must be asked and answered because it already aligns with concerns about validity, reliability, and applicability to diverse populations.",
    "### Automated Essay Scoring\n\nAn instructive example is Automated Essay Scoring (AES).\n\nTo become strong writers, which is a valuable life skill, students need regular and specific feedback.\n\nHowever, reviewing and providing feedback on essays is very time-consuming for humans.\n\nHence, Ellis Page provided a first vision for computer programs that could review and provide feedback on student essays in 1966-57, and much effort has gone into AES technologies in the intervening 56 years.\n\nMany research review articles are available to summarize the progress, which has been impressive.\n\nFurther, some of today’s applications of AES technologies will be familiar to readers, such as Grammarly, Turnitin, and the various essay analysis engines used by publishers and assessment companies.\n\nAlso note that while the traditional AES functionality emphasizes scoring or rating essays, newer AI-enabled products focus more on providing students with constructive criticism and developing their skills as writers.\n\nWriting is a life skill that is important to the pursuit of college and career ambitions, and developing writers require comprehensive feedback.\n\nIf developers could inexpensively augment human feedback to developing writers with AI feedback, it’s possible that support for learning to write could become more equitable.\n\nAnd yet, AES is an instructive example because researchers have analyzed limitations, too.\n\nAES technologies in AI can analyze some features of student essays but can also be misled by the length of an essay, by a student who places appropriate keywords in sentences that don’t make sense, and other flaws that a human reader would easily notice.\n\nIn a telling quote, one team that reviewed the state of the art wrote this: The authors further note that while human and AI judgments of essays may correlate, people and computers are not noticing the same things in student writing.\n\nDue to these limitations, we must continue to emphasize a human in the loop foundation for AI-enhanced formative assessment.\n\nAI may support but not replace high-quality, human-led processes and practices of formative assessment in schools.",
    "### Key Opportunities for AI in Formative Assessment\n\nBased on the listening sessions we held, we see three key areas of opportunity in supporting formative assessment using AI systems and models.\n\nFirst, we recommend a strong focus on measuring what matters and particularly those things that have not been easily measured before and that many constituents would like to include in feedback loops.\n\nThe example above, AES, was chosen because writing remains a valuable academic, workplace, and life skill.\n\nLooking at community goals through the lens of their visions for their high school graduates, we see that families/caregivers, students, and community leaders want to nurture graduates who solve problems adaptively, who communicate and collaborate well, who persevere and self-regulate when they experience challenges.\n\n“What matters” today reaches beyond a sole focus on the core academic content measured by large-scale summative assessments, to support students and teachers with actionable feedback that nurtures the broader skills students need to succeed and thrive.\n\nFurther, within core academic content, AI may help us to provide feedback on the more realistic and complex aspects of doing math, for example, or investigating scientific phenomena, understanding history, or discussing literature.\n\nSecond, we’d like to see a strong focus on improving help-seeking and help-giving.\n\nAsking for and giving help is crucial to learning and practicing a growth-mindset and central to the notion of human feedback loops.\n\nStudents may not always know when they need help.\n\nIn one example, computer algorithms can detect a student who is “wheel spinning” (working hard on mastering content but not making progress).\n\nA student who is working hard may not feel like they need help, and the teacher may not be aware that the student is struggling if he or she appears to be “on task.” AI may also be helpful by highlighting for students and teachers what forms of assistance have been most useful to the student in the recent past so that an educator can expand access to specific assistance that works for that individual student.\n\nFinally, educators may learn things from AI-enabled systems and tools that give feedback and hints during the completion of homework, utilizing that feedback to later reinforce concepts in direct instruction and strengthen the one-on-one support provided to students.\n\nAI-enabled systems and tools can provide teachers with additional information about the students’ recent work, so their instructor has a greater contextual sense as they begin to provide help.\n\nThird, we advocate for teachers and students to be strongly involved in designing feedback loops as developers produce AI-enhanced formative assessments so they can directly voice what would make assessments less onerous and more convenient and valuable to them.\n\nEarlier in the Teaching section, we emphasized how important it is to involve teachers in designing, selecting, and evaluating AI-enhanced technologies.\n\nStudents need to be centered, too.\n\nThey are experiencing AI in their everyday lives, and they have strong opinions on what is valuable and safe.\n\nThere are local and cultural variations in how people provide and receive feedback, so adjusting feedback to align with community norms is important.",
    "### Key Recommendation: Harness Assessment Expertise to Reduce Bias\n\nBias and fairness are important issues in assessment design and administration, and they hold relevance for the area of AI-enabled assessment.\n\nIn traditional assessment, a test item might be biased if unnecessary details are included that differentially advantage some students (e.g., a story-based item that references a sport that only boys play regularly may be less helpful to girls).\n\nAs discussed earlier, with AI, we now must worry about algorithmic discrimination which can arise due to the manner in which AI algorithms are developed and improved from large datasets of parameters and values that may not represent all cohorts of learners.\n\nAlgorithmic discrimination is not just about the measurement side of formative assessment; it is also about the feedback loop and the instructional interventions and supports that may be undertaken in response to data collected by formative assessments.\n\nThere is a question both about access to such interventions and the quality or appropriateness of such interventions or supports.\n\nWhen an algorithm suggests hints, next steps, or resources to a student, we have to check whether the help-giving is unfair because one group systematically does not get useful help which is discriminatory.\n\nFairness goes beyond bias as well.\n\nIn AI-enabled formative assessment, both the opportunity to learn through feedback loops, as well as the quality of learning in and outside of such loops, should be addressed.\n\nIssues of bias and fairness have arisen in traditional assessments, and the field of psychometrics has already developed valuable tools to challenge and address these issues.\n\nAssessment as a field may have a head start on tackling bias and fairness for AI in education.\n\nAnd yet the issues expand with AI, so the work is not done.\n\nStrong and deliberate attention to bias and fairness is needed as future formative assessments are developed.",
    "### Related Questions\n\nAs indicated, formative assessment is an area in which AI is expanding along a continuum that can be guided by visions already in place, such as the 2017 NETP.\n\nIt is an area in which AI is poised to grow, especially with capabilities that power more feedback loops in student learning.\n\nAs this growth takes place, we suggest ongoing attention to the following questions:\n- Is formative assessment bringing benefits to the student learning experience and to the efficacy of classroom instruction?\n\n- Are humans being centered in AI-enabled formative assessment and feedback loops?\n\n- Are we providing empowering professional development to teachers so they can leverage feedback loops and safeguard against concerns?\n\n- To what extent are the developers and implementers of AI-enabled systems and tools tackling new sources of algorithmic bias and continuing to make assessment fairer?\n\n- Are governance policies regarding who owns, controls, and can view or use AI-enabled formative assessment data appropriate and adequate?\n\n- Do we have sufficient guardrails against misuse of formative assessment data or automatically generated interpretations of student achievement and learning, such as on dashboards?\n\n- Is trust in an AI-enabled assessment system, feedback loops, and data generated by such assessments growing or diminishing?",
    "### Research and Development\n\nPolicy relies upon research-based knowledge; likewise, improving practice depends on feedback loops that analyze empirical evidence.\n\nConsequently, the 2010 NETP specified a series of “grand challenges” which were “R&D problems that might be funded and coordinated at a national level.” One 2010 NETP grand challenge was to create personalized learning systems that continuously improve as they are used:\n\n“Design and validate an integrated system that provides real-time access to learning experiences tuned to the levels of difficulty and assistance that optimize learning for all learners and that incorporates self-improving features that enable it to become increasingly effective through interaction with learners.”\n\nSince 2010, much R&D has addressed this challenge.\n\nConferences about learning analytics, educational data mining, and learning at scale have blossomed.\n\nDevelopers have created platforms that use algorithms and the analysis of big data to tune learning experiences.\n\nThe challenge has not been fully achieved, and further work on this challenge is still relevant today.",
    "### Insight: Research Can Strengthen the Role of Context in AI\n\nDespite the relevance of 2010’s grand challenges, it has become apparent that the R&D community is now looking to expand their attention.\n\nThe 2010 challenges were stated as technical problems.\n\nToday’s researchers want to more deeply investigate context, and today’s tech companies want to develop platforms that are responsive to the learners’ characteristics and situations more broadly—not just in terms of narrow cognitive attributes.\n\nWe see a push to transform R&D to address context sensitivity.\n\nWe look forward to new meanings of “adaptive” that broaden outward from what the term has meant in the past decade.\n\nFor example, “adaptive” should not always be a synonym of “individualized” because people are social learners.\n\nResearchers therefore are broadening “adaptivity” to include support for what students do as they learn in groups, a form of learning that is prevalent in schools across the U.S.\n\nThe focus on context is not an accident.\n\nContext is a traditional challenge in AI.\n\nThus, researchers and developers are wise to prioritize context.\n\nUnless we invest more in AI that is context-sensitive, it is quite likely that AI will break and fail to achieve educational goals.\n\nAgreeing to prioritize context won’t be easy.\n\nAs illustrated above, there will be a tension between depth of context and pace of technological advances in AI R&D.\n\nOn the one hand, AI is sometimes presented as a race to be the first to advance new techniques or scale new applications—innovation is sometimes portrayed as rapidly going to scale with a minimally viable product, failing fast, and only after failure, dealing with context.\n\nOn the other hand, researchers and developers see that achieving good innovations with AI in education will clearly require bringing more context into the process early and often.\n\nFor example, researchers highlight that humans must be continually adjusting the goals for technology and have noted that when we set forth goals, we often don’t yet fully understand context; and as we learn about context, the goals must change.\n\nThis suggests that context must be prioritized early and habitually in R&D; we don’t want to win a race to the wrong finish line.",
    "### Further, Intensifying Focus on Context\n\nIn this work will change the nature of the R&D.\n\nThere won’t be just one type of change in R&D because context has multiple meanings.\n\nAttendees in our listening sessions described four types of context necessary for the future.\n\nWe list these four types of context below and then expand on each one in its own section.\n\nThese four types emerged as topics of provocations to think differently about R&D but certainly do not exhaust the important ways of investigating context.\n\n1.\n\nFocus on the Long Tail: How could we use big data and AI to pay more attention to the “long tail” of edtech use—going beyond a few “most typical” ways of using emerging technology and instead solving digital equity and inclusion?\n\n2.\n\nPartnership in Design-Based Research: How can we change who is involved and influential in designing the future of AI in education to more centrally include students, teachers, and other educational constituents?\n\n3.\n\nConnect with Public Policy: How can work on AI in education build on general advances in AI ethics, safety, and regulation and contribute additional advances specific to educational policy?\n\n4.\n\nRethink Teacher Professional Development: How can we solve for new systems of teacher professional development (both pre-service and in-service) that align with the increasingly core role of technology in the teaching profession?",
    "### Attention to the Long Tail of Learner Variability\n\nAt the core of R&D of AI in education, innovators will be building models that fit available data.\n\nThe increasing scale and prevalence of technologies means that the data is coming from and including a wide range of different contexts and varied ways that people in those contexts engage in teaching and learning.\n\nResearchers in our listening sessions drew attention to the promise of AI for addressing “context” by reference to the long tail of learner variability.\n\nAs depicted, learners vary in their strengths and needs.\n\nThe most frequently occurring mix of strengths and needs (also known as “teaching to the middle”) is depicted leftmost, with less frequently occurring mixes spreading to the right.\n\nRising upward, the figure depicts the number of learners who benefit from a particular learning design, pathway, or approach.\n\nWe argue that AI can bring opportunities to address a wider spectrum of strengths and needs but only if developers and innovators focus on the long tail and not only “teaching to the middle.”\n\nFor the sake of argument, the figure indicates three zones.\n\nIn the first zone, curricular resources are mostly standardized, with perhaps a dimension or two of adaptivity.\n\nFor example, many existing products adapt based on the correctness of student answers and may also provide options to read or hear text in a second language.\n\nHowever, the core of the instructional approach is highly standardized.\n\nIn a second zone, there is greater balance between how much standardization and how much adaptivity students can access.\n\nUniversal Design for Learning (UDL) is one set of recommendations for providing learning opportunities in multiple formats and for accommodating different learning progressions.\n\nUDL can enable accommodating more ways in which learners vary, and as teachers know, there are many more important ways to adapt to students than found in today’s edtech products.\n\nStudents are neurodiverse.\n\nThey bring different assets from their experiences at home, in their communities, and in their cultures.\n\nThey have different interests and motivations.\n\nAnd they learn in varied settings—classrooms and schools differ, and at-home students learn in informal settings in ways that could complement school learning.\n\nThese are all important dimensions of “context.” Zone 3 indicates highly adaptive learning, where standardization is less successful and where we need to discover a wider variety of approaches to engage learners and sustain powerful learning.\n\nResearchers in our listening sessions noted the promise of Zone 3 because AI’s ability to recognize patterns in data can extend beyond the most common patterns and because AI's ability to generate customized content can extend beyond what people can reasonably generate on their own.\n\nNotice that although the Zone 1 bar appears to be the tallest, and thus tends to attract initial attention, there are more students in Zones 2 and 3, the regions where AI can provide more help.\n\nThus, it’s important to ask where AI researchers and developers are directing their attention.\n\nWhen we say a model “fits,” are we saying it fits the most common and typical uses by teachers and learners?\n\nThis sort of R&D is easier to do.\n\nHowever, machine learning and AI can also tailor a model to the less common and more culturally specific contexts too.\n\nTherefore, how can constituents cultivate interdisciplinary expertise to direct attention among researchers and developers to focus on the long tail?\n\nIf we do, the quality of what we do for those represented in that tail can be more adaptive and more context-sensitive.\n\nAnd to be most effective, it will require the integration of contextual, content, and technical expertise.\n\nWithin the long-tail challenge, the community is wondering how we can get to research insights that are both general and specific enough.\n\nWhen research produces very general abstractions about learning, it often doesn’t give developers enough guidance on exactly how to adjust their learning environments.\n\nConversely, when research produces a specific adaptive algorithm that works on one educational platform, it often remains hard to apply to additional platforms; research can be too detailed as well.\n\nThe research community is also thinking about new partnerships that could bring more data and more diverse perspectives to the table, the topic of the next section.\n\nFocusing on the long tail of learner variability is particularly important to addressing a long-standing key research question: “Do new AI-enhanced approaches work to improve learning, and for whom and under what conditions?”",
    "### Partnership in Design-Based Research\n\nOf course, teachers must be included in rethinking their own professional development.\n\nThis thought leads to another priority aspect of context: partnership in design-based research.\n\nWith regard to inclusive design, attendees in our listening sessions brought up a variety of co-design and other participatory processes and goals that can be used in R&D.\n\nBy co-design, they mean sharing power with non-researchers and non-developers through all the phases of design and development, which would result in more influence by teachers, students, and other constituents in the shape of AI-enabled edtech.\n\nThe shift toward co-design was palpable throughout our listening sessions, but as researchers and developers have not standardized on one particular co-design method, we share some representative examples.\n\nYouth can powerfully participate in design when researcher methods include participant co-design.\n\nSuch research can investigate how to improve edtech while educating students.\n\nA listening session attendee asked about developing students’ awareness of what data are being collected and how data are being used by developers.\n\nThere is a near future need to go beyond representation so that co-designed solutions consider more generous contexts for broader possibilities, according to attendees.\n\nThe shift of power dynamics is another research-worthy interest of the panel and attendees to understand the balance between a teacher’s agency and a machine’s suggestions.\n\nLikewise, such longitudinal research will require both the infrastructure and institutional support to fund necessary experimentation and requisite failures to elicit positive results and safe innovation.\n\nThere is a desire for rapid cycle evaluations with inclusive feedback loops that return to the educators themselves as essential relative to traditional research approaches.\n\nMany researchers also mentioned a focus on explainable AI as essential to enable participation in the design and evaluation of emerging AI approaches in education.\n\nThe conversations raised this question: how can co-design provide an empowering form of participation in design and thus achieve digital inclusion goals?\n\nSuch digital inclusion can span many layers of design, including diverse representation in the design of policies around data, design of adaptivity, and other user experiences in AI systems, design of plans for cultivating AI literacy for users of new platforms, and lastly, the design of plans to evaluate systems.",
    "### Re-thinking Teacher Professional Development\n\nWith regard to teachers as professionals, both researchers and other educators attending our listening sessions were highly concerned about the disconnect between how teachers are prepared versus how they are expected to work with emerging technology.\n\nWhen we discuss learning, teachers are central actors, and thus the contexts in which they are prepared is centrally important to their ability to do great work in current and emerging technological environments.\n\nTeacher professional development, professional learning, and leadership (PD or PL) for emerging technologies was seen as an area needing intense re-thinking, and research could lead the way.\n\nToday, few who prepare to become a teacher in an established pre-service program learn about the effective use of educational technology in schools and classrooms; those who do have the opportunity to investigate technology rarely think about the structures that shape its use in the classroom and in educational leadership.\n\nConsequently, a troubling dichotomy arises between a small set of investigators who specifically consider educational technology in their research on teaching and a broader group of educators who see educational technology as a generic instructional resource.\n\nThe challenge is high because teacher professional development will remain highly varied by local contexts.\n\nYet insufficient attention to teachers as leaders in the use and further development of effective educational technology is widespread in teacher professional development research.\n\nOne response can be in terms of investigating how to nurture greater AI literacy for all teachers.\n\nAI literacy is not only important to protect educators and students from possible dangers but also valuable to support teachers in harnessing the good and doing so in innovative ways.\n\nA panelist reminded the group that this work implies how we prepare educators with a baseline AI literacy and understanding.\n\nMore transparency and authentic dialogue can foster trust, which was mentioned by a researcher as a chief concern for all teachers and students.\n\nThis is not to suggest that AI literacy is a complete or even a simple fix.\n\nResearchers want to ask fundamental questions about what it means for teachers to be professionals, especially as emerging technologies gain ground in schools and classrooms—our teachers’ professional workplaces.\n\nResearchers want to broadly reconceptualize teacher professionalism and to stop treating technology as an add-on element of professional development.",
    "# Implications of AI in Education\n\nFor example, power dynamics that arise between companies and consumers in society around issues like data ownership will also arise in the education-specific ecosystem.\n\nFurther, the public discourse in which people are discussing ethics, bias, responsibility, and many other necessary concepts will be happening simultaneously in public policy and in educational ecosystems.\n\nOne clear implication in our listening sessions was that efforts to improve AI literacy in education could be important and helpful to society more generally.\n\nFor example, one panelist said that an overarching goal of improving AI literacy is necessary if they are to contribute to how those technologies are designed.\n\nAnother researcher was interested in how edtech can provide environments where students can experience having difficult discussions across perspectives, an issue which is endemic to present society.\n\nA third researcher noted the insufficiencies of prior efforts to contend with algorithmic bias, ethics, and inclusion due to a classroom’s complex social dynamics.\n\nResearchers want to take a lead in going beyond checkbox approaches to take these issues seriously.\n\nAnd they also acknowledge that engaging with policy is often a new form of context for edtech and AI researchers, many of whom don’t have long experiences in policy arenas.\n\nLikewise, developers often do have experience with some policy issues, such as data privacy and security, but are now needing to become part of new conversations about ethics, bias, transparency, and more, a problem that the EdSAFE AI Alliance is addressing through multi-sector working groups and policy advocacy.75",
    "# Key Recommendation: Focus R&D on Addressing Context\n\nAttendees who have participated in listening sessions leading up to this report were exceptionally clear that their view of future R&D involved a shift from narrow technical questions to richer contextual questions.\n\nThis expansive shift toward context, as detailed below, is the foundational orientation that the listening session attendees saw as being necessary to advancing R&D.\n\nAttendees included these as dimensions of context:\n- learner variability, e.g., in disabilities, languages spoken, and other relevant characteristics;\n- interactions with peers, teachers, and others in the learning settings;\n- relationships across home, school, and community settings, including cultural assets;\n- instructional resources available while learning;\n- teacher preparation; and\n- policies and systems that structure teaching and learning.\n\nTo more fully represent the context of teaching and learning, including these and other dimensions of text, researchers will have to work in partnership with others to understand which aspects of context are most relevant to teaching and learning and how they can be usefully incorporated into AI models.",
    "# Ongoing Questions for Researchers\n\nAs mentioned earlier, people are good at context; AI—not so much.\n\nR&D investment in context-rich edtech thus could serve multiple national interests because finding ways to do a better job with context would be a fundamental advancement in AI.\n\nIndeed, questions like these reverberate across all applications of AI in society, and education is a centrally good context for investigating them:\n- Are AI systems moving beyond the tall portions of the “long tail” to adapt to a greater range of conditions, factors, and variations in how people learn?\n\n- To what extent are AI technologies enhancing rather than replacing human control and judgment of student learning?\n\n- How will users understand the legal and ethical implications of sharing data with AI enabled technologies and how to mitigate privacy risks?\n\n- To what extent does technology account for the complex social dynamics of how people work and learn together, or is technology leading humans to narrow or oversimplify?\n\n- How can we more clearly define what we mean by a context-sensitive technology in terms that are both concrete and broad enough?\n\nHow can we measure it?\n\nTo what extent are technical indicators and human observations of bias or unfairness working together with human observations?\n\nHow can concerns about ethics and equity in AI technologies become actionable both in R&D, and later, when AI is widely used?\n\nAre we learning for whom and under what conditions AI systems produce desired benefits and impacts and avoid undesirable discrimination, bias, or negative outcomes?",
    "# Desired National R&D Objectives\n\nAttendees sought immediate progress on some key R&D issues, such as these:\n- Clarifying and achieving a consensus on the terms that go beyond data privacy and data security, including ideas like human-centered, value-sensitive, responsible, ethical, and safe so constituents can advocate for their needs meaningfully and consistently\n- Creating and studying effective programs for AI literacy for students, teachers, and educational constituents in general, including literacy with regard to the ethics and equity issues specific to AI in educational settings\n- Advancing research and development to increase fairness, accountability, transparency, and safety in AI systems used in educational settings\n- Defining participatory or co-designed research processes that include educators in the development and conduct of research related to the development, use, and efficacy of AI-enabled systems and tools\n- Highlighting and advancing R&D efforts that empower the participation and voices of youth regarding research, data, and design of AI applications for teaching and learning\n\nLonger term desires for a national R&D program include some of the following objectives:\n- Funding sustainable partnerships that uncover what context means and how it can be addressed over longer periods of time\n- Better connecting goals for “broadening participation” (for example, in STEM learning pathways) to strategies for addressing learner variability and diversity\n- Prioritizing research to revitalize support for instructors in light of the increasingly technological nature of K-12, higher education, and workplace learning settings\n- Creating infrastructure and new ways of working together beyond individual field-initiated grants so that R&D with big data and leveraging emerging AI capabilities becomes safer and more productive",
    "# Recommendations\n\nEarlier, we asked two guiding questions:\n- What is our collective vision of a desirable and achievable educational system that leverages automation while protecting and centering human agency?\n\n- On what timeline will we be ready with necessary guidelines and guardrails along with convincing evidence of positive impacts, so that we can ethically and equitably implement this vision widely?\n\nAnswers to the first question are provided throughout the Learning, Teaching, Assessment, and Research sections.\n\nThis section turns to a call to action to education leaders and to recommendations.\n\nCore to the Department’s perspective is that education will need leadership specific to our sector.\n\nLeadership should recognize and build on prior accomplishments in edtech (such as strong prior work on student privacy and school data security) as well as broad frameworks for safe AI (such as the Blueprint for an AI Bill of Rights).\n\nLeadership must also reach beyond these accomplishments and frameworks to address emerging opportunities and risks that are specific to novel capabilities and uses of AI in education.",
    "# Insight: Aligning AI to Policy Objectives\n\nIndividual sections of this policy report provided insights in each of four areas—learning, teaching, assessment, and research.\n\nThese insights, synthesized from extensive stakeholder consultation and listening sessions, show that the advances in AI can bring opportunities to advance the Department’s policy objectives:\n\nIn support of our objective of attracting and retaining teachers, our nation could focus on AI assistants that make teaching jobs better and provide teachers with the information they need to work closely and empathically with students.\n\nAn emphasis on teachers in the loop could ensure that AI-enabled classroom technologies keep teachers in the know, in touch with their students, and in control of important instructional decisions.\n\nKeeping the teacher in the loop is important to managing risks, as well.\n\nIn support of equitable learning, especially for those most affected by the pandemic, AI could shift edtech from a current deficit-based model to a strengths-based alternative.\n\nIn addition to finding student weaknesses and assigning fixes, edtech could make recommendations based on strengths that students bring to learning and how adapting to the whole student—a cognitive, social, and self-regulating person—could enable more powerful learning.\n\nAdapting to the whole student should include supporting students with disabilities as well as English learners.\n\nWith regard to equity, we must remain highly attuned to the challenges of bias (which are inherent to how AI systems are developed) and take firm action to ensure fairness.\n\nWith regard to growth trajectories to successful careers, AI-enabled assessments could provide students and teachers with formative guidance on a wider range of valuable skills, focusing on providing information that enhances learning.\n\nAligned with the human-centric view, we should take a systems view of assessments where students, teachers, and others remain at the center of instructional decision making.\n\nWith regard to equity, as research advances and brings more context into AI, we will be better able to use AI to support goals that require customization of learning resources, such as enabling teachers to more easily transform materials to support neurodiverse learners and increase responsiveness to local communities and cultures.\n\nGoing forward, educational leaders need to bring these and their own policy priorities to the table at every discussion about AI, driving the conversation around human priorities and not only their excitement about what new technology might do.\n\nFundamentally, AI seeks to automate processes that achieve goals, and yet, AI should never set goals.\n\nThe goals must come from educators’ vision of teaching and learning and educators’ understanding of students’ strengths and needs.",
    "# Calling Education Leaders to Action\n\nWe summarize seven recommendations for policy action.\n\nThese recommendations are for education leaders.\n\nIn the introduction, we note the necessity of involving education constituents in determining policies for AI.\n\nWe also observed throughout our listening sessions that people coming from many different roles in education all have passion, knowledge, and insights to contribute.\n\nIn our view, all types of constituents can be education leaders.\n\nWe are reluctant to suggest any constituent role is more important to advance any of the recommendations, but we call out specific needs for action within some of the recommendations where it is warranted.",
    "## Recommendation #1: Emphasize Humans in the Loop\n\nWe start with a central recommendation throughout this report.\n\nThis recommendation was a clear constituent favorite.\n\nIndeed, across more than 700 attendees in our listening sessions, the predominant discussion tackled how constituents can achieve a consensus vision for AI-enabled edtech where humans are firmly at the center.\n\nThe Blueprint for an AI Bill of Rights similarly calls for “access to timely human consideration and remedy by a fallback and escalation process if an automated system fails, it produces an error, or you would like to appeal or contest its impacts…” Building on this consensus, we call upon all constituents to adopt “humans in the loop” as a key criterion for educational use of AI.\n\nWe envision a technology-enhanced future more like an electric bike and less like robot vacuums.\n\nOn an electric bike, the human is fully aware and fully in control, but their burden is less, and their effort is multiplied by a complementary technological enhancement.\n\nRobot vacuums do their job, freeing the human from involvement or oversight.\n\nAlthough teachers should not be the only humans involved in loops, Figure 5 provided examples of three types of teacher loops that are central to education and can be used to illustrate what “human in the loop” means.\n\nHere, we use the example of an AI chatbot to elaborate on the meaning of the loops.\n\nFirst, as students become involved in extended interactions with AI chatbots, teachers will need to educate students about safe AI use, monitor their use, and provide human recourse when things go astray.\n\nSecond, teachers are beginning to use chatbots to plan personalized instruction for their students; they will need to be involved in loops with other teachers to understand effective prompts, to know how to analyze AI-generated lesson plans for flaws, and to avoid the human tendency to overly trust AI systems and under apply human judgement.\n\nThird, teachers need to be involved in the design and evaluation of AI systems before they are used in classrooms and when needs for improvement are observed.\n\nIn one example, to design AI-generated homework support for students, teachers’ in-depth understanding of the cognitive, motivational, and social supports their students need will provide much-needed guidance as a homework-support chatbot is designed.\n\nIn framing AI in education, this report advances a key recommendation of “human in the loop” AI because the phrase readily communicates a criterion that everyone can use as they determine which AI-enabled systems and tools are appropriate for use in teaching and learning.\n\nIn a rather technical field, human in the loop is an approachable and humanistic criterion.\n\nRather than suggesting that AI-enabled systems and tools should replace teachers, this term instead solidifies the central role of educators as instructors and instructional decision makers, while reinforcing the responsibility of teachers to exercise judgement and control over the use of AI in education.\n\nIt resonates with the important idea of feedback loops, which are highly important to how people teach and learn.\n\nIt also aligns with the ideas of inspectable, explainable, severable, and overridable AI.\n\nThe Department agrees with listening session participants who argued that teachers should not be the only humans in the loop and calls upon parents, families, students, policy makers, and system leaders to likewise examine the “loops” for which they are responsible, critically analyze the increasing role of AI in those loops, and determine what they need to do to retain support for the primacy of human judgement in educational systems.",
    "## Recommendation #2: Align AI Models to a Shared Vision for Education\n\n“All models are wrong, but some are useful.”\n—George Box, Statistician\n\nAs we have discussed across every section of this report, AI technologies are grounded in models, and these models are inevitably incomplete in some way.\n\nIt is up to humans to name educational goals and measure the degree to which models fit and are useful—or don’t fit and might be harmful.\n\nSuch an assessment of how well certain tools serve educational priorities may seem obvious, but the romance of technology can lead to a “let’s see what the tech can do'' attitude, which can weaken the focus on goals and cause us to adopt models that fit our priorities poorly.\n\nHere we call upon educational policy and decision makers at the local, state, and federal level to use their power to align priorities, educational strategies, and technology adoption decisions to place the educational needs of students ahead of the excitement about emerging AI capabilities.\n\nWe want to strengthen their attention to existing state, district, and school-level policies that guide edtech adoption and use, such as the four levels of evidence in ESSA, the privacy requirements of FERPA, and enhanced policies to come.\n\nLocal education leaders know best what their urgent educational priorities are.\n\nEvery conversation about AI (or any emerging technology) should start with the educational needs and priorities of students front and center and conclude with a discussion about the evaluation of effectiveness re-centered on those needs and priorities.\n\nEquity, of course, is one of those priorities that requires constant attention, especially given the worrisome consequences of potentially biased AI models.\n\nWe especially call upon leaders to avoid romancing the magic of AI or only focusing on promising applications or outcomes, but instead to interrogate with a critical eye how AI-enabled systems and tools function in the educational environment.\n\nWe ask leaders to distrust broad claims and ask six types of questions, listed below.\n\nThroughout this report, we elaborated on which characteristics of AI model use in education are most important to evaluate for alignment to intended educational goals.\n\nTo aid leaders, we summarize our insights about AI models and their use in educational tools and systems in Figure 14.\n\nFigure 14: Recommendation for desired qualities of AI tools and systems in education\n\nIn this figure, we center teaching and learning in all considerations about the suitability of an AI model for an educational use.\n\nHumans remain in the loop of defining, refining, and using AI models.\n\nWe highlight the six desirable characteristics of AI models for education (elaborating from principles in the Blueprint for an AI Bill of Rights to fit the specifics of educational systems):\n- Alignment of the AI Model to Educators’ Vision for Learning: When choosing to use AI in educational systems, decision makers prioritize educational goals, the fit to all we know about how people learn, and alignment to evidence-based best practices in education.\n\n- Data Privacy: Ensuring security and privacy of student, teacher, and other human data in AI systems is essential.\n\n- Notice and Explanation: Educators can inspect edtech to determine whether and how AI is being incorporated within edtech systems.\n\nEducators’ push for AI models can explain the basis for detecting patterns and/or for making recommendations, and people retain control over these suggestions.\n\n- Algorithmic Discrimination Protections: Developers and implementers of AI in education take strong steps to minimizing bias and promoting fairness in AI models.\n\nSafe and Effective Systems: The use of AI models in education is based on evidence of efficacy (using standards already established in education for this purpose) and work for diverse learners and in varied educational settings.\n\nHuman Alternatives, Consideration and Feedback: AI models that support transparent, accountable, and responsible use of AI in education by involving humans in the loop to ensure that educational values and principles are prioritized.\n\nAlthough we first address our recommendation to interrogate how educational systems use AI models to educational leaders who adopt technologies, other leaders also have integral roles to play.\n\nTeachers and students, as well as their families/caregivers, contribute significantly to adoption decisions also.\n\nAnd leaders and parents must support educators when they question or override an AI model based on their professional wisdom.\n\nDevelopers of technologies need to be forthcoming about the models they use, and we may need policymakers to create requirements for disclosure so that the marketplace can function on the basis of information about AI models and not only by the claims of their benefits.\n\nWe also emphasize the need for a government role.\n\nAI models are made by people and are only an approximation to reality.",
    "We also emphasize the need for a government role.\n\nAI models are made by people and are only an approximation to reality.\n\nThus, we need policies that require transparency about the AI models that are embedded in educational systems, as well as models that are inspectable, explainable, and overridable.\n\nOur listening sessions featured constituent calls for government doing more to hold developers accountable for disclosing the types of AI models they employ in large-scale products and the safeguards included in their systems.\n\nGovernment leaders can make a positive contribution to market conditions that enable building trust as AI systems are procured and implemented in education.\n\nWe discuss these guidelines more in recommendation #4, which is about building trust.",
    "## Recommendation #3: Design Using Modern Learning Principles\n\nWe call for the R&D sector to ensure that product designs are based on best and most current principles of teaching and learning.\n\nThe first decade of adaptivity in edtech drew upon many important principles, for example, around how to sequence learning experiences and how to give students feedback.\n\nAnd yet the underlying conception was often deficit-based.\n\nThe system focused on what was wrong with the student and chose pre-existing learning resources that might fix that weakness.\n\nGoing forward, we must harness AI’s ability to sense and build upon learner strengths.\n\nLikewise, the past decade of approaches was individualistic, and yet we know that humans are fundamentally social and that learning is powerfully social.\n\nGoing forward, we must build on AI capabilities that connect with principles of collaborative and social learning and which respect the student not just for their cognition but also for the whole human skill set.\n\nGoing forward, we also must seek to create AI systems that are culturally responsive and culturally sustaining, leveraging the growth of published techniques for doing so.\n\nFurther, most early AI systems had few specific supports for students with disabilities and English learners.\n\nGoing forward, we must ensure that AI-enabled learning resources are intentionally inclusive of these students.\n\nThe field has yet to develop edtech that builds upon each student’s ability to make choices and to self-regulate in increasingly complex environments.\n\nWe have to develop edtech that expands students’ abilities to learn in creative modes and to expand their ability to discuss, write, present, and lead.\n\nWe also call upon educators to reject uses of AI that are based solely on machine learning from data—without triangulation based on learning theory and knowledge from practice.\n\nAchieving effective and equitable educational systems requires more than processing “big data,” and although we want to harness insights from data, human interpretation of data remains highly important.\n\nWe reject a technological determinism in which patterns in data, on their own, tell us what to do.\n\nApplications of AI in education must be grounded in established, modern learning principles, the wisdom of educational practitioners, and should leverage the expertise in the educational assessment community around detecting bias and improving fairness.",
    "## Recommendation #4: Prioritize Strengthening Trust\n\nTechnology can only help us to achieve educational objectives when we trust it.\n\nYet, our listening sessions revealed the ways in which distrust of edtech and AI is commonplace.\n\nConstituents distrust emerging technologies for multiple reasons.\n\nThey may have experienced privacy violations.\n\nThe user experience may be more burdensome than anticipated.\n\nPromised increases in student learning may not be backed by efficacy research.\n\nThey may have experienced unanticipated consequences.\n\nUnexpected costs may arise.\n\nConstituents may distrust complexity.\n\nTrust needs to incorporate safety, usability, and efficacy.\n\nThe Department firmly takes the stance that constituents want AI that supports teachers and rejects AI visions that replace teachers.\n\nAnd yet, teachers, students, and their families/caregivers need support to build appropriate levels of trust in systems that affect their work.\n\nIn the broader ecosystem, trustworthy AI is recognized as a multidimensional problem (including the dimensions of Figure 14, above).\n\nIf every step forward does not include strong elements of trust building, we worry that distrust will distract from innovation serving the public good that AI could help realize.\n\nWe expect that associations and societies have a key role in strengthening trust.\n\nSome important associations like the State Educational Technology Directors Association and the Consortium for School Network work with edtech leaders, and parallel organizations like EDUCAUSE work with postsecondary leaders.\n\nOther associations and societies work with teachers, education leaders, and education staff developers.\n\nIndustry networks, like the EdSAFE AI Alliance, can bring together industry leaders to work together to foster trust.\n\nAdditional societies bring researchers together.\n\nThese societies and associations have the reach necessary to bring all parts of the educational ecosystem into discussions about trust and also the ability to represent the views of their constituents in cross-cutting policy discussions.",
    "## Recommendation #5: Inform and Involve Educators\n\nOur listening sessions also asked for more specific direction on the question of what education leaders should do (see Figure 15).\n\nThe most frequent responses fit three clusters: the need for guidelines and guardrails, strengthening the role of teachers, and re-focusing research and development.\n\nThese are activities that constituents are asking for and that could expand trust.\n\nThe recommendations that follow respond to these requests.\n\nFigure 15: Listening session attendees prioritized involving practitioners, research, and evaluation and the need for guidelines and guardrails.\n\nIn particular, one concern that repeatedly arose in our listening sessions was the potential for AI to result in less respect for educators or less value for their skills.\n\nAcross the nation, we are now responding to decreasing interest in entering or remaining in the teaching profession.\n\nNow is the time to show the respect and value we hold for educators by informing and involving them in every step of the process of designing, developing, testing, improving, adopting, and managing AI-enabled edtech.\n\nThis includes involving educators in reviewing existing AI-enabled systems, tools, and data use in schools, designing new applications of AI based on teacher input, carrying out pilot evaluations of proposed new instructional tools, collaborating with developers to increase the trustworthiness of the deployed system, and raising issues about risks and unexpected consequences as the system is implemented.\n\nWe have already seen educators rise to the challenge of creating overall guidelines, designing specific uses of available AI-enabled systems and tools, and ferreting out concerns.\n\nAnd yet, the influence of educators in the future of AI-enabled products cannot be assumed; instead, constituents need policies that put muscle behind it.\n\nCould we create a national corps of leading educators representing every state and region to provide leadership?\n\nCould we commit to developing necessary professional development supports?\n\nCan we find ways to compensate educators so they can be at the forefront of designing the future of education?\n\nOur policies should enable educators to be closely involved in design of AI-enabled educational systems.\n\nAlthough we know that the responsibility for informing and involving educators must be distributed at all levels of national and school governance, the Office of Educational Technology can play a key role in informing and involving educators through its reports, events, outreach, and in a future NETP.\n\nAlthough examples above refer to K-12 teachers, higher education instructors must also be included.\n\nWe also call on the edtech industry to involve educators throughout their design and development processes.\n\nFor example, AI-enabled teaching assistants are only likely to help teachers do their job if teachers are thoroughly involved as the assistants are designed.\n\nWe call upon institutions that prepare teachers to integrate technology more systematically into their programs; for example, the use of technology in teaching and learning methods should be part of teacher training courses.",
    "# Recommendation #6: Focus R&D on Addressing Context and Enhancing Trust and Safety\n\nResearch that focuses on how AI-enabled systems can adapt to context (including variability among learners) in instructional approaches and across educational settings is essential to answering the question of, “Do specific applications of AI work in education, and if so, for whom and under what conditions?” The italicized phrase points to variability among learners and diversity in the settings for learning.\n\nWe call upon innovators in R&D to focus their efforts to advance AI on the long tail of learning variability, where large populations of students would benefit from customization of learning.\n\nWe also call on R&D to lead by establishing how trust can be strengthened in AI-enabled systems, building on the Blueprint’s call for safe and effective systems yet also including education-specific requirements, such as how teachers can be meaningfully involved in design phases, not only in implementation and evaluation.\n\nAlthough many products today are adaptive, some adapt on just one or a few dimensions of variability, such as student’s accuracy in problem solving.\n\nAs teachers know, there are many more important ways to adapt to students’ strengths and needs.\n\nStudents are neurodiverse and may have specific disabilities.\n\nThey bring different assets from their experiences at home, in communities, and in their cultures.\n\nThey have different interests and motivations.\n\nThey are in different places in their journeys to master the English language.\n\nAnd they learn in varied settings.\n\nClassrooms and schools are different, and at home, students learn in informal settings in ways that could complement school learning.\n\nWe recommend attention to “context” as a means for expressing the multiple dimensions that must be considered when elaborating the phrase “for whom and under what conditions.” We also acknowledge the role of researchers in conducting evaluations, which must now consider not only efficacy but must also explore where harm may arise and the system problems that can occur through weak trust or over-trust in AI systems.\n\nR&D must take the lead in making AI models more context-sensitive and ensuring that they are effective, safe, and trustworthy for use with varied learners in diverse settings.\n\nAlthough AI has capabilities to find patterns beyond the limited number of variables that people normally think about, AI is not particularly good at understanding and working with context in the ways people do.\n\nOver time, we’ve seen learning sciences grow to be less about individualistic cognitive principles and more encompassing first of social learning and then of the many dimensions of context that matter in learning.\n\nOur use of AI needs to follow this trajectory toward context to support educational applications.\n\nTo achieve human-centric vision, listening session attendees argued that teams will need time and freedom to explore how best to manage the tension between the pace of technological advancement and the need for broader contextual insights—for trust and for safety.\n\nThey will need time and freedom to pioneer new processes that better involve teachers and students as co-designers, with attention to balancing power dynamics.\n\nAnd they will need to shift attention from older ways of framing priorities (such as achievement gaps) to new ways of prioritizing digital equity.\n\nWe call on R&D funders to focus resources on the long tail of learner variability, the need for AI-enabled systems that better incorporate context, and time required to get contextual considerations right.\n\nWe call upon researchers and developers to prioritize challenges of context, trust, and safety in their work to advance AI.",
    "# Recommendation #7: Develop Education-Specific Guidelines and Guardrails\n\nOur final recommendation is central to policymakers.\n\nA feature of the American educational system is the emphasis on local decision making.\n\nWith technology growing in complexity at such a rapid pace, it is becoming difficult for local leaders to make informed decisions about the deployment of artificial intelligence.\n\nAs we have discussed, the issues are not only data privacy and security but extend to new topics such as bias, transparency, and accountability.\n\nIt will be harder to evaluate promising edtech platforms that rely on AI systems against this evolving, complex set of criteria.\n\nRegulations related to key student and family data privacy laws like the Family Educational Rights & Privacy Act (FERPA), the Children’s Internet Privacy Act (CIPA), and the Children’s Online Privacy Protection Act (COPPA) warrant review and further consideration in light of new and emerging technologies in schools.\n\nLaws such as the Individuals with Disabilities Education Act (IDEA) may likewise be considered as new situations arise in the use of AI-enabled learning technologies.\n\nAs discussed throughout this document, the Blueprint for an AI Bill of Rights is an important framework throughout this work.\n\nThe Department encourages parallel work by constituents in all levels of the educational system.\n\nIn addition to the key federal laws cited immediately above, many states have also passed privacy laws that govern the use of educational technology and edtech platforms in classrooms.\n\nFurther constituents can expect general frameworks for responsible AI in parallel sectors like health, safety, and consumer products to be informative but not sufficient for education’s specific needs.\n\nLeaders at every level need awareness of how this work reaches beyond implications for privacy and security (e.g., to include awareness of potential bias and unfairness), and they need preparation to effectively confront the next level of issues.",
    "# Next Steps\n\nWe are heartened to see intensifying discussions throughout the educational ecosystem about the role of AI.\n\nWe see progress that we can build upon occurring, as constituents discuss these three types of questions: What are the most significant opportunities and risks?\n\nHow can we achieve trustworthy educational AI?\n\nHow can we understand the models at the heart of applications of AI and ensure they have the qualities that align to educational aspirations?\n\nThe Department developed this report with awareness of contributions arising from many types of organizations and collectives.\n\nInternationally, we recognize parallel efforts to consider AI in the European Union, at the United Nations, and indeed throughout the world.\n\nWe are aware of progress being led by organizations such as UNESCO, the EdSAFE AI Alliance, and research organizations in many countries.\n\nWe plan to continue cross-agency work, for example, by continuing to coordinate with the Office of Science and Technology Policy and other Federal agencies as agencies implement next steps guided by the Blueprint for an AI Bill of Rights.\n\nWe see a broad and fertile context for necessary next steps:\n\nWorking within this context and with others, the Department will consider specific policies and regulations so that educators can realize the opportunities of AI in edtech while minimizing risks.\n\nFor example, the Department is developing a set of AI usage scenarios to strengthen the process of evaluating and enhancing policies and regulations.\n\nThe principles and practices in the Blueprint for an AI Bill of Rights will be used to ensure the scenarios mitigate important risks and harms.\n\nWorking with constituents (including education leaders; teachers, faculty, support staff, and other educators; researchers; policymakers; funders; technology developers; community members and organizations; and above all, learners and their families/caregivers), we will develop additional resources and events to increase understanding of AI and to involve those who will be most affected by these new technologies.\n\nWorking across sectors, such as education, innovation, research, and policy, we will revise and update the NETP to guide all constituents toward safe, equitable, and effective AI in education in the United States, in alignment with our overall educational priorities.",
    "# Common Acronyms and Abbreviations\n\nAES: Automated Essay Scoring  \nAI: Artificial Intelligence  \nCIPA: Children’s Internet Protection Act  \nCOPPA: Children’s Online Privacy Protection Act  \nEdtech: Educational Technology  \nESEA: Elementary and Secondary Education Act  \nESSA: Every Student Succeeds Act  \nFERPA: Family Educational Rights and Privacy Act  \nIA: Intelligence Augmentation  \nIDEA: Individuals with Disabilities Education Act  \nIEP: Individualized Education Program  \nITS: Intelligent Tutoring Systems  \nNETP: National Education Technology Plan  \nR&D: Research & Development",
    "## Project Team\n\nArtificial Intelligence and the Future of Teaching and Learning was developed under the leadership and guidance of Roberto J. Rodríguez, Assistant Secretary for the Office of Planning, Evaluation, and Policy Development, Kristina Ishmael, Deputy Director of the Office of Educational Technology, and Bernadette Adams, Senior Policy Advisor for the Office of Educational Technology at the U.S. Department of Education.\n\nSupport for the creation of this document was provided by Digital Promise, led by Jeremy Roschelle with Carly Chillmon, Judi Fusco, Gabrielle Lue, Eric Nentrup, My Nguyen, Pati Ruiz, and Zohal Shah.\n\nSpecial thanks to Center for Integrative Research in Computing and Learning Sciences postdocs Michael Chang and Aditi Mallavarapu.",
    "## Listening Session Panelists and Hosts\n\nHal Abelson  \nRyan Baker  \nNancye Blair Black  \nMarcelo Aaron Bonilla Worsley  \nMichael Chang  \nCarly Chillmon  \nSherice Clarke  \nTammy Clegg  \nSidney d’Mello  \nJudi Fusco  \nDragan Gasevic  \nKip Glazer  \nJanice Gobert  \nSarah Hampton  \nKristina Ishmael  \nJim Larimore  \nNicol Turner Lee  \nSherry Loftin  \nGabrielle Lue  \nAditi Mallavarapu  \nOle Molvig  \nPeter Norvig  \nThomas Philip  \nVidula Plante  \nJeremy Roschelle  \nPati Ruiz  \nAlina Von Davier  \nErin Walker  \nDiego Zapata  \n\nWe also thank 1,075 people who registered for Listening Sessions and 700 who attended.",
    "# References\n\nAkgun, S., Greenhow, C. (2022).\n\nArtificial intelligence in education: Addressing ethical challenges in K-12 settings.\n\nAI Ethics, 2, 431–440.\n\nAleven, V., McLaughlin, E. A., Glenn, R. A., & Koedinger, K. R. (2016).\n\nInstruction based on adaptive learning technologies.\n\nIn Mayer, R.E.\n\n& Alexander, P.A., Handbook of research on learning and instruction, 522-560.\n\nISBN: 113883176X\n\nBaker, R.S., Esbenshade, L., Vitale, J., & Karumbaiah, S. (2022).\n\nUsing demographic data as predictor variables: A questionable choice.\n\nBlack, P. & Wiliam, D. (1998).\n\nInside the black box: Raising standards through classroom assessment.\n\nPhi Delta Kappan, 92(1), 81-90.\n\nBlack, P., & Wiliam, D. (2009).\n\nDeveloping the theory of formative assessment.\n\nEducational Assessment, Evaluation and Accountability, 21(1), 5-31.\n\nBoden, M.A.\n\n(2018).\n\nArtificial intelligence: A very short introduction.\n\nOxford.\n\nISBN: 978-0199602919 \n\nBryant, J., Heitz,C., Sanghvi, S., & Wagle, D. (2020, January 14).\n\nHow artificial intelligence will impact K-12 teachers.\n\nMcKinsey.\n\nCelik, I., Dindar, M., Muukkonen, H. & Järvelä, S. (2022).\n\nThe promises and challenges of artificial intelligence for teachers: A systematic review of research.\n\nTechTrends, 66, 616–630.\n\nCenter for Integrative Research in Computing and Learning Sciences (CIRCLS).\n\n(2022, Feb.).\n\nFrom Broadening to empowering: Reflecting on the CIRCLS’21 Convening.\n\nChen, C., Park, H.W.\n\n& Breazeal, C. (2020).\n\nTeaching and learning with children: Impact of reciprocal peer learning with a social robot on children’s learning and emotive engagement.\n\nComputers & Education, 150, \n\nChen, G., Clarke, S., & Resnick, L.B.\n\n(2015).\n\nClassroom Discourse Analyzer (CDA): A discourse analytic tool for teachers.\n\nTechnology, Instruction, Cognition and Learning, 10(2), 85-105\n\nDieterle, E., Dede, C. & Walker, M. (2022).\n\nThe cyclical ethical effects of using artificial intelligence in education.\n\nAI & Society.\n\nDoewes, A.\n\n& Pechenizkiy, M. (2021).\n\nOn the limitations of human-computer agreement in automated essay scoring.\n\nIn Proceedings of the 14th International Conference on Educational Data Mining (EDM21).\n\nEnglebart, D.C. (October 1962).\n\nAugmenting human intellect: A conceptual framework.\n\nSRI Summary Report AFOSR-3223.\n\nErsozlu, Z., Ledger, S., Ersozlu, A., Mayne, F., & Wildy, H. (2021).\n\nMixed-reality learning environments in teacher education: An analysis of TeachLivETM Research.\n\nSAGE Open, 11(3).\n\nEuropean Commission, Directorate-General for Education, Youth, Sport and Culture.\n\n(2022).\n\nEthical guidelines on the use of artificial intelligence (AI) and data in teaching and learning for educators.\n\nPublications Office of the European Union.\n\nForsyth, S., Dalton, B., Foster, E.H., Walsh, B., Smilack, J., & Yeh, T. (2021, May).\n\nImagine a more ethical AI: Using stories to develop teens' awareness and understanding of artificial intelligence and its societal impacts.\n\nIn 2021 Conference on Research in Equitable and Sustained Participation in Engineering, Computing, and Technology (RESPECT).\n\nIEEE.\n\nFriedman, L., Blair Black, N., Walker, E., & Roschelle, J.\n\n(November 8, 2021).\n\nSafe AI in education needs you.\n\nAssociation of Computing Machinery BLOG@ACM, \n\nGardner, J., O'Leary, M. & Yuan, L. (2021).\n\nArtificial intelligence in educational assessment: \"Breakthrough?\n\nOr buncombe and ballyhoo?\"\n\nJournal of Computer Assisted Learning, 37(5), 1207–1216.\n\nGartner (n.d.) Gartner glossary: Augmented intelligence.\n\nGartner.\n\nGay, G. (2018).\n\nCulturally responsive teaching: Theory, research, and practice.\n\nTeachers College Press.\n\nISBN: 978-0807758762\n\nGodwin-Jones, R. (2021).\n\nBig data and language learning: Opportunities and challenges.\n\nLanguage Learning & Technology, 25(1), 4–19.\n\nHammerness, K., Darling-Hammond, L., & Bransford, J.\n\n(2005).\n\nPreparing teachers for a changing world: What teachers should learn and be able to do.\n\nJossey-Bass.\n\nISBN: 0787996343\n\nHolmes, W. & Porayska-Pomsta, K.\n\n(Eds.)\n\n(2022).\n\nThe ethics of artificial intelligence in education.\n\nRoutledge.\n\nISBN 978-0367349721\n\nHolstein, K., McLaren, B.M., & Aleven, V. (2019).\n\nCo-designing a real-time classroom orchestration tool to support teacher–AI complementarity.\n\nJournal of Learning Analytics, 6(2).\n\nIEEE-USA Board of Directors.\n\n(February 10, 2017).\n\nArtificial intelligence research, development and regulation.\n\nIEEE \n\nJensen, E., Dale, M., Donnelly, P.J., Stone, C., Kelly, S., Godley, A.\n\n& D'Mello, S.K.\n\n(2020).\n\nToward automated feedback on teacher discourse to enhance teacher learning.\n\nIn Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20).\n\nKai, S., Almeda, M.V., Baker, R. S., Heffernan, C., & Heffernan, N. (2018).\n\nDecision tree modeling of wheel-spinning and productive persistence in skill builders.\n\nJournal of Educational Data Mining, 10(1), 36–71.\n\nKaplan, R.M., & Saccuzzo, D.P.\n\n(2017).\n\nPsychological testing: Principles, applications, and issues.\n\nCengage Learning.",
    "Journal of Educational Data Mining, 10(1), 36–71.\n\nKaplan, R.M., & Saccuzzo, D.P.\n\n(2017).\n\nPsychological testing: Principles, applications, and issues.\n\nCengage Learning.\n\nKe, Z., & Ng, V. (2019).\n\nAutomated essay scoring: A survey of the state of the art.\n\nIn Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, 6300–6308.\n\nKhosravi, H., Shum, S.B., Chen, G, Conati, C., Tsai,Y-S., Kay, J., Knight, S., Martinez-Maldonado, R., Sadiq, S., Gašević, D. (2022).\n\nExplainable artificial intelligence in education.\n\nComputers and Education: Artificial Intelligence, 3.\n\nKulik, J.A., & Fletcher, J.D.\n\n(2016).\n\nEffectiveness of intelligent tutoring systems: A meta-analytic review.\n\nReview of Educational Research, 86(1), 42–78\n\nMa, W., Adescope, O.O, Nesbit, J.C. & Liu, Q.\n\n(2014).\n\nIntelligent tutoring systems and learning outcomes: A meta-analysis.\n\nJournal of Educational Psychology, 106(4), 901–918.\n\nMaslej, N., Fattorini, L., Brynjolfsson E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Ngo, H., Niebles, J.C., Parli, V., Shoham, Y., Wald, R., Clark, J. and Perrault, R., (2023).\n\nThe AI index 2023 annual report.\n\nStanford University: AI Index Steering Committee, Institute for Human-Centered AI.\n\nMerrill, S. (2020).\n\nIn schools, are we measuring what matters?\n\nEdutopia.\n\nMolenaar, I.\n\n(2022).\n\nTowards hybrid human-AI learning technologies.\n\nEuropean Journal of Education, 00, 1–14.\n\nMostow, J., Aist, G., Burkhead, P., Corbett, A., Cuneo, A., Eitelman, S., Huang, C., Junker, B., Sklar, M.B., & Tobin, B.\n\n(2003).\n\nEvaluation of an automated reading tutor that listens: Comparison to human tutoring and classroom instruction.\n\nJournal of Educational Computing Research, 29(1), 61–117.\n\nMousavinasab, E., Zarifsanaiey, N., R. Niakan Kalhori, S., Rakhshan, M., Keikha, L., & Ghazi Saeedi, M. (2021).\n\nIntelligent tutoring systems: A systematic review of characteristics, applications, and evaluation methods.\n\nInteractive Learning Environments, 29(1), 142–163.\n\nNational Academies of Sciences, Engineering, and Medicine.\n\n2018.\n\nHow people learn II: Learners, contexts, and cultures.\n\nThe National Academies Press.\n\nNational Research Council.\n\n2000.\n\nHow people learn: Brain, mind, experience, and school.\n\nThe National Academies Press.\n\nNentrup, E. (2022).\n\nHow Policymakers Can Support Educators and Technology Vendors Towards SAFE AI.\n\nEdSAFE AI Alliance.\n\nPage, E.B.\n\n(1966).\n\nThe imminence of grading essays by computer.\n\nPhi Delta Kappan, 47(5), 238–243\n\nParis, D., & Alim, H.S.\n\n(Eds.).\n\n(2017).\n\nCulturally sustaining pedagogies: Teaching and learning for justice in a changing world.\n\nTeachers College Press.\n\nISBN: 978-0807758342\n\nPlass, J.L., & Pawar, S. (2020).\n\nToward a taxonomy of adaptivity for learning.\n\nJournal of Research on Technology in Education, 52(3), 275–300.\n\nRegona, Massimo & Yigitcanlar, Tan & Xia, Bo & Li, R.Y.M.\n\n(2022).\n\nOpportunities and adoption challenges of AI in the construction industry: A PRISMA review.\n\nJournal of Open Innovation Technology Market and Complexity, 8(45).\n\nReynolds, C.R., & Suzuki, L.A. (2012).\n\nBias in psychological assessment: An empirical review and recommendations.\n\nHandbook of Psychology, Second Edition.\n\nRitter, S., Anderson, J.R., Koedinger, K.R.\n\n& Corbett, A.\n\n(2007).\n\nCognitive Tutor: Applied research in mathematics education.\n\nPsychonomic Bulletin & Review, 14, 249–255/ \n\nRoll, I., Aleven, V., McLaren, B.M., Koedinger, K.R.\n\n(2011).\n\nImproving students’ help-seeking skills using metacognitive feedback in an intelligent tutoring system, Learning and Instruction, 21(2), 267–280.\n\nRoschelle, J., Dimitriadis, Y.\n\n& Hoppe, U.\n\n(2013).\n\nClassroom orchestration: Synthesis.\n\nComputers & Education, 69, 512-526.\n\nRoschelle, J., Feng, M., Murphy, R. & Mason, C.A.\n\n(2016).\n\nOnline mathematics homework increases student achievement.\n\nAERA Open, 2(4), 1-12.\n\nDOI: 10.1177/2332858416673968\n\nRoschelle, J., Penuel, W., & Shechtman, N. (2006).\n\nCo-design of innovations with teachers: definition and dynamics.\n\nIn Proceedings of the 7th International Conference on Learning Sciences, Bloomington, IN.\n\nRose, D. (2000).\n\nUniversal design for learning.\n\nJournal of Special Education Technology, 15(4), 47-51.\n\nRuiz, P. & Fusco, J.\n\n(2022).\n\nTeachers partnering with artificial intelligence: Augmentation and automation.\n\nDigital Promise.\n\nRussell, S. (2019).\n\nHuman compatible: Artificial intelligence and the problem of control.\n\nViking.\n\nISBN 978-0-525-55861-3.\n\nShao, Q., Sniffen, A., Blanchet, J., Hillis, M.E., Shi, X., Haris, T.K., & Balkcom, D. (2020).\n\nTeaching american sign language in mixed reality.\n\nProceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(4), 1-27.\n\nSharples, M. & Pérez y Pérez, R. (2022).\n\nStory machines: How computers have become creative writers.\n\nRoutledge.\n\nISBN 9780367751951\n\nShemshack, A., Spector, J.M.\n\n(2020) A systematic literature review of personalized learning terms.",
    "Story machines: How computers have become creative writers.\n\nRoutledge.\n\nISBN 9780367751951\n\nShemshack, A., Spector, J.M.\n\n(2020) A systematic literature review of personalized learning terms.\n\nSmart Learning Environments, 7(33).\n\nShute, V J.\n\n(2008).\n\nFocus on formative feedback.\n\nReview of Educational Research, 78(1), 153–189.\n\nShute, V. J., Ventura, M., & Kim, Y. J.\n\n(2013).\n\nAssessment and learning of qualitative physics in Newton's Playground.\n\nThe Journal of Educational Research, 106(6), 423-430.\n\nSwiecki, Z., Khosravi, H., Chen, G., Martinez-Maldonado, R., Lodge, J.M., Milligan, S., Selwyn, B.\n\n& Gašević,D.\n\n(2022).\n\nAssessment in the age of artificial intelligence.\n\nComputers and Education: Artificial Intelligence, 3.\n\nThe White House (February 17, 2023).\n\nExecutive order on further advancing racial equity and support for underserved communities through the federal government.\n\nThe White House (September 8, 2022).\n\nReadout of White House listening session on tech platform accountability.\n\nU.S. Department of Education, Office of Educational Technology (2022).\n\nAdvancing digital equity for all: Community-based recommendations for developing effective digital equity plans to close the digital divide and enable technology-empowered learning.\n\nUS Department of Education.\n\nU.S. Department of Education, Office of Educational Technology.\n\n(2010).\n\nTransforming American Education: Learning Powered by Technology.\n\nU.S. Department of Education.\n\np. 78\n\nVan Lehn, K. (2011) The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems.\n\nEducational Psychologist, 46(4), 197-221.\n\nWagner, A.R., Borenstein, J.\n\n& Howard, A.\n\n(September 2018).\n\nOvertrust in the robotics age.\n\nCommunications of the ACM, 61(9), 22-24.\n\nWalker, E., Rummel, N. & Koedinger, K.R.\n\n(2015).\n\nAdaptive intelligent support to improve peer tutoring in algebra.\n\nInternational Journal of Artificial Intelligence in Education, 24, 33–61 \n\nWalton Family Foundation (March 1, 2023).\n\nTeachers and students embrace ChatGPT for education.\n\nWebb, N.M., & Farivar, S. (1994).\n\nPromoting helping behavior in cooperative small groups in middle school mathematics.\n\nAmerican Educational Research Journal, 31(2), 369–395.\n\nWhite House Office of Science and Technology Policy (October 2022), Blueprint for an AI bill of rights: Making automated systems work for the American people.\n\nThe White House Office of Science and Technology Policy.\n\nWiggins, G. (2015).\n\nSeven keys to effective feedback.\n\nACSD.\n\nWinne, P.H.\n\n(2021).\n\nOpen learner models working in symbiosis with self-regulating learners: A research agenda.\n\nInternational Journal of Artificial Intelligence in Education, 31(3), 446-459.\n\nZacamy, J.\n\n& Roschelle, J.\n\n(2022).\n\nNavigating the tensions: How could equity-relevant research also be agile, open, and scalable?\n\nDigital Promise.\n\nZhai, X., He, P., Krajcik, J.\n\n(2022).\n\nApplying machine learning to automatically assess scientific models.\n\nJournal of Research in Science Teaching.\n\nZhang, H., Lee, I., Ali, S., DiPaola, D., Cheng, Y., & Breazeal, C. (2022).\n\nIntegrating ethics and career futures with technical learning to promote AI literacy for middle school students: An exploratory study.\n\nInternational Journal of Artificial Intelligence in Education, 1–35.",
    "# Data Security Concerns With AI Services Like ChatGPT\n\nTara Stone, CCIT Communications  \nMay 19, 2023\n\nThe rise in capabilities and ease of access of ChatGPT and other AI services provides exciting potential as a tool for research, programming, data processing and other applications.\n\nHowever, when using these AI services, as with any technology, we are obligated to protect and preserve the data we use at Clemson every day.\n\nClemson is committed to protecting the privacy of its students, alumni, faculty, and staff, as well as protecting the confidentiality, integrity, and availability of information.\n\nA recent disclosure of sensitive data and information by engineers at a large technology company into ChatGPT sheds light on the potential for the disclosure of sensitive information at institutions like Clemson to an AI developer.\n\nAI services like ChatGPT retain user input data to further train itself, leading to possibly sensitive data in the hands of these companies.\n\nTherefore, putting data into ChatGPT or similar services is equivalent to disclosing that data to the public.\n\nThere are currently no legal agreements providing assurance of data confidentiality between Clemson University and any AI developers.\n\nAny sensitive data disclosed to the public could be considered a breach under FERPA, HIPAA, PCI, GLBA or other Federal or State Statute.\n\nExamples include, but are not limited to:\n\n- Social Security Numbers \n- Credit Card Numbers\n- Personally identifiable medical information \n- Financial Aid information\n- Student education records\n\nAdditionally, great caution is suggested with the following information:\n\n- Research data\n- Intellectual Property \n- Source code\n- Proprietary data\n- Internal meeting notes\n- Hardware related information \n- Presentation notes\n- Emails\n\nFor more information on commonly used data types at Clemson and which security controls are required for related systems and applications, please refer to Clemson’s data classification policy.\n\nWhile AI may prove to be a valuable tool, its use at Clemson is limited by our control over how sensitive data is stored and accessed.\n\nPlease be cognizant of our data stewardship responsibilities as you explore these new technologies and their capabilities.",
    "# Notices\n\n- Changes To Clemson Email Quarantine\n- FCC/FEMA Emergency Alert System Test on October 4, 2023\n- Check the status of university systems on Status Hub\n- Security upgrades to impact email spam retrieval beginning July 12, 2023\n- Campus network and TikTok — July 10, 2023\n- Kronos to require log-in through Duo mobile application on July 21, 2023\n- PeopleSoft and Banner Outage on June 11, 2023\n- Upcoming changes to Google Workspace\n- Temporary restrictions to iROAR access – Nov. 30, 2022\n- Welcome Back: Important IT Updates for Faculty and Staff\n- Welcome Back: Important IT Updates for Students\n- Eduroam Update – July 15, 2022\n- New Student Laptop Models Available for 2023-2024\n- Adobe Software is now available using your Clemson login\n- CCIT Chat Support is now Available\n\nPrevious  \nCCIT Home  \nSkip to Main Content  \nPopular",
    "## Generative AI Policy for Students (August 2023)\n\nDear students,\n\nYou may have received an email this week from the University’s Chief Information Officer highlighting the need for caution when using generative AI (such as ChatGPT, etc.)\n\nwith personal and/or sensitive information and the need to protect confidential data.\n\nAnother area of concern is the possibility that inappropriate use of these tools will interfere with your ability to successfully engage with and master course material, given the potential that they create for academic dishonesty.\n\nBelow is the University’s policy regarding the use of generative AI in your courses.\n\nThis new technology offers numerous ways to support your education, such as making study guides or flash cards or providing help with understanding difficult concepts.\n\nHowever, misuse of generative AI impedes the University’s mission to develop the gifts and talents that you bring to the ND community, and using it as a substitute for genuine engagement with your coursework runs counter to the heart of education itself.\n\nThink carefully about the difference between supplementing your education and replacing it.\n\nWith this in mind, remember that representing work that you did not produce as your own, including work generated or materially modified by AI, constitutes academic dishonesty.\n\nUse of generative AI in a way that violates an instructor’s articulated policy, or using it to complete coursework in a way not expressly permitted by the faculty member, will be considered a violation of the Honor Code.\n\nFinally, Undergraduate Education at Notre Dame and the Office of Academic Standards would also like to support you as you navigate the impacts of generative AI on your education and career path.\n\nIf you have concerns about academic dishonesty, or about generative AI impacting learning in your classes, email  and let us know.\n\nWelcome back to campus, and best wishes for the fall semester.\n\n---\n\nUndergraduate Academic Code of Honor  \n217N Main Building  \nNotre Dame, IN 46556 USA  \nPhone 574-631-0626  \n  \n© 2024 University of Notre Dame",
    "# ChatGPT and Generative AI Tools: Sample Syllabus Policy Statements\n\nLarge language model generative artificial intelligence (generative AI) tools, such as ChatGPT and others, have recently emerged and become available for wide use.\n\nWhile generative AI tools can offer inspiration and new possibilities, they should not be seen as unacknowledged substitutes for the content created by students in their courses.\n\nRegarding the potential use of generative AI tools, no changes in university policy are required.\n\nIt is already a violation of policy for students to represent work they did not do as their own, and work generated by an AI system that is not credited to that system falls under that policy.\n\nWhile there are clear limitations on the use of these tools in certain contexts at UT, instructors have the discretion to explore them in the classroom.\n\nAt present, the CTL recommends that UT faculty and instructors decide whether or not these tools fit within their pedagogical aims and clearly state their course policies in a designated section of their syllabi.\n\nIf adopted, students should learn how to use AI text generators (i.e., ChatGPT) and other AI tools to expand rather than impede their developing abilities as writers, coders, creators, and thinkers.\n\nEnsuring equitable access to these tools is essential, as are fair grading policies and student evaluation for both those who do and do not use AI tools in their courses.\n\nThe following suggested syllabus statements focused on generative AI tools in the classroom are culled from CTL discussions as well as openly available Web resources.\n\nEach section contains several possible ways of framing the instructor’s intent.\n\nDue to the nuance of generative AI, the categories do not stand alone, so you may find areas of overlap.\n\nTo that end, these statements are intended to spur your own thinking, and so you are welcome to use, edit, or adapt any of the selections below for your own purposes.\n\nPlease feel free to contact the CTL with any questions about how to craft your own statements or explore our webpage, \"5 Things to Know about ChatGPT,\" which contains links to additional UT resources and recommendations.\n\n> \"This course assumes that work submitted by students – all process work, drafts, brainstorming artifacts, final works – will be generated by the students themselves, working individually or in groups as directed by class assignment instructions.\n\nThis policy indicates the following constitute violations of academic honesty: a student has another person/entity do the work of any substantive portion of a graded assignment for them, which includes purchasing work from a company, hiring a person or company to complete an assignment or exam, and/or using generative AI tools (such as ChatGPT).\"\n\n> \"In this course, every element of class assignments must be fully prepared by the student.\n\nThe use of generative AI tools for any part of your work will be treated as plagiarism.\n\nIf you have questions, please contact me.\"\n\n> \"All assignments should be fully prepared by the student.\n\nDeveloping strong competencies in the skills associated with this course, from student-based brainstorming to project development, will prepare you for success in your degree pathway and, ultimately, a competitive career.\n\nTherefore, the use of generative AI tools to complete any aspect of assignments for this course are not permitted and will be treated as plagiarism.\n\nIf you have questions about what constitutes a violation of this statement, please contact me.\"\n\n> \"This course assumes that work submitted for a grade by students – all process work, drafts, brainstorming artifacts, final works – will be generated by the students themselves, working individually or in groups as directed by class assignment instructions.\n\nThis policy indicates the following constitute violations of academic honesty: a student has another person/entity do the work of any substantive portion of a graded assignment for them, which includes purchasing work from a company, hiring a person or company to complete an assignment or exam, and/or using generative AI tools (such as ChatGPT).\"\n\n> \"The emergence of generative AI tools (such as ChatGPT and DALL-E) has sparked interest among many students in our discipline.\n\nThe use of these tools for brainstorming ideas, exploring possible responses to questions or problems, and creative engagement with the materials may be useful for you as you craft responses to class assignments.\n\nWhile there is no substitute for working directly with your instructor, the potential for generative AI tools to provide automatic feedback, assistive technology and language assistance is clearly developing.\n\nPlease feel free to reach out to me well in advance of the due date of assignments for which you may be using generative AI tools and I will be happy to discuss what is acceptable.\"",
    "Please feel free to reach out to me well in advance of the due date of assignments for which you may be using generative AI tools and I will be happy to discuss what is acceptable.\"\n\n> \"In this course, students shall give credit to AI tools whenever used, even if only to generate ideas rather than usable text or illustrations.\n\nWhen using AI tools on assignments, add an appendix showing (a) the entire exchange, highlighting the most relevant sections; (b) a description of precisely which AI tools were used (e.g.\n\nChatGPT private subscription version or DALL-E free version), (c) an explanation of how the AI tools were used (e.g.\n\nto generate ideas, turns of phrase, elements of text, long stretches of text, lines of argument, pieces of evidence, maps of the conceptual territory, illustrations of key concepts, etc.\n\n); (d) an account of why AI tools were used (e.g.\n\nto save time, to surmount writer’s block, to stimulate thinking, to handle mounting stress, to clarify prose, to translate text, to experiment for fun, etc.).\n\nStudents shall not use AI tools during in-class examinations, or assignments unless explicitly permitted and instructed.\n\nOverall, AI tools should be used wisely and reflectively with an aim to deepen understanding of subject matter.\"\n\n> \"It is a violation of university policy to misrepresent work that you submit or exchange with your instructor by characterizing it as your own, such as submitting responses to assignments that do not acknowledge the use of generative AI tools.\n\nPlease feel free to reach out to me with any questions you may have about the use of generative AI tools before submitting any content that has been substantially informed by these tools.\"\n\n> \"In this course, we may use generative AI tools (such as ChatGPT) to examine the ways in which these kinds of tools may inform our exploration of the topics of the class.\n\nYou will be informed as to when and how these tools will be used, along with guidance for attribution if/as needed.\n\nAny use of generative AI tools outside of these parameters constitutes plagiarism and will be treated as such.\"\n\n> \"Understanding how and when to use generative AI tools (such as ChatGPT, DALL-E) is quickly emerging as an important skill for future professions.\n\nTo that end, you are welcome to use generative AI tools in this class as long as it aligns with the learning outcomes or goals associated with assignments.\n\nYou are fully responsible for the information you submit based on a generative AI query (such that it does not violate academic honesty standards, intellectual property laws, or standards of non-public research you are conducting through coursework).\n\nYour use of generative AI tools must be properly documented and cited for any work submitted in this course.\"\n\n> \"To ensure all students have an equal opportunity to succeed and to preserve the integrity of the course, students are not permitted to submit text that is generated by artificial intelligence (AI) systems such as ChatGPT, Bing Chat, Claude, Google Bard, or any other automated assistance for any classwork or assessments.\n\nThis includes using AI to generate answers to assignments, exams, or projects, or using AI to complete any other course-related tasks.\n\nUsing AI in this way undermines your ability to develop critical thinking, writing, or research skills that are essential for this course and your academic success.\n\nStudents may use AI as part of their research and preparation for assignments, or as a text editor, but text that is submitted must be written by the student.\n\nFor example, students may use AI to generate ideas, questions, or summaries that they then revise, expand, or cite properly.\n\nStudents should also be aware of the potential benefits and limitations of using AI as a tool for learning and research.\n\nAI systems can provide helpful information or suggestions, but they are not always reliable or accurate.\n\nStudents should critically evaluate the sources, methods, and outputs of AI systems.\n\nViolations of this policy will be treated as academic misconduct.\n\nIf you have any questions about this policy or if you are unsure whether a particular use of AI is acceptable, please do not hesitate to ask for clarification.\"\n\n> \"The use of generative AI is encouraged with certain tasks and with attribution: You can choose to use AI tools to help brainstorm assignments or projects or to revise existing work you have written.\n\nWhen you submit your assignment, I expect you to clearly attribute what text was generated by the AI tool (e.g., AI-generated text appears in a different colored font, quoted directly in the text, or use an in-text parenthetical citation).\"\n\n> \"Designers commonly use AI-content generation tools in their work.\n\nIn this course, using AI-content generation tools is permitted and will be a normal and regular part of our creative process when it is used according to the below criteria.",
    "In this course, using AI-content generation tools is permitted and will be a normal and regular part of our creative process when it is used according to the below criteria.\n\nIn this course, neglecting to follow these requirements may be considered academic dishonesty.\n\n(1) For each assignment, you are required to include a paragraph that explains what AI content-generation tool you used, the dates you used it, and the prompts you used to generate the content according to the MLA style guide.\n\n(2) During critique, it is important to describe the precedents you used and how any source content was transformed.\n\nWhen showing or presenting images or other content you generated using an AI-tool, cite that image or content following the MLA style guide.\n\nIf you need help referencing your creative work, contact me to collaborate.\"\n\n> \"Students are invited to use AI platforms to help prepare for assignments and projects (e.g., to help with brainstorming or to see what a completed essay might look like).\n\nI also welcome you to use AI tools to help revise and edit your work (e.g., to help identify flaws in reasoning, spot confusing or underdeveloped paragraphs, or to simply fix citations).\n\nWhen submitting work, students must clearly identify any writing, text, or media generated by AI.\n\nThis can be done in a variety of ways.\n\nIn this course, parts of essays generated by AI should appear in a different colored font, and the relationship between those sections and student contributions should be discussed in cover letters that accompany the essay submission.\"",
    "## Note on Citation\n\nWhen thinking about your options, we urge instructors to explore these tools and think carefully about how to cite content from them as well as the role of citation in student work (i.e., brainstorming possibilities, developing arguments from sources, aligning AI tool output and authorship, etc.).\n\nConsidering UT-Austin's policy that \"ChatGPT or similar AI Tools must not be used to generate output that would be considered non-public,\" it is important to consider the shareable or retrievable nature of any AI-informed work.\n\nRemind students that these tools cannot be used to create content that could be private (i.e., used for research purposes) or as their intellectual property.\n\nThe following, most commonly used style guides should be useful in crafting language for your syllabi and assignments:\n\n- How to cite generative AI output (MLA)\n- How to cite generative AI output (APA)\n- How to cite generative AI output (Chicago)"
]